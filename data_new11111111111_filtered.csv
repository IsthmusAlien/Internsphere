title,category,reference,summary
crypto Currency Price Info,HTML & CSS,https://github.com/surabhiojha12/Certificate_creation-,"# Certificate_creation-  Overview ---------- Create certificate with HTML,CSS AND JSPDF, for events. The same logic can be used for ticket booking.  Features ---------- * Details entry page. * Certificate display page(html and css) with download pdf option. * Pdf display note: javascript is written inside html.  User Interface --------------- * #### Details Page  ![home](index.PNG?raw=true ""Optional Title"") -----------------------------------------------  * #### Certificate page  ![cert](certificate.PNG?raw=true ""Optional Title"") -----------------------------------------------  * #### Pdf  ![pdf](cert_pdf.PNG?raw=true ""Optional Title"") -----------------------------------------------   "
Responsive Portfolio,HTML & CSS,https://github.com/bedimcode/portfolio-responsive-complete,"# Portfolio Responsive Complete ## [Watch it on youtube](https://youtu.be/AKNvTxWOdKw) ### Portfolio Responsive Complete  - Responsive Personal Portfolio Website HTML CSS & JavaScript. - Contains animations when scrolling. - Smooth scrolling in each section. - Developed first with the Mobile First methodology, then for desktop. - Compatible with all mobile devices and with a beautiful and pleasant user interface.  üíô Join the channel to see more videos like this. [Bedimcode](https://www.youtube.com/@Bedimcode)  ![preview img](/preview.png) "
Roadmap for Developers,HTML & CSS,https://github.com/kamranahmedse/developer-roadmap,"<p align=""center"">   <a href=""https://roadmap.sh/""><img src=""public/images/brand.png"" height=""128""></a>   <h2 align=""center""><a href=""https://roadmap.sh"">roadmap.sh</a></h2>   <p align=""center"">Community driven roadmaps, articles and resources for developers<p>   <p align=""center"">     <a href=""https://roadmap.sh/roadmaps"">     	<img src=""https://img.shields.io/badge/%E2%9C%A8-Roadmaps%20-0a0a0a.svg?style=flat&colorA=0a0a0a"" alt=""roadmaps"" />     </a>     <a href=""https://roadmap.sh/best-practices"">     	<img src=""https://img.shields.io/badge/%E2%9C%A8-Best%20Practices-0a0a0a.svg?style=flat&colorA=0a0a0a"" alt=""best practices"" />     </a>     <a href=""https://roadmap.sh/questions"">     	<img src=""https://img.shields.io/badge/%E2%9C%A8-Questions-0a0a0a.svg?style=flat&colorA=0a0a0a"" alt=""videos"" />     </a>     <a href=""https://www.youtube.com/channel/UCA0H2KIWgWTwpTFjSxp0now?sub_confirmation=1"">     	<img src=""https://img.shields.io/badge/%E2%9C%A8-YouTube%20Channel-0a0a0a.svg?style=flat&colorA=0a0a0a"" alt=""roadmaps"" />     </a>   </p> </p>  <br>  ![](https://i.imgur.com/waxVImv.png)  Roadmaps are now interactive, you can click the nodes to read more about the topics.  ### [View all Roadmaps](https://roadmap.sh) &nbsp;&middot;&nbsp; [Best Practices](https://roadmap.sh/best-practices) &nbsp;&middot;&nbsp; [Questions](https://roadmap.sh/questions)  ![](https://i.imgur.com/waxVImv.png)  Here is the list of available roadmaps with more being actively worked upon.  > Have a look at the [get started](https://roadmap.sh/get-started) page that might help you pick up a path.  - [Frontend Roadmap](https://roadmap.sh/frontend) / [Frontend Beginner Roadmap](https://roadmap.sh/frontend?r=frontend-beginner) - [Backend Roadmap](https://roadmap.sh/backend) / [Backend Beginner Roadmap](https://roadmap.sh/backend?r=backend-beginner) - [DevOps Roadmap](https://roadmap.sh/devops) / [DevOps Beginner Roadmap](https://roadmap.sh/devops?r=devops-beginner) - [Full Stack Roadmap](https://roadmap.sh/full-stack) - [Git and GitHub](https://roadmap.sh/git-github) - [API Design Roadmap](https://roadmap.sh/api-design) - [Computer Science Roadmap](https://roadmap.sh/computer-science) - [Data Structures and Algorithms Roadmap](https://roadmap.sh/datastructures-and-algorithms) - [AI and Data Scientist Roadmap](https://roadmap.sh/ai-data-scientist) - [AI Engineer Roadmap](https://roadmap.sh/ai-engineer) - [AWS Roadmap](https://roadmap.sh/aws) - [Cloudflare Roadmap](https://roadmap.sh/cloudflare) - [Linux Roadmap](https://roadmap.sh/linux) - [Terraform Roadmap](https://roadmap.sh/terraform) - [Data Analyst Roadmap](https://roadmap.sh/data-analyst) - [MLOps Roadmap](https://roadmap.sh/mlops) - [Product Manager Roadmap](https://roadmap.sh/product-manager) - [Engineering Manager Roadmap](https://roadmap.sh/engineering-manager) - [QA Roadmap](https://roadmap.sh/qa) - [Python Roadmap](https://roadmap.sh/python) - [Software Architect Roadmap](https://roadmap.sh/software-architect) - [Game Developer Roadmap](https://roadmap.sh/game-developer) / [Server Side Game Developer](https://roadmap.sh/server-side-game-developer) - [Software Design and Architecture Roadmap](https://roadmap.sh/software-design-architecture) - [JavaScript Roadmap](https://roadmap.sh/javascript) - [TypeScript Roadmap](https://roadmap.sh/typescript) - [C++ Roadmap](https://roadmap.sh/cpp) - [React Roadmap](https://roadmap.sh/react) - [React Native Roadmap](https://roadmap.sh/react-native) - [Vue Roadmap](https://roadmap.sh/vue) - [Angular Roadmap](https://roadmap.sh/angular) - [Node.js Roadmap](https://roadmap.sh/nodejs) - [PHP Roadmap](https://roadmap.sh/php) - [GraphQL Roadmap](https://roadmap.sh/graphql) - [Android Roadmap](https://roadmap.sh/android) - [iOS Roadmap](https://roadmap.sh/ios) - [Flutter Roadmap](https://roadmap.sh/flutter) - [Go Roadmap](https://roadmap.sh/golang) - [Rust Roadmap](https://roadmap.sh/rust) - [Java Roadmap](https://roadmap.sh/java) - [Spring Boot Roadmap](https://roadmap.sh/spring-boot) - [Design System Roadmap](https://roadmap.sh/design-system) - [PostgreSQL Roadmap](https://roadmap.sh/postgresql-dba) - [SQL Roadmap](https://roadmap.sh/sql) - [Redis Roadmap](https://roadmap.sh/redis) - [Blockchain Roadmap](https://roadmap.sh/blockchain) - [ASP.NET Core Roadmap](https://roadmap.sh/aspnet-core) - [System Design Roadmap](https://roadmap.sh/system-design) - [Kubernetes Roadmap](https://roadmap.sh/kubernetes) - [Cyber Security Roadmap](https://roadmap.sh/cyber-security) - [MongoDB Roadmap](https://roadmap.sh/mongodb) - [UX Design Roadmap](https://roadmap.sh/ux-design) - [Docker Roadmap](https://roadmap.sh/docker) - [Prompt Engineering Roadmap](https://roadmap.sh/prompt-engineering) - [Technical Writer Roadmap](https://roadmap.sh/technical-writer) - [DevRel Engineer Roadmap](https://roadmap.sh/devrel)  There are also interactive best practices:  - [Backend Performance Best Practices](https://roadmap.sh/best-practices/backend-performance) - [Frontend Performance Best Practices](https://roadmap.sh/best-practices/frontend-performance) - [Code Review Best Practices](https://roadmap.sh/best-practices/code-review) - [API Security Best Practices](https://roadmap.sh/best-practices/api-security) - [AWS Best Practices](https://roadmap.sh/best-practices/aws)  ..and questions to help you test, rate and improve your knowledge  - [JavaScript Questions](https://roadmap.sh/questions/javascript) - [Node.js Questions](https://roadmap.sh/questions/nodejs) - [React Questions](https://roadmap.sh/questions/react) - [Backend Questions](https://roadmap.sh/questions/backend) - [Frontend Questions](https://roadmap.sh/questions/frontend)  ![](https://i.imgur.com/waxVImv.png)  ## Share with the community  Please consider sharing a post about [roadmap.sh](https://roadmap.sh) and the value it provides. It really does help!  [![GitHub Repo stars](https://img.shields.io/badge/share%20on-reddit-red?logo=reddit)](https://reddit.com/submit?url=https://roadmap.sh&title=Interactive%20roadmaps,%20guides%20and%20other%20educational%20content%20for%20Developers) [![GitHub Repo stars](https://img.shields.io/badge/share%20on-hacker%20news-orange?logo=ycombinator)](https://news.ycombinator.com/submitlink?u=https://roadmap.sh) [![GitHub Repo stars](https://img.shields.io/badge/share%20on-twitter-03A9F4?logo=twitter)](https://twitter.com/share?url=https://roadmap.sh&text=Interactive%20roadmaps,%20guides%20and%20other%20educational%20content%20for%20Developers) [![GitHub Repo stars](https://img.shields.io/badge/share%20on-facebook-1976D2?logo=facebook)](https://www.facebook.com/sharer/sharer.php?u=https://roadmap.sh) [![GitHub Repo stars](https://img.shields.io/badge/share%20on-linkedin-3949AB?logo=linkedin)](https://www.linkedin.com/shareArticle?url=https://roadmap.sh&title=Interactive%20roadmaps,%20guides%20and%20other%20educational%20content%20for%20Developers)  ## Development  Clone the repository, install the dependencies and start the application  ```bash git clone git@github.com:kamranahmedse/developer-roadmap.git cd developer-roadmap npm install npm run dev ```  Note: use the `depth` parameter to reduce the clone size and speed up the clone.  ```sh git clone --depth=1 https://github.com/kamranahmedse/developer-roadmap.git ```  ## Contribution  > Have a look at [contribution docs](./contributing.md) for how to update any of the roadmaps  - Add content to roadmaps - Add new roadmaps - Suggest changes to existing roadmaps - Discuss ideas in issues - Spread the word  ## Thanks to all contributors ‚ù§   <a href = ""https://github.com/kamranahmedse/developer-roadmap/graphs/contributors"">    <img src = ""https://contrib.rocks/image?repo=kamranahmedse/developer-roadmap""/>  </a>  ## License  Have a look at the [license file](./license) for details"
Photography Site,HTML & CSS,https://github.com/JoaoFranco03/photography-portfolio,"<!-- Improved compatibility of back to top link: See: https://github.com/othneildrew/Best-README-Template/pull/73 -->  <a name=""readme-top""></a>  <!-- *** Thanks for checking out the Best-README-Template. If you have a suggestion *** that would make this better, please fork the repo and create a pull request *** or simply open an issue with the tag ""enhancement"". *** Don't forget to give the project a star! *** Thanks again! Now go create something AMAZING! :D -->  <!-- PROJECT SHIELDS --> <!-- *** I'm using markdown ""reference style"" links for readability. *** Reference links are enclosed in brackets [ ] instead of parentheses ( ). *** See the bottom of this document for the declaration of the reference variables *** for contributors-url, forks-url, etc. This is an optional, concise syntax you may use. *** https://www.markdownguide.org/basic-syntax/#reference-style-links -->  [![LinkedIn][linkedin-shield]][linkedin-url]  <!-- PROJECT LOGO --> <br /> <div align=""center"">   <a href=""https://github.com/JoaoFranco03/photography-portfolio"">     <img src=""dist/assets/Logo.jpg"" alt=""Logo"" width=""80"" height=""80"">   </a>    <h3 align=""center"">Photography Portfolio</h3>    <p align=""center"">     A Website created using Tailwind CSS, HTML, CSS <br /> and JavaScript that can be used as a Photography Portfolio.     <br />     <br />     <a href=""https://photography-portfolio-joaofranco03.netlify.app/"">View Demo</a>     <br />     <br />        </p> </div>  <!-- TABLE OF CONTENTS --> <details>   <summary>Table of Contents</summary>   <ol>     <li><a href=""#about-the-project"">About The Project</a></li>     <li>       <a href=""#built-with"">Built With</a>       <ul>         <li><a href=""#html-badge"">HTML</a></li>         <li><a href=""#css-badge"">CSS</a></li>         <li><a href=""#js-badge"">JavaScript</a></li>         <li><a href=""#tailwind-badge"">Tailwind CSS</a></li>       </ul>     </li>     <li><a href=""#quick-start"">Quick Start</a></li>     <li><a href=""#getting-started"">Getting Started</a>       <ul>         <li><a href=""#installation"">Installation</a></li>       </ul>     </li>     <li><a href=""#license"">License</a></li>     <li><a href=""#contact"">Contact</a></li>     <li><a href=""#acknowledgments"">Acknowledgments</a></li>   </ol> </details>  <!-- ABOUT THE PROJECT -->  # üìã About The Project <a name=""about-the-project""></a>  [![Product Name Screen Shot][product-screenshot]](https://github.com/JoaoFranco03/photography-portfolio)  This project is a web-based portfolio that beautifully showcases the photography work of Sophia Williams, a fictional photographer. The portfolio was skillfully built using a combination of powerful front-end technologies, including Tailwind CSS, CSS, HTML, and JavaScript.  <p align=""right"">(<a href=""#readme-top"">back to top</a>)</p>  ## üõ†Ô∏è Built With <a name=""built-with""></a>  - [![HTML][html-badge]][html-url] - [![CSS][css-badge]][css-url] - [![JavaScript][js-badge]][js-url] - [![Tailwind][tailwind-badge]][tailwind-url]  <p align=""right"">(<a href=""#readme-top"">back to top</a>)</p>  ## üöÄ Quick Start <a name=""quick-start""></a>  Create your own page with one click on [Netlify](https://app.netlify.com/signup):  [<img src=""https://www.netlify.com/img/deploy/button.svg"" alt=""Deploy to Netlify"" />](https://app.netlify.com/start/deploy?repository=https://github.com/JoaoFranco03/photography-portfolio) <p align=""right"">(<a href=""#readme-top"">back to top</a>)</p>  ## üéØ Getting Started <a name=""getting-started""></a>  This is an example of how you may give instructions on setting up your project locally. To get a local copy up and running follow these simple example steps.  ### üèóÔ∏è Installation <a name=""installation""></a>  1. Clone the repo  ```sh  git clone https://github.com/JoaoFranco03/photography-portfolio/.git ```  2.  Run the following command:  ```sh  npx tailwindcss -i ./src/input.css -o ./dist/output.css --watch ``` 3.  Run the Project in a Server  4.  Change it with your own photos, about me and contact info.  5.  Publish it using your preferred hosting platform.  <p align=""right"">(<a href=""#readme-top"">back to top</a>)</p>  <!-- Ko-fi -->  ## ‚òï Support Me on Ko-fi  If you find this project useful, consider supporting me on Ko-fi. Thanks for checking it out!    <a href=""https://ko-fi.com/joaofranco03"" target=""_blank"">     <img src=""https://ko-fi.com/img/githubbutton_sm.svg"" alt=""Support me on Ko-fi"" style=""height:40px;""> </a>  <p align=""right"">(<a href=""#readme-top"">back to top</a>)</p>  <!-- LICENSE -->  ## üìú License <a name=""license""></a>  Distributed under the GPL-3.0 License. See `LICENSE.txt` for more information.  <p align=""right"">(<a href=""#readme-top"">back to top</a>)</p>  <!-- CONTACT -->  ## üìß Contact <a name=""contact""></a>  Jo√£o Franco - https://www.linkedin.com/in/jo√£o-franco-452161195/  Project Link: [https://github.com/JoaoFranco03/photography-portfolio/](https://github.com/JoaoFranco03/photography-portfolio/)  <p align=""right"">(<a href=""#readme-top"">back to top</a>)</p>  <!-- ACKNOWLEDGMENTS -->  ## üåü Acknowledgments <a name=""acknowledgments""></a>  - [FancyBox](https://fancyapps.com/fancybox/) - [Unsplash](https://unsplash.com/) - [Tailwind Documentation](https://tailwindcss.com/docs/installation)  <p align=""right"">(<a href=""#readme-top"">back to top</a>)</p>  <!-- MARKDOWN LINKS & IMAGES --> <!-- https://www.markdownguide.org/basic-syntax/#reference-style-links -->  [contributors-shield]: https://img.shields.io/github/contributors/othneildrew/Best-README-Template.svg?style=for-the-badge [contributors-url]: https://github.com/othneildrew/Best-README-Template/graphs/contributors [forks-shield]: https://img.shields.io/github/forks/othneildrew/Best-README-Template.svg?style=for-the-badge [forks-url]: https://github.com/othneildrew/Best-README-Template/network/members [stars-shield]: https://img.shields.io/github/stars/othneildrew/Best-README-Template.svg?style=for-the-badge [stars-url]: https://github.com/othneildrew/Best-README-Template/stargazers [issues-shield]: https://img.shields.io/github/issues/othneildrew/Best-README-Template.svg?style=for-the-badge [issues-url]: https://github.com/othneildrew/Best-README-Template/issues [tailwind-badge]: https://img.shields.io/badge/Tailwind_CSS-62BAF3?style=for-the-badge&logo=tailwind-css&logoColor=white [tailwind-url]: https://tailwindcss.com [html-badge]: https://img.shields.io/badge/HTML-239120?style=for-the-badge&logo=html5&logoColor=white [html-url]: https://developer.mozilla.org/en-US/docs/Web/HTML [css-badge]: https://img.shields.io/badge/CSS-239120?&style=for-the-badge&logo=css3&logoColor=white [css-url]: https://developer.mozilla.org/en-US/docs/Web/CSS [js-badge]: https://img.shields.io/badge/JavaScript-F7DF1E?style=for-the-badge&logo=javascript&logoColor=black [js-url]: https://developer.mozilla.org/en-US/docs/Web/JavaScript [license-url]: https://github.com/JoaoFranco03/photography-portfolio/blob/main/LICENSE.md [linkedin-shield]: https://img.shields.io/badge/-LinkedIn-black.svg?style=for-the-badge&logo=linkedin&colorB=555 [linkedin-url]: https://www.linkedin.com/in/jo√£o-franco-452161195/ [product-screenshot]: dist/assets/mockup.png"
Technical Documentation Wiki,HTML & CSS,https://github.com/naemazam/Technical-Documentation-Page,"# Technical Documentation Page   ### Technology Stack You can use HTML, JavaScript, and CSS to complete this project. Plain CSS is recommended because that is what the lessons have covered so far and you should get some practice with plain CSS. You can use Bootstrap or SASS if you choose. Additional technologies (just for example jQuery, React, Angular, or Vue) are not recommended for this project, and using them is at your own risk. Other projects will give you a chance to work with different technology stacks like React. We will accept and try to fix all issue reports that use the suggested technology stack for this project. Happy coding!     "
Survey Forms,HTML & CSS,https://github.com/lazy-geek/survey-form,"# Survey Form  A Survey Form That I Made To Learn Css Concepts  ## What I Learned  1. How To Make Custom CheckBox. 2. About Css Positioning. 3. How To Bind Checked Event To `Label` (give `checkbox` an `id` and in `Label` set `for` attribute to that `id` , now we can check the checkbox by clicking anything that is inside the `Label`).  ## Demo Visit [codepen.io](https://codepen.io/abrar-malek/full/poeMBZd) or https://lazy-geek.github.io/survey-form For Live Demo"
Comic Reading Site,HTML & CSS,https://github.com/afzafri/Web-Comic-Reader/tree/master,"# Web Comic Reader Web Based cbr/cbz/cbt comic book reader. Written in HTML5 and Javascript (with jQuery) <br> Demo: https://afzafri.github.io/Web-Comic-Reader/  # Screenshots <img src=""https://cloud.githubusercontent.com/assets/14824387/25302185/e8353cee-276a-11e7-9e78-d58eac26b16f.png"" width=""500px""/><br> <img src=""https://cloud.githubusercontent.com/assets/14824387/25302186/eb8f0064-276a-11e7-9e1d-0f4dd7f7f5bd.png"" width=""500px""/><br>  # Credit - Uncompress.js: https://github.com/workhorsy/uncompress.js - lightGallery: https://github.com/sachinchoolur/lightGallery - jQuery: https://jquery.com/  # License This library is under ```MIT license```, please look at the LICENSE file"
Fashion eCommerce Website,HTML & CSS,https://github.com/wpcodevo/lc28-fashion-ecommerce-website,"# Responsive Fashion E-commerce Website Template with HTML, CSS & JavaScript  ![Responsive Fashion E-commerce Website Template with HTML, CSS and JavaScript](https://raw.githubusercontent.com/wpcodevo/lc28-fashion-ecommerce-website/starter/fashion%20ecommerce%20website%20html%20css%20scss%20javascript.png 'Responsive Fashion E-commerce Website Template with HTML, CSS and JavaScript')  The Figma file of the Responsive Fashion E-commerce Website Template can be found on my [website](https://www.codevoweb.com)"
Restaurant Management System,HTML & CSS,https://github.com/ankithcrgowda/Responsive-Restaurant-Website,"# Restaurant-Management-System Frontend of a restaurant management system website crafted with HTML5, CSS3, JavaScript, and Bootstrap."
Event Management Website,HTML & CSS,https://github.com/roniy68/event-management,"<a name=""readme-top""></a>  <div align=""center"">    <img src=""images/logo.jpg"" alt=""logo"" width=""140""  height=""auto"" />   <br/>    <h3><b>Event Management</b></h3>  </div>  <!-- TABLE OF CONTENTS --> # üìó Table of Contents  - [üìó Table of Contents](#-table-of-contents) - [üìñ Event-Management  ](#-event-management--)   - [üõ† Built With ](#-built-with-)     - [Tech Stack ](#tech-stack-)     - [Key Features ](#key-features-)   - [üöÄ Live Demo ](#-live-demo-)   - [Demonstration :](#demonstration-)   - [üíª Getting Started ](#-getting-started-)     - [Prerequisites](#prerequisites)     - [Setup](#setup)     - [Install](#install)     - [Usage](#usage) - [To check for css errors](#to-check-for-css-errors) - [To check for html errors](#to-check-for-html-errors)   - [üë• Authors ](#-authors-)   - [üî≠ Future Features ](#-future-features-)   - [ü§ù Contributing ](#-contributing-)   - [‚≠êÔ∏è Show your support ](#Ô∏è-show-your-support-)   - [üôè Acknowledgments ](#-acknowledgments-)   - [Designed by :](#designed-by-)   - [Developed By:](#developed-by)   - [‚ùì FAQ ](#-faq-)   - [üìù License ](#-license-)  <!-- PROJECT DESCRIPTION -->  # üìñ Event-Management  <a name=""about-project""></a>  > This Project is to create Event-Management for our Resume >       <br><b> Module One day 2 Project [solo]</b>  **Event-Management** is a template to build your Event-Management website beautiful.  ## üõ† Built With <a name=""built-with""></a>  ### Tech Stack <a name=""tech-stack""></a>  >HTML and CSS  <details>   <summary>Client</summary>   <ul>     <li><a href=""https://w3school.com/"">HTML</a></li>   </ul> </details> <br>  ### Key Features <a name=""key-features""></a>  > THIS IS A SIMPLE TEMPLATE APP FOR NOW  <p align=""right"">(<a href=""#readme-top"">back to top</a>)</p>  <!-- LIVE DEMO -->  ## üöÄ Live Demo <a name=""live-demo""></a>  > Deployed Link Below:  >  soon will be available  - [Live Demo Link](https://roniy68.github.io/event-management)      <p align=""right"">(<a href=""#readme-top"">back to top</a>)</p>  ## Demonstration :   > Watch this video:  - Demonstraion: <br/> [![Watch the video](https://media.giphy.com/media/3oz8xNVZFhMdjNnoic/giphy.gif)](https://www.loom.com/share/dca16ee0b02741dd8e9e763f2d3e1c2b) <!-- GETTING STARTED -->  ## üíª Getting Started <a name=""getting-started""></a>  > An Event Management website where Engineers from different fields collaborate together.   To get a local copy up and running, follow these steps.  ### Prerequisites  In order to run this project you need:   - NODE    - ESlint set up  ### Setup - install node and eslint    ```sh   npm install eslint   npx eslint --init ``` <br>   ### Install  Install this project with:  Clone this repository to your desired folder:    commands:  ```sh   git clone https://github.com/roniy68/event-management.git   cd hello-world   npm install -y ``` <br><br>  ### Usage  To run the project, execute the following command:   -install serve with : npm install -g serve  ```sh   serve -s . ```     # To check for css errors  ```sh     npx stylelint ""**/*.{css,scss}""  ``` # To check for html errors ```sh     npx hint . ```   <p align=""right"">(<a href=""#readme-top"">back to top</a>)</p>  <!-- AUTHORS -->  ## üë• Authors <a name=""authors""></a>   üë§ **Author**  - GitHub: [@roniy68](https://github.com/roniy68) - Twitter: [@ahroniy](https://twitter.com/ahroniy) - LinkedIn: [LinkedIn](https://linkedin.com/in/ahroniy)    <p align=""right"">(<a href=""#readme-top"">back to top</a>)</p>  <!-- FUTURE FEATURES -->  ## üî≠ Future Features <a name=""future-features""></a>  >  1 - 3 features I will add to the project.  - [ ] **javascript** - [ ] **frontend** - [ ] **backend**  <p align=""right"">(<a href=""#readme-top"">back to top</a>)</p>  <!-- CONTRIBUTING -->  ## ü§ù Contributing <a name=""contributing""></a>  Contributions, issues, and feature requests are welcome!  Feel free to check the [issues page](../../issues/).  <p align=""right"">(<a href=""#readme-top"">back to top</a>)</p>  <!-- SUPPORT -->  ## ‚≠êÔ∏è Show your support <a name=""support""></a>  >  I need Support and Guidance to become a Developer.  If you like this project...  <p align=""right"">(<a href=""#readme-top"">back to top</a>)</p>  <!-- ACKNOWLEDGEMENTS -->  ## üôè Acknowledgments <a name=""acknowledgements""></a>  ## Designed by : >[Cindy Shin from behance ](https://www.behance.net/adagio07) <br>  ## Developed By:  > [`Ahmed Hasan Rony`](https://www.linkedin.com/in/ahroniy)  <p align=""right"">(<a href=""#readme-top"">back to top</a>)</p>  <!-- FAQ (optional) -->  ## ‚ùì FAQ <a name=""faq""></a>    - **How you clone the repo?**    - git clone **\<repo name\>**  - **How you install node?**    - https://radixweb.com/blog/installing-npm-and-nodejs-on-windows-and-mac  <p align=""right"">(<a href=""#readme-top"">back to top</a>)</p>  <!-- LICENSE -->  ## üìù License <a name=""license""></a>  This project is [MIT](./LICENSE) licensed.  <p align=""right"">(<a href=""#readme-top"">back to top</a>)</p>"
Product Landing Page,HTML & CSS,https://github.com/FreeCodeCamp-Solutions/Product-Landing-Page,"# Build-A-Product-Landing-Page FreeCodeCamp.com Responsive Web Design Certification Project - ""Build A Product Landing Page"" <https://www.freecodecamp.org/learn/responsive-web-design/responsive-web-design-projects/build-a-product-landing-page> FreeCodeCamp - Build a Product Landing Page.  Responsive Web Design Projects - Build a Product Landing Page Objective: Build a CodePen.io app that is functionally similar to this: https://codepen.io/freeCodeCamp/full/RKRbwL.  Fulfill the below user stories and get all of the tests to pass. Give it your own personal style.  You can use HTML, JavaScript, and CSS to complete this project. Plain CSS is recommended because that is what the lessons have covered so far and you should get some practice with plain CSS. You can use Bootstrap or SASS if you choose. Additional technologies (just for example jQuery, React, Angular, or Vue) are not recommended for this project, and using them is at your own risk. Other projects will give you a chance to work with different technology stacks like React. We will accept and try to fix all issue reports that use the suggested technology stack for this project. Happy coding!  User Story #1: My product landing page should have a header element with a corresponding id=""header"".  User Story #2: I can see an image within the header element with a corresponding id=""header-img"". A company logo would make a good image here.  User Story #3: Within the #header element I can see a nav element with a corresponding id=""nav-bar"".  User Story #4: I can see at least three clickable elements inside the nav element, each with the class nav-link.  User Story #5: When I click a .nav-link button in the nav element, I am taken to the corresponding section of the landing page.  User Story #6: I can watch an embedded product video with id=""video"".  User Story #7: My landing page has a form element with a corresponding id=""form"".  User Story #8: Within the form, there is an input field with id=""email"" where I can enter an email address.  User Story #9: The #email input field should have placeholder text to let the user know what the field is for.  User Story #10: The #email input field uses HTML5 validation to confirm that the entered text is an email address.  User Story #11: Within the form, there is a submit input with a corresponding id=""submit"".  User Story #12: When I click the #submit element, the email is submitted to a static page (use this mock URL: https://www.freecodecamp.com/email-submit).  User Story #13: The navbar should always be at the top of the viewport.  User Story #14: My product landing page should have at least one media query.  User Story #15: My product landing page should utilize CSS flexbox at least once.  You can build your project by forking this CodePen pen. Or you can use this CDN link to run the tests in any environment you like: https://cdn.freecodecamp.org/testable-projects-fcc/v1/bundle.js  Once you're done, submit the URL to your working project with all its tests passing.  Live Site(solution):  https://freecodecamp-solutions.github.io/Product-Landing-Page/"
Travel Agency Landing Page,HTML & CSS,https://github.com/adityamamta/travel-one-page-website,"# Travel Agency Landing Page üåç  ## <a href=""https://adityamamta.github.io/travel-one-page-website/""><img src=""img/readme-btn.png"" alt=""Click to view live website"" height=""120""></a>  This is a fully responsive travel agency landing page built with **HTML**, **CSS**, and **JavaScript**. The design showcases a professional and user-friendly interface, providing essential features for booking tours, exploring destinations, and viewing testimonials.  ### Features - **Hero Section** with a captivating headline and call-to-action buttons. - **Best Services** section highlighting key offerings like guided tours, flights, and medical insurance. - **Tour Packages** with eye-catching visuals and details about popular destinations. - **Testimonials** slider showcasing client reviews. - **Footer Section** with navigation links and a subscription form.  ### Tools & Technologies - **HTML5** for semantic structure. - **CSS3** for responsive and attractive styling. - **JavaScript** for interactivity, including sliders and animations.  ### Highlights - Responsive layout for seamless viewing across all devices. - Professional travel-themed design with high-quality images. - Modern UI/UX to enhance the user experience.   ### üíº Contact me  - linkedin. [Linkedin](https://www.linkedin.com/in/adityamamta/) - Email: adityamamta4@gmail.com  <!-- ![preview img](image/card-hover-effect-mockup.png) -->"
Tribute Page,HTML & CSS,https://github.com/kalawaja/freeCodeCamp_tributePage,"# Tribute Page  > `Certification Project` > ## Tribute Page > - This is one of the required projects to earn your certification. > - For this project, you will build a tribute page for a subject of your choosing, fictional or real.  > ### **Instructions** > **Build a Tribute Page** <br> > <br> > **Objective:** Build an app that is functionally similar to [Tribute Page](https://tribute-page.freecodecamp.rocks) <br> > <br> > **User Stories:** > 1. Your tribute page should have a `main` element with a corresponding `id` of `main`, which contains all other elements > 2. You should see an element with an `id` of `title`, which contains a string (i.e. text), that describes the subject of the tribute page (e.g. ""Dr. Norman Borlaug"") > 3. You should see either a `figure` or a `div` element with an `id` of `img-div` > 4.  Within the `#img-div` element, you should see an `img` element with a corresponding `id=""image""` > 5. Within the `#img-div` element, you should see an element with a corresponding `id=""img-caption""` that contains textual content describing the image shown in `#img-div` > 6. You should see an element with a corresponding `id=""tribute-info""`, which contains textual content describing the subject of the tribute page > 7. You should see an `a` element with a corresponding `id=""tribute-link""`, which links to an outside site, that contains additional information about the subject of the tribute page. HINT: You must give your element an attribute of `target` and set it to `_blank` in order for your link to open in a new tab > 8. Your `#image` should use `max-width` and `height` properties to resize responsively, relative to the width of its parent element, without exceeding its original size > 9. Your `img` element should be centered within its parent element <br>  > Fulfill the user stories and pass all the tests below to complete this project. Give it your own personal style. Happy Coding! <br>  > **Note:** Be sure to add ```<link rel=""stylesheet"" href=""styles.css"">``` in your HTML to link your stylesheet and apply your CSS <br>  ---  > ## **Solutions** <br> > **Follow Step by Step** <br><br>  > **Step 1** <br> Your tribute page should have a `main` element with a corresponding `id` of `main`, which contains all other elements  ```html #index.html     <main id=""main"">      </main> ``` ```css #styles.css html {   /* Setting a base font size of 10px give us easier rem calculations        Info: 1rem === 10px, 1.5rem === 15px, 2rem === 20px and so forth      */   font-size: 10px; } ``` > **Step 2** <br> You should see an element with an `id` of `title`, which contains a string (i.e. text), that describes the subject of the tribute page (e.g. ""Dr. Norman Borlaug"")  ```html #index.html     <main id=""main"">       <h1 id=""title"">Dr. Norman Borlaug</h1>       <p>The man who saved a billion lives</p>      </main> ``` ```css #styles.css body {   /* Native font stack https://getbootstrap.com/docs/4.2/content/reboot/#native-font-stack */   font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto',     'Helvetica Neue', Arial, sans-serif;   font-size: 1.6rem;   line-height: 1.5;   text-align: center;   color: #333;   margin: 0; }  h1 {   font-size: 4rem;   margin-bottom: 0; }  @media (max-width: 460px) {   h1 {     font-size: 3.5rem;     line-height: 1.2;   } }  h2 {   font-size: 3.25rem; } ``` > **Step 3** <br> You should see either a `figure` or a `div` element with an `id` of `img-div`  ```html #index.html     <main id=""main"">       <h1 id=""title"">Dr. Norman Borlaug</h1>       <p>The man who saved a billion lives</p>       <figure id=""img-div"">                <figure>     </main> ``` > **Step 4** <br> Within the `#img-div` element, you should see an `img` element with a corresponding `id=""image""`  ```html #index.html     <main id=""main"">       <h1 id=""title"">Dr. Norman Borlaug</h1>       <p>The man who saved a billion lives</p>       <figure id=""img-div"">         <img id=""image"" src=""https://cdn.freecodecamp.org/testable-projects-fcc/images/tribute-page-main-image.jpg"" alt=""Dr. Norman Borlaug seen standing in Mexican wheat field with a group of biologists"">       <figure>     </main> ``` > **Step 5** <br> Within the `#img-div` element, you should see an element with a corresponding `id=""img-caption""` that contains textual content describing the image shown in `#img-div`  ```html #index.html     <main id=""main"">       <h1 id=""title"">Dr. Norman Borlaug</h1>       <p>The man who saved a billion lives</p>       <figure id=""img-div"">         <img           id=""image""           src=""https://cdn.freecodecamp.org/testable-projects-fcc/images/tribute-page-main-image.jpg""           alt=""Dr. Norman Borlaug seen standing in Mexican wheat field with a group of biologists""         />         <figcaption id=""img-caption"">           Dr. Norman Borlaug, third from the left, trains biologists in Mexico           on how to increase wheat yields - part of his life-long war on hunger.         </figcaption>       </figure>     </main> ``` > **Step 6** <br> You should see an element with a corresponding `id=""tribute-info""`, which contains textual content describing the subject of the tribute page  ```html #index.html       <section id=""tribute-info"">         <h3 id=""headline"">Here's a time line of Dr. Borlaug's life:</h3>         <ul>           <li><strong>1914</strong> - Born in Cresco, Iowa</li>           <li>             <strong>1933</strong> - Leaves his family's farm to attend the             University of Minnesota, thanks to a Depression era program known as             the ""National Youth Administration""           </li>           <li>             <strong>1935</strong> - Has to stop school and save up more money.             Works in the Civilian Conservation Corps, helping starving             Americans. ""I saw how food changed them"", he said. ""All of this left             scars on me.""           </li>           <li>             <strong>1937</strong> - Finishes university and takes a job in the             US Forestry Service           </li>           <li>             <strong>1938</strong> - Marries wife of 69 years Margret Gibson.             Gets laid off due to budget cuts. Inspired by Elvin Charles Stakman,             he returns to school study under Stakman, who teaches him about             breeding pest-resistent plants.           </li>           <li>             <strong>1941</strong> - Tries to enroll in the military after the             Pearl Harbor attack, but is rejected. Instead, the military asked             his lab to work on waterproof glue, DDT to control malaria,             disinfectants, and other applied science.           </li>           <li>             <strong>1942</strong> - Receives a Ph.D. in Genetics and Plant             Pathology           </li>           <li>             <strong>1944</strong> - Rejects a 100% salary increase from Dupont,             leaves behind his pregnant wife, and flies to Mexico to head a new             plant pathology program. Over the next 16 years, his team breeds             6,000 different strains of disease resistent wheat - including             different varieties for each major climate on Earth.           </li>           <li>             <strong>1945</strong> - Discovers a way to grown wheat twice each             season, doubling wheat yields           </li>           <li>             <strong>1953</strong> - crosses a short, sturdy dwarf breed of wheat             with a high-yeidling American breed, creating a strain that responds             well to fertilizer. It goes on to provide 95% of Mexico's wheat.           </li>           <li>             <strong>1962</strong> - Visits Delhi and brings his high-yielding             strains of wheat to the Indian subcontinent in time to help mitigate             mass starvation due to a rapidly expanding population           </li>           <li><strong>1970</strong> - receives the Nobel Peace Prize</li>           <li>             <strong>1983</strong> - helps seven African countries dramatically             increase their maize and sorghum yields           </li>           <li>             <strong>1984</strong> - becomes a distinguished professor at Texas             A&M University           </li>           <li>             <strong>2005</strong> - states ""we will have to double the world             food supply by 2050."" Argues that genetically modified crops are the             only way we can meet the demand, as we run out of arable land. Says             that GM crops are not inherently dangerous because ""we've been             genetically modifying plants and animals for a long time. Long             before we called it science, people were selecting the best breeds.""           </li>           <li><strong>2009</strong> - dies at the age of 95.</li>         </ul>         <blockquote           cite=""http://news.rediff.com/report/2009/sep/14/pm-pays-tribute-to-father-of-green-revolution-borlaug.htm""         >           <p>             ""Borlaug's life and achievement are testimony to the far-reaching             contribution that one man's towering intellect, persistence and             scientific vision can make to human peace and progress.""           </p>           <cite>-- Indian Prime Minister Manmohan Singh</cite>         </blockquote>         <h3>           If you have time, you should read more about this incredible human           being on his         </h3>       </section> ``` ```css #styles.css #headline {   margin: 50px 0;   text-align: center; }  ul {   max-width: 550px;   margin: 0 auto 50px auto;   text-align: left;   line-height: 1.6; }  li {   margin: 16px 0; }  blockquote {   font-style: italic;   max-width: 545px;   margin: 0 auto 50px auto;   text-align: left; } ``` > **Step 7** <br> You should see an `a` element with a corresponding `id=""tribute-link""`, which links to an outside site, that contains additional information about the subject of the tribute page. HINT: You must give your element an attribute of `target` and set it to `_blank` in order for your link to open in a new tab  ```html #index.html         <h3>           If you have time, you should read more about this incredible human           being on his           <a             id=""tribute-link""             href=""https://en.wikipedia.org/wiki/Norman_Borlaug""             target=""_blank""             >Wikipedia entry</a           >.         </h3> ``` ```css #styles.css a {   color: #477ca7; }  a:visited {   color: #74638f; } ``` > **Step 8** <br> Your `#image` should use `max-width` and `height` properties to resize responsively, relative to the width of its parent element, without exceeding its original size  ```css #styles.css img {   max-width: 100%;   display: block;   height: auto;   margin: 0 auto; }  #img-div {   background: white;   padding: 10px;   margin: 0; }  #img-caption {   margin: 15px 0 5px 0; }  @media (max-width: 460px) {   #img-caption {     font-size: 1.4rem;   } } ``` > **Step 9** <br> Your `img` element should be centered within its parent element  ```css #styles.css #main {   margin: 30px 8px;   padding: 15px;   border-radius: 5px;   background: #eee; }  @media (max-width: 460px) {   #main {     margin: 0;   } } ```  > **Note:** Be sure to add ```<link rel=""stylesheet"" href=""styles.css"">``` in your HTML to link your stylesheet and apply your CSS <br> > <br>  ```html #index.html <!DOCTYPE html> <html>   <head>     <link rel=""stylesheet"" href=""styles.css"" />   </head>   <body>     <main id=""main"">       <h1 id=""title"">Dr. Norman Borlaug</h1>       <p>The man who saved a billion lives</p>       <figure id=""img-div"">         <img           id=""image""           src=""https://cdn.freecodecamp.org/testable-projects-fcc/images/tribute-page-main-image.jpg""           alt=""Dr. Norman Borlaug seen standing in Mexican wheat field with a group of biologists""         />         <figcaption id=""img-caption"">           Dr. Norman Borlaug, third from the left, trains biologists in Mexico           on how to increase wheat yields - part of his life-long war on hunger.         </figcaption>       </figure>       <section id=""tribute-info"">         <h3 id=""headline"">Here's a time line of Dr. Borlaug's life:</h3>         <ul>           <li><strong>1914</strong> - Born in Cresco, Iowa</li>           <li>             <strong>1933</strong> - Leaves his family's farm to attend the             University of Minnesota, thanks to a Depression era program known as             the ""National Youth Administration""           </li>           <li>             <strong>1935</strong> - Has to stop school and save up more money.             Works in the Civilian Conservation Corps, helping starving             Americans. ""I saw how food changed them"", he said. ""All of this left             scars on me.""           </li>           <li>             <strong>1937</strong> - Finishes university and takes a job in the             US Forestry Service           </li>           <li>             <strong>1938</strong> - Marries wife of 69 years Margret Gibson.             Gets laid off due to budget cuts. Inspired by Elvin Charles Stakman,             he returns to school study under Stakman, who teaches him about             breeding pest-resistent plants.           </li>           <li>             <strong>1941</strong> - Tries to enroll in the military after the             Pearl Harbor attack, but is rejected. Instead, the military asked             his lab to work on waterproof glue, DDT to control malaria,             disinfectants, and other applied science.           </li>           <li>             <strong>1942</strong> - Receives a Ph.D. in Genetics and Plant             Pathology           </li>           <li>             <strong>1944</strong> - Rejects a 100% salary increase from Dupont,             leaves behind his pregnant wife, and flies to Mexico to head a new             plant pathology program. Over the next 16 years, his team breeds             6,000 different strains of disease resistent wheat - including             different varieties for each major climate on Earth.           </li>           <li>             <strong>1945</strong> - Discovers a way to grown wheat twice each             season, doubling wheat yields           </li>           <li>             <strong>1953</strong> - crosses a short, sturdy dwarf breed of wheat             with a high-yeidling American breed, creating a strain that responds             well to fertilizer. It goes on to provide 95% of Mexico's wheat.           </li>           <li>             <strong>1962</strong> - Visits Delhi and brings his high-yielding             strains of wheat to the Indian subcontinent in time to help mitigate             mass starvation due to a rapidly expanding population           </li>           <li><strong>1970</strong> - receives the Nobel Peace Prize</li>           <li>             <strong>1983</strong> - helps seven African countries dramatically             increase their maize and sorghum yields           </li>           <li>             <strong>1984</strong> - becomes a distinguished professor at Texas             A&M University           </li>           <li>             <strong>2005</strong> - states ""we will have to double the world             food supply by 2050."" Argues that genetically modified crops are the             only way we can meet the demand, as we run out of arable land. Says             that GM crops are not inherently dangerous because ""we've been             genetically modifying plants and animals for a long time. Long             before we called it science, people were selecting the best breeds.""           </li>           <li><strong>2009</strong> - dies at the age of 95.</li>         </ul>         <blockquote           cite=""http://news.rediff.com/report/2009/sep/14/pm-pays-tribute-to-father-of-green-revolution-borlaug.htm""         >           <p>             ""Borlaug's life and achievement are testimony to the far-reaching             contribution that one man's towering intellect, persistence and             scientific vision can make to human peace and progress.""           </p>           <cite>-- Indian Prime Minister Manmohan Singh</cite>         </blockquote>         <h3>           If you have time, you should read more about this incredible human           being on his           <a             id=""tribute-link""             href=""https://en.wikipedia.org/wiki/Norman_Borlaug""             target=""_blank""             >Wikipedia entry</a           >.         </h3>       </section>     </main>   </body> </html> ``` ```css #styles.css html {   /* Setting a base font size of 10px give us easier rem calculations        Info: 1rem === 10px, 1.5rem === 15px, 2rem === 20px and so forth      */   font-size: 10px; }  body {   /* Native font stack https://getbootstrap.com/docs/4.2/content/reboot/#native-font-stack */   font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto',     'Helvetica Neue', Arial, sans-serif;   font-size: 1.6rem;   line-height: 1.5;   text-align: center;   color: #333;   margin: 0; }  h1 {   font-size: 4rem;   margin-bottom: 0; }  @media (max-width: 460px) {   h1 {     font-size: 3.5rem;     line-height: 1.2;   } }  h2 {   font-size: 3.25rem; }  a {   color: #477ca7; }  a:visited {   color: #74638f; }  #main {   margin: 30px 8px;   padding: 15px;   border-radius: 5px;   background: #eee; }  @media (max-width: 460px) {   #main {     margin: 0;   } }  img {   max-width: 100%;   display: block;   height: auto;   margin: 0 auto; }  #img-div {   background: white;   padding: 10px;   margin: 0; }  #img-caption {   margin: 15px 0 5px 0; }  @media (max-width: 460px) {   #img-caption {     font-size: 1.4rem;   } }  #headline {   margin: 50px 0;   text-align: center; }  ul {   max-width: 550px;   margin: 0 auto 50px auto;   text-align: left;   line-height: 1.6; }  li {   margin: 16px 0; }  blockquote {   font-style: italic;   max-width: 545px;   margin: 0 auto 50px auto;   text-align: left; } ```"
Parallax Site,HTML & CSS,https://github.com/VineetKumar02/Parallax-Website,"# Parallax Scrolling Website  Welcome to the Parallax Scrolling Website project! This project showcases an interactive and engaging website with a parallax scrolling effect. The website is designed using HTML5, CSS3, and JavaScript to create an immersive experience for users.  [![Project Preview](https://drive.google.com/uc?id=1GFbBHTi7w8weRVqLJ7bkofezEEGg4U30)](https://parallax-website-by-vineet.netlify.app/)  ## Table of Contents  - [Introduction](#introduction) - [Features](#features) - [Getting Started](#getting-started) - [Usage](#usage) - [Technologies Used](#technologies-used) - [Contributing](#contributing)  ## Introduction  This project is a demonstration of parallax scrolling ‚Äî a web design technique that adds depth and interactivity to web pages. As users scroll down the website, the background elements move at different speeds, creating a visually captivating experience.  ## Features  - **Parallax Scrolling:** Engaging parallax scrolling effect that adds a dynamic and immersive element to the website. - **Interactive Sections:** Divided into sections, each with its own parallax effect, allowing users to explore different visual layers. - **Responsive Design:** The website is designed to be responsive, ensuring a seamless experience across various devices and screen sizes. - **Informative Content:** Informative text sections explain the concept of parallax scrolling and its applications in web design. - **Smooth Animations:** Utilizes animations and transitions to create smooth and visually appealing effects. - **Navigation:** The header provides easy navigation links to different sections of the website.   ## Getting Started  ### 1. To access the project, you have two options:  1. **Clone the Repository:**      If you have Git installed, you can clone the repository using the following command:          ```bash    git clone https://github.com/VineetKumar02/Parallax-Website.git    ```      After cloning, navigate to the project directory:      ```bash     cd Parallax-Website     ```  2. **Download ZIP:**      If you don't have Git installed, you can download the ZIP archive of the project. Click the ""Code"" button on the repository page and choose ""Download ZIP."" Extract the downloaded ZIP file to a location of your choice.  ### 2. To run the project, you have two options:  1. If you have the Live Server extension installed in your code editor:    - Open the project folder in your code editor.    - Right-click on the `index.html` file and select ""Open with Live Server.""     Live Server will launch the page in your default web browser, allowing you to use the view the website.  2. If you don't have the Live Server extension:     - Open the project folder in your preferred code editor.     - Double-click on the `index.html` file to open it in your web browser.  ## Usage  1. Open the project folder and explore the HTML, CSS, and JavaScript files to see how the parallax scrolling effect is implemented.  2. Feel free to modify and experiment with the code to understand how different elements contribute to the overall interactive experience.  ## Technologies Used  - HTML5 - CSS3 - JavaScript  ## Contributing  If you're interested in contributing to this project, whether it's fixing a bug, adding new features, or improving the documentation, your contributions are welcome! Please create a pull request with your changes."
Weather Suggestions (Landing Page),HTML & CSS,https://github.com/Himanshu3412/Weather-Forecast,"# Weather_Forecast    This project is a simple weather forecast website built using HTML, CSS, and JavaScript. It provides users with current weather conditions.    # Features    - **Current Weather:** Display of current weather conditions including temperature, humidity, wind speed, and weather description.  - **Search:** Ability for users to search for weather forecasts for specific locations.    # Technologies Used    - **HTML:** Used for structuring the webpage and its content.  - **CSS:** Styled the website to make it visually appealing and user-friendly.  - **JavaScript:** Implemented functionality for fetching weather data from a weather API and dynamically updating the webpage.    # Preview    ![Screenshot 2024-03-24 224200](https://github.com/Himanshu3412/Weather_Forecast/assets/163979859/932f5749-f7d4-47cd-9498-0ec8344fb515)  ![Screenshot 2024-03-24 224317](https://github.com/Himanshu3412/Weather_Forecast/assets/163979859/a1502c91-1a98-40c8-8212-95126c3646c7)  ![Screenshot 2024-03-24 224507](https://github.com/Himanshu3412/Weather_Forecast/assets/163979859/b3c0d3a4-6c14-4c90-ba10-4a112d4a09b8)  ![Screenshot 2024-03-24 224645](https://github.com/Himanshu3412/Weather_Forecast/assets/163979859/68dca5c2-ea1e-4782-bf35-79d66efd6537)  ![Screenshot 2024-03-24 224723](https://github.com/Himanshu3412/Weather_Forecast/assets/163979859/71955859-6f18-4e6e-b3f5-9e68da26a1a1)  ![Screenshot 2024-03-24 225134](https://github.com/Himanshu3412/Weather_Forecast/assets/163979859/aa329b87-d562-4e97-9920-d76a77465541)  ![Screenshot 2024-03-24 225549](https://github.com/Himanshu3412/Weather_Forecast/assets/163979859/68365c0f-5534-4d3c-9a17-83170dd6bb14)    # Go Live    Check out the live here [Live](https://weather-forecast-one-gamma.vercel.app/)    ##  Contributing    Contributions to this project are welcome! Before contributing, please take a moment to review the following guidelines:    1. **Fork** the repository on GitHub.  2. **Clone** the forked repository to your local machine.  3. **Create a new branch** off of the `main` branch for your changes.  4. **Make your modifications** and ensure they adhere to the project's coding style and conventions.  5. **Test your changes** thoroughly to ensure they work as expected.  6. **Commit your changes** with descriptive commit messages.  7. **Push your changes** to your forked repository.  8. **Submit a pull request** (PR) to the original repository.    Please include a clear description of your changes in the pull request, along with any necessary context or motivation behind the changes. Be responsive to any feedback or questions that may arise during the review process.    Before submitting a pull request, ensure that your changes:    - Pass all automated tests (if applicable).  - Adhere to the project's coding standards.  - Are properly documented (e.g., code comments, README updates).    Thank you for your support and contributions!    ##  Get in Touch    Thank you for exploring the Netflix Homepage Clone! If you have any questions, suggestions, or feedback, feel free to reach out to me via GitHub:    - GitHub: [Himanshu3412](https://github.com/Himanshu3412)    ##  All the best!     [![built with love](https://forthebadge.com/images/badges/built-with-love.svg)](https://github.com/Himanshu3412/Weather_Forecast)"
Event or Conference Page,HTML & CSS,https://github.com/harshi0102/ConferencePage,"<a name=""readme-top""></a>   <!-- TABLE OF CONTENTS -->  # üìó Table of Contents  - [üìó Table of Contents](#-table-of-contents) - [üìñ \[Conference Web Page -Capstone Project\] ](#-conference-web-page--capstone-project-)   - [Video Presentation ](#video-presentation-)   - [üõ† Built With ](#-built-with-)     - [Tech Stack ](#tech-stack-)     - [Key Features ](#key-features-)   - [üöÄ Live Demo ](#-live-demo-)   - [üíª Getting Started ](#-getting-started-)     - [Prerequisites](#prerequisites)     - [Setup](#setup)     - [Install](#install)     - [Usage](#usage)     - [Run tests](#run-tests)     - [Deployment](#deployment)   - [üë• Authors ](#-authors-)   - [üî≠ Future Features ](#-future-features-)   - [ü§ù Contributing ](#-contributing-)   - [‚≠êÔ∏è Show your support ](#Ô∏è-show-your-support-)   - [üôè Acknowledgments ](#-acknowledgments-)   - [üìù License ](#-license-)  <!-- PROJECT DESCRIPTION -->  # üìñ [Conference Web Page -Capstone Project] <a name=""about-project""></a>  **[Conference Web Page ]** is a conference web page built on HTML,CSS and Javascript. I used semantic HTML tags in this project. -I used CSS selectors correctly. -I Used CSS box model. -I used Flexbox to place elements in the page. -I Demonstrated the ability to create UIs adaptable to different screen sizes using media queries. -I used GitHub Pages to deploy web pages. -I Applied JavaScript best practices and language style guides in this code. -I Used JavaScript to manipulate DOM elements. -I Used JavaScript events. -I used objects to store and access data.    [Live Demo](https://harshi0102.github.io/ConferencePage/)   <!--Video--> ## Video Presentation <a name=""video-presentation""></a> [Click Here for Video Presentation](https://www.loom.com/share/37fdc5ee0b2b4ab9994e54aa0f504f6a)  Desktop Version <a name=""desktop-version""></a> ![Screenshot 2023-03-30 120408](https://user-images.githubusercontent.com/108334376/228751937-ea1224a9-04c2-4c62-a6f0-f8c243c7133f.jpg)   Mobile Version <a name=""mobile-version""></a> ![image](https://user-images.githubusercontent.com/108334376/228752143-0fd65800-0a86-43b8-b37a-e69e57661fb2.png)   ## üõ† Built With <a name=""built-with""></a>  ### Tech Stack <a name=""tech-stack""></a>  > - Front-end: HTML, CSS, JavaScript, Flexbox, Grid, Media Queries, Semantic HTML.   - Back-end: Not applicable as the project is a static website.   - Database: Not applicable as the project is a static website.   - Deployment: The website can be hosted on a web server or using a static site generator like Github Pages.   <details>   <summary>HTML</summary>   <ul>     <li><a href=""https://www.w3schools.com/html/"">HTML5</a></li>   </ul> </details>  <details>   <summary>CSS</summary>   <ul>     <li><a href=""https://www.w3schools.com/Css/"">CSS3</a></li>   </ul> </details>  <details> <summary>Javascript</summary>   <ul>     <li><a href=""https://javascript.info/"">Javascript</a></li>   </ul> </details>  <!-- Features -->  ### Key Features <a name=""key-features""></a>  > Describe between 1-3 key features of the application.  - **[Implemeted HTML,css and Javascript ]** - **[Usage of Grid, media queries Semantic HTML]**  <p align=""right"">(<a href=""#readme-top"">back to top</a>)</p>  <!-- LIVE DEMO -->  ## üöÄ Live Demo <a name=""live-demo""></a>  >  - [Live Demo Link](https://harshi0102.github.io/ConferencePage/)  <p align=""right"">(<a href=""#readme-top"">back to top</a>)</p>  <!-- GETTING STARTED -->  ## üíª Getting Started <a name=""getting-started""></a>  > Create a local directory that you want to clone the repository.  - Open the command prompt in the created directory.  - On the terminal run this command git clone https://github.com/harshi0102/ConferencePage.git  - Go to the repository folder using command prompt cd.  - Install the dev dependencies for linters run npm install.  To get a local copy up and running, follow these steps.  ### Prerequisites  In order to run this project you need: A web browser: You will need a modern web browser such as Google Chrome, Mozilla Firefox, or Safari to view and test your project. - A code editor: You will need a code editor such as Visual Studio Code, Sublime Text, or Atom to write, edit, and manage your project's files. - A local server: You will need a local server such as XAMPP or WAMP to run your project locally and test it on your own computer. - Knowledge of HTML, CSS, and JavaScript: You will need to have a basic understanding of HTML, CSS, and JavaScript in order to build a web development project. - Familiarity with Git and GitHub: If you are using Git and GitHub to manage your project's codebase, you will need to be familiar with how to use them.  <!-- Example command:  ```sh  gem install rails ```  -->  ### Setup  Clone this repository to your desired folder:  - Open the command prompt in the created directory.  - On the terminal run this command git clone https://github.com/harshi0102/ConferencePage.git  - Go to the repository folder using command prompt cd.  - Install the dev dependencies for linters run npm install.    ### Install  No installation required   <!-- Example command:  ```sh   cd my-project   gem install ``` --->  ### Usage  To run the project, open index.html in any web browser.  <!-- Example command:  ```sh   rails server ``` --->  ### Run tests  To run tests, open index.html in any web browser.  <!-- Example command:  ```sh   bin/rails test test/models/article_test.rb ``` --->  ### Deployment  You can check the live version here:https://harshi0102.github.io/ConferencePage/  <!-- Example:  ```sh  ```  -->  <p align=""right"">(<a href=""#readme-top"">back to top</a>)</p>  <!-- AUTHORS -->  ## üë• Authors <a name=""authors""></a>   üë§ **Author1**  - GitHub: [harshi0102](https://github.com/harshi0102) - Twitter: [@harshika0102me](https://twitter.com/harshika0102me) - LinkedIn: [LinkedIn](https://www.linkedin.com/in/harshikagovind)     <p align=""right"">(<a href=""#readme-top"">back to top</a>)</p>  <!-- FUTURE FEATURES -->  ## üî≠ Future Features <a name=""future-features""></a>   - Implement some UX improvements: add the ""More"" button on the home page, include transitions and/or animation  - Implement additional pages, like the tickets page and the schedule page. Make sure that you have a decent mobile design for them   <p align=""right"">(<a href=""#readme-top"">back to top</a>)</p>  <!-- CONTRIBUTING -->  ## ü§ù Contributing <a name=""contributing""></a>  Contributions, issues, and feature requests are welcome! Feel free to check my issues page: https://github.com/harshi0102/ConferencePage/issues  <p align=""right"">(<a href=""#readme-top"">back to top</a>)</p>  <!-- SUPPORT -->  ## ‚≠êÔ∏è Show your support <a name=""support""></a>  >   If you like this project...Give a ‚≠êÔ∏è if you like this project!   <p align=""right"">(<a href=""#readme-top"">back to top</a>)</p>  <!-- ACKNOWLEDGEMENTS -->  ## üôè Acknowledgments <a name=""acknowledgements""></a>    [Cindy Shin](https://www.behance.net/gallery/29845175/CC-Global-Summit-2015) for the Amazing Design Template.  [Fontawsome](https://fontawesome.com/) Special thanks to fontawsome for the icons assets.    <p align=""right"">(<a href=""#readme-top"">back to top</a>)</p>  <!-- FAQ (optional) -->  <!-- LICENSE -->  ## üìù License <a name=""license""></a>  This project is [MIT](https://github.com/harshi0102/ConferencePage/blob/main/LICENSE) licensed.   <p align=""right"">(<a href=""#readme-top"">back to top</a>)</p>"
Online Store Page,HTML & CSS,https://github.com/lassiecoder/E-CommerceWebsite,"# E-CommerceWebsite  A mobile responsive sample of E-Commerce Website using HTML, CSS, JavaScript and API's       ### Home page ![1](https://user-images.githubusercontent.com/17312616/65086776-b1beb080-d9d0-11e9-9983-143d61ed8fdc.png)    ### Content Description page ![2](https://user-images.githubusercontent.com/17312616/65086777-b1beb080-d9d0-11e9-9e2b-af3b7210bdf3.png)    ### Ordered List page ![3](https://user-images.githubusercontent.com/17312616/65086778-b2574700-d9d0-11e9-9377-8e4886f582a8.png)    ### Order confirm page ![4](https://user-images.githubusercontent.com/17312616/65086779-b2efdd80-d9d0-11e9-95d5-4b1a48eafe04.png)"
Restaurant (Landing Page),HTML & CSS,https://github.com/SrajanAgrawal/Restaurant-Landing-Page,# Restaurant-Landing-Page Using HTML And CSS
Quiz Game,HTML & CSS,https://github.com/jamesqquick/Build-A-Quiz-App-With-HTML-CSS-and-JavaScript,"# Build a Quiz App with HTML, CSS, and JavaScript  ![Home Screen](./images/cover.png)  Video Playlist: https://www.youtube.com/playlist?list=PLB6wlEeCDJ5Yyh6P2N6Q_9JijB6v4UejF  Build a Quiz App with HTML, CSS, and JavaScript to improve your Core Web Development  > If you have questions, please ask them in the [Learn Build Teach Discord.](https://learnbuildteach.com/)  Want to improve your **core Web Develoment skills**? Want to improve your knowledge of **HTML, CSS, and JavaScript**? In this course, you're going to learn how to build a Quiz application **without the assistance of libraries or frameworks**. Here are some of the topic we will cover!  -   Save high scores in Local Storage -   Create a progress bar -   Create a spinning loader icon -   Dynamically generate HTML in JavaScript -   Fetch trivia questions from Open Trivia DB API  -   JavaScript - Array Functions (splice, map, sort), Local Storage, Fetch API -   ES6 JavaScript Features - Arrow Functions, the Spread Operator, Const and Let, Template Literals -   CSS - Flexbox, Animtations, and REM units  ## Course Intro and Resources  Welcome to ""Build a Quiz App with HTML, CSS, and JS"". I'm so excited you decided to take the initiative to improve your core Web Development skills!  In this course, we are going to use fundamental HTML, CSS, and JavaScript skills to build a quiz application. This application will be able to load questions from a 3rd party API, track and display high scores, and so much more! You'll learn how to use Local Storage, create animations, use modern ES6 JavaScript features, and more.  Resources  -   [Course Source Code](https://github.com/jamesqquick/Design-And-Build-A-Quiz-App) -   [Learn Build Teach Discord](https://learnbuildteach.com/)  ## 1. Create and Style the Home Page  In this video, we are going to create the home page along with a good chunk of the necessary CSS. The home page will consist of a few links for the Game and High Scores pages. We will also create helper CSS classes for Flexbox, buttons, and hiding elements.  I encourage you all to take a look at Emmet snippets for generating HTML and CSS.  Resources  -   [Emmet in Visual Studio Code](https://www.youtube.com/watch?v=5guZjNDcVnA) -   [Understanding REM Units](https://www.sitepoint.com/understanding-and-using-rem-units-in-css/) -   [A Complete Guide to Flexbox](https://css-tricks.com/snippets/css/a-guide-to-flexbox/)  ## 2. Create and Style the Game Page  In this video, we will create the Game Page and display static question and answer information. Eventually, we will load questions from an API, but for now, we will hard code one question so to establish styling.  ## 3. Display Hard Coded Question and Answers  In this video, we will load questions from a hard coded array and iterate through available questions as the user answers them. We will use custom data attributes, the ES6 spread operator, and JavaScript arrow functions.  Resources  -   [Creating Code Snippets in Visual Studio Code](https://www.youtube.com/watch?v=K3gLlZm-m_8) -   [Using Data Attributes](https://developer.mozilla.org/en-US/docs/Learn/HTML/Howto/Use_data_attributes) -   [Document Query Selector](https://developer.mozilla.org/en-US/docs/Web/API/Document_object_model/Locating_DOM_elements_using_selectors) -   [Document Get by ID](https://developer.mozilla.org/en-US/docs/Web/API/Document/getElementById) -   [Spread Operator](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/Spread_syntax) -   [Arrow Functions](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Functions/Arrow_functions)  ## 4. Display Feedback for Correct/Incorrect Answers  In this video, we check the user's answer for correctness and display feedback to the user before loading the next question.  Resources  -   [Bootstrap 4 Colors](https://www.w3schools.com/bootstrap4/bootstrap_colors.asp) -   [Triple vs Double Equals](https://codeburst.io/javascript-double-equals-vs-triple-equals-61d4ce5a121a)  ## 5. Create Head's Up Display (HUD)  In this video, we will create a Heads Up Display (HUD) for our quiz app. This will display the user's score and current question number.  Resources  -   [ES6 Template Literals](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Template_literals)  ## 6. Create a Progress Bar  In this video, we will take our HUD one step further by creating a visual progress bar to track the user's progress through the questions.  ## 7. Create and Style the End Page  In this video, we will create our End page where we will display the user's achieved score. This screen will provide a form for saving the score and links for playing again or going home.  Resources  -   [Local Storage](https://www.w3schools.com/jsref/prop_win_localstorage.asp)  ## 8. Save High Scores in Local Storage  In this video, we will save and maintain a high scores array in Local Storage. To do this, we will need to JSON.stringify() and JSON.parse() to convert our high score array to a string and visa versa.  Resources  -   [Local Storage](https://www.w3schools.com/jsref/prop_win_localstorage.asp)  ## 9. Load and Display High Scores from Local Storage  In this video, we will create our High Scores page. We will have to load the high scores from Local Storage, iterate through them, and display them on the screen.  Resources  -   [JSON Parse and Stringify](https://alligator.io/js/json-parse-stringify/) -   [Array Sort](https://www.w3schools.com/js/js_array_sort.asp) -   [Array Map](https://www.w3schools.com/jsref/jsref_map.asp) -   [Array Join](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/join)  ## 10. Fetch API to Load Questions From Local JSON File  In this video, we will move our sample questions from a hard coded array to an external .json file. This will help clean up our Game.js file and set ourselves up to request questions from an API in the next video.  Resources  -   [How to Use the Fetch API](https://scotch.io/tutorials/how-to-use-the-javascript-fetch-api-to-get-data) -   [Promises](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Promise_)  ## 11. Fetch API to Load Questions from Open Trivia API  In this video, we will use Fetch to request a list of questions from the Open Trivia DB API.  Reources  -   [How to Use the Fetch API](https://scotch.io/tutorials/how-to-use-the-javascript-fetch-api-to-get-data) -   [Open Trivia DB](https://opentdb.com/) -   [Array Map](https://www.w3schools.com/jsref/jsref_map.asp) -   [Array For Each](https://www.w3schools.com/jsref/jsref_foreach.asp)  ## 12. Create a Spinning Loader  In this video, we will create a simple spinning loader in CSS that will be displayed until we are finished requesting/loading questions from the API.  Resources  -   [Create a CSS Loader](https://www.w3schools.com/howto/howto_css_loader.asp)  ## 13. Closing  Thank you so much for going through this course. I truly hope that you enjoyed it and that you have improved your core Web Development skills!!"
Podcast Website,HTML & CSS,https://github.com/codewithsadee/micro,"<div align=""center"">      ![GitHub repo size](https://img.shields.io/github/repo-size/codewithsadee/micro)   ![GitHub stars](https://img.shields.io/github/stars/codewithsadee/micro?style=social)   ![GitHub forks](https://img.shields.io/github/forks/codewithsadee/micro?style=social) [![Twitter Follow](https://img.shields.io/twitter/follow/codewithsadee_?style=social)](https://twitter.com/intent/follow?screen_name=codewithsadee_)   [![YouTube Video Views](https://img.shields.io/youtube/views/_9oK9CKuaeI?style=social)](https://youtu.be/_9oK9CKuaeI)    <br />   <br />      <img src=""./readme-images/project-logo.png"" />    <h2 align=""center"">Micro - Podcast website</h2>    Micro is a fully responsive Podcast website, <br />Responsive for all devices, built using HTML, CSS, and JavaScript.    <a href=""https://codewithsadee.github.io/micro/""><strong>‚û• Live Demo</strong></a>  </div>  <br />  ### Demo Screeshots  ![Micro Desktop Demo](./readme-images/desktop.png ""Desktop Demo"") ![Micro Mobile Demo](./readme-images/mobile.png ""Mobile Demo"")  ### Prerequisites  Before you begin, ensure you have met the following requirements:  * [Git](https://git-scm.com/downloads ""Download Git"") must be installed on your operating system.  ### Installing Micro  To install **Micro**, run this command on your git bash:  Linux and macOS:  ```bash sudo git clone https://github.com/codewithsadee/micro.git ```  Windows:  ```bash git clone https://github.com/codewithsadee/micro.git ```  ### Contact  If you want to contact with me you can reach me at [Twitter](https://www.twitter.com/codewithsadee).  ### License  This project is **free to use** and does not contains any license."
Courses Website,HTML & CSS,https://github.com/keerti1924/E-Learning-Website-HTML-CSS,"# E-Learning Website using HTML5, CSS3, Bootstrap5 and JavaScript     ![E-Learning Website](preview.jpg)    The project involves the creation of an e-learning website using HTML, CSS, Bootstrap 5, and JavaScript. The website aims to provide a user-friendly and responsive platform for learners to access educational content.       ## Introduction    Secret Coder is an e-learning website designed to provide a user-friendly and responsive platform for learners to access educational content. The project incorporates HTML, CSS, Bootstrap 5, and JavaScript to create an interactive and visually appealing learning experience.    ## Features    - Responsive design using Bootstrap 5.  - Structured course catalog with detailed descriptions.  - Interactive lessons.  - User can register and login.  - User authentication for personalized learning experiences.  - Instructor application form.  - User can also contact us.    ## Getting Started    ### Prerequisites    Before you begin, ensure you have the following prerequisites:    - Web browser (e.g., Chrome, Firefox, Safari)  - Text editor (e.g., Visual Studio Code, Sublime Text)    ### Installation    1. Clone the repository:     ```bash     git clone https://github.com/keerti-1924/E-Learning-Website-HTML-CSS.git    2. Open the project in your preferred text editor.  3. Launch the `index.html` file in a web browser.    ## Technologies Used    - HTML  - CSS  - Bootstrap 5  - JavaScript    üì± Moreover, I've ensured that the website is fully responsive on all screens, making it accessible and user-friendly across various devices. üì±üí°    ## Contributing     Contributions, issues, and feature requests are welcome! Feel free to check the [issues page](/issues).    ## Show your support     Give a ‚≠êÔ∏è if you like this project!      ## License    This project is **free to use** and does not contains any license."
Food Delivery Tracking Website,HTML & CSS,https://github.com/shivamjain1/Online-food-ordering-app,"# Online-food-ordering-app Online Food Ordering App like Swiggy and Zomato built using Html, CSS and Pure JavaScript. No frameworks or libraries <br/><br/><br/>  Live Demo => https://shivamjain1.github.io/Online-food-ordering-app  <br/><br/><br/>  ![image](https://user-images.githubusercontent.com/28086341/102119032-855d4280-3e66-11eb-8070-59a73b68a918.png)  <br/>  Functionalities Implemented <br/> * Search of a Restaurant with <b>Debouncing</b> * Sorting of Restaurant   * Sort by Rating   * Sort by ETA * Mark Restaurant as Favourite   <b>Pull Requests Are Welcome:</b> This project is built part of my UI Tech round preparation. While I do appreciate if people trying to make some things prettier or adding new features."
Music Store Page,HTML & CSS,https://github.com/Zulhaditya/orbis-store,"# Orbis Store > Music store website based on HTML, CSS, and Javascript. > Live demo [_here_](https://orbis-store.netlify.app/).  ## Table of Contents * [General Info](#general-information) * [Technologies Used](#technologies-used) * [Features](#features) * [Screenshots](#screenshots) * [Setup](#setup) * [Project Status](#project-status) * [Contact](#contact)  ## General Information - This is a music store website that features various musical instruments such as guitars, drums and others. - This website applies a simple and minimalist style so that users are easier to access  ## Technologies Used - HTML5 - CSS3 - Javascript ES6  ## Features - Homepage - Product List - Product Details - Shopping Cart  ## Screenshots ![Example screenshot](./screenshot-orbis.png)  ## Setup To run this project, running it locally using live-server or open the index.html file.  ## Project Status Project is: _in progress._ <!-- / _complete_ / _no longer being worked on_. reason ? -->  ## Contact  Created by [@Zulhaditya](https://zulhaditya.vercel.app) - feel free to contact me!"
Simple Admission form,HTML & CSS,https://github.com/kumarsuraj345678/Student-Registration-Form,"[![Netlify Status](https://api.netlify.com/api/v1/badges/168507ed-6e7d-40a4-bb68-f9d12ad4df4b/deploy-status)](https://app.netlify.com/sites/srf/deploys)  # üéì Student Registration Form  ## üìñ Overview A modern and fully responsive **Student Registration Form** with built-in validation, real-time error handling, and an elegant UI. This project ensures a seamless and intuitive experience for students to register online.  ## ‚ú® Features - üìù **Comprehensive Form**: Includes personal details, contact information, security settings, and additional preferences. - üîç **Real-time Validation**: Ensures accurate data entry with instant feedback. - üîë **Password Strength Indicator**: Visual representation of password security. - üé≠ **Interactive UI**: Smooth animations and clear error messaging. - üì∏ **Profile Photo Upload**: Allows users to upload profile pictures. - üì± **Fully Responsive**: Optimized for all screen sizes. - üìÉ **Netlify Forms**: Utilizes Netlify Forms for seamless form submission and handling.  ## üõ†Ô∏è Technologies Used - **HTML5** - **CSS3** *(Including animations and custom styles)* - **JavaScript (ES6+)** *(Form validation, interactive elements)*  ## üöÄ Getting Started ### Prerequisites - A modern web browser (Chrome, Firefox, Edge, Safari, etc.)  ### Installation 1. Clone this repository:    ```sh    git clone https://github.com/yourusername/student-registration-form.git    ``` 2. Navigate to the project folder:    ```sh    cd student-registration-form    ``` 3. Open `index.html` in your preferred browser.  ## üåê Live Demo Check out the live demo [here](https://srf.netlify.app/).  ## üìö Usage 1. Open the form in a web browser. 2. Fill in the necessary details. 3. Click ""Register"" to complete the process.  ## üì∏ Screenshots ### üìå Home Page ![Macbook-Air-1559x2141](https://github.com/user-attachments/assets/012c63ee-4a73-40a1-ae67-f10f2046bb78)  ## ü§ù Contributing Contributions are welcome! Follow these steps: 1. **Fork** the repository. 2. Create a **feature branch** (`git checkout -b feature-branch`). 3. Commit your changes (`git commit -m 'Add new feature'`). 4. Push to the branch (`git push origin feature-branch`). 5. Open a **Pull Request**.  ## üìÑ License This project is licensed under the MIT License.  ## üôè Acknowledgements - [FontAwesome](https://fontawesome.com/) for icons. - [Google Fonts](https://fonts.google.com/) for beautiful typography. - [Netlify Forms](https://docs.netlify.com/forms/setup/) for easy form handling.  --- üî• **Star** this repository if you found it useful! Happy coding! üöÄ"
CSS Image Gallery,HTML & CSS,https://github.com/Jonesdl-2785/image-gallery-html-css,"# image-gallery-html-css HTML and  CSS (some JS) Image Gallery  Simple image gallery created with HTML, CSS and some JavaScript.   ## Features 1.  Responsive using Media Queries 2.  Hover using transitions and opacity 3.  Top navigation links to each section 4.  Scroll Top using simple JavaScript   ## View Project Project can be viewed [here](https://jonesdl-2785.github.io/image-gallery-html-css/)."
Score Landing Page,HTML & CSS,https://github.com/mnassarhub/responsive-landing-page,"# Landing Page Project  ## Table of Contents  -   [Instructions](#instructions) -   [Development](#Development) -   [License](#License) -   [Footer](#Footer)  ## Instructions  The starter project has some HTML and CSS styling to display a static version of the Landing Page project. You'll need to convert this project from a static project to an interactive one. This will require modifying the HTML and CSS files, but primarily the JavaScript file.  To get started, open `js/app.js` and start building out the app's functionality  For specific, detailed instructions, look at the project instructions in the Udacity Classroom.  ## Development  Let‚Äôs start to explain my code:-  ## Interface and Architecture  1- Architecture:- All files attached as request in rubrics css  -   styles.css       index.html     js -   app.js     README.md     2- Usability:- checked the accessibility from all platforms (All features are usable across modern desktop, tablet, and phone browsers.).     3- Styling:- Styling has been added for active states.     4- HTML Structure:- for inserting fourth section used cloning code in js  ---  ## Landing Page Behavior  1- Navigation:- Navigation is built dynamically as an unordered list. Start with empty ul and dynamically build navigation creating array from sections to use function and create navigation bar elements. Then make a function to check if sections in viewport, then made another function to toggle class "" your- active- class"" 2- Section Active State:- used scroll event and offset top to determine which active section to add class "" active"" to Li related to the active section and scroll to the list item in small screens. 3- Scroll to Anchor:- When clicking an item from the navigation menu, the link should scroll to the appropriate section and used create element to create button "" go to top"" when user click on it, and hide the button and header navigation bar when the page on top. And when user scroll down they are appeared to deal with them, and finally used event click to make the button responsive and go to the top of the page. 4- Used media query in js code to add some features to nav bar and hide it in mobiles screens  ## License  First project: Landing Page This code write by Mohamed Nassar copyrights Udacity Most of my searches in w3school and MDN websites topics but I wrote every code alone.  ## Footer  thanks to check my code and feed me back with your opinion and notes thanks"
Info Page using apis,HTML & CSS,https://github.com/BoddepallyVenkatesh06/Build--Website-using-an-API-with-HTML-JavaScript-and-JSON,"## Build--Website-using-an-API-with-HTML-JavaScript-and-JSON  ## Get Started  ### Demo <a href=""https://drive.google.com/file/d/1Z6bihDrFMUPy8j5nIqzwnqP4pl_OgPXc/view"" style=""text-decoration:none;"">üé•</a>  #### Website for this repository : <https://blogzen-gdsc.netlify.app/>  <br>  ### Steps for Contributing  <strong>Frontend</strong>  - Fork and clone the Repo by typing the following commands in the terminal  ``` $ git clone https://github.com/<your-github-username>/blogzen.git $ cd blogzen ```  <!-- - Open this folder in your favourite IDE. <br> - Run `npm install`.<br> - Run `git pull` command to sync with remote repo.<br>   <br> --> <!-- - Run `npm start` for starting server. -->  - Now to add your resource to website, add an object with keys same as listed in existing objects in the file.<br> - Save and commit your code.<br> - Push to your fork of the repository , navigate to original repository and make a pull request.<br>  <!-- <strong>Backend</strong>  > **Note**: You must have Nodejs installed  - Fork and clone the Repo by typing the following commands in the terminal  ``` $ git clone https://github.com/DSC-JSS-NOIDA/QuickLearn.git $ cd QuickLearn ``` -->  [![fork.png](https://i.postimg.cc/xTPqkF38/fork.png)](https://postimg.cc/BXXJkpyf)  <hr>  [![clone.png](https://i.postimg.cc/5t2F51kr/clone.png)](https://postimg.cc/K1CzxXb7)<hr>  [![clone-git-Bash.png](https://i.postimg.cc/kgcbtDw8/clone-git-Bash.png)](https://postimg.cc/CRR13h3L)  <hr>  - Make changes to the code(for ex- add an update route) - Stage your changes using:  ``` $ git add . ```  - Commit your changes using:  ``` $ git commit -m ""add any comment"" ```  - Push the changes to the forked repository using:  ``` $ git push ```  - Navigate to the original repository and make a pull request ``` Showing Your Remotes  $ git remote  $ git remote -v ```  Adding Remote Repositories ``` git remote add upstream https://github.com/DSC-JSS-NOIDA/blogzen.git ``` Pulling from Your Remote ``` $ git pull upstream main ```  ## Design - The Prospective design of this project is here [Blogzen - Flutter](https://www.figma.com/file/lTsgJbWw8MRxWLa3c0PwWh/Blog-Project?type=design&node-id=0%3A1&t=9fkc7cb59hN4TlHW-1)  	 ## Resources  - **Git and Github**: [Git and Github for Beginners](https://www.youtube.com/watch?v=RGOj5yH7evk) - **Frontend**: [Frontend development for Beginners](https://www.youtube.com/playlist?list=PL9ooVrP1hQOH2k1SANK5rvq_EAgUKTPoK) <!-- - **Backend**: [Node.js for Beginners](https://www.youtube.com/playlist?list=PL4cUxeGkcC9gcy9lrvMJ75z9maRw4byYp) -->  #### Happy?? Star ‚≠ê this Repo. ü§©  [![ForTheBadge uses-git](http://ForTheBadge.com/images/badges/uses-git.svg)](https://github.com/DSC-JSS-NOIDA/blogzen) [![ForTheBadge uses-html](http://ForTheBadge.com/images/badges/uses-html.svg)](https://github.com/DSC-JSS-NOIDA/blogzen) [![ForTheBadge uses-css](http://ForTheBadge.com/images/badges/uses-css.svg)](https://github.com/DSC-JSS-NOIDA/blogzen) [![ForTheBadge uses-js](http://ForTheBadge.com/images/badges/uses-js.svg)](https://github.com/DSC-JSS-NOIDA/blogzen)  > Made By GDSC JSS NOIDA with ‚ù§Ô∏è  <br><br>  [![ForTheBadge built-with-love](http://ForTheBadge.com/images/badges/built-with-love.svg)](https://github.com/DSC-JSS-NOIDA/blogzen) [![ForTheBadge built-by-developers](http://ForTheBadge.com/images/badges/built-by-developers.svg)](https://github.com/DSC-JSS-NOIDA/blogzen)  > **_Need help?_** > **_Feel free to contact us @ [dscjssnoida@gmail.com](mailto:idscjssnoida@gmail.com?Subject=DSCHackFest2023)_**  ## License  Licensed under the [MIT license](LICENSE)."
Virtual Classroom,HTML & CSS,https://github.com/nirmalnishant645/Virtual-Classroom,# Virtual-Classroom Teaching tool to assist each and every student to learn in an interactive manner in today‚Äôs situation of a pandemic
ToDo App,JavaScript,https://github.com/CodeStudio-Content/To-Do-List-JavaScript,"<h1 align=""center""> To-Do-List-JavaScript </h1>   ![chrome-capture-2023-1-26 (1)](https://user-images.githubusercontent.com/77020164/221394835-eb92ac02-53e3-42bf-96ac-b5114eb543a6.gif)  ## About The Project  The To-Do List project is a simple web application that allows users to create and manage a list of tasks they need to complete. With a clean and intuitive interface, users can quickly add, edit, and delete tasks, as well as mark tasks as complete. This project is built with JavaScript and is a great example of a basic web application using DOM manipulation and event listeners.   ## Blog  Check out our project blog post for more information on the development process and our thoughts on the To Do List project:  * [To Do List Using JS](https://www.codingninjas.com/codestudio/library/building-a-todo-list-using-javascript?utm_source=github&utm_medium=organic&utm_campaign=blog-building-a-todo-list-using-javascript)    ## Getting Started  To get a local copy up and running follow these simple example steps.  ### Installation  To run this application locally, you need to clone this repository to your local machine. You can do this by running the following command in your terminal: 1. Clone the repo `https://github.com/CodeStudio-Content/To-Do-List-JavaScript.git`  2. `cd To-Do-List-JavaScript`   ## Usage  - Open the index.html file in your browser  ### 1. Adding a task To add a task to the to-do list, simply enter the task description in the input field at the top of the page and press the ""Add"" button. The task will be added to the list.  ### 2. Updating a task To update a task, click on the task you want to update. This will open a modal window where you can edit the task description. Once you have made your changes, click the ""Save"" button to update the task.  ### 3. Completing a task To mark a task as complete, simply click the checkbox next to the task description. The task will be crossed out to indicate that it has been completed.  ### 4. Deleting a task To delete a task, click the ""Delete"" button next to the task you want to delete. This will remove the task from the to-do list.   ## Requirements  - HTML - CSS - Javascript   ## Files  * `index.html` :      This is the main file that contains the html code for the To-Do List. * `style.css` :      This file contains the styling for the To-Do List. * `script.js` :      This file contains the logic for the To-Do List, including adding, editing, and deleting tasks, and filtering the task list. "
Tic Tac Toe Game,JavaScript,https://github.com/vasanthk/tic-tac-toe-js,"# Tic Tac Toe  **A basic Tic Tac Toe game built using HTML, JavaScript, and CSS. No dependencies required.**  [![View The Demo](https://www.mtb.com/personal/onlineservices/PublishingImages/alt-banking-button-view-demo-cs5452.jpg)](http://codepen.io/vasanthkay/pen/KVzYzG?editors=001)  ## How to Get Started 1. To make a move, the player will use a single mouse click to mark a space. In this version, there is no provision to undo a move. Once a move is made, the game proceeds to the next player's turn. 2. At each move, the game will indicate whose turn (Player A or Player B) it is. When the game ends, it displays one of the following outcomes:    * Winner: Player A    * Winner: Player B    * Draw"
JS Quiz App,JavaScript,https://github.com/yashcrest/JavaScript-Quiz-App,"# JavaScript Quiz App using OpenTrivia API  This is a simple JavaScript Quiz App that lets users take a quiz on different categories by fetching questions from the [OpenTrivia API](https://opentdb.com/api_config.php).  # Live Demo [Live Demo](https://quizapp.yashshrestha.net)  ## Features - **Category Selection:** Users can choose a category for their quiz. - **Question Navigation:** Navigate through questions using next and previous buttons. - **Correct Answer Indication:** If the user answers incorrectly, the correct answer is shown temporarily. - **Prevent Skipping:** Users cannot proceed to the next question without selecting an answer. - **End of Quiz Summary:** Displays the user's total score at the end of the quiz.  ## Different Category in quiz ![Quiz](https://github.com/yashcrest/quiz.github.io/assets/79971012/5a3fe0b8-79bb-48c1-9e9a-05b532dc92c5)   ## Working Demo of the app  ![Quiz](https://github.com/yashcrest/quiz.github.io/assets/79971012/fb43556e-3ce5-4993-9f08-405f38a9ab48)  ## How to Use  1. **Clone the Repository:**  ```git clone https://github.com/yashcrest/quiz.github.io.git```  2. **Open `index.html` File:** Navigate to the project folder and open `index.html` in your browser.  3. **Choose a Category:** Select the desired category from the dropdown.  4. **Begin the Quiz:** Click on ""Begin"" to start the quiz.  5. **Answer the Questions:** Select the answers using the radio buttons and navigate using ""Next"" and ""Previous.""  6. **View Your Score:** At the end of the quiz, your total score will be displayed.  ## Technologies Used  - JavaScript (ES6) - HTML5 - CSS3 - Bootstrap 5  ## Contributing  Feel free to contribute to this project by forking the repository and submitting a pull request, or opening an issue with suggestions and bug reports.  ## License  This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details."
Expense Manager,JavaScript,https://github.com/Jdecho1118/Expense-Tracker,"# Expense Tracker Web App  This Expense Tracker Web App is a simple tool built with HTML, CSS, and JavaScript to help users manage their finances effectively. Keep track of your income and expenses, add new transactions, and visualize your financial health.  ## Features - **Transaction Management:** Add income or expenses with ease by entering the transaction details. - **Dynamic Updates:** Real-time updates of balance, income, and expenses displayed on the interface. - **Transaction Removal:** Remove unwanted transactions seamlessly with a delete button.  ## Usage 1. Open the web app in your browser. 2. Enter the transaction details, including the transaction text and amount. 3. Press the ""Add Transaction"" button to update the list and balances. 4. View your current balance, income, and expenses. 5. Delete transactions by clicking the ""x"" button next to each entry.  ## Technologies Used - **HTML:** Structure the web page. - **CSS:** Style the user interface for a visually appealing experience. - **JavaScript:** Implement the dynamic functionality of the Expense Tracker.  ## Code Overview - The JavaScript code handles transaction addition, deletion, and updates to the local storage. - Transactions are stored locally to maintain data persistence between sessions.  ## How to Run Simply open the `index.html` file in a web browser to start using the Expense Tracker.  Feel free to enhance and customize the code to suit your preferences or integrate additional features!"
COVID CheckUp Test,JavaScript,https://github.com/pranjay-poddar/Covinex,"# Covinex Hosted Link To Website- https://covinex.netlify.app  A Covid-19 web-app, which enables users to access some outstanding features like self-analysis for covid testing, Covid case tracker at global and country level, locating testing centers, and most importantly I have collated Pan-India resources like available hospital beds, contact information of oxygen cylinder distributors, plasma doners and medical assistance which are regularly updated. <br>  ![forthebadge](https://forthebadge.com/images/badges/made-with-javascript.svg) ![forthebadge](https://forthebadge.com/images/badges/built-with-love.svg) ![forthebadge](https://forthebadge.com/images/badges/uses-brains.svg) ![forthebadge](https://forthebadge.com/images/badges/check-it-out.svg) <p align=""center""> <img src=""./src/assets/images/logo.png"" width=""150"" title=""hover text""> </p > <br>  <img src=""./src/assets/images/web-view1.png"" title=""web-view"">  <br> <h4 align=""center"">Completely Responsive For Std Sized Screens:</h4>  <img src=""./src/assets/images/mobile-view1.png"" title=""mobile-view"">  <br>  ## List Of Features * The main feature of this application is that it has simple and user understandable questions with the help of their answers user can deduce whether he/she needed to be tested for covid-19 or not. *  If the user has to go for testing, the user can pre-register for the covid-19 testing in there nearby location. *  A global Covid tracker in which live tracking of active, deceased and recovered count of patients can be viewed. *  A Country Wise Covid tracker in which live tracking of active, deceased and recovered count of patients for a specific country can be viewed. *  Resources from Pan Indian which are updated regularly. * An interactive Chat Bot to solve User Queries.  ## Installation And Project Setup This project was generated with [Angular CLI](https://github.com/angular/angular-cli) version 11.2.1.  ## Development server Run `ng serve` for a dev server. Navigate to `http://localhost:4200/`. The app will automatically reload if you change any of the source files.  ## Code scaffolding   Run `ng generate component component-name` to generate a new component. You can also use `ng generate directive|pipe|service|class|guard|interface|enum|module`.      ## Build Run `ng build` to build the project. The build artifacts will be stored in the `dist/` directory. Use the `--prod` flag for a production build.  ## Running unit tests Run `ng test` to execute the unit tests via [Karma](https://karma-runner.github.io).  ## Running end-to-end tests Run `ng e2e` to execute the end-to-end tests via [Protractor](http://www.protractortest.org/).  ## Further help To get more help on the Angular CLI use `ng help` or go check out the [Angular CLI Overview and Command Reference](https://angular.io/cli) page.  ## Release History -   0.1     -   Initial Version -   0.2     -   New look! -   0.3     -   Update In Country Level Tracker -   0.4     -   Work in progress  ## Author Pranjay Poddar - pranjaypoddar@outlook.com Distributed under the MIT License  Copyright (c) 2021 Pranjay Poddar  * Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  * The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.  THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.  ## Contributing 1.  Fork it ([https://github.com/pranjay-poddar/Covinex.git](https://github.com/pranjay-poddar/Covinex.git)) 2.  Create your feature branch (`git checkout -b feature/xyz`) 3.  Commit your changes (`git commit -am 'Add some xyz'`) 4.  Push to the branch (`git push origin feature/xyz`) 5.  Create a new Pull Request   <h4 align=""center""> ¬© Pranjay Poddar 2021 </h4>"
Animation in Web Design,JavaScript,https://github.com/web-animations/web-animations-js," What is Web Animations? -----------------------  A new JavaScript API for driving animated content on the web. By unifying the animation features of SVG and CSS, Web Animations unlocks features previously only usable declaratively, and exposes powerful, high-performance animation capabilities to developers.  What is in this repository? ---------------------------  A JavaScript implementation of the Web Animations API that provides Web Animation features in browsers that do not support it natively. The polyfill falls back to the native implementation when one is available.  Quick start -----------  Here's a simple example of an animation that fades and scales a `<div>`.   [Try it as a live demo.](http://jsbin.com/yageyezabo/edit?html,js,output)  ```html <!-- Include the polyfill --> <script src=""web-animations.min.js""></script>  <!-- Set up a target to animate --> <div class=""pulse"" style=""width: 150px;"">Hello world!</div>  <!-- Animate! --> <script>     var elem = document.querySelector('.pulse');     var animation = elem.animate({         opacity: [0.5, 1],         transform: ['scale(0.5)', 'scale(1)'],     }, {         direction: 'alternate',         duration: 500,         iterations: Infinity,     }); </script> ```  Documentation -------------  * [Codelab tutorial](https://github.com/web-animations/web-animations-codelabs) * [Examples of usage](/docs/examples.md) * [Live demos](https://web-animations.github.io/web-animations-demos) * [MDN reference](https://developer.mozilla.org/en-US/docs/Web/API/Element/animate) * [W3C specification](https://drafts.csswg.org/web-animations/)  We love feedback! -----------------  * For feedback on the API and the specification:     * File an issue on GitHub: <https://github.com/w3c/web-animations/issues/new>     * Alternatively, send an email to <public-fx@w3.org> with subject line ""[web-animations] ... message topic ..."" ([archives](http://lists.w3.org/Archives/Public/public-fx/)).  * For issues with the polyfill, report them on GitHub: <https://github.com/web-animations/web-animations-js/issues/new>.  Keep up-to-date ---------------  Breaking polyfill changes will be announced on this low-volume mailing list: [web-animations-changes@googlegroups.com](https://groups.google.com/forum/#!forum/web-animations-changes).  More info ---------  * [Technical details about the polyfill](/docs/support.md)     * [Browser support](/docs/support.md#browser-support)     * [Fallback to native](/docs/support.md#native-fallback)     * [Feature list](/docs/support.md#features)     * [Feature deprecation and removal processes](/docs/support.md#process-for-breaking-changes) * To test experimental API features, try one of the   [experimental targets](/docs/experimental.md)"
Flappy Bird Game,JavaScript,https://github.com/aaarafat/JS-Flappy-Bird,# JS-Flappy-Bird Remake of the Original FlappyBird Using **JS** &amp; **HTML Canvas**  # [Play it.](https://aaarafat.github.io/JS-Flappy-Bird/index.html) # Demo ![Demo](https://user-images.githubusercontent.com/44725090/67148880-e7dba280-f2a4-11e9-8dbf-d154842ee0cf.gif)
Snake Game,JavaScript,https://github.com/mohdriyaan/snake-game,"<h1>Snake-Game Using HTML,CSS,JS</h1>  This is a simple implementation of the classic Snake game using HTML, CSS, and JavaScript. The game is played on a canvas element and controlled using the arrow keys on the keyboard.  <h2>Getting Started</h2>  To play the game, simply open the link <a href=""https://mohdriyaan.github.io/snake-game/"">Snake-Game</a> in your web browser. The game will start automatically, and you can control the snake using the arrow keys on your keyboard.  <h2>How To Play</h2>  The objective of the game is to eat as many apples as possible without colliding with the walls or the snake's own body. As the snake eats more apples, it will grow in length and move faster.  You can control the direction of the snake using the arrow keys on your keyboard: <ul>   <li>Up: Press the up arrow key to move the snake upwards.</li>   <li>Down: Press the down arrow key to move the snake downwards.</li>   <li>Left: Press the left arrow key to move the snake to the left.</li>   <li>Right: Press the right arrow key to move the snake to the right.</li> </ul>  <h2>Built With</h2> <ul>   <li>HTML</li>   <li>CSS</li>   <li>JavaScript</li> </ul>  <h2>Acknowledgements</h2>  This game was inspired by the classic Snake game that was popular in the late 1990s. Thanks to the developers of that game for providing the inspiration for this project."
Maze Game,JavaScript,https://github.com/husseinaltaaf/Maze-Game-JavaScript,"# Labyrinthus - A Maze Game [PLAY NOW](https://husseinaltaaf.github.io/labyrinthus_javascript_game/)  This is simply a Maze game where you have to start from one point and go to the finish point as fast as you can!  ## LEVEL & DIFFICULTIES  There are 5 levels in total & each new level increases the difficulty!  ## INSTRUCTION  You have to use your **W,S,A,D** keys (_W - UP_, _S - DOWN_, _A - LEFT_, _D - RIGHT_) to navigate throught the maze!  ### Made By: `Hamod Altaaf`  ## About Game and Referrences  I have taken all needed code from the website referrenced below, customised it and added as many feature as i can!  ## Referrences  - [Existing original game](https://html5.litten.com/make-a-maze-game-on-an-html5-canvas/) - [Maze GIF Creator](http://hereandabove.com/maze/mazeorig.form.html) - [Timer](https://stackoverflow.com/a/5517836)"
JavaScript Map,JavaScript,https://github.com/heremaps/maps-api-for-javascript-examples,"# Maps API for JavaScript  This repository holds a series of JavaScript based examples using the **HERE Maps API for JavaScript**. More information about the API can be found on [https://here.com/docs](https://www.here.com/docs/category/here-sdk-for-js) website. To run the examples, clone this repo to a folder on your Desktop. Replace the credentials in the **test-credentials.js** file with your own credentials.  > **Note:** In order to get the sample code to work, you **must** replace all instances of `window.apikey` within the code and use your own **HERE** credentials.  > You can obtain a set of credentials from the [HERE Base Plan Pricing](https://here.com/get-started/pricing) page on here.com.  See the [LICENSE](LICENSE) file in the root of this project for license details.  ## Maps API for JavaScript  All of the following examples use **version 3.1** of the API  * [Adding an Overlay to the Map](https://heremaps.github.io/maps-api-for-javascript-examples/custom-tile-overlay/demo.html) - Display custom map tiles as an overlay * [Animated Markers](https://heremaps.github.io/maps-api-for-javascript-examples/markers-update-position-with-animation/demo.html) - Update marker position with animation * [Calculating a Location from a Mouse Click](https://heremaps.github.io/maps-api-for-javascript-examples/position-on-mouse-click/demo.html) - Obtain the latitude and longitude of a location within the map * [Changing from the Metric System](https://heremaps.github.io/maps-api-for-javascript-examples/map-scale-bar-changing-from-the-metric-system/demo.html) - Display a map including a scale bar in miles or yards * [Circle on the Map](https://heremaps.github.io/maps-api-for-javascript-examples/circle-on-the-map/demo.html) - Display a map highlighting a circular region * [DOM Marker](https://heremaps.github.io/maps-api-for-javascript-examples/map-with-dom-marker/demo.html) - Display a marker that is capable of receiving DOM events * [DOM Marker rotation](https://heremaps.github.io/maps-api-for-javascript-examples/dom-marker-rotation/demo.html) - Rotate DOM Marker's content using CSS * [Display KML Data](https://heremaps.github.io/maps-api-for-javascript-examples/display-kml-on-map/demo.html) - Parse a KML file and display the data on a map * [Display GeoJSON Data](https://heremaps.github.io/maps-api-for-javascript-examples/display-geojson-on-map/demo.html) - Parse a GeoJSON file and display the data on a map * [Display an Indoor Map](https://heremaps.github.io/maps-api-for-javascript-examples/indoor-map/demo.html) - Use the HERE Indoor Maps API to load and visualize an indoor map * [UI interactions on Indoor Map](https://heremaps.github.io/maps-api-for-javascript-examples/indoor-map-ui-interaction/demo.html) - HERE Indoor Maps API with UI interactions on the map * [Restrict map movement with Indoor Map](https://heremaps.github.io/maps-api-for-javascript-examples/indoor-map-movement/demo.html) - Restrict the map movement within the indoor map bounds * [Draggable Marker](https://heremaps.github.io/maps-api-for-javascript-examples/draggable-marker/demo.html) - Display a moveable marker on a map * [Draggable geo shapes](https://heremaps.github.io/maps-api-for-javascript-examples/draggable-shapes/demo.html) - Display moveable geometric shapes on a map * [Extruded geo shapes](https://heremaps.github.io/maps-api-for-javascript-examples/extruded-objects/demo.html) - 3D extrusion of the geometric shapes * [Finding the Nearest Marker](https://heremaps.github.io/maps-api-for-javascript-examples/finding-the-nearest-marker/demo.html) - Find a marker nearest to the click location * [Image overlay](https://heremaps.github.io/maps-api-for-javascript-examples/image-overlay/demo.html) - Display an animated weather radar * [Interactive Map Layer](https://heremaps.github.io/maps-api-for-javascript-examples/transit-iml/demo.html) - Visualize Data from Interactive Map Layer on Map * [Interleave vector and object layers](https://heremaps.github.io/maps-api-for-javascript-examples/interleave-layers/demo.html) - Display an object under the buildings * [Map Objects Event Delegation](https://heremaps.github.io/maps-api-for-javascript-examples/map-objects-event-delegation/demo.html) - Use event delegation on map objects * [Map Objects Events](https://heremaps.github.io/maps-api-for-javascript-examples/map-object-events-displayed/demo.html) - Handle events on various map objects * [Map at a specified location](https://heremaps.github.io/maps-api-for-javascript-examples/map-at-specified-location/demo.html) - Display a map at a specified location and zoom level * [Map using View Bounds](https://heremaps.github.io/maps-api-for-javascript-examples/map-using-view-bounds/demo.html) - Display a map of a given area * [Map with custom MapSettings UI Control](https://heremaps.github.io/maps-api-for-javascript-examples/map-with-custom-map-settings-ui-control/demo.html) - Switch between different layers using a custom MapSettings UI control. * [Map with Driving Route from A to B](https://heremaps.github.io/maps-api-for-javascript-examples/map-with-route-from-a-to-b/demo.html) - Request a driving route from A to B and display it on the map. * [Map with Pedestrian Route from A to B](https://heremaps.github.io/maps-api-for-javascript-examples/map-with-pedestrian-route-from-a-to-b/demo.html) - Request a walking route from A to B and display it on the map. * [Map with Route from A to B using Public Transport](https://heremaps.github.io/maps-api-for-javascript-examples/map-with-route-from-a-to-b-using-public-transport/demo.html) - Request a route from A to B using public transport and display it on the map. * [Map with isoline route](https://heremaps.github.io/maps-api-for-javascript-examples/map-with-isoline-route/demo.html) - Request a range for the EV vehicle. * [Marker Clustering](https://heremaps.github.io/maps-api-for-javascript-examples/marker-clustering/demo.html) - Cluster multiple markers together to better visualize the data * [Marker Clustering with Custom Theme](https://heremaps.github.io/maps-api-for-javascript-examples/custom-cluster-theme/demo.html) - Cluster multiple markers and customize the theme * [Marker on the Map](https://heremaps.github.io/maps-api-for-javascript-examples/markers-on-the-map/demo.html) - Display a map highlighting points of interest * [Markers with Altitude](https://heremaps.github.io/maps-api-for-javascript-examples/markers-with-altitude/demo.html) - Display markers at different altitudes * [Multi-language support](https://heremaps.github.io/maps-api-for-javascript-examples/map-multi-language-support/demo.html) - Display a map with labels in a foreign language * [Opening an Infobubble on a Mouse Click](https://heremaps.github.io/maps-api-for-javascript-examples/open-infobubble/demo.html) - Open an infobubble when a marker is clicked * [Ordering Overlapping Markers](https://heremaps.github.io/maps-api-for-javascript-examples/ordering-overlapping-markers/demo.html) - Arrange the order in which a series of map objects are displayed * [Panning the Map](https://heremaps.github.io/maps-api-for-javascript-examples/panning-the-map/demo.html) - Programmatically pan the map so that it is continually in motion * [Polygon on the Map](https://heremaps.github.io/maps-api-for-javascript-examples/polygon-on-the-map/demo.html) - Display a map highlighting a region or area * [Polyline on the Map](https://heremaps.github.io/maps-api-for-javascript-examples/polyline-on-the-map/demo.html) - Display a map with a line showing a known route * [Rectangle on the map](https://heremaps.github.io/maps-api-for-javascript-examples/rectangle-on-the-map/demo.html) - Display a map highlighting a retangular region or area * [Resizable geo Polygon](https://heremaps.github.io/maps-api-for-javascript-examples/resizable-polygon/demo.html) - Display resizable polygon on a map * [Resizable geo Polyline](https://heremaps.github.io/maps-api-for-javascript-examples/resizable-polyline/demo.html) - Display resizable polyline on a map * [Resizable geo Circle](https://heremaps.github.io/maps-api-for-javascript-examples/resizable-circle/demo.html) - Display resizable circle on a map * [Resizable geo Rect](https://heremaps.github.io/maps-api-for-javascript-examples/resizable-rect/demo.html) - Display resizable rectangle on a map * [Restrict Map Movement](https://heremaps.github.io/maps-api-for-javascript-examples/restrict-map/demo.html) - Restrict a moveable map to a given rectangular area * [SVG Graphic Markers](https://heremaps.github.io/maps-api-for-javascript-examples/map-with-svg-graphic-markers/demo.html) - Display a map with custom SVG markers * [Search for a Landmark](https://heremaps.github.io/maps-api-for-javascript-examples/search-for-landmark/demo.html) - Request the location of a landmark and display it on the map. * [Search for a Location based on an Address](https://heremaps.github.io/maps-api-for-javascript-examples/geocode-a-location-from-address/demo.html) - Request a location using a free-form text input and display it on the map. * [Search for a Location given a Structured Address](https://heremaps.github.io/maps-api-for-javascript-examples/geocode-a-location-from-structured-address/demo.html) - Request a location from a structured address and display it on the map. * [Search for the Address of a Known Location](https://heremaps.github.io/maps-api-for-javascript-examples/reverse-geocode-an-address-from-location/demo.html) - Request address details for a given location and display it on the map. * [Set a map style at the load time](https://heremaps.github.io/maps-api-for-javascript-examples/change-style-at-load/demo.html) - Set a style of the whole map during the map instantiation * [Set a map style exported from the HERE Style Editor](https://heremaps.github.io/maps-api-for-javascript-examples/change-harp-style-at-load/demo.html) - Set a style exported from the [HERE Style Editor](https://platform.here.com/style-editor) during the map instantiation * [Synchronising Two Maps](https://heremaps.github.io/maps-api-for-javascript-examples/synchronising-two-maps/demo.html) - Synchronise a static map with an interactive map * [Take a Snapshot of the Map](https://heremaps.github.io/maps-api-for-javascript-examples/capture-map-area/demo.html) - Capture an area on the map * [Truck routing road restrictions](https://heremaps.github.io/maps-api-for-javascript-examples/truck-routing-road-restrictions/demo.html) Show a various truck routes with the truck related road restrictions highlighted on the map * [Zoom into Bounds](https://heremaps.github.io/maps-api-for-javascript-examples/custom-zooming-into-bounds/demo.html) - Zoom into bounds limiting maximum level * [Zooming to a Set of Markers](https://heremaps.github.io/maps-api-for-javascript-examples/zoom-to-set-of-markers/demo.html) - Alter the viewport  to ensure a group of objects are visible"
Random Quotes Generator,JavaScript,https://github.com/RichardJamesWard/JS-Random-Quote-Generator,"# random-quote-generator ### Treehouse Techdegree Project #1 - Random Quote Generator  In this project, you'll create an app that displays random famous quotes each time a button is clicked. You can display a quote from a famous athlete, politician, or historical figure:  ""The only thing we have to fear is fear itself."" ‚Äî Franklin Delano Roosevelt.  You'll need to use your knowledge of basic JavaScript syntax, including variables, loops and object literals, to complete this project.  This project is a fun and effective way for you to practice fundamental JavaScript skills. It also gives you a simple interactive portfolio piece to show off your understanding of JavaScript.    # Project Requirements  ### Create Array of Objects - [x] Array of objects is named quotes - [x] Array contains at least 5 quote objects ### Quote Object - [x] Objects in array include quote and source properties. If known, citation and date properties are present ### getRandomQuote function - [x] Function is named getRandomQuoteand returns a random object from the quotes array ### printQuote function - [x] Function named printQuote calls the randomQuote function - [x] Function prints the quote to the page using the template supplied in the project instructions ### Code comments - [x] Comments are included in the code.   # View Project [Live Demo]( https://richardjamesward.github.io/JS-Random-Quote-Generator/) of this project for peer review.   # Appraiser Comments Awesome work, Richard! Your project looks great and after seeing so many of these, I appreciate you changing the styling up.  Nice job ensuring that your function returns the entire quote object and then adding in those extra features! I added a hint in the individual rubric if you want to try creating a non-repeating randomizer.  You're ready to move on to the next project and get further into JavaScript. Have fun!  Thanks for being a Techdegree student."
Light Switch Effect,JavaScript,https://github.com/romeojeremiah/javascript-light-switch-project,# javascript-light-switch-project
Sudoku Solver,JavaScript,https://github.com/ayushichoudhary-19/SudokuSolver,"# üß© SudokuSolver  Welcome to the Sudoku Solver, a web application designed to solve Sudoku puzzles using the powerful Backtracking algorithm. This interactive tool is built with JavaScript, providing an intuitive way to solve Sudoku puzzles and gain insights into this classic number puzzle game. <br>  ### [Please give a ‚≠ê to this repo if you like my work :)]  <br>   ## üåê Live Website Demo   üîó https://ayushichoudhary-19.github.io/SudokuSolver/   <p align=""center"">   <img src=""https://github.com/ayushichoudhary-19/SudokuSolver/assets/73214455/1686de45-c5c3-4f53-ad1d-6a57a86111db"" alt=""Image"" width=""600"" height=""auto""> </p>  ## üß© Sudoku Puzzle  <div align=""center"">    ``` 1Ô∏è‚É£ 3Ô∏è‚É£ 5Ô∏è‚É£ | 9Ô∏è‚É£ 2Ô∏è‚É£ 4Ô∏è‚É£ | 6Ô∏è‚É£ 8Ô∏è‚É£ 7Ô∏è‚É£ 7Ô∏è‚É£ 8Ô∏è‚É£ 6Ô∏è‚É£ | 1Ô∏è‚É£ 3Ô∏è‚É£ 5Ô∏è‚É£ | 4Ô∏è‚É£ 9Ô∏è‚É£ 2Ô∏è‚É£ 2Ô∏è‚É£ 4Ô∏è‚É£ 9Ô∏è‚É£ | 6Ô∏è‚É£ 7Ô∏è‚É£ 8Ô∏è‚É£ | 3Ô∏è‚É£ 5Ô∏è‚É£ 1Ô∏è‚É£ ------------------------------- 4Ô∏è‚É£ 5Ô∏è‚É£ 7Ô∏è‚É£ | 8Ô∏è‚É£ 6Ô∏è‚É£ 3Ô∏è‚É£ | 9Ô∏è‚É£ 2Ô∏è‚É£ 1Ô∏è‚É£ 6Ô∏è‚É£ 1Ô∏è‚É£ 8Ô∏è‚É£ | 5Ô∏è‚É£ 9Ô∏è‚É£ 2Ô∏è‚É£ | 7Ô∏è‚É£ 4Ô∏è‚É£ 3Ô∏è‚É£ 9Ô∏è‚É£ 2Ô∏è‚É£ 3Ô∏è‚É£ | 7Ô∏è‚É£ 4Ô∏è‚É£ 1Ô∏è‚É£ | 8Ô∏è‚É£ 6Ô∏è‚É£ 5Ô∏è‚É£ ------------------------------- 5Ô∏è‚É£ 7Ô∏è‚É£ 2Ô∏è‚É£ | 3Ô∏è‚É£ 1Ô∏è‚É£ 6Ô∏è‚É£ | 4Ô∏è‚É£ 3Ô∏è‚É£ 8Ô∏è‚É£ 3Ô∏è‚É£ 6Ô∏è‚É£ 1Ô∏è‚É£ | 4Ô∏è‚É£ 8Ô∏è‚É£ 7Ô∏è‚É£ | 5Ô∏è‚É£ 1Ô∏è‚É£ 9Ô∏è‚É£ 8Ô∏è‚É£ 9Ô∏è‚É£ 4Ô∏è‚É£ | 2Ô∏è‚É£ 5Ô∏è‚É£ 9Ô∏è‚É£ | 2Ô∏è‚É£ 7Ô∏è‚É£ 6Ô∏è‚É£ ```   </div>   ## üõ†Ô∏è Tech Stack <p align=""center"">   <img src=""https://img.icons8.com/color/96/000000/html-5.png"" alt=""HTML"" style=""margin: 10px;"">   <img src=""https://img.icons8.com/color/96/000000/css3.png"" alt=""CSS"" style=""margin: 10px;"">   <img src=""https://img.icons8.com/color/96/000000/javascript.png"" alt=""JavaScript (JS)"" style=""margin: 10px;"">   <img src=""https://img.icons8.com/color/96/000000/visual-studio-code-2019.png"" alt=""VS Code"" style=""margin: 10px;""> </p>  - **JavaScript**: Utilized for the Backtracking algorithm and interactivity. - **HTML**: Used for structuring the web page. - **CSS**: Provides styling to create an appealing user interface.   ## ü§î What's a Sudoku Puzzle?   A Sudoku puzzle is a number puzzle or logic game. The puzzle consists of a `9x9` grid, divided into nine `3x3` subgrids, also known as ""regions"" or ""boxes."" The objective of Sudoku is to fill in the entire grid with numbers so that each row, each column, and each `3x3` subgrid contains all of the digits from `1 to 9`, with **no repetition of numbers in any of these regions**.  ## üìú Basic Rules :  1. Each row must contain all the numbers from 1 to 9, with no repetition.  2. Each column must also contain all the numbers from 1 to 9, with no repetition.  3. Each of the nine 3x3 subgrids must contain all the numbers from 1 to 9, with no repetition.   ## üìö How to Use the Solver  1. **Input Your Puzzle**: Enter the Sudoku puzzle by filling in the numbers in the grid. Use ""0"" or leave cells empty for unknown values.  2. **Click ""Solve""**: Once you've input your puzzle, click the ""Solve"" button.  3. **View the Solution**: The solver will utilize the Backtracking algorithm to find a solution. Once solved, the completed puzzle will be displayed.  ## üß∞ Solving with Backtracking  The Sudoku Solver employs the Backtracking algorithm to find solutions efficiently. Backtracking is a recursive technique that explores potential solutions and backtracks when it encounters inconsistencies, ultimately leading to a valid solution or indicating that no solution exists. This algorithm is particularly well-suited for solving Sudoku puzzles, as it efficiently explores the puzzle space and ensures the validity of each move.   # üìÑ License  This project is open source and available under the MIT License.  ---  **Note:** You are welcome to clone and customize this Sudoku Solver for your own projects. Explore the code and adapt it to your needs, or contribute to its development on [GitHub](https://github.com/ayushichoudhary-19/SudokuSolver).   "
Nano ID,JavaScript,https://github.com/ai/nanoid,"# Nano ID  <img src=""https://ai.github.io/nanoid/logo.svg"" align=""right""      alt=""Nano ID logo by Anton Lovchikov"" width=""180"" height=""94"">  **English** | [–†—É—Å—Å–∫–∏–π](./README.ru.md) | [ÁÆÄ‰Ωì‰∏≠Êñá](./README.zh-CN.md) | [Bahasa Indonesia](./README.id-ID.md)  A tiny, secure, URL-friendly, unique¬†string ID¬†generator for¬†JavaScript.  > ‚ÄúAn amazing level of senseless perfectionism, > which is simply impossible not to respect.‚Äù  * **Small.** 118 bytes (minified and brotlied). No¬†dependencies.   [Size Limit] controls the size. * **Safe.** It uses hardware random generator. Can be used in clusters. * **Short IDs.** It uses a¬†larger alphabet than UUID (`A-Za-z0-9_-`).   So ID size was reduced from¬†36¬†to¬†21¬†symbols. * **Portable.** Nano ID was ported   to over [20 programming languages](./README.md#other-programming-languages).  ```js import { nanoid } from 'nanoid' model.id = nanoid() //=> ""V1StGXR8_Z5jdHi6B-myT"" ```  ---  <img src=""https://cdn.evilmartians.com/badges/logo-no-label.svg"" alt="""" width=""22"" height=""16"" />¬†¬†Made at <b><a href=""https://evilmartians.com/devtools?utm_source=nanoid&utm_campaign=devtools-button&utm_medium=github"">Evil Martians</a></b>, product consulting for <b>developer tools</b>.  ---  [online tool]: https://gitpod.io/#https://github.com/ai/nanoid/ [with Babel]:  https://developer.epages.com/blog/coding/how-to-transpile-node-modules-with-babel-and-webpack-in-a-monorepo/ [Size Limit]:  https://github.com/ai/size-limit   ## Table of Contents  - [Table of Contents](#table-of-contents) - [Comparison with UUID](#comparison-with-uuid) - [Benchmark](#benchmark) - [Security](#security) - [Install](#install)   - [ESM](#esm)   - [CommonJS](#commonjs)   - [JSR](#jsr)   - [CDN](#cdn) - [API](#api)   - [Blocking](#blocking)   - [Non-Secure](#non-secure)   - [Custom Alphabet or Size](#custom-alphabet-or-size)   - [Custom Random Bytes Generator](#custom-random-bytes-generator) - [Usage](#usage)   - [React](#react)   - [React Native](#react-native)   - [PouchDB and CouchDB](#pouchdb-and-couchdb)   - [CLI](#cli)   - [TypeScript](#typescript)   - [Other Programming Languages](#other-programming-languages) - [Tools](#tools)   ## Comparison with UUID  Nano ID is quite comparable to UUID v4 (random-based). It has a similar number of random bits in the ID (126¬†in¬†Nano¬†ID¬†and¬†122¬†in¬†UUID), so¬†it¬†has¬†a similar¬†collision¬†probability:  > For there to be a one in a billion chance of duplication, > 103 trillion version 4 IDs must be generated.  There are two main differences between Nano ID and UUID v4:  1. Nano ID uses a bigger alphabet, so a similar number of random bits    are packed in just 21¬†symbols¬†instead of 36. 2. Nano ID code is **4 times smaller** than `uuid/v4` package:    130 bytes instead of 423.   ## Benchmark  ```rust $ node ./test/benchmark.js crypto.randomUUID          7,619,041 ops/sec uuid v4                    7,436,626 ops/sec @napi-rs/uuid              4,730,614 ops/sec uid/secure                 4,729,185 ops/sec @lukeed/uuid               4,015,673 ops/sec nanoid                     3,693,964 ops/sec customAlphabet             2,799,255 ops/sec nanoid for browser           380,915 ops/sec secure-random-string         362,316 ops/sec uid-safe.sync                354,234 ops/sec shortid                       38,808 ops/sec  Non-secure: uid                       11,872,105 ops/sec nanoid/non-secure          2,226,483 ops/sec rndm                       2,308,044 ops/sec ```  Test configuration: Framework 13 7840U, Fedora 39, Node.js 21.6.   ## Security  *See a good article about random generators theory: [Secure random values (in Node.js)]*  * **Unpredictability.** Instead of using the unsafe `Math.random()`, Nano ID   uses the `crypto` module in Node.js and¬†the¬†Web¬†Crypto¬†API¬†in¬†browsers.   These modules use unpredictable hardware random generator. * **Uniformity.** `random % alphabet` is a popular mistake to make when coding   an ID generator. The distribution will not be even; there¬†will be¬†a¬†lower   chance for some symbols to appear compared to others. So, it will reduce   the number of tries when¬†brute-forcing. Nano ID uses a [better algorithm]   and is tested for uniformity.    <img src=""img/distribution.png"" alt=""Nano ID uniformity""      width=""340"" height=""135"">  * **Well-documented:** all Nano ID hacks are documented. See comments   in [the source]. * **Vulnerabilities:** to report a security vulnerability, please use   the [Tidelift security contact](https://tidelift.com/security).   Tidelift¬†will¬†coordinate¬†the¬†fix¬†and¬†disclosure.  [Secure random values (in Node.js)]: https://gist.github.com/joepie91/7105003c3b26e65efcea63f3db82dfba [better algorithm]:                  https://github.com/ai/nanoid/blob/main/index.js [the source]:                        https://github.com/ai/nanoid/blob/main/index.js   ## Install  ### ESM  Nano ID 5 works with ESM projects (with `import`) in tests or Node.js scripts.  ```bash npm install nanoid ```  ### CommonJS  Nano ID can be used with CommonJS in one of the following ways:  - You can use `require()` to import Nano ID. You need to use latest Node.js   22.12 (works out-of-the-box) or Node.js 20   (with `--experimental-require-module`).  - For Node.js 18 you can dynamically import Nano ID as follows:    ```js   let nanoid   module.exports.createID = async () => {     if (!nanoid) ({ nanoid } = await import('nanoid'))     return nanoid() // => ""V1StGXR8_Z5jdHi6B-myT""   }   ```  - You can use Nano ID 3.x (we still support it):    ```bash   npm install nanoid@3   ```  ### JSR  [JSR](https://jsr.io) is a replacement for npm with open governance and active development (in contrast to npm).  ```bash npx jsr add @sitnik/nanoid ```  You can use it in Node.js, Deno, Bun, etc.  ```js // Replace `nanoid` to `@sitnik/nanoid` in all imports import { nanoid } from '@sitnik/nanoid' ```  For Deno install it by `deno add jsr:@sitnik/nanoid` or import from `jsr:@sitnik/nanoid`.   ### CDN  For quick hacks, you can load Nano ID from CDN. Though, it is not recommended to be used in production because of the lower loading performance.  ```js import { nanoid } from 'https://cdn.jsdelivr.net/npm/nanoid/nanoid.js' ```  ## API  Nano ID has 2 APIs: normal and non-secure.  By default, Nano ID uses URL-friendly symbols (`A-Za-z0-9_-`) and returns an ID with 21 characters (to¬†have¬†a¬†collision¬†probability¬†similar to¬†UUID v4).   ### Blocking  The safe and easiest way to use Nano ID.  In rare cases could block CPU from other work while noise collection for hardware random generator.  ```js import { nanoid } from 'nanoid' model.id = nanoid() //=> ""V1StGXR8_Z5jdHi6B-myT"" ```  If you want to reduce the ID size (and increase collisions probability), you can pass the size as¬†an¬†argument.  ```js nanoid(10) //=> ""IRFa-VaY2b"" ```  Don‚Äôt forget to check the¬†safety of your ID size in our [ID collision probability] calculator.  You can also use a [custom alphabet](#custom-alphabet-or-size) or a [random generator](#custom-random-bytes-generator).  [ID collision probability]: https://zelark.github.io/nano-id-cc/   ### Non-Secure  By default, Nano ID uses hardware random bytes generation for security and low collision probability. If you are not so concerned with security, you can use it for environments without hardware random generators.  ```js import { nanoid } from 'nanoid/non-secure' const id = nanoid() //=> ""Uakgb_J5m9g-0JDMbcJqLJ"" ```   ### Custom Alphabet or Size  `customAlphabet` returns a function that allows you to create `nanoid` with your own alphabet and ID size.  ```js import { customAlphabet } from 'nanoid' const nanoid = customAlphabet('1234567890abcdef', 10) model.id = nanoid() //=> ""4f90d13a42"" ```  ```js import { customAlphabet } from 'nanoid/non-secure' const nanoid = customAlphabet('1234567890abcdef', 10) user.id = nanoid() ```  Check the¬†safety of your custom alphabet and ID size in our [ID collision probability] calculator. For¬†more¬†alphabets,¬†check¬†out¬†the¬†options in¬†[`nanoid-dictionary`].  Alphabet must contain 256 symbols or less. Otherwise, the security of the internal generator algorithm is not guaranteed.  In addition to setting a default size, you can change the ID size when calling the function:  ```js import { customAlphabet } from 'nanoid' const nanoid = customAlphabet('1234567890abcdef', 10) model.id = nanoid(5) //=> ""f01a2"" ```  [ID collision probability]: https://alex7kom.github.io/nano-nanoid-cc/ [`nanoid-dictionary`]:      https://github.com/CyberAP/nanoid-dictionary   ### Custom Random Bytes Generator  `customRandom` allows you to create a `nanoid` and replace alphabet and the default random bytes generator.  In¬†this¬†example,¬†a¬†seed-based¬†generator is used:  ```js import { customRandom } from 'nanoid'  const rng = seedrandom(seed) const nanoid = customRandom('abcdef', 10, size => {   return (new Uint8Array(size)).map(() => 256 * rng()) })  nanoid() //=> ""fbaefaadeb"" ```  `random` callback must accept the array size and return an array with random numbers.  If you want to use the same URL-friendly symbols with `customRandom`, you can get the default alphabet using¬†the¬†`urlAlphabet`.  ```js const { customRandom, urlAlphabet } = require('nanoid') const nanoid = customRandom(urlAlphabet, 10, random) ```  Note, that between Nano ID versions we may change random generator call sequence. If you are using seed-based generators, we do not guarantee the same result.   ## Usage  ### React  There‚Äôs no correct way to use Nano ID for React `key` prop since it should be consistent among renders.  ```jsx function Todos({todos}) {   return (     <ul>       {todos.map(todo => (         <li key={nanoid()}> /* DON‚ÄôT DO IT */           {todo.text}         </li>       ))}     </ul>   ) } ```  You should rather try to reach for stable ID inside your list item.  ```jsx const todoItems = todos.map((todo) =>   <li key={todo.id}>     {todo.text}   </li> ) ```  In case you don‚Äôt have stable IDs you'd rather use index as `key` instead of `nanoid()`:  ```jsx const todoItems = todos.map((text, index) =>   <li key={index}> /* Still not recommended but preferred over nanoid().                       Only do this if items have no stable IDs. */     {text}   </li> ) ```  In case you just need random IDs to link elements like labels and input fields together, [`useId`] is recommended. That hook was added in React 18.  [`useId`]: https://reactjs.org/docs/hooks-reference.html#useid   ### React Native  React Native does not have built-in random generator. The following polyfill works for plain React Native and Expo starting with `39.x`.  1. Check [`react-native-get-random-values`] docs and install it. 2. Import it before Nano ID.  ```js import 'react-native-get-random-values' import { nanoid } from 'nanoid' ```  [`react-native-get-random-values`]: https://github.com/LinusU/react-native-get-random-values   ### PouchDB and CouchDB  In PouchDB and CouchDB, IDs can‚Äôt start with an underscore `_`. A prefix is required to prevent this issue, as Nano ID might use a `_` at the start of the ID by default.  Override the default ID with the following option:  ```js db.put({   _id: 'id' + nanoid(),   ‚Ä¶ }) ```   ### CLI  You can get unique ID in terminal by calling `npx nanoid`. You need only Node.js in the system. You do not need Nano ID to be installed anywhere.  ```sh $ npx nanoid npx: installed 1 in 0.63s LZfXLFzPPR4NNrgjlWDxn ```  Size of generated ID can be specified with `--size` (or `-s`) option:  ```sh $ npx nanoid --size 10 L3til0JS4z ```  Custom alphabet can be specified with `--alphabet` (or `-a`) option (note that in this case `--size` is required):  ```sh $ npx nanoid --alphabet abc --size 15 bccbcabaabaccab ```  ### TypeScript  Nano ID allows casting generated strings into opaque strings in TypeScript. For example:  ```ts declare const userIdBrand: unique symbol type UserId = string & { [userIdBrand]: true }  // Use explicit type parameter: mockUser(nanoid<UserId>())  interface User {   id: UserId   name: string }  const user: User = {   // Automatically casts to UserId:   id: nanoid(),   name: 'Alice' } ```  ### Other Programming Languages  Nano ID was ported to many languages. You can use these ports to have the same ID generator on the client and server side.  * [C](https://github.com/lukateras/nanoid.h) * [C#](https://github.com/codeyu/nanoid-net) * [C++](https://github.com/mcmikecreations/nanoid_cpp) * [Clojure and ClojureScript](https://github.com/zelark/nano-id) * [ColdFusion/CFML](https://github.com/JamoCA/cfml-nanoid) * [Crystal](https://github.com/mamantoha/nanoid.cr) * [Dart & Flutter](https://github.com/pd4d10/nanoid-dart) * [Elixir](https://github.com/railsmechanic/nanoid) * [Gleam](https://github.com/0xca551e/glanoid) * [Go](https://github.com/matoous/go-nanoid) * [Haskell](https://github.com/MichelBoucey/NanoID) * [Haxe](https://github.com/flashultra/uuid) * [Janet](https://sr.ht/~statianzo/janet-nanoid/) * [Java](https://github.com/wosherco/jnanoid-enhanced) * [Kotlin](https://github.com/viascom/nanoid-kotlin) * [MySQL/MariaDB](https://github.com/viascom/nanoid-mysql-mariadb) * [Nim](https://github.com/icyphox/nanoid.nim) * [OCaml](https://github.com/routineco/ocaml-nanoid) * [Perl](https://github.com/tkzwtks/Nanoid-perl) * [PHP](https://github.com/hidehalo/nanoid-php) * Python [native](https://github.com/puyuan/py-nanoid) implementation   with [dictionaries](https://pypi.org/project/nanoid-dictionary)   and [fast](https://github.com/oliverlambson/fastnanoid) implementation (written in Rust) * Postgres [Extension](https://github.com/spa5k/uids-postgres)   and [Native Function](https://github.com/viascom/nanoid-postgres) * [R](https://github.com/hrbrmstr/nanoid) (with dictionaries) * [Ruby](https://github.com/radeno/nanoid.rb) * [Rust](https://github.com/nikolay-govorov/nanoid) * [Swift](https://github.com/antiflasher/NanoID) * [Unison](https://share.unison-lang.org/latest/namespaces/hojberg/nanoid) * [V](https://github.com/invipal/nanoid) * [Zig](https://github.com/SasLuca/zig-nanoid)  For other environments, [CLI] is available to generate IDs from a command line.  [CLI]: #cli   ## Tools  * [ID size calculator] shows collision probability when adjusting   the ID alphabet or size. * [`nanoid-dictionary`] with popular alphabets to use with [`customAlphabet`]. * [`nanoid-good`] to be sure that your ID doesn‚Äôt contain any obscene words.  [`nanoid-dictionary`]: https://github.com/CyberAP/nanoid-dictionary [ID size calculator]:  https://zelark.github.io/nano-id-cc/ [`customAlphabet`]:    #custom-alphabet-or-size [`nanoid-good`]:       https://github.com/y-gagar1n/nanoid-good"
Prettier,JavaScript,https://github.com/prettier/prettier,"[![Prettier Banner](https://unpkg.com/prettier-logo@1.0.3/images/prettier-banner-light.svg)](https://prettier.io)  <h2 align=""center"">Opinionated Code Formatter</h2>  <p align=""center"">   <em>     JavaScript     ¬∑ TypeScript     ¬∑ Flow     ¬∑ JSX     ¬∑ JSON   </em>   <br />   <em>     CSS     ¬∑ SCSS     ¬∑ Less   </em>   <br />   <em>     HTML     ¬∑ Vue     ¬∑ Angular   </em>   <br />   <em>     GraphQL     ¬∑ Markdown     ¬∑ YAML   </em>   <br />   <em>     <a href=""https://prettier.io/docs/plugins"">       Your favorite language?     </a>   </em> </p>  <p align=""center"">   <a href=""https://github.com/prettier/prettier/actions?query=workflow%3AProd+branch%3Amain"">     <img alt=""Github Actions Build Status"" src=""https://img.shields.io/github/actions/workflow/status/prettier/prettier/prod-test.yml?label=Prod&style=flat-square""></a>   <a href=""https://github.com/prettier/prettier/actions?query=workflow%3ADev+branch%3Amain"">     <img alt=""Github Actions Build Status"" src=""https://img.shields.io/github/actions/workflow/status/prettier/prettier/dev-test.yml?label=Dev&style=flat-square""></a>   <a href=""https://github.com/prettier/prettier/actions?query=workflow%3ALint+branch%3Amain"">     <img alt=""Github Actions Build Status"" src=""https://img.shields.io/github/actions/workflow/status/prettier/prettier/lint.yml?label=Lint&style=flat-square""></a>   <a href=""https://codecov.io/gh/prettier/prettier"">     <img alt=""Codecov Coverage Status"" src=""https://img.shields.io/codecov/c/github/prettier/prettier.svg?style=flat-square""></a>   <a href=""https://twitter.com/acdlite/status/974390255393505280"">     <img alt=""Blazing Fast"" src=""https://img.shields.io/badge/speed-blazing%20%F0%9F%94%A5-brightgreen.svg?style=flat-square""></a>   <br/>   <a href=""https://www.npmjs.com/package/prettier"">     <img alt=""npm version"" src=""https://img.shields.io/npm/v/prettier.svg?style=flat-square""></a>   <a href=""https://www.npmjs.com/package/prettier"">     <img alt=""weekly downloads from npm"" src=""https://img.shields.io/npm/dw/prettier.svg?style=flat-square""></a>   <a href=""#badge"">     <img alt=""code style: prettier"" src=""https://img.shields.io/badge/code_style-prettier-ff69b4.svg?style=flat-square""></a>   <a href=""https://twitter.com/PrettierCode"">     <img alt=""Follow Prettier on Twitter"" src=""https://img.shields.io/badge/%40PrettierCode-9f9f9f?style=flat-square&logo=x&labelColor=555""></a> </p>  ## Intro  Prettier is an opinionated code formatter. It enforces a consistent style by parsing your code and re-printing it with its own rules that take the maximum line length into account, wrapping code when necessary.  ### Input  <!-- prettier-ignore --> ```js foo(reallyLongArg(), omgSoManyParameters(), IShouldRefactorThis(), isThereSeriouslyAnotherOne()); ```  ### Output  ```js foo(   reallyLongArg(),   omgSoManyParameters(),   IShouldRefactorThis(),   isThereSeriouslyAnotherOne(), ); ```  Prettier can be run [in your editor](https://prettier.io/docs/editors) on-save, in a [pre-commit hook](https://prettier.io/docs/precommit), or in [CI environments](https://prettier.io/docs/cli#list-different) to ensure your codebase has a consistent style without devs ever having to post a nit-picky comment on a code review ever again!  ---  **[Documentation](https://prettier.io/docs/)**  [Install](https://prettier.io/docs/install) ¬∑ [Options](https://prettier.io/docs/options) ¬∑ [CLI](https://prettier.io/docs/cli) ¬∑ [API](https://prettier.io/docs/api)  **[Playground](https://prettier.io/playground/)**  ---  ## Badge  Show the world you're using _Prettier_ ‚Üí [![code style: prettier](https://img.shields.io/badge/code_style-prettier-ff69b4.svg?style=flat-square)](https://github.com/prettier/prettier)  ```md [![code style: prettier](https://img.shields.io/badge/code_style-prettier-ff69b4.svg?style=flat-square)](https://github.com/prettier/prettier) ```  ## Contributing  See [CONTRIBUTING.md](CONTRIBUTING.md)."
Space Invaders Game,JavaScript,https://github.com/yojoecool/SpaceInvaders,"# Space Invaders  ![Logo](https://i.ytimg.com/vi/k9oyDTR0EwQ/maxresdefault.jpg ""Space Invaders"")  ## Rules of the Game Space Invaders is the classic single-player arcade game introduced in 1978. The objective of my version is to make it through 3 levels of increasing difficulty while avoiding getting hit by enemy bullets and preventing the enemies from reaching the bottom of the screen. Difficulty between levels increases by having enemies become faster and take more hits in order to die.  ## Controls Players use the left and right arrow keys to move their ship and the spacebar to shoot. When prompted, the player can press the Enter key to restart the game.  ## Technologies HTML5, CSS, and JavaScript were used to create the website. Canvas was used to render the graphics and run the game.  ## Approach/Process After creating the wireframe, I started just playing around with Canvas and piecing together how to accomplish the features listed in my user stories. I first learned how to draw on the canvas and create a figure. Drawing the figure turned to figuring out how to move it around, which turned to figuring out how to move it around with keydown events, etc. So I just tried to learn how to accomplish one of the user stories and build off the knowledge for the following ones I approached.  To help with the creation of the objects needed for the game and make the game easier to scale, I used classes from ES6 to make the game more object oriented, storing properties like the coordinates, width, height, along with functions to draw and update the objects on the canvas. This, along with using functions to create new levels that draw the enemies and set the enemy speeds, makes it easy for me to not only create new characters/enemies, but also easily add levels.  After finishing the basics of the game that followed the user stories, I created a list of features that I thought would make the game more interesting and more fleshed out, such as a score counter, multiple lives, difficulty adjustments, sprites and animations, background animations, sound effects, and more.  ## Wireframe ![wireframeImg](https://github.com/yojoecool/SpaceInvaders/blob/master/docs/wireframe.png ""Wireframe"")  ## Biggest challenges and wins The biggest challenge was probably learning Canvas in the first place since I had never used anything like that before. Along with learning Canvas, making sure I kept my scope realistic and avoiding feature creep given the limited time to work on the project was also a challenge.   My biggest win was getting things animated smoothly with the controls being responsive. I didn't originally plan to use the animation loop I ended up with to constantly redraw the image on the screen, but deciding to go in that direction made things a lot smoother.  ## Bugs None that I know of  ## Future Features More levels, bosses, high scores, touch-screen controls  ## User Stories https://trello.com/b/sCXykRc4/space-invaders"
Memory (Card game),JavaScript,https://github.com/taniarascia/memory,"# ‚≠ê JavaScript Memory Game  [![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)  Fun and simple memory game, like the one from Super Mario 3, made with plain JavaScript.  ### [Read the tutorial](https://www.taniarascia.com/how-to-create-a-memory-game-super-mario-with-plain-javascript/) | [View the demo](http://taniarascia.github.io/memory)  ## Instructions  Just want to view the source of all the steps from start to finish? [View steps](https://gist.github.com/taniarascia/a3b550d568f3e6b693e89786eb333988).  - Display 12 cards. - Duplicate the cards to have 2 sets of 12. - Randomize the display of cards. - Add selected style for selected cards. - Only allow two cards to be selected at a time. - Determine if two selected cards are a match and hide them. - Reset guess count after 2. - Add delay to selections. - Show back of card initially and flip on select - Finished game!  ## Author  - [Tania Rascia](https://www.taniarascia.com)  ## License  This project is open source and available under the [MIT License](LICENSE)."
Drum Kit,JavaScript,https://github.com/git-vaibhav30/Drum-Kit,"# Drum Kit Project  DESCRIPTION: The Drum Kit project is a fun and interactive web application that allows users to play virtual drums by clicking on corresponding buttons or using keyboard keys. This project is built using HTML, CSS, and JavaScript, providing an engaging way to create different drum sounds and beats.  FEATURES: Interactive Drumming: Users can simulate playing drums by clicking on the drum pads or pressing the corresponding keys on their keyboard. Multiple Drum Sounds: Each drum pad corresponds to a specific drum sound, providing a variety of sounds to experiment with. Keyboard Shortcuts: Users can use keyboard keys to play the drum sounds, making it easier to create rhythms. Visual Feedback: When a drum pad is clicked or a keyboard key is pressed, the pad visually responds with animation, enhancing the user experience.  HOW TO PLAY: Play by Clicking: Click on any of the drum pads with the labels ""w"", ""a"", ""s"", ""d"", ""j"", ""k"", ""l"" to trigger the corresponding drum sound. Play by Keyboard: Alternatively, you can use the keyboard keys ""w"", ""a"", ""s"", ""d"", ""j"", ""k"", ""l"" to play the drum sounds. Press the respective key to produce the sound associated with that drum pad. Create Beats: Experiment with different combinations of drum sounds to create your own beats and rhythms.  TECHNOLOGIES USED: HTML: Used to structure the drum kit interface and elements. CSS: Styled the drum pads and overall design for a visually appealing experience. JavaScript: Implemented the functionality of playing drum sounds, handling user interactions, and adding animations.  Enjoy creating awesome beats with the Drum Kit! Thankyou!"
The RGB Color Game,JavaScript,https://github.com/GovindCodes/ColorGame,"# ColorGame  <!-- PROJECT LOGO --> <br /> <p align=""center"">   <a href=""https://github.com/govindk11/ColorGame"">     <img src=""images/readme.png"" alt=""Logo"" width=""80"" height=""80"">   </a>    <h3 align=""center"">Color Game Project-README</h3>    <p align=""center"">     An intresting Game to boost Konwledge of Colors      <br />     <a href=""https://colorgamebygovind.netlify.app/""><strong>View Demo ¬ª</strong></a>     <br />     <br />     <a href=""https://github.com/govindk11/ColorGame/issues"">Report Bug</a>     ¬∑     <a href=""https://github.com/govindk11/ColorGame/issues"">Request Feature</a>   </p> </p>    <!-- ABOUT THE PROJECT --> ## About The Project  [Also Check The Game Here](https://colorgamebygovind.netlify.app/)   The RGB Color Game is a simple project developed using JavaScript, CSS, and HTML. This project is an interesting color guessing game. The user/player has to guess the result of the given RGB color combination and find the correct output of the color mixture. The user can guess the color until the option finishes.    The Game has two levels: * easy * hard  :large_orange_diamond::large_orange_diamond::large_orange_diamond::large_orange_diamond::large_orange_diamond:  * Guess the color :dart: ![Guess the Color](https://github.com/govindk11/ColorGame/blob/master/images/guessColor.png?raw=true)  * Guessed the correct color :collision: ![Guessed the color](https://github.com/govindk11/ColorGame/blob/master/images/colorGuessed.png?raw=true)    ### Built With This section should list any major frameworks that you built your project using. Leave any add-ons/plugins for the acknowledgements section. Here are a few examples. * Vanilla JavaSctipt * HTML * CSS   <!-- CONTRIBUTING --> ## Contributing  Contributions are what make the open source community such an amazing place to be learn, inspire, and create. Any contributions you make are **greatly appreciated**.  1. Fork the Project 2. Create your Feature Branch (`git checkout -b feature/AmazingFeature`) 3. Commit your Changes (`git commit -m 'Add some AmazingFeature'`) 4. Push to the Branch (`git push origin feature/AmazingFeature`) 5. Open a Pull Request    <!-- CONTACT --> ## Contact  Govind Kumar - [@Govind_Govindk](https://twitter.com/Govind_Govindk) - govindkumar3006@gmail.com  Project Link: [https://github.com/govindk11/ColorGame](https://github.com/govindk11/ColorGame)       "
Dice Game,JavaScript,https://github.com/Hashuudev/Dice-Game,"# Dice-Game This is a Web-based implementation of the classic Dice Game, created using HTML, CSS, and vanilla JavaScript, an awesome challenge provided by Jonas Schmedtmann ( Instructor )  # How-to-Play The objective of the game is to be the first player to reach a total score of 100 or more points. Each turn, a player repeatedly rolls a die until either a 1 is rolled or the player decides to ""hold"" and end their turn:  If the player rolls a 1, their turn ends and they receive no points for the turn. If the player rolls any other number, it is added to their turn total and they can choose to either roll again or hold. If they choose to hold, their turn total is added to their score, and the turn ends. Players can also choose to use additional rules or variations, such as requiring a minimum score to start adding to their total, playing with two or more dice, or allowing the option to ""double"" or ""triple"" their score with certain rolls.  # How-to-run To run the Pig Dice Game, simply open the index.html file in a web browser. No additional installation or setup is required.  # Features This implementation of the Dice Game includes the following features:  Two players take turns rolling a single die. Each player's turn total and total score are displayed. Players can choose to hold their turn total and add it to their total score. Rolling a 1 ends the player's turn and forfeits their turn total. The game automatically switches turns after each turn ends. The game ends and declares a winner when a player reaches a total score of 100 points.  # Screenshot  ![1](https://github.com/Hashuudev/Dice-Game/assets/94761963/65f06a0e-ab78-4fdd-970d-d56e3a21cdda)"
Music App,JavaScript,https://github.com/codewithsadee/music-player,"<div align=""center"">      ![GitHub repo size](https://img.shields.io/github/repo-size/codewithsadee/music-player)   ![GitHub stars](https://img.shields.io/github/stars/codewithsadee/music-player?style=social)   ![GitHub forks](https://img.shields.io/github/forks/codewithsadee/music-player?style=social) [![Twitter Follow](https://img.shields.io/twitter/follow/codewithsadee_?style=social)](https://twitter.com/intent/follow?screen_name=codewithsadee_)   [![YouTube Video Views](https://img.shields.io/youtube/views/jbMd2NVFrZk?style=social)](https://youtu.be/jbMd2NVFrZk)    <br />   <br />    <h2 align=""center"">Web Music Player</h2>    A fully responsive web music player using vanilla javascript, <br />Responsive for all devices, build using html, css, and javascript.    <a href=""https://codewithsadee.github.io/music-player/""><strong>‚û• Live Demo</strong></a>  </div>  <br />  ### Demo Screeshots  ![Music Player Desktop Demo](./readme-images/desktop.png ""Desktop Demo"")  ### Prerequisites  Before you begin, ensure you have met the following requirements:  * [Git](https://git-scm.com/downloads ""Download Git"") must be installed on your operating system.  ### Run Locally  To run **Music Player** locally, run this command on your git bash:  Linux and macOS:  ```bash sudo git clone https://github.com/codewithsadee/music-player.git ```  Windows:  ```bash git clone https://github.com/codewithsadee/music-player.git ```  ### Contact  If you want to contact with me you can reach me at [Twitter](https://www.twitter.com/codewithsadee).  ### License  This project is **free to use** and does not contains any license."
Tip Calculator,JavaScript,https://github.com/nidhiupman568/TIP-CALCULATOR-HTML-CSS-JS-,"  # üí∞ Tip Calculator Project üí∞  üéâ Welcome to the **Tip Calculator** project! üéâ This practical tool helps you quickly calculate the tip amount and total bill after adding a tip percentage. Built with **HTML** üìù, **CSS** üé®, and **JavaScript** üíª, this calculator simplifies the process of determining tips. üí∞  Project Demo: https://nidhiupman568.github.io/TIP-CALCULATOR-HTML-CSS-JS-/  ## üìã Description  The **Tip Calculator** computes the tip amount based on the bill total and the selected tip percentage. It provides you with the total bill including the tip, making it easier to split bills or manage expenses at restaurants and cafes. It‚Äôs perfect for anyone who wants to quickly calculate tips without hassle. üçΩÔ∏è  In this project, we will build a tip calculator which takes the billing amount, type of service, and the number of persons as input. You can choose the percentage of tip you want to give and can calculate the tip per person. üí°  ## üõ†Ô∏è Built With  - **HTML**: For creating the structure of the calculator interface. - **CSS**: For styling to make the calculator visually appealing. - **JavaScript**: For implementing the tip calculation logic.  ## üöÄ How to Use  1. Clone the repository: 'https://github.com/nidhiupman568/TIP-CALCULATOR-HTML-CSS-JS-.git' üìÅüíª    2. Open `index.html` in your web browser to start using the Tip Calculator. üåê  ## üì∏ Screenshots (Output)  Here‚Äôs a preview of the Tip Calculator in action:  ![TIP CALCULATOR](https://github.com/nidhiupman568/TIP-CALCULATOR-HTML-CSS-JS-/assets/130860182/936204e4-58f5-49e2-b020-b69fe6afa355)  Calculate tips effortlessly with the Tip Calculator! üí° Whether you‚Äôre dining out or grabbing coffee, this tool ensures you tip accurately and conveniently. Feel free to share your feedback or contribute to the project by making a pull request. Enjoy calculating tips! üåü"
Color Palette App,JavaScript,https://github.com/katecoded/color-palette-app,"# Color Palette Generator  This Python application, made using Flask, generates color palettes based on themed nature photos. The user can either choose a theme and amount of image choices, or choose ""surprise me!"" to get a medium amount of random images, and an assortment of nature photos (chosen by a randomizer service) will be displayed. Choosing one of these photos will generate and reveal a color palette based on the colors within the image.  This application was created as a portfolio project for CS 361: Software Engineering I.  ## Screenshots of the Final Application ### Home Page ![The home page of the application with buttons for either choosing a theme or allowing for a random theme.](screenshots/home_page.png) ### Images Page ![A page full of images based on the ocean theme and a medium amount of images.](screenshots/images_page.png) ### Palette Page ![The palette page featuring an ocean image with the associated color palette of five colors below.](screenshots/palette_page.png)"
Virtual Drums App,JavaScript,https://github.com/Alexoid1/Virtual-Drum,"# Virtual-Drum ## Emulate a drum with the keyboard     ![screenshot](./screen.png)  Additional description about the project and its features.  ## Built With  - HTML, - JavaScript - Css  ## Live Demo  [Live Demo Link](https://alexoid1.github.io/Virtual-Drum/)   ## Getting Started  **Navigation bar** - Go to (https://alexoid1.github.io/Virtual-Drum/) and have a look around.      In this project:  - Add border efect on every keyboard event. - Add diferent sounds in every keyboard and click event.     To get a local copy  and to set it up and running follow these simple example steps.  ### Prerequisites  - Browser - Internet - Download the code from repository (https://github.com/Alexoid1/Virtual-Drum)   ## Author  üë§ **Pablo Alexis Zambrano Coral**  - Github: [@Alexoid1](https://github.com/Alexoid1) - Twitter: [@pablo_acz](https://twitter.com/pablo_acz) - Linkedin: [linkedin](https://www.linkedin.com/in/pablo-alexis-zambrano-coral-7a614a189/)    ## Show your support  Give a ‚≠êÔ∏è if you like this project!    ## üìù License  This project is [MIT](LICENSE) licensed.#"
Dice Roll,JavaScript,https://github.com/dice-roller/rpg-dice-roller,"<p align=""center"">     <img src=""https://dice-roller.github.io/documentation/dice-roller-logo.png"" alt=""RPG Dice Roller"" style=""max-width: 100%;"" width=""200""/> </p>  # RPG Dice Roller  [![npm (scoped)](https://img.shields.io/npm/v/@dice-roller/rpg-dice-roller?label=version)](https://www.npmjs.com/package/@dice-roller/rpg-dice-roller) [![Build Status](https://github.com/dice-roller/rpg-dice-roller/actions/workflows/build.yml/badge.svg)](https://github.com/dice-roller/rpg-dice-roller/actions/workflows/build.yml) [![Coverage Status](https://coveralls.io/repos/github/dice-roller/rpg-dice-roller/badge.svg?branch=main)](https://coveralls.io/github/dice-roller/rpg-dice-roller?branch=main) ![npm type definitions](https://img.shields.io/npm/types/@dice-roller/rpg-dice-roller) [![License](https://img.shields.io/npm/l/@dice-roller/rpg-dice-roller)](./licence.txt) [![npm downloads](https://img.shields.io/npm/dm/@dice-roller/rpg-dice-roller)](https://www.npmjs.com/package/@dice-roller/rpg-dice-roller)  A JS based dice roller that can roll various types of dice and modifiers, along with mathematical equations.   ## Install  ```bash npm install @dice-roller/rpg-dice-roller ```  ## Documentation  Check out the documentation at https://dice-roller.github.io/documentation   ## Usage in the wild  ### Official  * [Vue components](https://github.com/dice-roller/vue) - For Tailwind, Bootstrap, basic HTML, and renderless * [Vuepress plugin](https://github.com/dice-roller/vuepress-plugin) - Dice roller plugin used in this documentation * [CLI](https://github.com/dice-roller/cli) - Command Line Interface for rolling dice   ## Contributing  We're always happy for community contributions. You can find our contributing guide in the docs: https://dice-roller.github.io/documentation/contributing   ## Licence  This dice roller has been released under the MIT licence, meaning you can do pretty much anything you like with it, so long as the original copyright remains in place.  You **can** use it in commercial products.  If the licence terminology in the licence.txt is confusing, check out this: https://www.tldrlegal.com/l/mit"
Weather App,JavaScript,https://github.com/Prince-Shivaram/Simple-Weather-App,"# ""Simple Weather Application using HTML, CSS &amp; JavaScript""  ## Overview of Weather App  It's a  Simple Weather Application made by using HTML, CSS &amp; JavaScript.  The app is created by [J. Siv Ram Shastri](https://www.linkedin.com/in/imsivram1999/) for helping out the beginners on how to make Simple Weather Application using HTML, CSS &amp; JavaScript  Live Demo:  https://prince-shivaram.github.io/Simple-Weather-App/  ## Show some :heart: and :star: the repo if you like the design.  ![WeatherApp](https://user-images.githubusercontent.com/42378118/99897986-fd02dc00-2cc3-11eb-9cac-f5b577bfef40.png) "
Hangman Game,JavaScript,https://github.com/samruddhisomani/js-hangman,"# JavaScript Hangman  *[Check out a live demo here!](https://samruddhisomani.github.io/js-hangman/)*  ## What Does It Do?  It allows the user to play hangman with the computer. This particular version is aquatic mammal themed.  ## How Was It Built?  This was built using vanilla HTML, CSS, and JavaScript.  ## Why Was This Built?  This project was built as part of the Vrbo 2019 Career Exploration Program Software Development part time bootcamp.  ## Credits  Background photo by –ê–ª–µ–∫—Å–∞–Ω–¥—Ä –ü—Ä–æ–∫–æ—Ñ—å–µ–≤ from Pexels"
Transaction Limiter (Splitwise App Clone),JavaScript,https://github.com/PawanSirsat/SplitWise,"<p align=""center"">   <img src=""https://github.com/user-attachments/assets/b44a514e-0a41-4f09-8786-fc529ed51cf9"" alt=""Splitwise clone Banner""/> </p>  ---  ### ü§ù **Looking for Collaborators**  I‚Äôm actively looking for collaborators to help add new features and enhance this project further!  If you have ideas, expertise, or just the enthusiasm to contribute, feel free to:  - Fork this repository and start contributing. - Reach out to me at [p1.sirsat1998@gmail.com](mailto:p1.sirsat1998@gmail.com) to discuss ideas or ask questions.  ‚ú® **Together, we can make this project even better!** ‚ú®  ---  # **üìÑ SplitWise Clone Installation Guide**  ### üìö **Table of Contents**  1. [üì• Clone the Repository](#-1-clone-the-repository) 2. [üì¶ Install Dependencies](#-2-install-dependencies) 3. [üé® Install and Configure Tailwind CSS](#-3-install-and-configure-tailwind-css) 4. [üóÑÔ∏è Setup Appwrite (Database Configuration)](#-4-setup-appwrite-database-configuration)    - [üîë Create an Appwrite Account](#-step-1-create-an-appwrite-account)    - [üìÅ Create a New Project](#-step-2-create-a-new-project)    - [üõ†Ô∏è Setup the Database](#-step-3-setup-the-database)    - [üèóÔ∏è Create Collections](#-step-4-create-collections)    - [üîí Update Collection Permissions](#-step-5-update-collection-permissions)    - [üîë Copy IDs to .env File](#-step-6-copy-ids-to-env-file) 5. [üöÄ Run the Project](#-5-run-the-project) 6. [üåê Deploy on Vercel](#-6-deploy-on-vercel)    - [üîå Configure Appwrite Integration](#-configure-appwrite-integration) 7. [üìß Need Help?](#-need-help) 8. [üé• Appwrite Database Guide Video](#-appwrite-database-guide-video) 9. [üìë Documentation](#-documentation)    - [üìÇ Google Drive](#google-drive)    - [üìÑ DOC PDF](#doc-pdf) 10. [üìä Database Design](#-database-design) 11. [üîÑ Flowchart](#-flowchart) 12. [üí∏ Simplify Debt Flowchart](#-simplify-debt-flowchart)  ---  ### üì• **1. Clone the Repository**  Begin by cloning the SplitWise repository to your local machine:  ```bash git clone https://github.com/PawanSirsat/SplitWise.git ```  ### üì¶ **2. Install Dependencies**  Navigate to the project directory and install the required Node.js packages:  ```bash cd splitwise npm install ```  ### üé® **3. Install and Configure Tailwind CSS**  Install Tailwind CSS and initialize it in your project:  ```bash npm install -D tailwindcss npx tailwindcss init ```  ---  ### üóÑÔ∏è **4. Setup Appwrite (Database Configuration)**  #### üöÄ **Quick Option: Import My Database Setup (Migration)**  **Skip the manual steps below and directly migrate my database configuration and data!**  1. Create a new project in Appwrite. 2. Go to **Settings** ‚Üí **Migrations** ‚Üí **Import Project Data**. 3. Use the following required details:    - **Endpoint**    - **Project ID**    - **API Key**  **I can provide these details to you. Just email me at [p1.sirsat1998@gmail.com](mailto:p1.sirsat1998@gmail.com), and I‚Äôll share them with you!**  This automated process ensures you avoid errors and get started instantly.  ---  #### üõ†Ô∏è **Manual Option: Create the Database Yourself**  If you prefer to set up the database manually, follow these steps:  #### üîë **Step 1: Create an Appwrite Account**  - Sign up for an Appwrite account at [Appwrite](https://appwrite.io).  #### üìÅ **Step 2: Create a New Project**  - In the Appwrite dashboard, create a new project (e.g., **Splitwise**).  #### üõ†Ô∏è **Step 3: Setup the Database**  - Go to the **Databases** section and create a new database (e.g., **Expense**).  #### üèóÔ∏è **Step 4: Create Collections**  - Create the following collections and there attributes within your database:  ‚ö†Ô∏è **Warning**: Ensure that you use the attribute names and related collections exactly as mentioned below. To avoid errors, **copy and paste the names directly** where applicable.  1. **Users**     - **UserName**: `string` (Default: `-`)    - **name**: `string` (Default: `-`)    - **email**: `email` (Default: `-`)    - **accountId**: `string` (Default: `-`)  2. **Groups**     - **groupName**: `string` (Default: `-`)    - **Creator**: `Relationship` (Two-way Relationship with `Users`; `Many to one`,      Attribute Key (related collection): `groups`, Cascade on delete)    - **Members**: `Relationship` (Two-way Relationship with **Users**; `Many to Many`,      Attribute Key (related collection): `UserMember`, Set Null on delete)  3. **Friends**     - **friendsId**: `Relationship` (Two-way Relationship with **Users**; `Many to many`,      Attribute Key (related collection): `friendCollection`, Set Null on delete)    - **CollectionId**: `Relationship` (Two-way Relationship with **Users**; `Many to one`,      Attribute Key (related collection): `List`, Set Null on delete)  4. **Activity**     - **Desc**: `string` (Default: `-`)    - **Time**: `DateTime` (Default: `-`)    - **Amout**: `string` (Default: `-`) _Note: If you change this spelling (Amout), update it in the React app._    - **IsSettled**: `boolean` (Default: `false`)    - **splitMember**: `Relationship` (Two-way Relationship with **Users**; `Many to many`,      Attribute Key (related collection): `members`, Set Null on delete)    - **PaidBy**: `Relationship` (Two-way Relationship with **Users**; `Many to one`,      Attribute Key (related collection): `activity`, Set Null on delete)    - **Group**: `Relationship` (Two-way Relationship with **Groups**; `Many to one`,      Attribute Key (related collection): `activity`, Cascade on delete)  5. **Transaction**    - **Amount**: `string` (Default: `-`)    - **Time**: `DateTime` (Default: `-`)    - **IsOld**: `boolean` (Default: `false`)    - **payerId**: `Relationship` (Two-way Relationship with **Users**; `Many to one`,      Attribute Key (related collection): `transaction`, Set Null on delete)    - **receiverId**: `Relationship` (Two-way Relationship with **Users**; `Many to one`,      Attribute Key (related collection): `transactionId`, Set Null on delete)  #### üîí **Step 5: Update Collection Permissions**  1. **Navigate to Collection Settings**:     - In your **Appwrite** dashboard, open the **Collection Settings** for each collection that requires permission changes.  2. **Modify Permissions**:     - For each collection, go to the **Settings** tab.    - Under the **Permissions** section, update the role to `Any`.    - Ensure that the following permissions are checked:      - **Create**      - **Read**      - **Update**      - **Delete**  3. **Save Changes**:    - Repeat the process for each collection, ensuring the correct permissions are applied.  ---  #### üîë **Step 6: Copy IDs to .env File**  1. In **Project Settings**, copy the **Project ID** and **API Endpoint**. 2. Copy the **Database ID** and all **Collection IDs** from the database. 3. Create a `.env.local` file and add the copied IDs as follows: 4. Now no need of `.env.sample`  #### **Sample .env.local File**  ```bash VITE_APPWRITE_URL='https://cloud.appwrite.io/v1' VITE_APPWRITE_PROJECT_ID='67c067565211fbcf173' VITE_APPWRITE_DATABASE_ID='657c0953b37f27853d8' VITE_APPWRITE_USER_COLLECTION_ID='657casd56db7f49cee3b20' VITE_APPWRITE_GROUPS_COLLECTION_ID='657c09839424664asd87496' VITE_APPWRITE_ACTIVITY_COLLECTION_ID='657c099dd2eda1ddebb' VITE_APPWRITE_FRIENDS_COLLECTION_ID='681b28b356casds5dd28d' VITE_APPWRITE_TRANSACTION_COLLECTION_ID='65aasd54f3a07aec3c8' ```  ---  ### üöÄ **5. Run the Project**  Finally, start the development server:  ```bash npm run dev ```  ---  ### üåê **6. Deploy on Vercel**  1. **Deploy on Vercel**:     - Go to [Vercel](https://vercel.com/) and sign in or sign up.    - Connect your **GitHub** account and select the Git repository of the project you want to deploy.    - Follow the prompts to deploy your project. Vercel will handle the deployment and provide you with a live URL once completed.    - Add .env.local in Vercel Spliwise Project Go in Settings Environment Variables And Paste Your all  .env.local in that  2. **Configure Appwrite Integration**:    - After deployment, copy the Vercel deployment URL (e.g., `https://your-project-name.vercel.app`).    - Log in to your **Appwrite** dashboard.    - Go to your **Project Overview** and scroll down to the **Integrations** section.    - Click **Add Platform** and select `Web App`.    - In the `Name` field, paste your Vercel deployment URL, and in the `Hostname` field, enter `*.vercel.app`. 3. **Complete Setup**:    - Skip any additional configurations unless required by your project setup.    - Your app is now deployed and integrated with Appwrite! üéâ  ---  ### üìß **Need Help?**  If you encounter any issues with the database or need further assistance, feel free to email me at: p1.sirsat1998@gmail.com.  ---  ### üé• **Appwrite Database Guide Video**  https://github.com/user-attachments/assets/a09c0edf-5d71-4417-b03d-39db91105be6  ---  ### üìë **Documentation**  #### üìÇ **Google Drive**  [Google Drive Complete Application DOC](https://drive.google.com/drive/folders/1nG6kY0vprGZ4sazl2pUZ6ee1TMno9BVI?usp=drive_link)  #### üìÑ **DOC PDF**  [splitwise_documentation.pdf](https://github.com/user-attachments/files/16871474/splitwise_documentation.pdf)  ---  ### üìä **Database Design**  ![DataBase Design 1](https://github.com/user-attachments/assets/26d84039-10e9-4d9b-b04d-442017fbcb80)  ---  ### üîÑ **Flowchart**  ![FlowChart](https://github.com/user-attachments/assets/6add1f3d-8f64-43e9-8c78-cedcd18032d4)  ---  ### üí∏ **Simplify Debt Flowchart**  ![Simplify Debt Flow](https://github.com/user-attachments/assets/9114c101-d851-48d5-a7ab-15f1b1b69f5c)  ---  # Login  ![Screenshot (463)](https://github.com/PawanSirsat/SplitWise/assets/48860105/6feaf149-4f67-474b-ac5b-a61f6eacbb63)  # Signup  ![Screenshot (464)](https://github.com/PawanSirsat/SplitWise/assets/48860105/71c31b5f-beee-4a61-87ff-3398fdd6e98f)  # Home  ![Screenshot (457)](https://github.com/PawanSirsat/SplitWise/assets/48860105/b09536d8-43a5-402d-8590-7b6c4edbfd59)  # All Activity  ![Screenshot (458)](https://github.com/PawanSirsat/SplitWise/assets/48860105/63a37885-204b-4d6c-b1e1-6d2f09b3dcd8)  # Group Activity  ![Screenshot (460)](https://github.com/PawanSirsat/SplitWise/assets/48860105/ae8b0631-8a98-49d1-96cb-9810a0673586)  # Profile  ![Screenshot (459)](https://github.com/PawanSirsat/SplitWise/assets/48860105/8f6f3be7-7883-483b-9e23-05aaf8fcc29f)  ### Built With  React - A JavaScript library for building user interfaces. React Router - Declarative routing for React.js. Tailwind CSS - A utility-first CSS framework.  # React + TypeScript + Vite  This template provides a minimal setup to get React working in Vite with HMR and some ESLint rules.  Currently, two official plugins are available:  - [@vitejs/plugin-react](https://github.com/vitejs/vite-plugin-react/blob/main/packages/plugin-react/README.md) uses [Babel](https://babeljs.io/) for Fast Refresh - [@vitejs/plugin-react-swc](https://github.com/vitejs/vite-plugin-react-swc) uses [SWC](https://swc.rs/) for Fast Refresh  ## Expanding the ESLint configuration  If you are developing a production application, we recommend updating the configuration to enable type aware lint rules:  - Configure the top-level `parserOptions` property like this:  ```js export default {   // other rules...   parserOptions: {     ecmaVersion: ""latest"",     sourceType: ""module"",     project: [""./tsconfig.json"", ""./tsconfig.node.json""],     tsconfigRootDir: __dirname,   }, }; ```  - Replace `plugin:@typescript-eslint/recommended` to `plugin:@typescript-eslint/recommended-type-checked` or `plugin:@typescript-eslint/strict-type-checked` - Optionally add `plugin:@typescript-eslint/stylistic-type-checked` - Install [eslint-plugin-react](https://github.com/jsx-eslint/eslint-plugin-react) and add `plugin:react/recommended` & `plugin:react/jsx-runtime` to the `extends` list"
Patatap Game,JavaScript,https://github.com/cdclaw/patatap-game,# Patatap Game Clone  Just for fun!  Built with [Paper.js](http://paperjs.org/) and [Howler.js](https://howlerjs.com/)  Check out [demo](https://cdclaw.github.io/patatap-game/)
URL Shortener,JavaScript,https://github.com/surishortlink/suri,"<h1 align=""center"" width=""100%"">   <img src=""logo.png"" width=""200"" alt=""Suri"" /> </h1>  <h3 align=""center"" width=""100%"">   <i>Your own short links as an easily deployed static site</i> </h3>  <h3 align=""center"" width=""100%"">   <a href=""https://www.npmjs.com/package/@surishortlink/suri"">     <img src=""https://img.shields.io/npm/v/@surishortlink/suri"" alt=""npm"">   </a>   <a href=""https://github.com/surishortlink/suri/actions"">     <img src=""https://github.com/surishortlink/suri/actions/workflows/ci.yml/badge.svg"" alt=""GitHub Actions"">   </a> </h3>  No server-side hosting, serverless cloud functions, or database necessary. Suri static sites can be deployed to Vercel, Netlify, and more (usually for free) in a few minutes.  Suri doesn't care about ""technically superior"" `3xx` server redirects. Suri just wants you to finally use that domain you spend \$59/year on and take back your short links from the Bitlys and TinyURLs of the web.  ## Try It Out  https://surishort.link/gh ‚áí https://github.com/surishortlink/suri  https://surishort.link is an example site that showcases Suri in action. You can check out [the repository for the site](https://github.com/surishortlink/surishort.link) and [the file that manages the links](https://github.com/surishortlink/surishort.link/blob/main/src/links.json) to see how it works.  ## Quick Start  Suri has template repositories that make it super easy to get started. Choose the platform you're deploying to and follow the step by step instructions:  - [Deploy to DigitalOcean](https://github.com/surishortlink/suri-deploy-digitalocean) - [Deploy to GitHub Pages](https://github.com/surishortlink/suri-deploy-github) - [Deploy to Netlify](https://github.com/surishortlink/suri-deploy-netlify) - [Deploy to Render](https://github.com/surishortlink/suri-deploy-render) - [Deploy to Vercel](https://github.com/surishortlink/suri-deploy-vercel)  Not deploying to one of those platforms? No worries. Here are a few generic options that cover most other scenarios, whether that's a different cloud provider or hosting it yourself:  - [Deploy with Docker](https://github.com/surishortlink/suri-deploy-docker) - [Deploy with Node.js](https://github.com/surishortlink/suri-deploy-nodejs)  ## How It Works  ### Manage Links  At the heart of Suri is the `links.json` file, located in the `src` directory, where you manage your links. All of the template repositories include this file seeded with a few examples:  ```json {   ""/"": ""https://www.youtube.com/watch?v=CsHiG-43Fzg"",   ""1"": ""https://fee.org/articles/the-use-of-knowledge-in-society/"",   ""gh"": ""https://github.com/surishortlink/suri"" } ```  It couldn't be simpler: the key is the ""short link"" path that gets redirected, and the value is the target URL. Keys can be as short or as long as you want, using whatever mixture of characters you want. `/` is a special entry for redirecting the root path.  ### Build Static Site  Suri ships with a `suri` executable file that generates the static site from the `links.json` file. The static site is output to a directory named `build`.  All of the template repositories are configured with a `build` script that invokes this executable, making the command you run simple:  ```bash npm run build ```  When you make a change to the `links.json` file, simply re-run this command to re-generate the static site, which can then be re-deployed. Many of the platforms that Suri has template repositories for are configured to do this automatically.  ### Config  Configuration is handled through the `suri.config.json` file in the root directory. There is only one option at this point:  | Option | Description                                                        | Type    | Default | | ------ | ------------------------------------------------------------------ | ------- | ------- | | `js`   | Whether to redirect with JavaScript instead of a `<meta>` refresh. | Boolean | `false` |  ### Public Directory  Finally, any files in the `public` directory will be copied over to the `build` directory without modification when the static site is built. This can be useful for files like `favicon.ico` or `robots.txt` (that said, Suri provides sensible defaults for both).  ## Upgrading v0 to v1  If you previously forked/cloned this repository when it was on version 0.1 through 0.5.1, you'll notice a few differences now with version 1.  Version 1 solves three main issues with version 0:  1. **Difficult to update.** Since you forked/cloned this repository in v0, you    had to manually merge in upstream changes if you wanted the latest version.    This often led to merge conflicts and wasn't an easy, intuitive process most    of the time. v1 fixes this by turning Suri into a proper npm package that you    can update just like any other dependency. 2. **Lots of unnecessary files.** v0 included deployment and config files for    every platform you could deploy to (Vercel, Netlify, etc.). Even if you    deployed to Vercel, you still had `render.yaml` for Render and `app.json` for    Heroku (among others) in your repository. v1 fixes this by having separate    template repositories for each platform, which only include the necessary    files for that platform. 3. **[Eleventy](https://www.11ty.dev/) was overkill.** v0 was built on it for    static site generation. While a great option for most static sites, it was    overkill for the tiny HTML page that Suri generates. Eleventy came with 34 of    its own dependencies, which ultimately resulted in 241 total dependencies    being installed. v1 fixes this by removing Eleventy in favor of a    purpose-built static site generator that doesn't require a single dependency.  So, how do you upgrade? If you only ever edited your `links.json` file, upgrading is simple:  1. Create a new repository based on the template repository for your platform    (see links above). 2. Copy over your `links.json` file. 3. If you changed any of the files in your `public` directory, copy those over. 4. If you set the environment variable `SURI_JS` to `1`, change `js` to `true`    in `suri.config.json`.  If you edited any of the Eleventy files ‚Äì such as the `links.njk` template ‚Äì you probably just want to stick to v0 and continue using Eleventy.  There are a few other noteworthy changes in v1 beyond that:  - The static site is now output to a directory named `build` instead of `_site`. - Configuration is now done through the `suri.config.json` file instead of   environment variables. - Node.js >= v18 is now required, up from v14, which has reached end-of-life. - Removed `npm run clean` to delete the build directory. `npm run build` does   this automatically before each new build. Otherwise, you can manually add it   back if you found it useful. - Removed `npm run dev` to build, watch, and serve the static site during   development. It's overkill for the tiny HTML page that Suri generates. - Removed `npm run lint` to lint with Prettier. You can manually add it back if   you found it useful. - Removed `npm run release` to release a new version of Suri. You can manually   add it back if you want to tag release versions of your repository. - Removed Heroku as a deploy platform because they no longer offer a free tier.   You can still deploy there quite easily if you're willing to pay. - This repository moved from my personal `jstayton` user on GitHub to a new   `surishortlink` organization for all Suri-related repositories.  ## Development  ### Prerequisites  The only prerequisite is a compatible version of Node.js (see `engines.node` in [`package.json`](package.json)).  ### Dependencies  Install dependencies with npm:  ```bash npm install ```  ### Tests  The built-in Node.js [test runner](https://nodejs.org/docs/latest/api/test.html) and [assertions module](https://nodejs.org/docs/latest/api/assert.html) is used for testing.  To run the tests:  ```bash npm test ```  During development, it's recommended to run the tests automatically on file change:  ```bash npm test -- --watch ```  ### Docs  [JSDoc](https://jsdoc.app/) is used to document the code.  To generate the docs as HTML to the (git-ignored) `docs` directory:  ```bash npm run docs ```  ### Code Style & Linting  [Prettier](https://prettier.io/) is setup to enforce a consistent code style. It's highly recommended to [add an integration to your editor](https://prettier.io/docs/en/editors.html) that automatically formats on save.  [ESLint](https://eslint.org/) is setup with the [""recommended"" rules](https://eslint.org/docs/latest/rules/) to enforce a level of code quality. It's also highly recommended to [add an integration to your editor](https://eslint.org/docs/latest/use/integrations#editors) that automatically formats on save.  To run via the command line:  ```bash npm run lint ```  ## Releasing  When the `development` branch is ready for release, [Release It!](https://github.com/release-it/release-it) is used to orchestrate the release process:  ```bash npm run release ```  Once the release process is complete, merge the `development` branch into the `main` branch, which should always reflect the latest release.  ![piratepx](https://app.piratepx.com/ship?p=e91ddd1b-31ad-4c36-b03e-be4a1e9a7678&i=suri)"
ID Card Generator,JavaScript,https://github.com/RahulBRB/ID-Card-Generator,"# ü™™ ID-Card-Generator ü™™ Welcome to This Repository which is contributing to Hacktoberfest2023! It's a mini fun project where you can apply your HTML, CSS as well as Javascript skill to transform the simple looking app completely.   ## üìå Introduction : This repository contains a simple and elegant web-based ID card maker. Users can input their information, including name, job title, location, and a brief ""About Me"" section (limited to 200 characters). They can also provide a URL for their profile image. The application generates an ID card with a realistic design, featuring rounded corners, shadows, and customizable user details.   <p align=""center"">     <img width=""1000"" src=""./assets/1.png"" alt=""banner""> </p>    ## ‚ùìHow to Contribute  We welcome contributions from everyone. To get started, follow these steps:  ### üëâ Getting Started  1. Fork the repository and clone it locally. 2. Install [dependency/tool] by running `command`.   ### üëâ Making Changes  - Explain the structure of your project. - Provide guidance on coding standards.   ### üëâ Creating a Pull Request  1. Make sure your changes are tested and linted. 2. Create a new branch: `git checkout -b feature/your-feature-name`. 3. Commit your changes: `git commit -m 'Add some feature'`. 4. Push to the branch: `git push origin feature/your-feature-name`. 5. Submit a pull request.  ## ü§ù Our Contributors ‚ù§Ô∏è <a href=""https://github.com/RahulBRB/ID-Card-Generator/graphs/contributors"">   <img src=""https://contrib.rocks/image?repo=RahulBRB/ID-Card-Generator"" /> </a>"
Movie Seat Booking App,JavaScript,https://github.com/nathanifill/movie-seat-booking,"# movie-seat-booking Vanilla JS Movie seat booking system using localStorage  ## Motivation  I wanted to create a real cinema feel on the desktop, using perspective and gradients to create the feel of looking down at the cinema screen from above.  This was also a chance for me to use localStorage to hold onto the selected seat data in the user's browser to ensure it was there should they refresh or come back to the page.  ## Tech used HTML, CSS and JavaScript. No frameworks were harmed in the making of this project.  ## Features The main feature, which is only visible on larger screens, is the movie screen which shows gifs of the movie which has been selected. This is easily updatable by adding to the JSON object in the JavaScript file.  ## Credits I got inspired to build this after seeing [Brad Traversy's Movie Seat Booking from his Vanilla Web Projects](https://github.com/bradtraversy/vanillawebprojects/tree/master/movie-seat-booking), so I've been heavily influenced by that.  I looked at his code, recreated it exactly, wrote down the project specifications, and then created this mock booking system from scratch based solely on those project specifications. Any other similarities are purely coincidental.  ## License This is under an MIT licence. You can do whatever you want wth the code as long as you include the original copyright and license notice in any copy of the software/source.  MIT ¬© [Nathan Ifill](https://www.nathanifill.com)"
Whack-A-Mole Game,JavaScript,https://github.com/kubowania/whac-a-mole,"# whac-a-mole Vanilla JavaScript game   Learn to make a simple grid-based game using HTML, CSS and JavaScript. The idea of whack-a-mole is simple! The player needs to hit the grid with the mole in as many times as possible until the time runs out. In this tutorial, we will cover:  - document.querySelector - textContent - forEach - addEventListener - setInterval - classList.add  ...and much more.  View the full tutorial [here]:(https://youtu.be/rJU3tHLgb_c)  As always, if you get stuck or have any questions, please do comment below and I will be sure to get back to you!  I would love to see what you made so please tag me on twitter on @ania_kubow or mention me on Youtube so I can find it and see."
Word Scramble,JavaScript,https://github.com/jeskodes/word-scramble,"# Animal Scramble Word Game  <br> <p align=""center"" width=""100%"">     <img width=""72%"" src=""documentation/animal-scramble-demo.gif""> </p> <br>  [Animal Scramble](https://jeskodes.github.io/word-scramble/) is an anagram game where the player has to unscramble the jumbled up animal words. The game has been designed to be quick to start with obvious game play and few explanations.  The target players are people of all ages who are looking to distract themselves for a few minutes with a lighthearted word game.  Play [Animal Scramble!](https://jeskodes.github.io/word-scramble/)  ## Design  ### Research  #### Features Research  The original idea for the project came from my enjoyment of anagram and code word games. The Conundra app is great to play to pass the time, especially while waiting for appointments or travelling. I wanted to develop something more basic, but similar to Conundra.  <br> <p align=""center"" width=""100%"">     <img width=""24%"" src=""documentation/conundra-demo.gif""> </p> <br>  Features of Conundra:  - Select three different levels: Practice, Easy, Hard. - Gameplay: Buttons to scramble the word again, pass, start again. Scoreboard at the end showing correct and incorrect guesses.  #### Design Research  - I researched word game apps for layout and colour palette ideas. - The main findings were that the apps used two - three colours, usually one being white, and green was a common main colour. - Astra Wordsearch and Wordle-type games are two examples:  <br> <p align=""center"" width=""100%"">     <img width=""60%"" src=""documentation/astra-wordsearch-research.png""> </p> <br> <br> <p align=""center"" width=""100%"">     <img width=""60%"" src=""documentation/wordle-research.png""> </p> <br>  ##### Tutorial Research and Development:  I searched for JavaScript word scramble tutorials to develop my knowlege of JavaScript and which included the basic features I wanted to include in the game.  A large part of the functionality of Animal Scramble is based on a tutorial by Laurence Svekis: [JavaScript Create 5 Fun Word Games by Laurence Svekis](https://www.udemy.com/course/javascript-games/learn/lecture/22686281?start=120#content). Additions, deletions, substitutions and changes are noted in the code.  ##### Word Scramble by Laurence Svekis - Final Game Play  <br> <p align = ""center"" style=""background-color: gray"">   <img src=""documentation/word-scramble-udemy-demo.gif"" width=""30%"" style = ""border:2px solid gray""> </p> </br>  I also referred to two similar tutorials by [CodingNepal.](https://www.youtube.com/watch?v=4-s3g_fU7Vg&t=992s)  ##### Word Scramble by CodingNepal - Final Game Play  <br> <p align = ""center"">   <img src=""documentation/codingnepal-word-scramble.gif"" width=""30%"">  </p> </br>  ### Design  - A minimally cluttered single gameplay container which the user can pick up and play almost immediately. - Intuitive game play: minimal text on the page and only a few instructions needed to understand game play. - Slight use of border-radius for container, buttons and input box. - Typeface: Serif font for labels such as name of game, buttons - slightly 70's style; used [google fonts](https://fonts.google.com/)   Patua One. Sans-serif font for input box and displaying correct word; used [google fonts](https://fonts.google.com/) Roboto. - Colour Palette: Despite research that word games tend to use a more muted colour palette with an emphasis on greens and white, the Animal Scramble game is targeted at users of all ages from 10yrs upwards. Animal Scramble's aim is to be lighthearted and fun. Therefore a more brightly coloured, retro theme and typeface was chosen with a colour palette to reflect this:  <br> <p align = ""center"">   <img src=""documentation/colour-palette-ws.png"" width=""39%""/> </p> </br>  <br> <p align = ""center"">   <img src=""documentation/animal-scramble-static-img.png"" width=""39%""/> </p> </br>  #### Wireframes  <br> <p align=""center"" width=""100%"">     <img width=""72%"" src=""documentation\wireframes-animal-scramble.png""> </p> <br>  #### Wireframes - Starting from a desktop first approach.  - Main gameplay in a fixed sized <div> container; media queries will be applied if needed for tablet and mobile.  - Three different displays of content controlled with JavaScript:    - Start Page   - Game Play   - Game Over  ## User Stories and Features  #### User Story 1:  1. As a child of approximately 8 - 13yrs who likes word games. 2. I want a quick game to keep me occupied. 3. So that I can test my skills and won't get bored travelling or waiting.  #### User Story 2:  1. As an adult who enjoys word games. 2. I want a quick game that will challenge me and keep me occupied. 3. So that I can relax with an engaging challenge.  #### User Story 3:  1. As an adult who enjoys word games. 2. I want a quick game that will challenge me and keep me occupied. 3. So that I can pass the time while waiting or travelling.  #### User Story 4:  1. As an older adult who enjoys word games and is retired. 2. I want a quick game that will challenge me. 3. So that I can keep my mind sharp.  ##### User Steps:  - General search: SEO tags - brain-training, conundrum, letters and words, anagram, game, guessing, words, scramble, animals.  - Easily open the app/webpage and play Animal Scramble straight away. The design communicates how to play without the need for lots of explanations.  ##### Minimum Viable Product: Features and Acceptance Criteria:  - Single page. - The user first arrives at the game title and game play. - Start Button. - Input box for guessing the word. - Answer: The correct answer is always shown after the guess, whether the user guesses correctly or incorrectly. - Next Button for next word. - A way to skip a guess. - A restart button to start a new game. - Scoring. - Accessibility: Tab controls can be used to play the game.  ## Technologies Used  - JavaScript, HTML and CSS. - VScode - Coding Editor. - Chrome Dev tools - [Google Fonts.](https://fonts.google.com/) - [Favicon Icons](https://favicon.io) - [fontawesome](https://fontawesome.com/) for social media icons. - [github](https://github.com/) version control. - [Repl:it](https://replit.com/) for experimenting with code. - [Axe dev tools](https://www.deque.com/axe/devtools/) to test accessibility. - [Webaim](https://wave.webaim.org/) WCAG Contrast checker. - [W3C CSS Validation Service](https://wave.webaim.org/) to validate css. - [W3C Markup Validation Service](https://validator.w3.org/#validate_by_input) to validate html. - [JSON Formatter - Convert Array to JSON](https://jsonformatter.curiousconcept.com/#) - [Validating JSON Object: jsonlint.com](https://jsonlint.com/) - [JavaScript Linting: jshint.com](https://jshint.com/) - [Convert mp4 to gif using ezgif.com](https://ezgif.com/)  ## Current Features  <br> <p align=""center"" width=""100%"">     <img src=""documentation/animal-scramble-demo.gif"" width=""68%""> </p> <br>  ### Main Game Area - Background image covers entire screen.  - One main section with fixed div container for whole game.  - Footer with link to [github profile](https://github.com/jeskodes).  #### Start Game - Title  - Rules - Start button <br> <p align=""center"" width=""100%"">     <img src=""documentation\start-game.png""width=""33%""> </p> <br>  #### Game Play - Title - Dynamic Scoreboard - Scrambled word - Input box to type in guess. - Placeholder text in textarea ""Enter to Skip"".  - Restart button <br> <p align=""center"" width=""100%"">     <img src=""documentation\gameplay.png""width=""33%""> </p> <br>  Make a guess, then - Title - Scrambled word unscrambles with correct or incorrect guess.  - Scrambled word background goes blue for correct or pink for incorrect.  - Button ""Click for Next Word"" - Restart button. <br> <p align=""center"" width=""100%"">     <img src=""documentation\guess-word.png""width=""33%""> </p> <br>  After 5 guesses (correct or incorrect) then - Game Over - Score - Restart button. <br> <p align=""center"" width=""100%"">     <img src=""documentation\game-over.png""width=""33%""> </p> <br>  #### Restart Button - Reloads page.  - Animation used to smooth transition.   #### Buttons - :hover used to change color.  - cursor pointer added.   ### Features for Future Versions:  - Fix accessibility issue so can use tab controls to play game; split game into more functions so greater flexibility.  - Add to Json File and create levels of difficulty, Easy, Medium, Hard.  - Add additional categories, e.g. Food, Countries, Actions; add different start pages and themes for each category.  - Add buttons to choose difficulty level.  - Store the 5 words per game in a temporary array with for loop and create scoreboard at end showing correct and incorrect words.  - Best of 5 games then final score.  - Timer. - Split scrambled word into individual tiles.  - Experiment with [""Fisher-Yates"" shuffle algorithim](https://www.geeksforgeeks.org/shuffle-a-given-array-using-fisher-yates-shuffle-algorithm/) to randomise selection of words.  - More celebratory and dynamic ""Game Over"" or ""You Won"".   ## Testing  ### BUGS  #### Accessibility Bugs  **BUG** - The player is using tab controls. - The player presses ""Enter"" to play the game and for the next word. - The winChecker() function - which checks if the words match - immediately checks the word and the player cannot make a guess. The game cannot be played without using the mouse.  <br> <p align = ""center"">   <img src=""documentation/bug-accessibility-can't-use-tab-controls.gif"" width=""60%""/> </p> </br>  **Fixes**  1. Make input textbox visible throughout. This resulted in two further bugs: - The target word persisted and any press on ‚ÄúEnter‚Äù resulted in getting an infinite number of correct scores on the same correct guess. - The input (inWord) could be displayed but was disabled. <p align = ""center"">   <img src=""documentation\fix1-bug-ax-tab.png"" width=""60%""/> </p>  2. Added eventListener to stop default behaviour when pressing Enter - did not fix(removed). 3. Separated out functions to create a separate function after winChecker(). 4. Added `btn.onfocus();` after function checked if words matched - this worked to automatically focus on the button but did not fix the bug.  **Result** - Unable to fix the bug. This would be targeted in later versions with a separate function at the point of refreshing the scrambled word.  It is disappointing that Animal Scramble V1 is not accessible using tab controls.  #### Game Play General Bugs  **BUG**  - The output(myWordsFromJson) does not unscramble the target word, even if the player guesses incorrectly so the player won't know what the right answer was; this could cause the player frustration.   **Fix**  - Add line of code to winChecker() fucntion - both correct and incorrect guesses:    <br> <p align = ""center"">   <img src=""documentation/bug-fix-unscramble-word.png"" width=""60%""/> </p> </br>    **BUG**  - Gameplay does not stop at 5 turns and can play an infinite amout of times if keep guessing incorrectly.     <br> <p align = ""center"">   <img src=""documentation/BUG-infinite-turns.png"" width=""33%""/> </p> </br>    **Fix**  -Create maxGuesses variable to count guesses and add to gamePlay() function; stop play at 5 guesses.     ```javascript   function gamePlay() {   if (myWordsFromJson.length <= 0 || maxGuesses === 5) {     //EDIT: Add in ""or"" maxGuesses for game over.     console.log(""game over"");     console.log(maxGuesses);`      ```  **BUG** - Game play stops at 5 turns but the player has no way to play again unless they refresh the page.   **Fix** - Add restart-btn.   ```javascript const restart = document.createElement(""button""); ```  - Add EventListener to restart button to refresh page when clicked.   ```javascript restart.addEventListener(""click"", (e) => {   window.location.reload(); }); ```  **BUG** - When refresh page the transition is jerky.   **Fix** - Add CSS animation to start of game to mask.   **BUG** - User testing reported that on android there was no way to press ""Enter"" to skip; the ""go"" button did not work.   **Fix**  - Added EventListener for pressing ""go"" when focus on input box = carry out winChecker function.   ```javascript  inWord.addEventListener(""keyup"", (e) => {   console.log(e);   if (inWord.value.length === game.sel.length || e.key === ""Enter"") {     winChecker(); //run the winChecker function   } });  ```  #### Textbox Specific Bugs  **BUG** - Initially used four animal words in array inside script.js to scramble; the words were four letters in length or shorter.  - Added larger array of animal words to separate js file and the text went outside of input and output boxes.   **Fix** - Added classes to inWord(textbox) and myWordsFromJson (scrambled words) and styled with CSS.  - Additionally added classes to start and refresh buttons in order to style in CSS.   **BUG** - User testing on ios reported that the game was showing an incorrect response as correct.  - Investigation revealed that the words were being autocorrected across multiple devices.   **Fix** - Add autocomplete = ""off"" spellcheck = ""false"" to input html.   ```html  <section class = ""game-play"">   <div id=""game-area"">     <h1 id=""heading-wordscramble"">Animal Scramble</h1>       <input type=""text"" class =""input-text"" id = ""input-word"" name = ""inputtext"" autocomplete = ""off"" spellcheck = ""false"" autocomplete = ""off"" placeholder = 'Press Enter to Skip' aria-label=""Type word here""   ```   **BUG**  - Can enter numbers and special characters into input box.   **Fix** - Add JavaScript code snippet to html to only allow letters.   ```javascript         onkeypress=""return (event.charCode > 64 &&  	    event.charCode < 91) || (event.charCode > 96 && event.charCode < 123)""        ```  **BUG**  - Input box is case sensitive and counting guesses in lowercase and caps as wrong.  - Investigation with DevTools showed correct guess in console log as incorrect:   <p align = ""center"">   <img src=""documentation/BUG-case-sensitive-textbox.png"" width=""60%""/> </p>   **Fix**  - Add toLowerCase() to winChecker() function.   ```javascript  function winChecker() {   if (inWord.value.toLowerCase() == game.sel) {     //EDIT: added .toLowerCase()      ``` ### Verification  ### Responsiveness    <br> <p align = ""center"">   <img src=""documentation/animal-scramble-responsive.png"" width=""60%""/> </p> </br>  [Animal Scramble](https://jeskodes.github.io/word-scramble/) is responsive across a large range of devices. There are no media queries needed as: -  The main game container div is set at a fixed size using rem. -  The input box, buttons and output div are also set to a fixed size using rem so the format does not break when resized.  - Responsiveness was tested using Chrome DevTools, [Responsivedesignchecker.com](https://responsivedesignchecker.com), and user testing on Android Phone, ios ipad air 2, iphone SE 2020, iphone 10, windows laptop 1600 x 1200.  ---  ### Axe Chrome DevTools and WebAIM Contrast Checker  - No issues raised.    <br> <p align = ""center"">   <img src=""documentation/ax-testing-animal-scramble.png"" width=""68%""/> </p> </br>  ---  ## Validator Testing  ### JSHint Validator  There were no warnings or errors when passed through the [JSHint](https://jshint.com/) validator.   <br> <p align = ""center"">   <img src=""documentation\jshint-validator.png"" width=""60%""/> </p> </br>  ### W3C CSS Validation Service  The [W3C CSS Validation Service](https://jigsaw.w3.org/css-validator/#validate_by_input) found no errors.   ### W3C Markup Validation Service  ##### The [W3C Markup Validation Service](https://validator.w3.org/#validate_by_input) was used to validate the HTML.  Following minor fixes such as an unclosed <div>, a quotation mark the wrong way round in the meta data and two duplicate attributes, the HTML passed with no warnings or errors.  ---  ## Deployment  [Animal Scramble](https://jeskodes.github.io/word-scramble/) was deployed using git, github and vscode.  ## Credits  - [JavaScript Create 5 Fun Word Games by Laurence Svekis](https://www.udemy.com/course/javascript-games/learn/lecture/22686281?start=120#content) - [Word Scramble Game Youtube Tutorial by Coding Nepal](https://www.youtube.com/watch?v=4-s3g_fU7Vg&t=992s) - [Word Scramble Game Tutorial by Coding Nepal](https://www.codingnepalweb.com/word-scramble-game-html-javascript/)  - [Youtube Tutorial: 3 Ways to code Rock Paper Scissors with Ania Kubow](https://www.youtube.com/watch?v=RwFeg0cEZvQ)  - [Git Hub Repo - Learning how to add reset button and replay](https://github.com/mariaalouisaa/pokemon-top-trumps/blob/main/index.html)  - [Tic Tac Toe Tutorial - Research how to refresh game](https://www.youtube.com/watch?v=JsErMawwdOw)  - [W3 Explanation of keyboard and mouse events: w3.org](https://www.w3.org/WAI/GL/WCAG20/WD-WCAG20-TECHS-20071102/SCR20.html)  - [JavaScript Keyboard Events Tutorial: the keyup and keydown Event Listeners](https://www.youtube.com/watch?v=OiYmhhe6Inc)  - [Youtube Tutorial Event Handlers and Event Listeners](https://www.youtube.com/watch?v=xogpUfUL5kY)  - [Tutorial Listening to Multiple Events in Vanilla JS](https://gomakethings.com/listening-to-multiple-events-in-vanilla-js/)  - [Animals Array](https://gist.github.com/borlaym/585e2e09dd6abd9b0d0a)  - [Colour Palettes and Hex Codes: coolor.co](https://coolors.co/)  - [Background Image from iStockPhoto.com](https://www.istockphoto.com/)  - [Make Reload of Page Smoother with CSS Animation and @keyframes: geeksforgeeks.org](https://www.geeksforgeeks.org/how-to-create-fade-in-effect-on-page-load-using-css/)  - [JSON Formatter - Convert Array to JSON](https://jsonformatter.curiousconcept.com/#)  - [Create new button from tutorial: www.3schools.in](<https://www.3schools.in/2022/08/how-to-create-button-with-id-js.html#:~:text=Use%20the%20createElement()%20method%20and%20set%20in%20a%20variable,an%20id%20to%20that%20button>)  - [Hex Colors from Background Image: imagecolorpicker.com](https://imagecolorpicker.com/)  - [YouTube: JavaScript Array Methods You Should Know | JSON | Object Arrays ](https://www.youtube.com/watch?v=kMVMxyp0KVo)  - [Validating JSON Object: jsonlint.com](https://jsonlint.com/)  - [Favicon Icons: favicon.io](favicon.io)  - [JavaScript Linting: jshint.com](https://jshint.com/)  - [Convert mp4 to gif using ezgif.com](https://ezgif.com/)  - [Center align an img in README.md from stack overflow](https://stackoverflow.com/questions/12090472/how-do-i-center-an-image-in-the-readme-md-file-on-github/62383408#62383408)  - [Video Bugs and Functionality Using Bandicam.com](https://www.bandicam.com/)"
Rock-Paper-Scissors Game,JavaScript,https://github.com/chauhan-hardik/Rock-Paper-Scissors,"# Rock, Paper, Scissors Game  ![Game Screenshot](Screenshots/desktop-1.png)   Welcome to the Rock, Paper, Scissors Game project! This simple and interactive web-based game is built using HTML5, CSS3, and JavaScript. Test your luck and strategy against the computer in the classic Rock, Paper, Scissors duel.  ## Live Demo  Check out the live demo of the Rock, Paper, Scissors Game [here](https://chauhan-hardik.github.io/Rock-Paper-Scissors/).  ## Features  - **User-friendly Interface:** Enjoy a clean and intuitive design for an immersive gaming experience.  - **Responsive Layout:** The game adapts to different screen sizes, ensuring a seamless play on various devices.  - **Score Tracking:** Keep track of your wins against the computer with a visible score display.  - **Engaging Messages:** Receive informative and entertaining messages throughout the game.  ## How to Play  1. **Choose Your Move:**    - Click on the Rock, Paper, or Scissors images to make your move.  2. **Game Result:**    - The game will determine the winner based on your choice and the computer's random move.  3. **Score Updates:**    - Track your score to see who is dominating the Rock, Paper, Scissors battlefield.  ## Installation  1. Clone the repository to your local machine:     ```bash    git clone https://github.com/chauhan-hardik/Rock-Paper-Scissors.git  ## Contributing Contributions are welcome! If you have ideas for improvements, find bugs, or want to enhance the project, please open an issue or submit a pull request."
BMI Calculator,JavaScript,https://github.com/AbinandhMJ/BMI-Calculator,"# BMI Calculator This BMI (Body Mass Index) calculator is a web application built using HTML, CSS, Bootstrap 5, and JavaScript. It allows users to quickly and easily calculate their BMI based on their height and weight. BMI is a simple way to determine if a person's weight is within a healthy range relative to their height.  ## Features - Input fields for height (in meters) and weight (in kilograms). - Calculates BMI when the ""Calculate BMI"" button is pressed. - Displays the BMI value along with a color-coded result (Underweight, Normal, Overweight, Obese). - Provides a brief explanation of the BMI result. - Responsive design for various screen sizes.  ## Technologies Used - HTML: Provides the structure and layout of the calculator. - CSS: Styles the calculator, including responsive design for different devices. - Bootstrap 5: Utilized for styling, layout, and components. - JavaScript: Performs the BMI calculation and updates the UI dynamically.  ## Installation 1. Clone this repository to your local machine: `git clone https://github.com/yourusername/bmi-calculator.git ` 2. Open the `index.html` file in your web browser or host it on a web server.  ## Usage 1. Open the BMI calculator in your web browser.  2. Enter your height in meters (e.g., 1.75) in the ""Height"" field.  3. Enter your weight in kilograms (e.g., 70) in the ""Weight"" field.  4. Click the ``""Calculate BMI""`` button.  5. Your BMI value and result will be displayed along with an explanation.  ## Demo You can try out a live demo of the BMI Calculator here. https://fitness-bmi-calculator.netlify.app/  ## Screenshots  ![image](https://github.com/AbinandhMJ/BMI-Calculator/assets/99226172/71284f1c-88f8-417f-baeb-b976799192be) ![image](https://github.com/AbinandhMJ/BMI-Calculator/assets/99226172/d4d2e28a-7a54-43fd-9024-78b8c4045bd7)  To calculate a new BMI, simply update the height and weight fields and click the button again.  ## Acknowledgments - This BMI calculator is based on the World Health Organization's (WHO) BMI classification. - BMI calculations should not be used as a sole indicator of a person's health. Consult with a healthcare professional for a comprehensive assessment.  ## Feedback and Contributions If you have any feedback or would like to contribute to this project, please create an issue or a pull request on the GitHub repository. Your input is greatly appreciated!"
Random Joke AJAX Project,JavaScript,https://github.com/ryanbyrne28/project-1,"# Tell Me a Joke  A simple jquery ajax app pulling API data.     I implemented a simple joke generator using jquery and ajax to pull data from an API source with all the said jokes.  Some of the challenges I faced: - Filtering the API with appropriate jokes - Overall AJAX functionality  - Not getting carried away with CSS features.  - ## Features  - Click a button to generate a random joke.    ## Technologies Used  - HTML - JavaScript - CSS    ## Future Enhancements   - Ability to input a keyword for jokes - option for several part jokes  ## Link To Get Joke https://ryanbyrne28.github.io/project-1/  <img width=""1437"" alt=""Screenshot 2023-03-22 at 3 31 50 PM"" src=""https://user-images.githubusercontent.com/128634690/227034141-6f9c947c-115c-43ea-a3af-0f23fce2d544.png"">  <img width=""1437"" alt=""Screenshot 2023-03-22 at 3 29 44 PM"" src=""https://user-images.githubusercontent.com/128634690/227034238-e2b2b556-c88d-4bfe-a892-a87f3deb64f9.png""> "
Countdown Timer Project,JavaScript,https://github.com/sanjudhritlahre/JS-CountDown-Timer,"# JS-CountDown-Timer ""JavaScript CountDown Timer"" is a handy web-based tool that enables users to set countdowns for various events. It's built using JavaScript, offering a sleek and intuitive interface for precise time tracking and anticipation of important deadlines.  # <h1> JavaScript CountDown Timer </h1>  <img src=""https://d35z3p2poghz10.cloudfront.net/apps/thirdparty/powr-countdown-timer/countdown-timer-banner.png""/>  A simple and customizable countdown timer implemented in JavaScript.  ## Features  - Set a specific countdown time in hours, minutes, and seconds. - Display the remaining time in a user-friendly format. - Start, pause, and reset the countdown timer. - Customizable visual styling using CSS. - Trigger a callback function when the countdown reaches zero.  ## Demo  Check out the live demo of the countdown timer: [Live Demo](https://example.com/countdown-timer-demo)  ## Getting Started  ### Installation  1. Clone the repository:     ```bash    git clone https://github.com/your-username/javascript-countdown-timer.git    ```  2. Include the `countdown-timer.js` file in your HTML:     ```html    <script src=""path/to/countdown-timer.js""></script>    ```  3. Add the necessary HTML markup to your page:     ```html    <div id=""countdown""></div>    ```  ### Usage  1. Create a new instance of the CountdownTimer:     ```javascript    const countdownTimer = new CountdownTimer('countdown', {      hours: 1,      minutes: 30,      seconds: 0,      onTick: handleTick,      onComplete: handleComplete    });    ```  2. Customize the countdown timer options as needed:     - `hours` (optional): Number of hours for the countdown (default: 0).    - `minutes` (optional): Number of minutes for the countdown (default: 0).    - `seconds` (optional): Number of seconds for the countdown (default: 0).    - `onTick` (optional): Callback function called on every tick of the timer.    - `onComplete` (optional): Callback function called when the timer reaches zero.  3. Start the countdown timer:     ```javascript    countdownTimer.start();    ```  ### Styling  The countdown timer can be styled using CSS. The main container has the `countdown-timer` class, and each unit (hours, minutes, seconds) has its own class (`countdown-timer__hours`, `countdown-timer__minutes`, `countdown-timer__seconds`).  ```css .countdown-timer {   /* Your styles here */ }  .countdown-timer__hours {   /* Your styles here */ }  .countdown-timer__minutes {   /* Your styles here */ }  .countdown-timer__seconds {   /* Your styles here */ } ```  ## Contributing  Contributions are welcome! If you find any issues or have suggestions for improvements, please open an issue or submit a pull request.  ## License  This project is licensed under the [MIT License](LICENSE)."
Fruit Cutter Game,JavaScript,https://github.com/AchillesKarki/fruits-slice-game,"* This is a fruit slicing game. It is built using HTML, CSS and JQuery. * Fruits will drop down from above, use your mouse cursor to slice them!! * The demo of this application can be found here: https://fruits-slice.netlify.com"
Seat Booking App,JavaScript,https://github.com/Rajnandanibais/seatbookingapp,# seatbookingapp  seat booking app using js
Chinese Checkers Game,JavaScript,https://github.com/fakegeek/Chinese-Checkers,# Chinese-Checkers HTML5 &amp; Canvas Implementation of the game Chinese Checkers
Time Table Maker,JavaScript,https://github.com/Sight-Innovation/Auto-TimeTable-Generator-for-Schools,# Auto Time Table Generator For Schools This repo contains all the algorithm for the auto time table generator. Also comments was added to the javascript file to aid the usage of the code by anyone.
Testimonial Slider,JavaScript,https://github.com/refinedguides/testimonial-slider,"# Testimonial Slider  A basic JavaScript implementation of a responsive and accessible testimonial slider.  ### Features  - [x] Prev / Next navigation - [x] Dot navigation - [x] Swipe navigation - [x] Auto slide on page load  ### Screenshots  ![Testimonial Slider](https://raw.githubusercontent.com/refinedguides/testimonial-slider/main/screenshot.png)  ### Support Me  If you'd like to support me, consider buying me a coffee. Thanks!  [![""Buy Me A Coffee""](https://www.buymeacoffee.com/assets/img/custom_images/orange_img.png)](https://www.buymeacoffee.com/refinedguides)"
Contact Form For Portfolio with Firebase as Realtime DB,JavaScript,https://github.com/bradtraversy/firebasecontact,"# Firebase Contact Form  Mobile first, responsive contact from that sends data to a firebase database"
Crossword Solver,JavaScript,https://github.com/ajvillegas/crossword-solver,"# Crossword Puzzle Solver  This app will return word recommendations based on given clues and known letters to help you solve your next crossword puzzle.  It uses the [Datamuse API](https://www.datamuse.com/api/) to find words that have similar meaning to the phrase entered, as well as words that match the provided spelling pattern.  ## Browser Support  ### Polyfills  This app is using the [Fetch API](https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API) which is based on the [Promise object](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Promise) and is supported by all modern browsers, [except Internet Explorer and older mobile browsers](https://caniuse.com/#search=fetch). If you want to provide support for those browsers you can add polyfills for the Promise object and the Fetch API:  * [Promise() Polyfill](https://vanillajstoolkit.com/polyfills/promise/) (Pushes support back to IE9) * [fetch() Polyfill](https://vanillajstoolkit.com/polyfills/fetch/) (Pushes support back to IE10)  You can also use the [Polyfill.io](https://polyfill.io/v3/) service in order to automate this process.  ### ES6 Syntax  The [ES6 template literals](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Template_literals) used in the app make it easier to add JSON data into a string, but are [not supported in Internet Explorer and some mobile browsers](https://caniuse.com/#search=template%20literals).  To add support for ES6 syntax such as template literals, you must transpile the code to ES5 syntax using a compiler tool like [Babel](https://babeljs.io/repl)."
ToDo App,React JS,https://github.com/ShaifArfan/react-todo-app,"![React Todo App](./banner.png)  # React Todo App.  A complete todo application with all features.  **live demo: [https://wc-react-todo-app.netlify.app/](https://wc-react-todo-app.netlify.app/)**  **Watch On Youtube: [https://youtu.be/W0Uf_xu350k](https://youtu.be/W0Uf_xu350k)**  ---  ### Made with ‚ù§Ô∏è by [Shaif Arfan](https://www.instagram.com/shaifarfan08/)  Like my works and want to support me?  <a href=""https://www.buymeacoffee.com/shaifarfan08"" target=""_blank""><img src=""https://cdn.buymeacoffee.com/buttons/v2/default-blue.png"" alt=""Buy Me A Coffee"" style=""height: 45px !important;width: 162.75px !important;"" ></a>  ---  ## Project Description  In the project, we will be creating a Complete Todo Application with all features. We will do all the CRUD operations. We will use `React.js` and to manage our states, we will use `Redux`. Also we will learn to make simple animations using `Framer Motion`. This will be a complete `beginner` friendly app. Hope you enjoy it.  ## What we are going to learn/use  - [React](https://reactjs.org/) - [React Redux](https://redux.js.org/) - [Framer Motion](https://framer.com/motion/) - [React icons](https://react-icons.netlify.com/) - [React Hot Toast](https://react-hot-toast.com/) - More...  ## Requirements  - Basic ReactJs knowledge - Basic HTML, CSS knowledge  ## Starter files  You can find all the starter files in `starter-files` branch. You can go to the `starter-files` branch and `download zip` the starter files or You can clone the project and git checkout to `starter-files` branch.  ## Getting Started  The recommended way to get started with the project is to follow the [YouTube tutorial](https://youtu.be/W0Uf_xu350k). You will find all the step-by-step guides. Or you can start the project on your own by following the guide below.  After getting the starter files, you need to go the file directory and run  ```shell npm install ```  and after that start the dev server.  ```shell npm start ```  ## Tools Used  1. Favicon: [Flaticon.com](https://www.flaticon.com/) 1. Code Editor: [VS Code](https://code.visualstudio.com/)  ## Other projects  üìö [All Web Cifar Project Tutorials](https://github.com/ShaifArfan/wc-project-tutorials)  ---  ## FAQ  ### Q: How can i get started?  You can get started by following the [YouTube tutorial](https://youtu.be/W0Uf_xu350k) of this project. Here is the full tutorial video link: [coming soon].  ### Q: What i need to start the project?  Just open your favorite code editor and follow along with the [YouTube tutorial](https://youtu.be/W0Uf_xu350k).  ### Q: What are the prerequisites?  basics of html, css, javascript and some basic knowledge of react is enough to start this project. Rest you will learn in the tutorial.  ### Q: Who the project is for?  The project is for the people who wanna get more skilled in `ReactJs`.  ---  ## Feedback  If you have any feedback, please reach out to us at [@web_cifar](http://instagram.com/web_cifar)  ## Support  For support, join our [Community Group](http://facebook.com/groups/webcifar).  ## License  [MIT](https://choosealicense.com/licenses/mit/)  Happy Coding! üöÄ"
Weather App,React JS,https://github.com/s-shemmee/React-Weather-App,"# React Weather App This is a web app developed as a final project for SheCodes React using React.js. It allows users to search for the weather conditions of any city in the world and provides current weather information. The app fetches weather data using the SheCodes Weather API.  ![Opera Snapshot_2024-02-08_163132_react-weather-app-shemmee netlify app](https://github.com/s-shemmee/React-Weather-App/assets/56132945/a3aea312-a726-40c6-93bb-45261ecddcad)  # Features - Users can search for weather conditions of any city in the world. - The app displays the current weather conditions, including temperature and weather description. - The app utilizes the SheCodes Weather API to fetch weather data.  # Technologies & Tools Used - React.js - HTML - CSS - JavaScript - VS Code - SheCodes Weather API  # Installation and Usage To use this app, you can follow these steps:  - Clone the repository or download the source code. - Open the project in your preferred code editor. - Run  `npm install` to install the necessary dependencies. - Run `npm start` to start the development server. - Open your browser and navigate to `http://localhost:3000` to use the app.  To use the app, simply type the name of the city you want to search for in the search bar and press Enter. The app will display the current weather conditions for the searched city.  # Demo A live demo of the React Weather App is available at [LINK TO LIVE DEMO](https://react-weather-app-shemmee.netlify.app).  # Credits - The weather app was created by [s-shemmee](https://github.com/s-shemmee) as a final project for the SheCodes React workshop. - The app uses the SheCodes Weather API to retrieve weather data.  # License This project is licensed under the MIT license."
Recipe search App,React JS,https://github.com/itisameerkhan/react-recipe-app,"# React Full Recipe App  Welcome to the React Full Recipe App! This app allows you to discover a wide range of delicious recipes, including popular items, veggie picks, and various cuisine categories such as Italian, Chinese, Thai, and American. You can also use the search feature to find your favorite recipes quickly.  ## Table of Contents  - [Features](#features) - [Technologies](#technologies) - [Getting Started](#getting-started) - [Usage](#usage) - [Contributing](#contributing) - [License](#license)  ## Features  - Browse popular recipes. - Explore veggie picks for healthy options. - Discover recipes from various cuisine categories: Italian, Chinese, Thai, and American. - Search for specific recipes by name or ingredients. - User-friendly interface built with React and Material-UI. - Routing provided by React Router. - Fast development with Vite.  ## Technologies  This project is built with the following technologies:  - React: A JavaScript library for building user interfaces. - React Router: For handling routing within the app. - Material-UI: A popular library for building beautiful and responsive UI components. - Vite: A fast build tool for modern web development.  ## Getting Started  To run this project locally, you'll need Node.js and npm (or yarn) installed on your computer.  1. Clone the repository:     ```bash    git clone https://github.com/itisameerkhan/react-full-recipe-app.git  2. Navigate to the project directory:    ```    cd react-full-recipe-app  3. Install dependencies:     ```    npm install    # or    yarn install  4. Start the development server:     ```    npm run dev    # or    yarn dev  # Usage  1. On the homepage, you'll find a selection of popular recipes. 2. Click on ""Veggie Picks"" to explore healthy and vegetarian recipes. 3. Browse recipes from different cuisines by selecting Italian, Chinese, Thai, or American from the navigation menu. 4. Use the search feature to find specific recipes by name or ingredients. 5. Enjoy your cooking and discover new culinary delights!  # Contributing  We welcome contributions from the open-source community. If you'd like to contribute to this project, please follow these steps:  # Fork the repository.  1. Create a new branch for your feature: git checkout -b feature-name. 2. Make your changes and commit them: git commit -m 'Add feature'. 3. Push your changes to your fork: git push origin feature-name. 4. Create a pull request. 5. Please make sure your code follows best practices and is well-documented.  # License  This repository does not have a specific license¬©, so you are free to use the code in any way you like without any restrictions.  If you have any questions or suggestions, feel free to open an issue in the repository.  Happy cooking!"
eCommerce app,React JS,https://github.com/ssahibsingh/React_E-Commerce,"# E-Commerce Website  A Ecommerce Website made with React.js Framework.   ## Demo  https://reactjs-ecommerce-app.vercel.app/  ## Features  - Easy to integrate with Backend - Fully Responsive   ## Screenshots  ![App Screenshot](https://i.ibb.co/fQ293tm/image.png)    ## Run Locally  Clone the project  ```bash   git clone https://github.com/ssahibsingh/React_E-Commerce ```  Go to the project directory  ```bash   cd React_E-Commerce ```  Install dependencies  ```bash   npm install ```  Start the server  ```bash   npm start ```    ## Tech Stack  * [React](https://reactjs.org/) * [Redux](https://redux.js.org/) * [Bootstrap](https://getbootstrap.com/) * [Fake Store API](https://fakestoreapi.com/)  ## Contributing  Contributions are always welcome! Just raise an issue, we will discuss it.   ## Feedback  If you have any feedback, please reach out to me [here](https://ssahibsingh.github.io/#contact)  "
An Entertainment App,React JS,https://github.com/yashbhanu/CineMatix-Entertainment-App-ReactJS,"## CineMatix - Entertainment App ReactJS <br />  > An entertainment app built using ReactJS, Material UI, and Movie Database API to show Top trending movies, Web series and search functionality to search for Movies and Web series and get Details about a movie or a web series. <br />  > It uses REST API to fetch data from the moviedb api endpoint which returns a JSON result that is mapped and rendered as Output/Result in UI. <br />  ## Tech Stack  - ReactJS  - Material UI  - Movie Database API  ## DEMO - [LIVE DEMO](https://cinematix-entertainment.netlify.app/)"
GitHub User Finding by Name,React JS,https://github.com/Subhampreet/Github-Profile-Finder-ReactJS,"<h1 align=""center"">Github Profile Finder Application üê±‚Äçüë§</h1>  ![0](https://user-images.githubusercontent.com/61475220/96919833-f701bb80-14c9-11eb-8eea-1c46ba92b906.jpg)  <b>GitHub</b>, Inc. is an American multinational corporation that provides hosting for software development and version control using Git. It offers the distributed version control and source code management (SCM) functionality of Git, plus its own features. It provides access control and several collaboration features such as bug tracking, feature requests, task management, and continuous integration.  <b>Github Profile Finder</b> is built using React JS. It simply searches the profile using the username and displays the profile details, along with the the repositories. It also labels the profile hireable or non-hireable according to contribution made and user activity. The app uses the github api to fetch data from github.  ### ‚úî Features :  - Fetches all profiles with the provided username - Displays all the details of the selected profile - Labels the profile as hireable or non-hireable  ### üê±‚Äçüíª Dependancies Installed :  - npm install react-router-dom - npm install react-bootstrap - npm install react-markdown - npm install @material-ui/core  ### üéâ Issues  Feel free to file a new issue with a respective title and description on the Pokedex NextJS repository. If you already found a solution to your problem, I would love to review your pull request! Have a look at our contribution guidelines to find out about the coding standards.  ### ‚ö° Contribution  When contributing to this repository, please first discuss the change you wish to make via issue, email, or any other method with the owners of this repository before making a change.  Please note we have a code of conduct, please follow it in all your interactions with the project."
Music Streaming Application (FrontEnd),React JS,https://github.com/onamkrverma/okv-music,"# Okv Music  - **[Okv Music](https://okv-music.netlify.app/) is a Progressive Web App (PWA) music app made with Javascript using React.js and YouTube Api that allows user to discover and listen to new music from around the world. The website features a clean and modern design, a user-friendly interface, and a powerful search feature of youtube.**  ## Screenshots  ![okv music](https://user-images.githubusercontent.com/106578262/224484464-6ef06a58-9aba-4f3e-99ac-150857672f2b.png)  ## Demo video  https://github.com/onamkrverma/okv-music/assets/106578262/86e5d3c1-25fd-42e9-96e8-8660537a31fa  **[Demo Link](https://okv-music.netlify.app/)**  ## Features  - Best Streaming Quality - Trending and New Released songs list - Play youtube song as audio only - YouTube Search Support - Controls player from notification - Next/Prev song track controls - Auto Song Recommendations - No Ads  #### Upcoming features to be add  - Controls audio quality (done ‚úî) - Direct click to watch youtube video (done ‚úî) - Get song info button (done ‚úî) - Import Youtube playlists (done ‚úî) - Add explore page for diffrent playist (done ‚úî) - Add trending page to show trending song list for global and india (done ‚úî) - Download audio as mp3 (done ‚úî) - many more..  ### Don't forget to :star: the repo  ## Technology used  - React js - CSS3 - Redux toolkit - React router dom - React loading skeleton - React icons  ### Backend  - Node js - Express js - Ytdl-core package  ### Note:  - Now this project is migrated from CRA to Vite + React app  ## Getting Started with Vite React App  - This project is bootstrapped with [Create Vite + React App](https://github.com/vitejs/vite/tree/main/packages/create-vite). - clone down this repositery. You will need to `node.js` and `git` installed globally on your machine.  ## installation and setup instructions  1. installation: `npm install` 2. In the project directory, you can run: `npm run dev`  Runs the app in the development mode.\ Open [http://localhost:5173](http://localhost:5173) to view it in your browser. The page will reload when you make changes.  ### Sample code for backend to get audio url with youtube id  - setup backend with nodejs, expressjs and install Ytdl-core package  ``` const express = require(""express""); const app = express(); const ytdl = require(""ytdl-core"");  app.get(""/song/:id"", async (req, res) => {   try {     let info = await ytdl.getInfo(req.params.id);     let audioFormatHigh = ytdl.chooseFormat(info.formats, {       quality: ""highest"",       filter: ""audioonly"",     });     let audioFormatLow = ytdl.chooseFormat(info.formats, {       quality: ""lowest"",       filter: ""audioonly"",     });     res.status(200).json({       audioFormatHigh: audioFormatHigh.url,       audioFormatLow: audioFormatLow.url,     });   } catch (err) {     // console.error(error);     if (err instanceof Error)       res.status(500).send(`internal server error ""${err.message}""`);   } ```  ## Queries  If you have any query or suggestion, feel free to [create an issue](https://github.com/onamkrverma/okv-music/issues) or [submit feedback here](https://okv-music.netlify.app/feedback)"
Crypto Currency Price Info,React JS,https://github.com/alimazhar4/Live-Crypto-Price-Tracker-in-React,"# Live Crypto Price Tracker in React Js A simple project to display real-time cryptocurrency prices of the top 200 cryptos by marketcap. It uses React Hooks and Axios to fetch API data Coin Gecko API.   <p align=""center"">   <img src=""https://user-images.githubusercontent.com/59063759/222639321-4e095190-bdde-4ec4-96e8-8594de393544.png"" width=48%>   <img src=""https://user-images.githubusercontent.com/59063759/222639722-0a788df9-98c3-4f57-962d-403256aeaf5b.png"" width=48%> </p>  ## Getting Started ### Prerequisites - Node.js and npm installed on your computer  ### Installation 1) Clone the respository on your local machine using: ```sh git clone https://github.com/alimazhar4/Live-Crypto-Price-Tracker-in-React.git ``` 2) Navigate to the project directory: ```sh cd Live-Crypto-Price-Tracker-in-React ``` 3) Install dependencies: ```sh npm install ``` 4) Start the developmental server: ```sh npm start ```  ## Contributing If you'd like to contribute to this project, feel free to create a pull request. Any contributions, whether it be a bug fix or a new feature, are greatly appreciated."
GitHub Finder,React JS,https://github.com/bradtraversy/github-finder,"# Github Finder  > React app to search Github profiles. This app uses the Context API along with the useContext and useReducer hooks for state management and is part of the ""Modern React Front To Back"" Udemy course  ## Usage  ### `npm install`  ### `npm start`  Runs the app in the development mode.<br> Open [http://localhost:3000](http://localhost:3000)  ### `npm run build`  Builds the app for production to the `build` folder.<br>"
Random Quote Generator,React JS,https://github.com/autumnchris/random-quote-machine-reactjs,"# Random Quote Machine (React.js)  A front-end web app that gets and displays an inspirational quote at random from my own Express.js [Quotes API](https://autumnchris-quotes.herokuapp.com). Built with React.js.  Inspired by the [Build a Random Quote Machine challenge](https://learn.freecodecamp.org/front-end-libraries/front-end-libraries-projects/build-a-random-quote-machine) as part of the curriculum for the [Front End Development Libraries Certification](https://www.freecodecamp.org/learn/front-end-libraries) on [freeCodeCamp](https://www.freecodecamp.org).  ---  ## Built With * [React.js](https://reactjs.org) * [Sass](http://sass-lang.com) * JavaScript * CSS3 * HTML5 * [Node.js](https://nodejs.org/en) * [Webpack](https://webpack.js.org) * [Axios](https://axios-http.com) * AJAX * [Babel](https://babeljs.io) * [Normalize.css](https://necolas.github.io/normalize.css) * [Font Awesome](https://fontawesome.com) * [Google Fonts](https://fonts.google.com)  ## Demo  View project demo at [https://autumnchris.github.io/random-quote-machine-reactjs](https://autumnchris.github.io/random-quote-machine-reactjs).  ## Instructions  After forking and cloning, navigate to the repository in your command line and install the NPM packages: ``` npm install ```  Run the following script in your command line to run the application: ``` npm start ```  Once the server is running, go to `http://localhost:8080` in your browser.  Before committing any changes, run the following script to update your static files for production: ``` npm run build ```"
Generate Devoloper Portfolio Website using LinkedIn API,React JS,https://github.com/waseemhnyc/LinkedIn-to-Portfolio-Site-Generator,"# Linkedin to Portfolio Site Generator  This project is a Python script that scrapes your [Linkedin PDF](https://www.linkedin.com/in/waseemhnyc/) and generates a customized portfolio site using OpenAI's GPT-4 model.  We interact with the GPT-4 model using [LangChain](https://github.com/hwchase17/langchain).  1. Embed the Linkedin PDF 2. Store the embeddings into a Chroma vector database 3. Query that database to get relevant information 4. Generate text with OpenAI's GPT-4 5. With the generated text we use the [Next JS portfolio site](https://github.com/vercel/nextjs-portfolio-starter), powered by [Nextra](https://nextra.site/), to create the main portfolio file 6. Build and deploy site on [Vercel](https://vercel.com/templates/next.js/portfolio-starter-kit)  ## Demo  For this demo, I used LangChain's Co-founder and CEO [Harrison Chase](https://twitter.com/hwchase17)'s LinkedIn.  You can find his deployed site here: <https://harrison-six.vercel.app/>  Video: https://www.youtube.com/watch?v=jY5UnSBq8sI  [![Demo Video](assets/screenshot.png)](https://www.youtube.com/watch?v=jY5UnSBq8sI)  ## Prerequisites  Before you begin, ensure you have met the following requirements:  - Installed a recent version of Python (3.7 or newer) installed and a way to create virtual environments (virtualenv or conda) - Created a Vercel account and have downloaded/login into Vercel CLI locally - Created OpenAI API account and obtain an OpenAI API key  ## Getting Started  Clone the repo  ```bash git clone https://github.com/waseemhnyc/LinkedIn-to-Portfolio-Site-Generator ```  Create a virutalenv and source the environment  ```bash python3 -m venv myenv source venv/bin/activate ```  Install the necessary libraries  ```bash pip install -r requirements.txt ```  Create a .env file and input your OpenAI API Key in the file  ```bash cp .env.example .env ```  ## Usage  To run the program, run the following command in the terminal:  ```bash python main.py ```  ## Ways to Improve  - Accept more inputs like resumes/CVs - Grab data from Github and Twitter - Integrate with other portfolio templates - Push to Github so you could make your own changes - Use LangChains output parser   ## Questions or Get in Touch  - Twitter: https://twitter.com/waseemhnyc - Email: waseemh.nyc@gmail.com  ## License  This project is licensed under the MIT License - see the LICENSE file for details."
Netflix Clone,React JS,https://github.com/JosinJojy/Netflix-reactjs,"# Netflix Clone React.js  ## Overview This project is a Netflix clone built using React.js, designed to enhance skills in web development. It features a fully interactive user interface with various functionalities, making it a comprehensive movie-watching experience. The project is powered by TMDB API, utilizing Firebase for database management.  ## Screenshots  ![PC screen](https://i.imgur.com/FLNs9Qy.jpg)    ### mobile experience  ![mobile screens](https://i.imgur.com/ForTeQi.jpg)   ## Key Functionalities - Sign In / Sign Up - Home Page for browsing movies - My List Section for user-specific movie selections - Liked Movies Page - Watched Movies Page - Profile Page - Play Movie Page - Search Movie Page  ## Technologies Used - [React.js](https://react.dev/) - [TMDB API](https://www.themoviedb.org/) - [Firebase](https://firebase.google.com/) - [Axios](https://www.npmjs.com/package/axios) - [Swiper.js](https://swiperjs.com/) - [React-Youtube](https://www.npmjs.com/package/react-youtube) - [Tailwind CSS](https://tailwindcss.com/)  ## Description This Netflix clone project was developed to deepen understanding and proficiency in React.js. Leveraging popular technologies and APIs like TMDB and Firebase, it encompasses a range of features, from user authentication to dynamic movie listings. The design is tailored to provide an immersive streaming experience, and the codebase reflects best practices in modern web development.  ## Link to the Site [Netflix Clone](https://netflicz-reactjs-rho.vercel.app/)"
Messaging App,React JS,https://github.com/DulanjaliSenarathna/react-chat-app,"# WeChat - React Js Chat App  **live demo: [https://react-chat-app-kohl.vercel.app/app](https://react-chat-app-kohl.vercel.app/app)**  ---  ### Made with ‚ù§Ô∏è by [Dulanjali Senarathna](https://www.linkedin.com/in/dulanjali-senarathna/)  ---  ## Project Description  Mainly focused on react js front-end development. I used css for styling. Also, this application used React Context API and redux for state management.  This chat app has basic chat app functionalities such as login, register, reset password, chat, conversation, group call, contact, starred messages, medial, files and links, settings, profile, edit profile, emoji pickers and etc. We can implement all the features using backend in future.   You can change theming colors and dark and light modes by clicking settings icon in bottom-left cornaer.  ## What I used  - [React](https://reactjs.org/) - [MAterial UI](https://mui.com/) - [React Context API](https://legacy.reactjs.org/docs/context.html) - [Redux](https://redux.js.org/) - [Faker js](https://fakerjs.dev/) - [phosphor-react](https://www.npmjs.com/package/phosphor-react) - [Emoji Mart](https://www.npmjs.com/package/emoji-mart) - [React Hook Form](https://react-hook-form.com/) - [Yup js](https://www.npmjs.com/package/yup) - More...  ## Requirements  - Basic ReactJs knowledge - Basic HTML, CSS knowledge  ## Getting Started  After getting the project files, you need to go the file directory and run  ```shell npm install ```  and after that start the dev server.  ```shell npm run start ```  ## Tools Used  1. Favicon: [Flaticon.com](https://www.flaticon.com/) 1. Code Editor: [VS Code](https://code.visualstudio.com/)  ---  ## FAQ  ### Q: What are the prerequisites?  basics of html, css, javascript and some basic knowledge of react is enough to start this project. Rest you will learn in the tutorial.  ### Q: Who the project is for?  The project is for the people who wanna get more skilled in `ReactJs`.  ---  ## Feedback  If you have any feedback, please reach out to us at [Dulanjali Senarathna](https://www.behance.net/dulanjasenarathna)  Happy Coding! üöÄ  # Website's screenshots  ![React js chat app 1](https://user-images.githubusercontent.com/59603716/236672849-f2577271-4a2b-4b25-bca2-97755ba0c9c2.jpg)  ![React js chat app 2](https://user-images.githubusercontent.com/59603716/236672853-2e1e2d1a-3e5a-44cd-a472-e0a4285de179.jpg)  ![React js chat app 3](https://user-images.githubusercontent.com/59603716/236672858-8b4a24ec-f287-4161-ae1f-7ed6115e35de.jpg)  ![React js chat app 4](https://user-images.githubusercontent.com/59603716/236672872-b12dbfbb-b73f-47e0-ab53-d53960083888.jpg)  ![React js chat app 5](https://user-images.githubusercontent.com/59603716/236672879-b3f9682c-5ced-4f8e-b3e7-dbbedf901d54.jpg)  ![React js chat app 6](https://user-images.githubusercontent.com/59603716/236672884-cddfe6bb-5550-484a-a215-08ec09e52365.jpg)  ![React js chat app 7](https://user-images.githubusercontent.com/59603716/236672886-080587d0-92f5-476f-ae24-263bfe8ab23d.jpg)  ![React js chat app 8](https://user-images.githubusercontent.com/59603716/236672893-bdd01215-3d0c-4a87-98e8-b8410735390c.jpg)  ![React js chat app 9](https://user-images.githubusercontent.com/59603716/236672900-8046f9d1-9d32-4f8b-b443-fa265fa059bd.jpg)  ![React js chat app 10](https://user-images.githubusercontent.com/59603716/236672903-d3139cb3-6490-427a-8573-cc9d4956ce60.jpg)  ## Dark mode  ![dark mode react js chat app](https://user-images.githubusercontent.com/59603716/236672907-66c3c842-66e8-40bb-9c44-ea1e6f7a29da.JPG)"
My Diary,React JS,https://github.com/allsimpledevcode/my-diaries-front-end,"This project was bootstrapped with [Create React App](https://github.com/facebook/create-react-app).  ## Available Scripts  In the project directory, you can run:  ### `yarn start`  Runs the app in the development mode.<br /> Open [http://localhost:3000](http://localhost:3000) to view it in the browser.  The page will reload if you make edits.<br /> You will also see any lint errors in the console.  ### `yarn test`  Launches the test runner in the interactive watch mode.<br /> See the section about [running tests](https://facebook.github.io/create-react-app/docs/running-tests) for more information.  ### `yarn build`  Builds the app for production to the `build` folder.<br /> It correctly bundles React in production mode and optimizes the build for the best performance.  The build is minified and the filenames include the hashes.<br /> Your app is ready to be deployed!  See the section about [deployment](https://facebook.github.io/create-react-app/docs/deployment) for more information.  ### `yarn eject`  **Note: this is a one-way operation. Once you `eject`, you can‚Äôt go back!**  If you aren‚Äôt satisfied with the build tool and configuration choices, you can `eject` at any time. This command will remove the single build dependency from your project.  Instead, it will copy all the configuration files and the transitive dependencies (webpack, Babel, ESLint, etc) right into your project so you have full control over them. All of the commands except `eject` will still work, but they will point to the copied scripts so you can tweak them. At this point you‚Äôre on your own.  You don‚Äôt have to ever use `eject`. The curated feature set is suitable for small and middle deployments, and you shouldn‚Äôt feel obligated to use this feature. However we understand that this tool wouldn‚Äôt be useful if you couldn‚Äôt customize it when you are ready for it.  ## Learn More  You can learn more in the [Create React App documentation](https://facebook.github.io/create-react-app/docs/getting-started).  To learn React, check out the [React documentation](https://reactjs.org/).  ### Code Splitting  This section has moved here: https://facebook.github.io/create-react-app/docs/code-splitting  ### Analyzing the Bundle Size  This section has moved here: https://facebook.github.io/create-react-app/docs/analyzing-the-bundle-size  ### Making a Progressive Web App  This section has moved here: https://facebook.github.io/create-react-app/docs/making-a-progressive-web-app  ### Advanced Configuration  This section has moved here: https://facebook.github.io/create-react-app/docs/advanced-configuration  ### Deployment  This section has moved here: https://facebook.github.io/create-react-app/docs/deployment  ### `yarn build` fails to minify  This section has moved here: https://facebook.github.io/create-react-app/docs/troubleshooting#npm-run-build-fails-to-minify"
Calculator,React JS,https://github.com/shakti1590/React-Calculator,"#### Problem statement:  - Create a simple calculator app.  #### Technology Used:  - React Js  - Java Script  - HTML  - CSS   ## Install  To install all the dependences of the project, run the following command:      git clone https://github.com/shakti1590/React-Calculator.git     yarn install     yarn start   #### Source Code:  - Link : https://github.com/shakti1590/React-Calculator   #### Deployment Link:  - Link : https://react-calculator-omega-navy.vercel.app/    ![Project Preview](./src/calc-app.png)    <p align=""center"">   Show some :heart: by starring the repository. </p>"
Emoji Search,React JS,https://github.com/PragatiVerma18/Emoji-Search,"This project was bootstrapped with [Create React App](https://github.com/facebook/create-react-app).  ## Available Scripts  In the project directory, you can run:  ### `npm start`  Runs the app in the development mode.<br /> Open [http://localhost:3000](http://localhost:3000) to view it in the browser.  The page will reload if you make edits.<br /> You will also see any lint errors in the console.  ### `npm test`  Launches the test runner in the interactive watch mode.<br /> See the section about [running tests](https://facebook.github.io/create-react-app/docs/running-tests) for more information.  ### `npm run build`  Builds the app for production to the `build` folder.<br /> It correctly bundles React in production mode and optimizes the build for the best performance.  The build is minified and the filenames include the hashes.<br /> Your app is ready to be deployed!  See the section about [deployment](https://facebook.github.io/create-react-app/docs/deployment) for more information.  ### `npm run eject`  **Note: this is a one-way operation. Once you `eject`, you can‚Äôt go back!**  If you aren‚Äôt satisfied with the build tool and configuration choices, you can `eject` at any time. This command will remove the single build dependency from your project.  Instead, it will copy all the configuration files and the transitive dependencies (webpack, Babel, ESLint, etc) right into your project so you have full control over them. All of the commands except `eject` will still work, but they will point to the copied scripts so you can tweak them. At this point you‚Äôre on your own.  You don‚Äôt have to ever use `eject`. The curated feature set is suitable for small and middle deployments, and you shouldn‚Äôt feel obligated to use this feature. However we understand that this tool wouldn‚Äôt be useful if you couldn‚Äôt customize it when you are ready for it.  ## Learn More  You can learn more in the [Create React App documentation](https://facebook.github.io/create-react-app/docs/getting-started).  To learn React, check out the [React documentation](https://reactjs.org/).  ### Code Splitting  This section has moved here: https://facebook.github.io/create-react-app/docs/code-splitting  ### Analyzing the Bundle Size  This section has moved here: https://facebook.github.io/create-react-app/docs/analyzing-the-bundle-size  ### Making a Progressive Web App  This section has moved here: https://facebook.github.io/create-react-app/docs/making-a-progressive-web-app  ### Advanced Configuration  This section has moved here: https://facebook.github.io/create-react-app/docs/advanced-configuration  ### Deployment  This section has moved here: https://facebook.github.io/create-react-app/docs/deployment  ### `npm run build` fails to minify  This section has moved here: https://facebook.github.io/create-react-app/docs/troubleshooting#npm-run-build-fails-to-minify"
Snap Shot,React JS,https://github.com/Yog9/SnapShot,"# Snap Shot [![Tweet](https://img.shields.io/twitter/url/http/shields.io.svg?style=social)](https://twitter.com/intent/tweet?text=See%20this%20react%20example&url=https://yog9.github.io/SnapShot/&hashtags=react,context-api,freecodecamp,developers)  [![Build Status](https://travis-ci.org/Yog9/SnapShot.svg?branch=master)](https://travis-ci.org/Yog9/SnapShot) [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT) [![HitCount](http://hits.dwyl.com/Yog9/SnapShot.svg)](http://hits.dwyl.com/Yog9/SnapShot)  [Demo of Snap Shot](https://yog9.github.io/SnapShot/)  ![](/snapscout.png)  ### Summary  Snap Shot is a gallery created using React,React Hooks, React Router and Context API. The Routes were setup for four default pages and a search page. Also the images were displayed using the Flickr API and axios to fetch data.  ### Motivation  The purpose of this project was to get familiar with React Hooks and Context API.  ### Getting Started  Click the demo link or clone/download the repository on your local machine. Create a config.js file in api folder inside src folders. In config.js file write `export const apiKey = ""YOUR_FLIKR_API_KEY"";`  ##### Install dependencies  `yarn install`  ##### Run Snap Shot from the root directory.  `yarn start`  ### Built With  - React js - React Router - React Hooks - Context API - Flickr API  ### Features  **1. Responsive Design.**  **2. Search functionality added to search photos from API.**  ### Coming Soon  - [ ] Cypress E2E Tests  ### Contributing  Everyone is welcomed to contribute to this project. You can contribute either by submitting bugs or suggesting improvements by opening an issue on GitHub. Please see the [CONTRIBUTING](CONTRIBUTING.md) guidelines for more information.  This project was bootstrapped with [Create React App](https://github.com/facebook/create-react-app)."
BMI Calculator,React JS,https://github.com/AnithaGunasekaran/react-bmi-calculator,"This project was bootstrapped with [Create React App](https://github.com/facebookincubator/create-react-app).  Below you will find some information on how to perform common tasks.<br> You can find the most recent version of this guide [here](https://github.com/facebookincubator/create-react-app/blob/master/packages/react-scripts/template/README.md).  ``` my-app/   README.md   node_modules/   package.json   public/     index.html     favicon.ico   src/     App.css     App.js     App.test.js     index.css     index.js     logo.svg ```  For the project to build, **these files must exist with exact filenames**:  * `public/index.html` is the page template; * `src/index.js` is the JavaScript entry point.  You can delete or rename the other files.  You may create subdirectories inside `src`. For faster rebuilds, only files inside `src` are processed by Webpack.<br> You need to **put any JS and CSS files inside `src`**, or Webpack won‚Äôt see them.  Only files inside `public` can be used from `public/index.html`.<br> Read instructions below for using assets from JavaScript and HTML.  You can, however, create more top-level directories.<br> They will not be included in the production build so you can use them for things like documentation.  ## Available Scripts  In the project directory, you can run:  ### `npm start`  Runs the app in the development mode.<br> Open [http://localhost:3000](http://localhost:3000) to view it in the browser.  The page will reload if you make edits.<br> You will also see any lint errors in the console.  ### `npm test`  Launches the test runner in the interactive watch mode.<br> See the section about [running tests](#running-tests) for more information.  ### `npm run build`  Builds the app for production to the `build` folder.<br> It correctly bundles React in production mode and optimizes the build for the best performance.  The build is minified and the filenames include the hashes.<br> Your app is ready to be deployed!  See the section about [deployment](#deployment) for more information.  ### `npm run eject`  **Note: this is a one-way operation. Once you `eject`, you can‚Äôt go back!**  If you aren‚Äôt satisfied with the build tool and configuration choices, you can `eject` at any time. This command will remove the single build dependency from your project.  Instead, it will copy all the configuration files and the transitive dependencies (Webpack, Babel, ESLint, etc) right into your project so you have full control over them. All of the commands except `eject` will still work, but they will point to the copied scripts so you can tweak them. At this point you‚Äôre on your own.  You don‚Äôt have to ever use `eject`. The curated feature set is suitable for small and middle deployments, and you shouldn‚Äôt feel obligated to use this feature. However we understand that this tool wouldn‚Äôt be useful if you couldn‚Äôt customize it when you are ready for it. "
Image Compressor,React JS,https://github.com/Rahul-Pandey7/react-image-compressor,# React Image Compressor  ## Overview  A simple image compressor built with [react](https://reactjs.org/) and [browser-image-compression](https://www.npmjs.com/package/browser-image-compression).  ## Functionalities  - Compress Image By Reducing Resolution and Size - Offline Compression  ## Built With  - ReactJS - React Bootstrap - Browser Image Compression  ## Development  1. Clone the repository and change directory.  ``` git clone https://github.com/RaulB-masai/react-image-compressor.git cd react-image-compressor ```  2. Install npm dependencies  ``` npm install ```  3. Run the app locally.  ``` npm start ```
Typing Speed Tester App,React JS,https://github.com/awran5/react-typing-speed-test-game,## React Typing-speed Test Game  1 minute Typing speed test game built with React. Test your typing speed online and find out how fast can you type in real world.   ## [Website](https://typingspeedtest.now.sh/)
Quiz App,React JS,https://github.com/VINAYAK9669/React-QuizApp,"# üöÄ React Quiz App   The ""React Quiz App"" is a dynamic and engaging web application built with React.js. This project offers a variety of features, including a complete responsive design for seamless use on different devices, a clean and intuitive user interface, and a timed quiz with a progress bar to track your progress.   As you answer questions, the app instantly updates your score and provides a comprehensive scorecard once the quiz is completed. The quiz also includes an automatic submission feature when the timer runs out.  # Demo   [WEBSITE-Demo](https://vinayak9669.github.io/React-QuizApp/)    [Check out the LinkedIn Post with Video Demo](https://www.linkedin.com/posts/vinay1998_reactjs-webdevelopment-frontenddevelopment-activity-7125437291241644032-3ko7?utm_source=share&utm_medium=member_desktop)  ## üõ†Ô∏èTechnology Used   - [React.js](https://reactjs.org/)  ## üìãProject Features   - üåê Complete Responsive Website: This quiz app is designed to work seamlessly on various devices and screen sizes. - üé® Clean and Simple User Interface: A user-friendly design for an enjoyable quiz experience. - ‚è≤Ô∏è Timer for Quiz: Each quiz question is timed, adding an element of challenge and excitement. - üìä Progress Bar: Keep track of your quiz progress in real-time. - üìà Score Update: Instant feedback on your score after answering each question. - üìú Score Card: Get a comprehensive scorecard with your quiz results. - ‚è±Ô∏è Auto-Submission: The quiz automatically submits when the time is up.  ## üìöLearning Points   - üì° Creating a Fake API: Learn how to create and deploy a mock API and integrate it into project. - ‚öôÔ∏è State Management: Explore state management techniques using the `useReducer` hook to efficiently handle application state. - üîÑ Fetching Data with `useEffect()`: Utilize the `useEffect` hook for fetching API data and managing timers.  ## How to Install and Run  1. Clone the repository:      ```bash    gh repo clone VINAYAK9669/React-QuizApp     ```  2. Install dependencies:      ```bash     cd React-QuizApp     npm install     ```  3. Start the development server:      ```bash     npm start     ```  ## üì±Responsiveness  ![ScreenShots](ScreenShots/React_Quiz_App_Responsivness.png) "
Uber Clone,React JS,https://github.com/DevlinRocha/uber-clone,"# Uber Clone  Full-stack Uber recreation built with React, Next.js, Firebase, styled-components, Tailwind, and Mapbox.  ## Getting Started  First, clone the repository and run the development server:  ``` git clone git@github.com:DevlinRocha/uber-clone.git cd uber-clone npm install npm run dev ```  Then open [http://localhost:3000](http://localhost:3000) with your browser.  ## Development  * [React](https://reactjs.org/) * [Next.js](https://nextjs.org/) * [Firebase](https://firebase.google.com/) * [styled-components](https://styled-components.com/) * [Tailwind](https://tailwindcss.com/) * [Mapbox](https://www.mapbox.com/)"
AirBnB Clone,React JS,https://github.com/CleverProgrammer/airbnb-clone,"This project was bootstrapped with [Create React App](https://github.com/facebook/create-react-app).  ## Available Scripts  In the project directory, you can run:  ### `npm start`  Runs the app in the development mode.<br /> Open [http://localhost:3000](http://localhost:3000) to view it in the browser.  The page will reload if you make edits.<br /> You will also see any lint errors in the console.  ### `npm test`  Launches the test runner in the interactive watch mode.<br /> See the section about [running tests](https://facebook.github.io/create-react-app/docs/running-tests) for more information.  ### `npm run build`  Builds the app for production to the `build` folder.<br /> It correctly bundles React in production mode and optimizes the build for the best performance.  The build is minified and the filenames include the hashes.<br /> Your app is ready to be deployed!  See the section about [deployment](https://facebook.github.io/create-react-app/docs/deployment) for more information.  ### `npm run eject`  **Note: this is a one-way operation. Once you `eject`, you can‚Äôt go back!**  If you aren‚Äôt satisfied with the build tool and configuration choices, you can `eject` at any time. This command will remove the single build dependency from your project.  Instead, it will copy all the configuration files and the transitive dependencies (webpack, Babel, ESLint, etc) right into your project so you have full control over them. All of the commands except `eject` will still work, but they will point to the copied scripts so you can tweak them. At this point you‚Äôre on your own.  You don‚Äôt have to ever use `eject`. The curated feature set is suitable for small and middle deployments, and you shouldn‚Äôt feel obligated to use this feature. However we understand that this tool wouldn‚Äôt be useful if you couldn‚Äôt customize it when you are ready for it.  ## Learn More  You can learn more in the [Create React App documentation](https://facebook.github.io/create-react-app/docs/getting-started).  To learn React, check out the [React documentation](https://reactjs.org/).  ### Code Splitting  This section has moved here: https://facebook.github.io/create-react-app/docs/code-splitting  ### Analyzing the Bundle Size  This section has moved here: https://facebook.github.io/create-react-app/docs/analyzing-the-bundle-size  ### Making a Progressive Web App  This section has moved here: https://facebook.github.io/create-react-app/docs/making-a-progressive-web-app  ### Advanced Configuration  This section has moved here: https://facebook.github.io/create-react-app/docs/advanced-configuration  ### Deployment  This section has moved here: https://facebook.github.io/create-react-app/docs/deployment  ### `npm run build` fails to minify  This section has moved here: https://facebook.github.io/create-react-app/docs/troubleshooting#npm-run-build-fails-to-minify"
Instagram Clone,React JS,https://github.com/Subhampreet/Instagram-Clone-ReactJS,"<h1 style=""center""> Instagram-Clone-ReactJS ‚ö°</h1>  <b>Instagram</b> is an American photo and video sharing social networking service owned by Facebook. This project is a Clone of the original Instagram UI. This Project provides some basic features, similar to the original application.  <b>P.S.</b> This project has been developed for learning purposes, and it has nothing to do with the original Application.  ![](https://raw.githubusercontent.com/Subhampreet/Instagram-Clone-ReactJS/main/public/images/screencapture.png)  ### Features :  - User Authentication : Sign In and Sign Up - Image Uploading for creating new Posts - User can add Comments to the Posts - An Awesome User-Interface  <b>To Create a new Post</b>, the user has to sign up for the apllication first using any mail ID (Works with an Invalid Mail ID too). For example : ""xyz@gmail.com"". User can Sign-In using the same credentials again and again.  <b>For Creating a Post</b> you need to sign-in first. Then click the ""UPLOAD PHOTO"" Button. Select a Photo from your device, add a suitable caption to the Post, and then click ""CREATE POST"" Button. Wait till the photo gets uploaded. And then BOOM!!! Your Post has been created(Scroll a bit if you don't find your post at the top).  #### Hope you have a great time, exploring the application in and out ‚úå !!!  [Instagram Clone](https://instagram-clone-react-70113.web.app)  ## Referances  - [Clever Programmer - Instagram Clone](https://www.youtube.com/watch?v=f7T48W0cwXM&list=PLgxM_xd-x_qHa6ErJkxh4wIRc7Q36AMfC&index=4) - [Image Uploader App using Firebase](https://www.youtube.com/watch?v=34f_SO7SWVA)"
Whatsapp Clone,React JS,https://github.com/muga-mark/WhatsApp-Clone-React,"## Table of contents * [General info](#general-info) * [Features](#features) * [Technologies](#technologies) * [Steps on how to clone](#clone) * [Steps on how to setup hosting](#hosting)  ## General info This a WhatsApp Web clone that is still a work in progress. Users can only create public rooms.  ## Features * Add/Delete Room (only admin or room owner can delete the room) * Update User Profile: Name, About, Profile Photo * Upload photos and videos (cannot upload multiple files) * Search rooms and messages * Emojis  ## Technologies Project is created with: * React * React Context API * React Router v5.2.0 * React Player v2.6.2 * React Toastify v6.0.8 * Emoji Mart v3.0.0"" * Material-Ui v4.11.0 * Firebase: Authentication, Cloud Firestore(db), Storage, Hosting  ## Steps on how to clone #### 1: Clone this repo using this command  `git clone https://github.com/muga-mark/WhatsApp-Clone-React.git`  #### 2: Enter  `cd WhatsApp-Clone-React`  #### 3: Next `npm install`  #### 4: Goto `src/components` & Find file named `firebase2.js` => Rename the file to `firebase.js` and input your config data.  #### 5: Now, Goto `src` directory again and find file named `.firebasesrc` Replace the line `""default"": ""your_project__name""` & save.  #### 6: Hit `npm start` in terminal.    ## Set Up Hosting in Firebase  #### 1: Hit command `firebase init`  #### 2: Enter Yes  #### 3: Goto the 4th option says - ""Configure and Deploy firebase Hosting sites"", Use Arrow key to go down & Hit spacebar to select the option and then hit Enter key  #### 4: Enter `build` for directory && select `Yes` for for as a single-page app and `No` for Overwrite.  #### 5: After initialization Completed. Enter  `npm run build`  #### 6: Now the last command Enter `firebase deploy`  #### You have succesfully hosted your firebase app. Click on the link and Enjoy!. Don't forget to rate the repository."
Facebook Clone,React JS,https://github.com/TusharKukra/Facebook-Clone-ReactJS,"# Screenshots:  ![image](https://user-images.githubusercontent.com/43209472/128313371-4cb0490e-3e03-4ef1-a600-9ca7a34a6669.png)  <br>  ![image](https://user-images.githubusercontent.com/43209472/128313417-8fe0926f-4dbb-422b-96cc-ab15f0e0c300.png)  # Getting Started with Create React App  This project was bootstrapped with [Create React App](https://github.com/facebook/create-react-app).  ## Available Scripts  In the project directory, you can run:  ### `npm start`  Runs the app in the development mode.\ Open [http://localhost:3000](http://localhost:3000) to view it in the browser.  The page will reload if you make edits.\ You will also see any lint errors in the console.  ### `npm test`  Launches the test runner in the interactive watch mode.\ See the section about [running tests](https://facebook.github.io/create-react-app/docs/running-tests) for more information.  ### `npm run build`  Builds the app for production to the `build` folder.\ It correctly bundles React in production mode and optimizes the build for the best performance.  The build is minified and the filenames include the hashes.\ Your app is ready to be deployed!  See the section about [deployment](https://facebook.github.io/create-react-app/docs/deployment) for more information.  ### `npm run eject`  **Note: this is a one-way operation. Once you `eject`, you can‚Äôt go back!**  If you aren‚Äôt satisfied with the build tool and configuration choices, you can `eject` at any time. This command will remove the single build dependency from your project.  Instead, it will copy all the configuration files and the transitive dependencies (webpack, Babel, ESLint, etc) right into your project so you have full control over them. All of the commands except `eject` will still work, but they will point to the copied scripts so you can tweak them. At this point you‚Äôre on your own.  You don‚Äôt have to ever use `eject`. The curated feature set is suitable for small and middle deployments, and you shouldn‚Äôt feel obligated to use this feature. However we understand that this tool wouldn‚Äôt be useful if you couldn‚Äôt customize it when you are ready for it.  ## Learn More  You can learn more in the [Create React App documentation](https://facebook.github.io/create-react-app/docs/getting-started).  To learn React, check out the [React documentation](https://reactjs.org/).  ### Code Splitting  This section has moved here: [https://facebook.github.io/create-react-app/docs/code-splitting](https://facebook.github.io/create-react-app/docs/code-splitting)  ### Analyzing the Bundle Size  This section has moved here: [https://facebook.github.io/create-react-app/docs/analyzing-the-bundle-size](https://facebook.github.io/create-react-app/docs/analyzing-the-bundle-size)  ### Making a Progressive Web App  This section has moved here: [https://facebook.github.io/create-react-app/docs/making-a-progressive-web-app](https://facebook.github.io/create-react-app/docs/making-a-progressive-web-app)  ### Advanced Configuration  This section has moved here: [https://facebook.github.io/create-react-app/docs/advanced-configuration](https://facebook.github.io/create-react-app/docs/advanced-configuration)  ### Deployment  This section has moved here: [https://facebook.github.io/create-react-app/docs/deployment](https://facebook.github.io/create-react-app/docs/deployment)  ### `npm run build` fails to minify  This section has moved here: [https://facebook.github.io/create-react-app/docs/troubleshooting#npm-run-build-fails-to-minify](https://facebook.github.io/create-react-app/docs/troubleshooting#npm-run-build-fails-to-minify)"
Frontend Pagination,React JS,https://github.com/bradtraversy/simple_react_pagination,"# Simple React Pagination  > Frontend pagination example using React with Hooks  ## Available Scripts  In the project directory, you can run:  ### `npm install`  ### `npm start`"
Digital Clock,React JS,https://github.com/esadakman/reactjs-digital-clock,"# Digital Clock - React    ## Objective    - Build a Digital Clock web site using ReactJS.    ## Description    - Remake of the javaScript digital clock project with React    ## Project Link  #### You can reach my project from [here](https://react-digital-clock-date.netlify.app/) üëà    ## Project Skeleton    ```  - Reviews Project (folder)  |  |----readme.md  SOLUTION  ‚îú‚îÄ‚îÄ public  ‚îÇ     ‚îî‚îÄ‚îÄ index.html  ‚îú‚îÄ‚îÄ src  ‚îÇ    ‚îú‚îÄ‚îÄ components  ‚îÇ    ‚îÇ       ‚îÇ‚îÄ‚îÄ fonts  ‚îÇ    ‚îÇ       ‚îÇ     ‚îú‚îÄ‚îÄ DS-DIGI.TTF  ‚îÇ    ‚îÇ       ‚îÇ     ‚îî‚îÄ‚îÄ DS-DIGIB.TTF   ‚îÇ    ‚îÇ       ‚îÇ‚îÄ‚îÄ clock.css  ‚îÇ    ‚îÇ       ‚îî‚îÄ‚îÄ clock.js  ‚îÇ    ‚îú‚îÄ‚îÄ App.js  ‚îÇ    ‚îú‚îÄ‚îÄ App.css  ‚îÇ    ‚îú‚îÄ‚îÄ index.js  ‚îÇ    ‚îî‚îÄ‚îÄ index.css  ‚îÇ  ‚îú‚îÄ‚îÄ package.json  ‚îî‚îÄ‚îÄ yarn.lock  ```    ### At the end of the project, following topics are to be covered;    - HTML   - CSS  - JS  - ReactJS    ```  $ git clone https://github.com/esadakman/reactjs-digital-clock.git  $ cd ../react-digital-clock  $ npm install / yarn  $ npm start / yarn start  ```    ### Preview of the Project    ![digital_clock](https://user-images.githubusercontent.com/98649983/174446111-dca936ed-dc73-41e8-a327-aca310e32509.gif)   "
Chinese Checkers Game,React JS,https://github.com/GabrielMioni/react-checkers,"# React Checkers  Playable version here: [http://www.gabrielmioni.com/react-checkers/](http://www.gabrielmioni.com/react-checkers/)  This is a 2 player checkers game built in React.js:  Features: * Mobile friendly design * Game board maintains state history and players can undo moves * Untold hours of checkers based entertainment  ### Playing Human players take turns selecting piece from the board. Board squares are highlighted to show the player legal moves available for their selected piece.  A piece can be de-selected either by clicking on the already selected piece or just choosing another piece.  Players win either by: - Jumping (and eliminating) all of the opponent's pieces. - Making it impossible for them to move.  Once a player has committed to a jump they must take all available jumps. However starting a jump isn't required (this differs from checkers tournament rules, but that's not how I played when I was kid).  The rules are otherwise normal checkers. Kings are made by reaching the opponents side, multi-jumping is possible (and satisfying), you would probably rather play Nintendo -- but you can't for whatever reason. You're stuck with checkers.  ### Code The project was started using the NPM create-react-app  The game's business logic lives in the ReactCheckers component. The board is initialized as an object where key values represent board coordinates.  Pieces for both players are stored as the values in the board object. Example: the value of ReactComponent.state.history[{boardState['a6']}] starts the game as a player1 piece object. When the piece moves, the value at boardState['a6'] in the new history array element will be null.  Each time a player moves (including multi-jumps, where the turn has not gone to the opponent), the game records the board's state. Moves can be undone by clicking the 'Undo' button in the lower right. This changes the value used to reference the current board state used to render the game.  When a pieces is selected, available moves (including jumps) are passed to the game's state.  Victory conditions are checked each time a player ends their turn."
Match Algo game,React JS,https://github.com/Pavan-Kiran-Chidirala/reactmatchgameappjs,"In this project, let's build a **Match Game** by applying the concepts we have learned till now.  ### Refer to the video below:  <br/> <div style=""text-align: center;"">   <video style=""max-width:80%;box-shadow:0 2.8px 2.2px rgba(0, 0, 0, 0.12);outline:none;"" loop=""true"" autoplay=""autoplay"" controls=""controls"" muted>     <source src=""https://assets.ccbp.in/frontend/content/react-js/match-game-output.mp4"" type=""video/mp4"">   </video> </div> <br/>  ### Design Files  <details> <summary>Click to view</summary>  - [Extra Small (Size < 576px) and Small (Size >= 576px)](https://assets.ccbp.in/frontend/content/react-js/match-game-sm-outputs.png) - [Medium (Size >= 768px), Large (Size >= 992px) and Extra Large (Size >= 1200px) - Match Game](https://assets.ccbp.in/frontend/content/react-js/match-game-lg-output.png) - [Medium (Size >= 768px), Large (Size >= 992px) and Extra Large (Size >= 1200px) - Scorecard](https://assets.ccbp.in/frontend/content/react-js/match-game-score-card-lg-output.png)  </details>  ### Set Up Instructions  <details> <summary>Click to view</summary>  - Download dependencies by running `npm install` - Start up the app using `npm start` </details>  ### Completion Instructions  <details> <summary>Functionality to be added</summary> <br/>  The app must have the following functionalities  - Initially,   - Score should be `0` and time should be `60` sec   - The image to be matched should have the src attribute value as the value of the key `imageUrl` from the first object in **imagesList** provided   - The **Fruits** tab should be active and the thumbnails with **FRUIT** as their category should be displayed - The timer should start running backwards from the `60` sec - When a tab is clicked, then the thumbnails in the corresponding category should be displayed - When a thumbnail is clicked, if that is matched with the image to be matched,   - Score is incremented by one   - The new image to be matched should be generated randomly among the value of the key `imageUrl` from **imagesList** provided - When a thumbnail is clicked, if it is not matched with the image to be matched,   - The game should end, and the [Scorecard](https://assets.ccbp.in/frontend/content/react-js/match-game-score-card-lg-output.png) view should be displayed   - When **PLAY AGAIN** button is clicked, then we should be able to play the game again     - The score and time values should be reset to `0` and `60` sec respectively     - The image to be matched should reset to the value of the key `imageUrl` from the first object in **imagesList** provided     - The active tab should reset to **Fruits**, and the thumbnails with **FRUIT** as their category should be displayed - When the timer reached `0` sec, then the game should end, and the [Scorecard](https://assets.ccbp.in/frontend/content/react-js/match-game-score-card-lg-output.png) view should be displayed - The App is provided with `tabsList`. It consists of a list of tabItem objects with the following properties in each tabItem object    |     Key     | Data Type |   | :---------: | :-------: |   |    tabId    |  String   |   | displayText |  String   |  - The App is provided with `imagesList`. It consists of a list of imageItem objects with the following properties in each imageItem object    |     Key      | Data Type |   | :----------: | :-------: |   |      id      |  String   |   |   imageUrl   |  String   |   | thumbnailUrl |  String   |   |   category   |  String   |  </details>  ### Important Note  <details> <summary>Click to view</summary>  <br/>  **The following instructions are required for the tests to pass**  - The image to be matched in the app should have the alt as **match** - The thumbnail images in the app should have the alt as **thumbnail**  </details>  ### Resources  <details> <summary>Image URLs</summary>  - [https://assets.ccbp.in/frontend/react-js/match-game-bg.png](https://assets.ccbp.in/frontend/react-js/match-game-bg.png) - [https://assets.ccbp.in/frontend/react-js/match-game-score-card-lg-bg.png](https://assets.ccbp.in/frontend/react-js/match-game-score-card-lg-bg.png) - [https://assets.ccbp.in/frontend/react-js/match-game-score-card-sm-bg.png](https://assets.ccbp.in/frontend/react-js/match-game-score-card-sm-bg.png) - [https://assets.ccbp.in/frontend/react-js/match-game-website-logo.png](https://assets.ccbp.in/frontend/react-js/match-game-website-logo.png) alt should be **website logo** - [https://assets.ccbp.in/frontend/react-js/match-game-timer-img.png](https://assets.ccbp.in/frontend/react-js/match-game-timer-img.png) alt should be **timer** - [https://assets.ccbp.in/frontend/react-js/match-game-play-again-img.png](https://assets.ccbp.in/frontend/react-js/match-game-play-again-img.png) alt should be **reset** - [https://assets.ccbp.in/frontend/react-js/match-game-trophy.png](https://assets.ccbp.in/frontend/react-js/match-game-trophy.png) alt should be **trophy**  </details>  <details> <summary>Colors</summary>  <br/>  <div style=""background-color:#2c0e3a; width: 150px; padding: 10px; color: white"">Hex: #2c0e3a</div> <div style=""background-color:#ffffff; width: 150px; padding: 10px; color: black"">Hex: #ffffff</div> <div style=""background-color:#fec653; width: 150px; padding: 10px; color: black"">Hex: #fec653</div> <div style=""background-color:#cf60c8; width: 150px; padding: 10px; color: black"">Hex: #cf60c8</div> </details>  <details> <summary>Font-families</summary>  - Roboto  </details>  > ### _Things to Keep in Mind_ > > - All components you implement should go in the `src/components` directory. > - Don't change the component folder names as those are the files being imported into the tests. > - **Do not remove the pre-filled code** > - Want to quickly review some of the concepts you‚Äôve been learning? Take a look at the Cheat Sheets."
Text To Speech Converter App,React JS,https://github.com/SahilAggarwal2004/react-text-to-speech,"# react-text-to-speech  An easy-to-use React.js component that leverages the [Web Speech API](https://developer.mozilla.org/en-US/docs/Web/API/Web_Speech_API) to convert text to speech.  ## Features  - **Text-to-Speech:** Converts text to speech using the Web Speech API. - **Text Highlighting:** Highlights text as it is read aloud using the [`useSpeech` hook](https://rtts.vercel.app/docs/usage/useSpeech#highlight-text) or [`Speech` component](https://rtts.vercel.app/docs/usage/speech#highlight-text). - **Error and Event Handling:** Provides APIs for managing errors and events via [`useSpeech` hook](https://rtts.vercel.app/docs/usage/useSpeech#handling-errors-and-events) or [`Speech` component](https://rtts.vercel.app/docs/usage/speech#handling-errors-and-events). - **Multiple Speech Instances:** Supports multiple speech instances using [`useSpeech` hook](https://rtts.vercel.app/docs/usage/useSpeech#multiple-instance-usage) or [`Speech` component](https://rtts.vercel.app/docs/usage/speech#multiple-instance-usage). - **Customization:** Fully customizable through the [`useSpeech` hook](https://rtts.vercel.app/docs/usage/useSpeech) or [`Speech` component](https://rtts.vercel.app/docs/usage/speech#full-customization). - **Unlimited Text Input:** Overcomes the [Web Speech API's text length limit](https://developer.mozilla.org/en-US/docs/Web/API/SpeechSynthesisUtterance/text) for continuous speech. - **Dynamic Controls:** Dynamically adjust `pitch`, `rate`, `volume`, `lang`, and `voiceURI` during speech. - **Auto Cleanup:** Automatically stops speech when the component unmounts.  ## Installation  Install `react-text-to-speech` using your preferred package manager:  ```bash # Using npm: npm install react-text-to-speech --save  # Using Yarn: yarn add react-text-to-speech  # Using pnpm: pnpm add react-text-to-speech  # Using Bun: bun add react-text-to-speech ```  ## Usage  **react-text-to-speech** provides two primary methods to integrate text-to-speech functionality into your React.js applications: the `useSpeech` hook and the `<Speech>` component.  ### `useSpeech` Hook  #### Basic Usage  ```tsx import React from ""react""; import { useSpeech } from ""react-text-to-speech"";  export default function App() {   const {     Text, // Component that renders the processed text     speechStatus, // Current speech status     isInQueue, // Indicates whether the speech is currently playing or waiting in the queue     start, // Starts or queues the speech     pause, // Pauses the speech     stop, // Stops or removes the speech from the queue   } = useSpeech({ text: ""This library is awesome!"" });    return (     <div style={{ display: ""flex"", flexDirection: ""column"", rowGap: ""1rem"" }}>       <Text />       <div style={{ display: ""flex"", columnGap: ""0.5rem"" }}>         {speechStatus !== ""started"" ? <button onClick={start}>Start</button> : <button onClick={pause}>Pause</button>}         <button onClick={stop}>Stop</button>       </div>     </div>   ); } ```  #### Detailed Usage  For more details on using the `useSpeech` hook, [refer to the documentation](https://rtts.vercel.app/docs/usage/useSpeech).  ### `<Speech>` Component  #### Basic Usage  ```tsx import React from ""react""; import Speech from ""react-text-to-speech"";  export default function App() {   return <Speech text=""This library is awesome!"" />; } ```  #### Detailed Usage  For more details on using the `<Speech>` component, [refer to the documentation](https://rtts.vercel.app/docs/usage/speech).  ## Demo  [Check out the live demo](https://rtts.vercel.app/demo) to see it in action.  ## Documentation  Explore the [documentation](https://rtts.vercel.app/docs/) to get started quickly.  ## Contribute  Show your ‚ù§Ô∏è and support by giving a ‚≠ê on [GitHub](https://github.com/SahilAggarwal2004/react-text-to-speech). You can also support the project by upvoting and sharing it on [Product Hunt](https://www.producthunt.com/posts/react-text-to-speech). Any suggestions are welcome! Take a look at the [contributing guide](CONTRIBUTING.md).  ## License  This project is licensed under the [MIT License](LICENSE)."
Text Utility App,React JS,https://github.com/AkshataGanbote/Text-Utility-App,"<h1><strong>Text-Utility-App</strong></h1>  <h3>üëã Introduction </h3> Text Utility App is a text to audio converter and can be used to manipulate the text in different ways. Additionally, it also counts words, characters and also shows average reading time for your text.    </br> </br>  ![Screenshot (184)](https://github.com/AkshataGanbote/Text-Utility-App/assets/117456092/36598f4c-4582-4e60-ac11-a9304d027206)   <h4> Dark Mode</h6>   ![Screenshot (185)](https://github.com/AkshataGanbote/Text-Utility-App/assets/117456092/e627efef-c7cf-4a3c-b6d3-78d6697c7dc6)    <h3>‚ñ∂Ô∏è Live Demo  :- https://text-utility-text-to-speech.netlify.app/      *** <h3>üî•Features</h3>  Following functionalities are implemented in the app :  - Text To Speech (Listen) - Convert to UPPERCASE - Convert to lowercase - Capitalize text - Remove Extra Space - Clear text   It also shows :  - Count of Characters - Count of Words - Count of Space - Reading Time  ***  <h3>üíª Technology Used</h3>  - React.JS  - Bootstrap  - Netlify  ***  <h2> Do not forget to give a star! ‚≠êü§ó </h2>"
fitness app,React JS,https://github.com/minnukota381/Fitness-Tracking-App-UI-React,"# Fitness Tracker Application  ## Table of Contents  - [Introduction](#introduction) - [Features](#features) - [Screenshots](#screenshots) - [Installation](#installation) - [Usage](#usage) - [Technologies](#technologies) - [Folder Structure](#folder-structure) - [License](#license) - [Acknowledgements](#acknowledgements)  ## Introduction  The Fitness Tracker Application is a React-based web application designed to help users monitor their fitness activities. It features an overview of their progress, activity cards, friends list, and more, making it easy to keep track of fitness goals and stay motivated.  ## Features  - **Overview Section**: Displays total time, steps, and target achievements. - **Activity Cards**: Show progress of various activities like biking, jogging, and walking. - **Friends List**: Displays friends' recent activities and progress. - **Header**: Includes a search bar and user profile picture. - **Sidebar**: Navigation menu with icons for home, activities, statistics, settings, and logout. - **Charts**: Visual representation of fitness data using line and doughnut charts.  ## Screenshots  ![Fitness Tracker Overview](./Screenshots/FTHOME.png)  ## Installation  1. Clone the repository:      ```bash     git clone https://github.com/minnukota381/Fitness-Tracking-App-UI-React.git     ```  2. Navigate to the project directory:      ```bash     cd Fitness-Tracking-App-UI-React     ```  3. Install the dependencies:      ```bash     npm install     ```  4. Start the development server:      ```bash     npm start     ```  ## Usage  1. Open your browser and go to `http://localhost:3000` to see the application in action. 2. Use the sidebar to navigate through different sections of the application. 3. View your fitness progress, check your friends' activities, and manage your fitness goals.  ## Technologies  - **React**: JavaScript library for building user interfaces. - **Chart.js**: For creating interactive charts. - **React-Icons**: For integrating icons in the UI. - **CSS3**: For styling the application.  ## Folder Structure  ```plaintext fitness-tracker/ ‚îÇ ‚îú‚îÄ‚îÄ public/ ‚îÇ   ‚îú‚îÄ‚îÄ index.html ‚îÇ   ‚îî‚îÄ‚îÄ ... ‚îÇ ‚îú‚îÄ‚îÄ src/ ‚îÇ   ‚îú‚îÄ‚îÄ Assets/ ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ kavya.jpg ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ bhargav.jpg ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sankar.jpg ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chaitu.jpg ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ minnu.JPG ‚îÇ   ‚îú‚îÄ‚îÄ components/ ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ActivityCards.js ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ DonutChart.js ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ FriendsList.js ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Header.js ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Overview.js ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Sidebar.js ‚îÇ   ‚îú‚îÄ‚îÄ styles/ ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ActivityCards.css ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ FriendsList.css ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Header.css ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Overview.css ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Sidebar.css ‚îÇ   ‚îú‚îÄ‚îÄ App.css ‚îÇ   ‚îú‚îÄ‚îÄ App.js ‚îÇ   ‚îî‚îÄ‚îÄ index.js ‚îÇ ‚îî‚îÄ‚îÄ package.json ```  ## License  This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.  ## Acknowledgements  - Thanks to the open-source community for providing valuable libraries and tools."
Text to Handwriting,React JS,https://github.com/hhhrrrttt222111/handReacting," <p  align=""center"">  <img  alt=""handreacting_logo""  src=""./src/media/mainlogo.png""  width=""250px""  />  </p>      # HandReacting  [![GitHub stars](https://img.shields.io/github/stars/hhhrrrttt222111/handReacting?color=ff69b4&style=flatsquare)](https://github.com/hhhrrrttt222111/handReacting/stargazers) [![GitHub forks](https://img.shields.io/github/forks/hhhrrrttt222111/handReacting?color=blueviolet&style=flatsquare)](https://github.com/hhhrrrttt222111/handReacting/network)      Are you tired and fed up of the multitude of written assignments that you have to submit?  ### *HandReacting* is the PERFECT solution to all your problems. It converts typed documents into handwritten ones üñã, saving you a hella lotta time. ‚è≥      <br><br><br>         <p  align=""center"">  <img  alt=""handreacting_image""  src=""./src/media/read.svg""  width=""250px""  />  </p>  ## About the Project HandReacting is an app which allows the user to generate handwritten text in the form of a downloadable image from the input given. It can be used by students to generate their assignments so that they don't have to go through the hassle of writing down everything physically.  The project has been made using React.  ## Prerequisites  Required to install and run the software:  -   [npm](https://www.npmjs.com/get-npm)  ## Installing and Running From the project folder, run these commands in console (terminal) to install dependencies and run the app: ``` npm install npm start ``` Open [http://localhost:3000](http://localhost:3000/) with your browser to see the result.  ## Usage Instructions   - Choose from 34 fonts that looks handwritten  - Adjust font size to choose number of words in a line  - Adjust font weight to change boldness of the text  - Letter spacing and word spacing helps arrange words more closely  - Change font color and paper page color to get paper like appearance  - Adjusting line height is important when page lines are enabled  - Page lines provide lines in the paper  - Scan effect provides a shadow to the page  - Page margin gives a margin to the page  ## Screenshots  * Converting Text to 34 different handwritten font faces.  <p  align=""center"">  <img  src=""./src/media/main.PNG""  alt=""""/>  </p>  <br>      * Extracting text from images before converting to various fonts.  <p  align=""center"">  <img  src=""./src/media/extract.PNG""  alt=""""/>  </p>  <br>      * Converting speech to text  <p  align=""center"">  <img  src=""./src/media/voice.PNG""  alt=""""/>  </p>      <br  ><br  ><br  >  ## Contributing  -   Refer the [CONTRIBUTING.md](https://github.com/hhhrrrttt222111/handReacting/blob/master/CONTRIBUTING.md) file for the guidelines for contributing to the project.  ## Notes HandReacting is a project made with React JS.   **For any suggestions or bug reports, refer [CONTRIBUTING.md](https://github.com/hhhrrrttt222111/handReacting/blob/master/CONTRIBUTING.md) and raise an issue.**   And yes, you can also thank me for making your life easier by giving a ‚≠ê for the HandReacting repository  ## Resources  - If you are new to Git - [Git & Github for Beginners](https://www.youtube.com/watch?v=SWYqp7iY_Tc) - For a more detailed understanding of the git flow - [Git Workflow Explained](https://medium.com/@swinkler/git-workflow-explained-a-step-by-step-guide-83c1c9247f03) - **Before making changes, refer - [CONTRIBUTING.md](https://github.com/hhhrrrttt222111/handReacting/blob/master/CONTRIBUTING.md)** - Writing good commit messages - [5 Useful Tips For A Better Commit Message](https://thoughtbot.com/blog/5-useful-tips-for-a-better-commit-message)    ### Thank You!"
Reddit Clone,React JS,https://github.com/SashenJayathilaka/Reddit-Clone,"<div align=""center"">    <img src=""https://user-images.githubusercontent.com/99184393/196572825-73d2a0dc-f96e-45af-884b-77ed7cf20184.png"" alt=""logo"" width=""200"" height=""auto"" />      <h1>Reddit Clone with REACT.JS</h1>      <p> Reddit Clone with REACTJS (Next.js, Firebase v9, Chakra UI, TypeScript, Recoil, (Image Uploading, Google Authentication, Create Community, Join Community, Leave Community, Upvote and Downvote Posts), Dark Mode & Light Mode, Data Encryption and Decryption)   </p>       <!-- Badges -->  <a href=""https://reddit-sclone.vercel.app"" target=""_blank"">![](https://img.shields.io/website-up-down-green-red/http/monip.org.svg)</a> ![](https://img.shields.io/badge/Maintained-Yes-indigo) ![](https://img.shields.io/github/forks/SashenJayathilaka/Reddit-Clone.svg) ![](https://img.shields.io/github/stars/SashenJayathilaka/Reddit-Clone.svg) ![](https://img.shields.io/github/issues/SashenJayathilaka/Reddit-Clone) ![](https://img.shields.io/github/last-commit/SashenJayathilaka/Reddit-Clone)  <h4>     <a href=""https://reddit-sclone.vercel.app"">View Demo</a>   <span> ¬∑ </span>     <a href=""https://github.com/SashenJayathilaka/Reddit-Clone/blob/master/README.md"">Documentation</a>   <span> ¬∑ </span>     <a href=""https://github.com/SashenJayathilaka/Reddit-Clone/issues"">Report Bug</a>   <span> ¬∑ </span>     <a href=""https://github.com/SashenJayathilaka/Reddit-Clone/issues"">Request Feature</a>   </h4> </div>  <br />  <!-- Table of Contents -->  ## :notebook_with_decorative_cover: Table of Contents  - [About the Project](#star2-about-the-project)   - [Screenshots](#camera-screenshots)   - [Tech Stack](#space_invader-tech-stack)   - [Environment Variables](#key-environment-variables) - [Getting Started](#toolbox-getting-started)   - [Prerequisites](#bangbang-prerequisites)   - [Installation](#gear-installation)   - [Run Locally](#running-run-locally)   - [Deployment](#triangular_flag_on_post-deployment) - [Contact](#handshake-contact)  <!-- About the Project -->  ## :star2: About the Project  <!-- Screenshots -->  ### :camera: Screenshots  - Create Community, Join Community, Leave Community, Upvote and Downvote Posts  <div align=""center""> <a href=""https://reddit-sclone.vercel.app""><img width='800rem' src='./demo/ezgif-5-bba0183ff4.gif' alt='image'/></a> </div>  <br />  - Dark Mode & Light Mode  <div align=""center""> <a href=""https://reddit-sclone.vercel.app""><img width='800rem' src='./demo/ezgif-5-d29f199523.gif' alt='image'/></a> </div>  <br />    - User Profile Section    <div align=""center""> <a href=""https://reddit-sclone.vercel.app""><img width='800rem' src='./demo/ezgif-4-c9902f3c2a.gif' alt='image'/></a> </div>  <br />    - Conversation Section (Reddit Clone Live Chat)    <div align=""center""> <a href=""https://reddit-sclone.vercel.app""><img width='800rem' src='./demo/ezgif-1-30bf5499cc.gif' alt='image'/></a> </div>  ## <a href=""https://reddit-sclone.vercel.app"" target=""_blank"">LIVE DEMO üí•</a>  #### üî¥ Open the camera app on your device and scan the code below (live demo)  <a href=""https://reddit-sclone.vercel.app""><img src=""./demo/qr/qr-code.png"" alt=""qr"" width=""150"" height=""150"" /></a> <a href=""#instagram""><img src=""https://user-images.githubusercontent.com/99184393/182557606-b36f2540-1260-42bf-b547-ed5832e3615e.png"" alt=""qr"" width=""150"" height=""150"" /></a>  ![forthebadge](https://forthebadge.com/images/badges/built-with-love.svg) ![forthebadge](https://forthebadge.com/images/badges/for-you.svg) ![forthebadge](https://forthebadge.com/images/badges/powered-by-coffee.svg)  ### :space_invader: Tech Stack  <details>   <summary>Client</summary>   <ul>     <li><a href=""https://#/"">Typescript</a></li>     <li><a href=""https://nextjs.org/"">Next.js</a></li>     <li><a href=""https://reactjs.org/"">React.js</a></li>     <li><a href=""https://chakra-ui.com/"">Chakra UI</a></li>   </ul> </details>  <details> <summary>Database</summary>   <ul>     <li><a href=""https://firebase.google.com"">Firebase</a></li>   </ul> </details>  <details>   <summary>Functions</summary>   <ul>     <li><a href=""https://firebase.google.com/docs/functions"">Cloud Functions for Firebase</a></li>   </ul> </details>  <br />  <table>     <tr>         <td> <a href=""#""><img src=""https://raw.githubusercontent.com/devicons/devicon/master/icons/react/react-original.svg"" alt="""" width=""30"" height=""30"" /></a>         </td>                         <td> <a href=""#""><img src=""https://user-images.githubusercontent.com/99184393/183096870-fdf58e59-d78c-44f4-bd1c-f9033c16d907.png"" alt=""Google"" width=""30"" height=""30"" /></a>         </td>                         <td> <a href=""#""><img src=""https://user-images.githubusercontent.com/99184393/180462270-ea4a249c-627c-4479-9431-5c3fd25454c4.png"" alt="""" width=""30"" height=""30"" /></a>         </td>                                 <td> <a href=""#""><img src=""https://user-images.githubusercontent.com/99184393/187247836-8df8fb4e-074c-4f3a-a150-555108f4c2c7.png"" alt="""" width=""30"" height=""30"" /></a>         </td>                                 <td> <a href=""#""><img src=""https://user-images.githubusercontent.com/99184393/177784603-d69e9d02-721a-4bce-b9b3-949165d2edeb.png"" alt="""" width=""30"" height=""30"" /></a>         </td>                                       <td> <a href=""#""><img src=""https://user-images.githubusercontent.com/99184393/187247589-11b213ce-0cbb-42f7-bbc2-e68634d23e51.png"" alt="""" width=""40"" height=""40"" /></a>         </td>     </tr> </table>  ## :toolbox: Getting Started  ### :bangbang: Prerequisites  - Sign up for a Firebase account <a href='https://firebase.google.com'>HERE</a> - Install Node JS in your computer <a href='https://nodejs.org/en/'>HERE</a>  <!-- Env Variables -->  ### :key: Environment Variables  To run this project, you will need to add the following environment variables to your .env file  `NEXT_PUBLIC_FIREBASE_API_KEY`  `NEXT_PUBLIC_FIREBASE_AUTH_DOMAIN`  `NEXT_PUBLIC_FIREBASE_PROJECT_ID`  `NEXT_PUBLIC_FIREBASE_STORAGE_BUCKET`  `NEXT_PUBLIC_FIREBASE_MESSAGING_SET`  `NEXT_PUBLIC_FIREBASE_APP_ID`  `NEXT_PUBLIC_BASE_URL`  `NEXT_PUBLIC_CRYPTO_SECRET_PASS`  This project was bootstrapped with [Create React App](https://github.com/facebook/create-react-app).  ### :gear: Installation  ![](https://img.shields.io/badge/React-20232A?style=for-the-badge&logo=react&logoColor=61DAFB) ![](https://img.shields.io/badge/next.js-20232A?style=for-the-badge&logo=next.js&logoColor=61DAFB)  Install my-project with npm  ``` npx create-next-app@latest --ts my-project ```  ``` cd my-project ```  Install dependencies  ### In your Next.js project, install Chakra UI  ![](https://img.shields.io/badge/UI-Chakra%20UI-green)  #### Installation  In your Next.js project, install Chakra UI by running either of the following:  ``` npm i @chakra-ui/react @emotion/react@^11 @emotion/styled@^11 framer-motion@^6 or yarn add @chakra-ui/react @emotion/react@^11 @emotion/styled@^11 framer-motion@^6 ```  Provider Setup After installing Chakra UI, you need to set up the `ChakraProvider` at the root of your application.  Go to `pages/_app.js` or `pages/_app.tsx` (create it if it doesn't exist) and wrap the `Component` with the `ChakraProvider`:  ```tsx // pages/_app.js import { ChakraProvider } from ""@chakra-ui/react"";  function MyApp({ Component, pageProps }) {   return (     <ChakraProvider>       <Component {...pageProps} />     </ChakraProvider>   ); }  export default MyApp; ```  <a href=""https://chakra-ui.com/getting-started/nextjs-guide"" target=""_blank"">üî∑ Customizing theme & More</a>  Install dependencies  <a href=""https://github.com/SashenJayathilaka/Reddit-Clone/blob/master/package.json"" target=""_blank"">üî∂ Dependency Info</a>  <!-- Run Locally -->  ### :running: Run Locally  ![](https://img.shields.io/badge/GIT-E44C30?style=for-the-badge&logo=git&logoColor=white)  Clone the project  ```bash   git clone https://github.com/SashenJayathilaka/Reddit-Clone.git ```  change directory  ```bash   cd Reddit-Clone ```  Install dependencies  ```bash   npm install ```  Start the server  ```bash   npm run dev ```  <hr />  This is a [Next.js](https://nextjs.org/) project bootstrapped with [`create-next-app`](https://github.com/vercel/next.js/tree/canary/packages/create-next-app).  <hr />  Open [http://localhost:3000](http://localhost:3000) with your browser to see the result.  You can start editing the page by modifying `pages/index.js`. The page auto-updates as you edit the file.  [API routes](https://nextjs.org/docs/api-routes/introduction) can be accessed on [http://localhost:3000/api/hello](http://localhost:3000/api/hello). This endpoint can be edited in `pages/api/hello.js`.  The `pages/api` directory is mapped to `/api/*`. Files in this directory are treated as [API routes](https://nextjs.org/docs/api-routes/introduction) instead of React pages.  ### Learn More  To learn more about Next.js, take a look at the following resources:  - [Next.js Documentation](https://nextjs.org/docs) - learn about Next.js features and API. - [Learn Next.js](https://nextjs.org/learn) - an interactive Next.js tutorial.  You can check out [the Next.js GitHub repository](https://github.com/vercel/next.js/) - your feedback and contributions are welcome!  <!-- Deployment -->  ### :triangular_flag_on_post: Deployment  To deploy this project run  ##### Deploy on Vercel  ![](https://img.shields.io/badge/Vercel-000000?style=for-the-badge&logo=vercel&logoColor=white)  The easiest way to deploy your Next.js app is to use the [Vercel Platform](https://vercel.com/new?utm_medium=default-template&filter=next.js&utm_source=create-next-app&utm_campaign=create-next-app-readme) from the creators of Next.js.  Check out our [Next.js deployment documentation](https://nextjs.org/docs/deployment) for more details.  ## :handshake: Contact  Sashen - [@twitter_handle](https://twitter.com/SashenHasinduJ) - sashenjayathilaka95@gmail.com  Project Link: [https://github.com/SashenJayathilaka/Reddit-Clone.git](https://github.com/SashenJayathilaka/Reddit-Clone.git)  <hr />"
SnapChat Clone,React JS,https://github.com/TowhidKashem/snapchat-clone,"<h1 align=""center"">üëª Snapchat Clone</h1>  <div align=""center"">   <img src=""./public/readme/tech-logos/react.svg"" width=""55"" alt=""React"" />   <img src=""./public/readme/tech-logos/redux.svg"" width=""55"" alt=""Redux"" />   <img     src=""./public/readme/tech-logos/typescript.svg""     width=""55""     alt=""TypeScript""   />   <img     src=""./public/readme/tech-logos/javascript.svg""     width=""55""     alt=""JavaScript""   />   <img src=""./public/readme/tech-logos/sass.svg"" width=""55"" alt=""Sass"" />   <img src=""./public/readme/tech-logos/webpack.svg"" width=""55"" alt=""Webpack"" />   <img src=""./public/readme/tech-logos/gulp.svg"" width=""35"" alt=""gulp"" />   <img src=""./public/readme/tech-logos/cypress.svg"" width=""55"" alt=""Cypress"" />   <img src=""./public/readme/tech-logos/jest.svg"" width=""55"" alt=""Jest"" />   <img src=""./public/readme/tech-logos/eslint.svg"" width=""55"" alt=""Eslint"" />   <img     src=""./public/readme/tech-logos/prettier.svg""     width=""55""     alt=""Prettier""   />   <img     src=""./public/readme/tech-logos/storybook.svg""     width=""50""     alt=""Storybook""   /> </div>  <h2 align=""center"">   <a href=""https://towhidkashem.github.io/snapchat-clone/"">[LIVE APP]</a> </h2>  <img src=""public/readme/filters.gif"" alt=""Preview"" />  <h3 align=""center"">   <a href=""https://towhidkashem.github.io/snapchat-clone/"">[Live App]</a>   &nbsp;&bull;&nbsp;   <a href=""https://www.youtube.com/embed/aRS88v-duKg?autoplay=1"">[Video Demo]</a> </h3>  <h2>‚ö°Ô∏èBreakdown</h2>  <ul>   <li>     Built with <code>React</code>     <ul>       <li>Only functional components using hooks</li>       <li>         Folder structure:         <ul>           <li>Flat - no greater than one level deep</li>           <li>             Modular - each folder contains all the relevant files needed to make             up a particular feature (components, styles, tests, actions, etc).             Having everything close at hand reduces cognitive load and deleting             a folder removes the feature entirely from the code base without             having to worry about left over code           </li>           <li>             Organized semantically by Feature (not by the traditional             ""components/containers"" model), this way of reasoning is more human             friendly           </li>           <li><code>components</code> directory houses all shared components</li>         </ul>       </li>       <li>Custom component library showcased in <code>Storybook</code></li>       <li>Relatively few prod dependencies</li>     </ul>   </li>   <li>     Global state management via <code>Redux</code>     <ul>       <li>         Uses <code>Redux Toolkit</code> - the official recommended         approach to using Redux which drastically cuts the need to write         boilerplate code       </li>       <li>         A single <code>store.ts</code> file for each feature contains all actions and         reducers (the creators are auto generated by RTK)       </li>       <li>Flat state tree avoids deeply nested properties</li>       <li>         RTK has built-in support for <code>ImmerJS</code> which allows state to         be safely mutated removing the need for messy object copying via spread         operators       </li>       <li>         Uses the <code>useDispatch</code> and <code>useSelector</code> hooks         provided by <code>react-redux</code> for accessing state values and         dispatching actions over the more verbose <code>connect</code> method       </li>       <li>Uses <code>thunk</code> for async operations</li>       <li>         Integrates the powerful         <code>Redux Devtools Extension</code> for ease of development       </li>     </ul>   </li>   <li>     Styled with <code>SASS</code>     <ul>       <li>         Each view's set of rules are scoped to a single parent element via         nesting to avoid style clashes       </li>       <li>         Use of variables, extendables and mixins to keep things DRY and uniform       </li>     </ul>   </li>   <li>     Written in <code>Typescript</code>     <ul>       <li>         To let the compiler catch bugs at build time instead of letting users         catch them at runtime!       </li>     </ul>   </li>   <li>Unit tested with <code>Jest</code> and <code>Enzyme</code></li>   <li>     End-to-end tested with <code>Cypress</code>     <ul>       <li>         Selectors use <code>data</code> attributes instead of classes or ids as         these can change often causing tests to break       </li>       <li>         Integration suite covers all essential feature happy paths       </li>     </ul>   </li>   <li>Linted using <code>Eslint</code></li>   <li>     Code is auto formatted using <code>Prettier</code> (ran as a pre-commit git     hook) before it gets pushed to the repo   </li>   <li>     Feels close to a native app if you ""add to homescreen"" on mobile   </li> </ul>  <h2>üíø Installation</h2>  <p>Run these commands in the terminal:</p>  <ol>   <li>     <code>$ git clone git@github.com:TowhidKashem/snapchat-clone.git</code>   </li>   <li><code>$ cd snapchat-clone</code></li>   <li>     <code>$ npm install</code>     <ul>       <li>         This will:         <ul>           <li>Install the dependencies in package.json</li>           <li>             Checkout             <a href=""https://github.com/jeeliz/jeelizFaceFilter"">jeelizFaceFilter</a>             package (used for the filters) and set it to the last version this             project was tested and confirmed to work with           </li>           <li>             Run <code>gulp</code> to concatenate, minify and transpile the files             located in <code>public/filters/source/*.js</code> into a single             file called <code>filters.min.js</code>           </li>         </ul>       </li>     </ul>   </li>   <li>     This part is optional but strongly recommended, without it you won't be able     to view any of the snap map features:     <ul>       <li>         Make a Mapbox account and         <a           href=""https://docs.mapbox.com/help/glossary/access-token/""           target=""_blank""           >get a free API key</a         >       </li>       <li>         In the <code>.env</code> file enter your new API key, for example:         <ul>           <li>             Before:             <code>REACT_APP_MAP_BOX_API_KEY=REPLACE_WITH_API_KEY</code>           </li>           <li>After: <code>REACT_APP_MAP_BOX_API_KEY=xy.abc123</code></li>         </ul>       </li>     </ul>   </li>   <li>     <code>$ npm start</code>     <ul>       <li>         The app should open automatically in your browser usually at         <code>https://localhost:3000/</code>         <ul>           <li>             In Chrome you will receive a ""Your connection is not private""             warning             <ul>               <li>                 Click ""Advanced"" &gt; ""Proceed to localhost (unsafe)""                 <ul>                   <li>                     You'll get this warning because the app uses a self signed                     <code>https</code> certificate. The                     <code>getUserMedia</code> API used by the camera requires                     the <code>https</code> protocol so we run the dev server in                     https mode.                   </li>                 </ul>               </li>             </ul>           </li>           <li>             After this you will be prompted to give access to your webcam, click             ""Allow""           </li>         </ul>       </li>     </ul>     <br />     <table>       <tbody>         <tr>           <th align=""center"">             Step 1           </th>           <th align=""center"">             Step 2           </th>           <th align=""center"">             Step 3           </th>         </tr>         <tr>           <td align=""center"" valign=""middle"">             <img src=""public/readme/step1.png"" />           </td>           <td align=""center"" valign=""middle"">             <img src=""public/readme/step2.png"" />           </td>           <td align=""center"" valign=""middle"">             <img src=""public/readme/camera.png"" />           </td>         </tr>       </tbody>     </table>   </li>   <li>You're all set! üéâ</li> </ol>  <table>   <tbody>     <tr>       <th colspan=""2"" align=""left"">         <h2>ü¶Æ Guides</h2>       </th>     </tr>     <tr>       <td align=""center"" valign=""top"">         <img src=""public/readme/guide.png"" />       </td>       <td valign=""top"">         Not all the buttons are actionable, many of them are there just for show         since this is a minimal demo. This         <a           href=""https://www.youtube.com/embed/aRS88v-duKg?autoplay=1""           target=""_blank""           >video</a         >         shows all the things you can currently do. Where it's not obvious which         buttons actually work I added red box-shadows as guides.       </td>     </tr>   </tbody> </table>  <table>   <tbody>     <tr>       <th colspan=""2"" align=""left"">         <h2>üõ† Tooling</h2>       </th>     </tr>     <tr>       <td valign=""top"">         <img src=""public/readme/storybook.png"" />         <p>           <strong>Storybook</strong> is used to showcase the app's custom           component library. You can run Storybook using the command           <code>npm run storybook</code>         </p>       </td>       <td valign=""top"">         <img src=""public/readme/redux-ext.png"" />         <p>           <strong>Redux Devtools Extension</strong> is implemented in the app,           it makes things like viewing the state tree, state flow and debugging           much easier, to use it you need to install the browser extension           <a             href=""https://chrome.google.com/webstore/detail/redux-devtools/lmhkpmbekcpmknklioeibfkpmmfibljd?hl=en""             >here</a           >           or           <a             href=""https://addons.mozilla.org/en-US/firefox/addon/reduxdevtools/""             >here</a           >         </p>       </td>     </tr>   </tbody> </table>  <h2>üß™ Testing</h2>  <table>   <tbody>     <tr>       <th colspan=""2"" align=""left"">         <strong>End-to-End Tests</strong>       </th>     </tr>     <tr>       <td align=""center"" valign=""top"">         <a href=""https://www.youtube.com/embed/tNrx6NlTYKI?autoplay=1"">           <img src=""public/readme/cypress.png"" width=""600"" />         </a>         <p>üëÜClick to see all tests run</p>       </td>       <td valign=""top"">         <ul>           <li>             All e2e tests are located in             <code>cypress/integration/*.spec.ts</code>             <ul>               <li>                 To run these first make sure the dev server is up and running                 via <code>npm start</code>, then use the command                 <code>npm run e2e</code>               </li>               <li>                 This will open the Cypress electron app. Click ""Run all specs""                 at the top right, you'll then get a Chrome instance and see all                 the tests being run               </li>             </ul>           </li>           <li>             Alternatively you can run the test suite in the terminal using the             command <code>npm run e2e-headless</code>. This command still             generates videos in <code>cypress/videos/*.mp4</code> of the tests             should you need them           </li>         </ul>       </td>     </tr>   </tbody> </table>  <table>   <tbody>     <tr>       <th colspan=""2"" align=""left"">         <strong>Unit Tests</strong>       </th>     </tr>     <tr>       <td valign=""top"">         <img src=""public/readme/unit.png"" width=""400"" />       </td>       <td valign=""top"">         <ul>           <li>             All the shared components in the <code>components</code> directory have             unit tests inside their respective folders. They end with a             <code>*.test.tsx</code> extension.           </li>           <li>             To run the unit test suite use the command             <code>npm run test</code>           </li>           <li>             These tests are also automatically run on each commit, if there are             any failures the commit will also fail           </li>         </ul>       </td>     </tr>   </tbody> </table>  <h2>üìù Misc Notes</h2>  <ul>   <li>     If you want to make changes to the filter files located in     <code>public/filters/src/*.js</code>, run the command     <code>npm run gulp watchJS</code> so that your changes get picked up   </li>   <li>     The project's <code>baseUrl</code> is set to the <code>src</code> directory     in tsconfig so you can use clean import paths like     <code>import Foo from 'components/Foo';</code> instead of messy ones like     <code>import Foo from '../../components/Foo';</code>. You can also use these in     the SASS files, e.g. <code>@import '~styles/foo';</code>   </li>   <li>     This is a purely front end demo, the ""API"" is nothing but a bunch of json     files with hard coded dummy data, they're located in     <code>/public/api/*.json</code>   </li> </ul>  <h2>‚ö†Ô∏è Contributing</h2>  <p>   Please note I won't be accepting PR's on this project since it's part of my   personal portfolio. You're more than welcome to fork and maintain your own   version if you like! </p>  <h2>‚öñÔ∏è License</h2>  <p>   The Snapchat name, artwork, trademark are all property of Snap Inc. This   project is provided for educational purposes only. It is not affiliated with   and has not been approved by Snap Inc. </p>"
Health Tracker App,React JS,https://github.com/ninalouw/health-tracker-react,"# Health Tracker App  ## Instructions  1. Clone or download the repo ``` git clone git@github.com:ninalouw/health-tracker-react.git npm install npm start ```  2. Or view the live app [here]().  ## Project Overview  This is a Health Tracker App built with a Rails API (see my other repo [health-tracker-rails](https://github.com/ninalouw/health-tracker-rails)) and React with Redux on the Front-End. It is inspired by the [FitBit](https://www.fitbit.com/en-ca/app) app, and allows users to track and visualize their calorie intake and their weight over time. It also allows them to set calorie and weight targets. It will use the [Nutrionix API](https://developer.nutritionix.com/) to fetch calorie data.   ## Project Resources  * [Nutrionix API](https://developer.nutritionix.com/) * Boilerplate [React Starter](https://github.com/mathemagics/react-starter) cloned from [@mathemagics](https://github.com/mathemagics). * [JWT Auth in Rails](http://www.thegreatcodeadventure.com/jwt-auth-in-rails-from-scratch/) * [JWT Auth with React-Redux](http://www.thegreatcodeadventure.com/jwt-authentication-with-react-redux/) * The incredible [Modern React with Redux](https://www.udemy.com/react-redux/learn/v4/content) taught by Stephen Grider."
Meme Generator,React JS,https://github.com/MarioWork/react-meme-generator,"# React Meme Generator This project consists of a React Meme Generator Application that allows the user to add two text inputs to a meme image and generating a custom meme, allowing the user to always change the image.  ## Technologies + Javascript + VsCode  + Html   + CSS + React + Node.js + JSX + REST + API  ## API Consumed [Meme Image API](https://imgflip.com/api)  ## Some UI Images ![memegenerator](https://user-images.githubusercontent.com/47696178/155040478-1ec38d0b-484c-4c77-baf4-0a75d7ec0a82.png)   ## Collaborators [Mario Vieira](https://github.com/MarioWork)  "
YouTube Clone,React JS,https://github.com/kennys-tech/react-youtube-clone,"# React YouTube clone  ## Demo of this app:  ![demo-gif](./readme_assets/youtube-clone-demo.gif)  ### Or Go to the live site and try it for yourself [here](https://react-youtubeclone.netlify.app/)  ## WARNING <strong> if the app does not load anything or the search function doesn't work, it is very likely the daily YouTube API quota has exceeded. There are only 5000 daily quotas for free usage, and each search costs 100 quotas. If the app doesn't work, I hope the animated GIF here can give you an idea of how it works. </strong>  ## Mobile view search function demo ![mobile-search-demo](./readme_assets/mobile-search-demo.gif)  ## What does this app do?  - It is a clone of YouTube HomePage and SearchPage. - HomePage displays the most popular videos of the selected country by querying data from the YouTube API. - HomePage utilizes infinite-scroll feature, so new videos thumbnails will load when the user keeps scrolling down the page. - Typing a word and clicking on search does a real search on YouTube API, 25 results are displayed on the SearchPage.  ## What is this project about?  - This is mainly a material-ui and styled-components learning project. I aimed at making the website look as close to the original YouTube as possible. - This project ends up turning into a state and complexity management exercise too because as the project progressed, I realized it is much bigger than I thought.  ## What technologies were used?  - react.js (create-react-app) - react-router - axios - styled-components - Material-UI v4 - jotai  ## Links to source code and live site:  - [Live site hosted on Netlify](https://react-youtubeclone.netlify.app/) - [Source code on Github](https://github.com/1codingguy/react-youtube-clone)  ## Detailed side-by-side comparison of the clone to the original  ![desktop_view_home](./readme_assets/desktop_view_home.png)  ![desktop_view_search](./readme_assets/desktop_view_search.png)  ![mobile_view_home](./readme_assets/mobile_view_home.png)  ![mobile_view_search](./readme_assets/mobile_view_search.png)  ![mobile_view_search_modal](./readme_assets/mobile_view_search_modal.png)  ## How to navigate this project? Click on the link for related source code:  1. Click on different countries in `ChipsBar` will display the most popular videos from that country, by querying the YouTube API. ([click here to view the `Chips` component](https://github.com/1codingguy/react-youtube-clone/blob/main/src/components/ChipsBar/Chips.jsx))  2. Header (Navbar) has different elements in different viewport size:    ![clone-header](./readme_assets/clone-header.gif) - `HamburgerMenuIcon` is hidden in mobile view. ([click here for `LeftContainer` component for details](https://github.com/1codingguy/react-youtube-clone/blob/main/src/components/Header/LeftContainer/LeftContainer.jsx)) - `SearchBox` is hidden in mobile view, a drawer will appear when clicked on the search icon. And ([Click here to view relevant code](https://github.com/1codingguy/react-youtube-clone/blob/main/src/components/Header/MiddleContainer/MiddleContainer.jsx#L67)) - `SearchPage` has a different YouTube logo in mobile view. There is also a filter button next to the search box (albeit not functional).  3. `HamburgerMenuIcon` has different roles:    ![sidebar-toggle](./readme_assets/Sidebar-toggle.gif) - In larger screen toggles between mini and full-width sidebar; - in smaller screen opens a drawer. - [Click here for `HamburgerMenuIcon` component](https://github.com/1codingguy/react-youtube-clone/blob/main/src/components/Header/LeftContainer/HamburgerMenuIcon.jsx) - Content of Sidebar is different depends on the screen size. [Click here to show the code that decide what Sidebar to show](https://github.com/1codingguy/react-youtube-clone/blob/main/src/components/Sidebar/SidebarToShow.jsx#L12)  4. A popup menu will appear if clicked on `MoreButton` in each `VideoCard`. [Click here to view `MoreButton` component](https://github.com/1codingguy/react-youtube-clone/blob/main/src/components/Videos/MoreButton.jsx)  5. A search result can be a video or a channel, they have different content and are displayed differently in the SearchPage. [Click here to view the relevant code](https://github.com/1codingguy/react-youtube-clone/blob/main/src/components/Search/ResultsVideoCard.jsx#L64)    ![search-results](/public/assets/search_results.png)  ## Some notable differences between original YouTube and my clone:  1. ChipsBar has no left and right button, and no blur out effect, unlike the original YouTube.    ![chipsbar-difference](./readme_assets/chipsbar-difference.png) 2. From 1952px, 5 columns of video thumbnails are displayed in the original YouTube. But since material-ui has a 12-column grid layout, there's no way I can have 5 columns since 12/5 is not a whole number.  ## Something you should expect when playing with the clone project:  - clicking on a row in any one of the popup menus will merely close the popup menu instead of routing to the another page. - clicking on most of the buttons won't do anything.  ## Why did I build the project this way?  1. Why ChipsBar lists different countries instead of keywords?     - If using different keywords, clicking on a single chip will perform a search based on that keyword.    - A search action costs 100 quotas on YouTube API, the daily quota is limited to 5000 for a free account.    - Querying popular videos from different countries only costs 1 quota.    - So ChipsBar is designed in this way because of the YouTube API quota.  2. Why use localStorage to save query results from YouTube?  - In the process of development I needed to load the SearchPage again and again, as each search action costs 100 quotas, daily limit quickly runs out. - Quota was running out once and I had to pause the development process. To avoid such interruption I opted to use localStorage. - Below picture shows my API quota was quickly used up without using localStorage.   ![api-quota](./readme_assets/api-quota.png)  ## How can you clone and tweak this project?  From your command line, first clone this repo:  ``` # Clone this repository $ https://github.com/1codingguy/react-youtube-clone.git  # Go into the repository $ cd react-youtube-clone  # Remove current origin repository $ git remote remove origin  ```  Then you can install the dependencies using NPM:  ``` # Install dependencies $ npm install  # Start development server $ npm start ```  Happy coding!  ---  ## Author  **coding-guy**  - [GitHub](https://github.com/1codingguy) - [Blog](https://blog.coding-guy.com/) - [Twitter](https://twitter.com/1codingguy)"
Library Management App,Spring Stack,https://github.com/saikat021/Library-Management-System,"# Library-Management-System ![](spring.PNG) ## Introduction  A Library Management System designed to see the books available in a college library. It allows students to register as a user and issue/return books from the college library hassle free. The backend is designed as a **Monolithic Architecture** with various nuances as discussed below. ## Technologies and Dependencies Used * [Maven](https://maven.apache.org/) used as a dependency management tool. * [Spring Boot](https://spring.io/projects/spring-boot) used to build hassle free web applications and writing REST APIs. * [Spring Security](https://spring.io/projects/spring-security) used for Authentication and Authorizations. * [Spring data JPA (Hibernate)](https://hibernate.org/) Used to reduce the time of writing hardcoded sql queries and instead allows to write much more readable and scalable code  * [MySQL](https://www.mysql.com/) used as a Java persistence store * [Project Lombok](https://projectlombok.org/) Reduces the time  of writing java boiler plate code.  ## Using Library Management System CLI--> ``` git clone https://github.com/saikat021/Library-Management-System.git cd Library-Management-System mvn package  java -jar target/Student-library-0.0.1-SNAPSHOT.jar ``` Intellij/Eclipse--> 1. Let maven resolve dependencies  2. run SpringBootApplication  ## Backend Design  ### Entities  Actors/Entities are inspired by the real world entities that can use the applications   1. **Student** having attributes: * unique primary key student_id, country, emailId, name, age, card_id(foreign key)  2. **Card** having attributes: * unique primary key card_id, createdOn, updatedOn, status(ACTVATED/DEACTIVATED) 3. **Book** having attributes: * Unique primary key book_id, isAvailable(True/False), genre, author_id(foreign key)  4. **Author** having attributes: * unique primary key author_id, country, name, emailId 5. **User** used mainly for authentication and authorization has attributes: * unique primary key user_id, Authorization--> (STUDENT/ADMIN or BOTH), Username(emailId for student), Password.   ### Relationships Between Entities and ER diagram An additional SQL table created to map the N:M mapping between the **Card** and the **Book** called **Transaction table**. The Transaction table has the following entities: * unique primary key transaction_id which is not given back to the client * Randomly Generated UUID given back to the calling client after the request is processed for future queries regarding the transactions * Card_id Foreign  key * Book_id Foreign key  * isIssue Operations (true for issue operations and false for return operation) * Transaction status (SUCCESSFUL/PENDING/FAILED) * date * fine amount (Applicable only while return operations and fine calculated based on a pre-defined Business logic written clearly in the Transaction Service class)   ER Diagram: ![](ER.PNG)  ### Functionalities Exposed  #### Student Controller class  The REST APIs exposed are  * CRUD APIs for the create, update, delete student information. The create student API **http://localhost:8080/student/createStudent** creates a student entity along with a card entry for that student and an user entity with Authorization as **STUDENT**. * Another API exposed is of changing password. The default login details of any user are Username:->emailId provided at the time of createStudent API hit and Password:->pass123(Bcrypt Encoded). This API used to change the login details mainly the password.   #### Book and Author Controller class  The REST APIs exposed are normal CRUD operations on Book and Author entities.  #### Transaction Controller  Two of the most important REST APIs exposed are: ##### Issue Book **https://localhost:8080/issueBook?bookId=_&cardId=_** goes through the following operations before issueing a book: Constraints : -->  1. Check if card is Activated? 2. Check if the book is available?  3. Check if the number of books issued with the requested card has gone past the maximum limit of number of books to be issued. Operations :--> 1. Book status marked unavailable in the Book table. 2. Book is mapped to a card  3. Transaction entry made in the transaction table and the Transaction unique UUID forwarded to the client in a Response Entity.   ##### Return Book **https://localhost:8080/returnBook?bookId=_&cardId=_** goes through the following operations before returning a book into the library. Constraints: --> 1. Check if the book_id and card_id given is valid? 2. Check if card_id is Activated(As when the student account is deleted by the student the card remains there in a Deactivated state for further accountability). Operations :--> 1. Make book available. 2. Make the card_id linked with the book null. 3. From the transaction Repository find the latest transaction entry in the table with the same card_id and book_id and is an issue opearation. Find the date from that and calculate the fine. 4. Make a new entry into the transaction table as an return operation and return the fine and unique UUID of the transaction entry to the calling client as a Response Entity.  Various transaction entries in the table: ![](transaction.PNG)   ### Security (Checkout Branch Security) Spring Security is used for Authentication and Authorization. For every API call it is checked whether the calling entity has cookies that make it a valid entity in the system and the Username(emailId in the student table) is the same as the details of whichever entity is changed by hitting a CRUD API regarding that table. * Few Examples: Each example API preceeded by ""http://localhost:8080""  * /student/all--> gives a list of all students in the system (ADMIN) * /student/findById --> gives a student with particular id (ADMIN) * /student/updateStudent--> update student details (STUDENT)[ADMIN should not be allowed to change the student details] * /student/changePassword--> Strictly STUDENT access * /transaction/all--> gives a list of all transactions in the system (ADMIN)  * /transaction/issueBook--> issues a book(STUDENT)   ..... ## Author and Developed by Saikat Chakraborty       "
Chatting App,Spring Stack,https://github.com/mtaghavian/simple-chat,"## What is this    This is a simple chat application demonstrating Spring Boot capabilities such as: * Authentication   * Using session validation (Login)   * Using cookies (Remember me functionality)   * Using basic authentication * Self-signed HTTPS connection * Web socket * Download/Upload files * JPA persistence * Redirecting to login page for logging in, then redirecting to desired page after that * Caching files in memory for better performance (Reloads file based on date-modified) * Clearing expired session after 1-hour idle * Keeping sessions in memory for better performance    This application also demonstrates some user interface capabilities such as: * Modal * Toast or notification * Web socket * Responsive * HttpRequest  ## How to use 1. Build application with ""mvn package"" 2. Run application with ""java -jar target/simplechat-0.0.1-SNAPSHOT.jar"" 3. Use Firefox or Chrome to open http://localhost:62600/  4. Sign-up at least two user accounts 5. Start chatting, you can do the following: 	* Send text massages 	* Upload files to create file messages 	* Download files by clicking on file-messages  ## About Masoud Taghavian (masoud.taghavian@gmail.com)   Enjoy!  ## Screenshots ![screenshot1](images/sc1.png ""Home page"") ![screenshot2](images/sc2.png ""Login page"") ![screenshot3](images/sc3.png ""Chat page 1"") ![screenshot4](images/sc4.png ""Chat page 2"") "
eWallet App,Spring Stack,https://github.com/dayalupatel/eWallet,## eWallet  ### website : https://dpatel-ewallet.up.railway.app/  ## What My Project Can Do :      * All Users can visit the site      * New Users can SignUp to use ewallet app     * Existing Users can Signin/login      * Users can see their Dashboard     * Users can Pay/Transfer the money to any existing User     * Users can Update their profile     * Users can check the balance in their wallet     * Users can add money in their wallet     * Users can see other users list on their dashboard     * Users can see their transactions list    ## About ewallet payment application is a full stack project. Spring boot is used as backend and html css as frontend and connected with thymeleaf.
Online Store App,Spring Stack,https://github.com/tuhinsaikh/Online-Shopping-App-SpringBoot-,"# Online-Shopping-App-SpringBoot Intro.. ----- This project is based on Online Shopping App using Java, Spring, SpringBoot, MySQL and Hibernate. There are 6 model classes in the project User, Customer, Address, Product, Cart and Order. The main goal of this project is to create a series of backend API‚Äôs so that the user can access the functionalities of the various types of functions of the models just like the functionalities one can get while shopping from any online website. This members responsible for the completion of this project are :  1. Tuhin Saikh  (User functionalities) -----------  2. Animesh Roy (Customer functionalities) -----------  3. Sampanna Chatterjee (Address functionalities) -------------------  4. Sourav Dhawa (Order functionalities) ------------  5. Shubham Mishra (Cart functionalities) --------------  6. Chandankumar Surendra Mourya (Product functionalities) ----------------------------  Entity Relationship Diagram ---------------------------  ![Annotation 2022-08-17 114603](https://user-images.githubusercontent.com/101566519/185047965-4bca8f2f-a99b-42bb-98e9-ecd71934242e.png)  Tech Stacks & Tools Used  ------------------------  Tech Stacks:  1. Java  2. MySQL  3. Spring  4. SpringBoot  5. Hibernate  Tools:  1. Spring Tool Suite  2. Swagger  3. Postman  Classes under the Model package -------------------------------  1. Address.java  2. User.java  3. Customer.java  4. MyOrder.java  5. Products.java  6. CategoryEnum  7. Cart.java  Some Endpoints to access the functionalities --------------------------------------------  Link to Swagger: http://localhost:8088/swagger-ui/index.html#/  To register as a user:  http://localhost:8088/regisrtration  To login as a user:  http://localhost:8088/login  To add new products  http://localhost:8088/newproducts  To get customer  http://localhost:8088/{customerId}  To add products to cart:  http://localhost:8088/Cart/addtocart/{id}/{custId}  To get all address:  http://localhost:8088/getAll  THANK YOU "
Employee Manager App,Spring Stack,https://github.com/Saad1929/Employee-Management-System,"# Employee Management System Back-end - Spring Boot & MySQL - **Front-end Component** using **React** is located here: https://github.com/Saad1929/Employee-Management-Frontend ## Contents 1. [ Brief Summary ](#summary) 2. [ Aims and Motivation ](#aims) 3. [ Technologies, Requirements and Software Tools ](#tech) 4. [ Design ](#design) 5. [ Application Screenshots ](#demo)  <a name=""summary""></a> ## Brief Summary - I developed a **RESTful API** using **Spring Boot and MySQL for the back-end**, tested it with **Postman**, and integrated it into a **React front-end using axios**. - **CREATE, READ, UPDATE and DELETE (CRUD)** appliction. - This repository contains the **back-end** of a **Full Stack** **personal project**, which is **responsible for storing employee information** and is implemented using the **Spring Boot Java Framework** which serves as the **web framework** for the **back-end**. - This application allows users to **view, add, remove, and edit** individuals within their management system. - This project is now **fully utilised** by my **community's sports club**. <a name=""aims""></a> ## üéØAims and Motivation - The main objective of this project was to create a thorough **Full Stack Application** using **Spring Boot & MySQL** as the **back-end** and **React** as the **front-end**. - Driven by an **unwavering enthusiasm** for **learning and self-improvement**, I dedicated my **personal time** to meticulously **develop and refine this Full Stack Application**. <a name=""tech""></a> ## ‚öôÔ∏èTechnologies, Requirements and Software Tools - Requirements listed below are for the **front-end and back-end**. ### Programming and Scripting Languages - Java - JavaScript - HTML - CSS - JSON ### Frameworks & Libraries - Spring Boot - **Back-end** - React - **Front-end** - Bootstrap - **Front-end & Back-end** ### npm Requirements (React) - nmp axios - npm react-router-dom ### Other Software Tools - **MySQLWorkbench** played a role where the **back-end** relied on this technology to **store user information**. - **DataGrip** was also used to support the **back-end** in storing user information. - **Postman** was used to **test PUT, GET, POST and DELETE requests to the database**. <a name=""design""></a> ## ‚úèÔ∏èDesign - Back-end ### Back-end Technology Stack - **Java** served as the **back-end programming language** for the web application, while the **Spring Boot framework** was employed to **facilitate its development and operation**. - **Spring Boot** was utilised to **create user models**, which were subsequently employed to **store employee information in the database**. The user models were established using the **Jakarta Persistence import** and **annotations**, including **@Entity, @Id, and @GeneratedValue**. This approach **facilitated the maintenance of the REST API**, making it more manageable and easier to maintain in the long run. - **MySQL Workbench and DataGrip** were employed to **store employee information**. The **back-end**, developed with **Spring Boot**, was **configured to connect with the database**, while the **front-end**, built with **React**, was responsible for **retrieving and displaying** this information. - **Spring Boot** was additionally utilised to **create custom exceptions**, specifically designed for **handling scenarios where the user ID was not found or encountered other exceptional conditions**. #### React & Spring Boot Summarisation Diagram  ![spring-boot-react-crud-example-rest-api-architecture](https://github.com/Saad1929/Employee-Management-System/assets/108022733/9951b53f-cf9c-4bf8-830f-654fa135789f)   ### Postman and Testing Sceenshots - **Postman** was used as an **API platform** to **design, build, test and iterate** the **RESTful API** which was **built using Spring Boot**. - Requests were dealt in **JSON** and **SQL**. #### GET Request (Normal Case)  <img width=""800"" alt=""GET"" src=""https://github.com/Saad1929/Employee-Management-System/assets/108022733/6f37bf38-071f-4d61-b4a7-7569e4730129"">   #### GET Request (Exceptional Case) - The customer error handling implemented in the Spring Boot back-end works successfully, as evidenced by the appropriate handling of the situation where a user with an ID of 20 does not exist. <img width=""800"" alt=""GET ERROR"" src=""https://github.com/Saad1929/Employee-Management-System/assets/108022733/ec068261-bc66-4f0e-b36f-d38889d2ae12"">   #### PUT Request (Normal Case) <img width=""1440"" alt=""PUT"" src=""https://github.com/Saad1929/Employee-Management-System/assets/108022733/2d978d20-75d9-43a1-96e2-d05f067df848"">   #### DELETE Request (Exceptional Case) <img width=""800"" alt=""DELETE ERROR"" src=""https://github.com/Saad1929/Employee-Management-System/assets/108022733/7c54018f-ee71-4e11-a222-70e17cd6970a"">   #### POST Request (Normal Case) <img width=""800"" alt=""Get   Post"" src=""https://github.com/Saad1929/Employee-Management-System/assets/108022733/ada4d1d1-fc66-4223-ac29-434f0d8fc446"">   #### Postman Summarisation Diagram ![postman](https://github.com/Saad1929/Employee-Management-System/assets/108022733/e045a6d9-48a6-46d8-8c78-daa6cb444606)   <a name=""demo""></a> ## Application Screenshots ### Home Page  <img width=""800"" alt=""Home Page"" src=""https://github.com/Saad1929/Employee-Management-System/assets/108022733/4d545b20-5119-4de1-a8e4-f9146811c9ac"">   ### Register User Page <img width=""800"" alt=""Register User Page"" src=""https://github.com/Saad1929/Employee-Management-System/assets/108022733/ff73e389-5e45-4b0d-ac50-921d58c4e570"">   ### View User Details Page <img width=""800"" alt=""View User Details Page"" src=""https://github.com/Saad1929/Employee-Management-System/assets/108022733/4edc3800-24a3-4753-9c3c-e1167b779352"">   ### Edit User Page <img width=""800"" alt=""Edit User Page"" src=""https://github.com/Saad1929/Employee-Management-System/assets/108022733/61cf2427-57ec-42bf-8bda-77dab6c32c4d"">  "
Payroll Management using Spring Boot,Spring Stack,https://github.com/AAdewunmi/Employee-Payroll-Management-System,"# Project Title:  Employee Payroll Management System (A RESTful API for Spring Boot)  ## 1. What is the project?  > ""Big picture: We‚Äôre going to create a simple payroll service that manages the employees of a company.  We‚Äôll store employee objects in a (H2 in-memory) database, and access them (via something called JPA).  Then we‚Äôll wrap that with something that will allow access over the internet (called the Spring MVC layer) ...""  > ""Introducing Spring HATEOAS, a Spring project aimed at helping you write hypermedia-driven outputs.  To upgrade your service to being RESTful, add this to your build: ... ""  -- Building REST services with Spring (Spring Guide)  #### This is an Employee Payroll Management RESTful API for Spring Boot, which performs CRUD operations on an in-memory database.  #### An implementation of a SPRING quide: ""Building REST services with Spring"".  ## 2. Tech Stack:  - Java 17 - Spring MVC - Spring Boot - Spring HATEOAS - Java JPA - H2 Database. - JSON - Postman  ## 3. Installing:  i. Clone the git repo  ``` https://github.com/AAdewunmi/Employee-Payroll-Management-System.git ```  ii. Open project folder  iii. Explore  ## 4. How To Use  i. Open project in preferred IDE (I'm using SpringToolSuit4)   ii. Run as a Spring Boot App  iii. Test RESTful API end points using Postman for CRUD operations:  - Get All Employees - Get Employee By ID - Post Employee - Put Employee - Delete Employee  ## 5. Demo  - Get All Employees  ![This is an image](src/main/java/com/payrollapplication/payroll/images/GetALLEmployees.png)  - Get Employee By ID  ![This is an image](src/main/java/com/payrollapplication/payroll/images/GetEmployeeByID.png)  - Post Employee  ![This is an image](src/main/java/com/payrollapplication/payroll/images/PostEmployee.png)  - Put Employee  ![This is an image](src/main/java/com/payrollapplication/payroll/images/PutEmployee.png)  - Delete Employee  ![This is an image](src/main/java/com/payrollapplication/payroll/images/DeleteEmployee_1.png)  - Check Employee has been deleted!  ![This is an image](src/main/java/com/payrollapplication/payroll/images/DeleteEmployee_2.png)  ## 6. Contributing:  Pull requests are welcome. For major changes, please open an issue first to discuss what you would like to change at:  Spring Guide Github Repo: https://github.com/spring-guides/tut-rest.   ## 7. Original Creator:  Author:  SPRING by VMware Tanzu  Tutorial Name: ""Building REST services with Spring""  Spring URL: https://spring.io/guides/tutorials/rest/   Github Project Name: Building REST Services with Spring  Github URL: https://github.com/spring-guides/tut-rest"
Flight Management System,Spring Stack,https://github.com/WebDesgns/Flight-Management-System,"# Flight Management System  The Flight Management System is a web application developed using Angular for the frontend and Spring Boot for the backend. This system allows users to manage flights, airlines, airports, and bookings.  ## Features  - User Registration and Authentication: Users can create an account and log in to access the system. - Flight Management: CRUD operations for flights, including creating, updating, and deleting flights. - Airline Management: Manage airlines, including adding, editing, and removing airlines. - Airport Management: CRUD operations for airports, allowing users to add, update, and delete airports. - Booking Management: Users can search for available flights and make bookings. - Role-based Access Control: Different roles such as admin and user with different permissions. - Data Validation: Input validation to ensure data integrity and prevent errors. - Error Handling: Proper handling of exceptions and errors with informative error messages. - User-friendly Interface: A clean and intuitive user interface for easy navigation and interaction.  ## Prerequisites  Make sure you have the following software installed:  - Node.js and npm - [Download here](https://nodejs.org) - Angular CLI - Install globally using npm: `npm install -g @angular/cli` - Java Development Kit (JDK) 8 or later - [Download here](https://www.oracle.com/java/technologies/javase-jdk11-downloads.html) - Apache Maven - [Download here](https://maven.apache.org/download.cgi)  ## Getting Started  ### Frontend Setup  1. Clone the repository:  ```shell git clone https://github.com/WebDesgns/Flight-Management-System.git ```  2. Navigate to the frontend directory:  ```shell cd flight-management-system/Client ```  3. Install the dependencies:  ```shell npm install ```  4. Start the development server:  ```shell ng serve ```  5. Open your browser and access the application at `http://localhost:4200`.  ### Backend Setup  1. Navigate to the backend directory:  ```shell cd flight-management-system/server ```  2. Build the project using Maven:  ```shell mvn clean install ```  3. Run the Spring Boot application:  ```shell mvn spring-boot:run ```  The backend server will start running on `http://localhost:8080`.  ## Configuration  ### Backend Configuration  The backend configuration file is located at `flight-management-system/backend/src/main/resources/application.properties`. Update the following properties according to your environment:  ```properties spring.datasource.url=jdbc:mysql://localhost:3306/flight_management_system spring.datasource.username=root spring.datasource.password=password ```  Replace the `spring.datasource.url` with your MySQL database URL, and update the `spring.datasource.username` and `spring.datasource.password` with your MySQL credentials.  ### Frontend Configuration  ```typescript export const environment = {   production: false,   apiUrl: 'http://localhost:8080' // Update with your backend URL }; ```  Replace `http://localhost:8080` with the URL where your backend server is running.  ## Usage  Once the application is set up and running, you can access it through your browser. Use the provided user registration functionality to create an account and log in.  As an admin, you will have access to manage flights, airlines, and airports. Regular users can search for available flights and make bookings.  ## Contributing  Contributions are welcome! If you want to contribute to this project, please follow these steps:  1. Fork this repository. 2. Create a new branch: `git checkout -b my-new-feature`. 3. Make   your changes and commit them: `git commit -am 'Add some feature'`. 4. Push to the branch: `git push origin my-new-feature`. 5. Submit a pull request.   ## Acknowledgments  - [Angular](https://angular.io/) - [Spring Boot](https://spring.io/projects/spring-boot) - [MySQL](https://www.mysql.com/)"
Determine If Your Food is Genetically Modified,Bio Technology,https://github.com/nfahlgren/plant_scientists_support_gmo/tree/master/data,"# Scopus publication analysis This repository contains data we analyzed from the petition ""Scientists in Support of GMO Technology for Crop Improvement"" (1) for the letter ""Plant scientists: GM technology is safe"" (2). An extended version is also available [here](https://github.com/nfahlgren/plant_scientists_support_gmo/blob/master/submitted_article.md).  ## Data collection Starting with the first ~1000 signatories of the petition ""Scientists in Support of GMO Technology for Crop Improvement"" (1), we identified signatories with academic affiliations. With the names and affiliation information, we searched the [Scopus database](http://www.scopus.com/) for publication records for each signatory. Scopus database searches yielded unambiguous results for 471 authors. Citation information for the publications of all 471 authors were downloaded in CSV format, and the data for individual authors can be found in the [data folder](https://github.com/nfahlgren/scopus_publications_analysis/tree/master/data).  ## Data curation During the course of processing the records, formatting inconsistencies were found among the Scopus records (e.g. failure to convert an author name to Lastname, Initials). In cases where formatting inconsistencies were found, records were manually reformatted. Retraction publications are listed as separate entries in the Scopus database, so both the retraction notices and the original retracted articles were removed from the data set. All changes made to the publication are tracked in the git history of this repository.  ## Data processing The Python script [coauthor_network.py](https://github.com/nfahlgren/scopus_publications_analysis/blob/master/coauthor_network.py) was used to extract data from the Scopus publication records.   ``` python coauthor_network.py -d ./data -o petitioner_publication_analysis_results ```  Erratum notices were skipped because they are redundant with the original article database entry, but all other publication types were used in our analysis. After filtering, 17,662 unique publications were used for further analysis.  ## Publication wordcloud The R script [wordcloud.R](https://github.com/nfahlgren/scopus_publications_analysis/blob/master/wordcloud.R) was used to do the wordcloud analysis. First, titles of the 17,662 unique publications were read into R. The tm package was used to generate a data frame of word frequency usage from the input titles (3,4). The titles were processed to convert all words to lowercase, remove numbers, remove English ""stopwords,"" remove punctuation characters, and remove whitespace. Cleaned title text was ""stemmed"" to truncate common suffixes and conjugations to collapse word variants to their root word. Stem completion was used to restore words that were inappropriately stemmed back to their full-length state. A term document matrix was generated from the processed text with words that were at least four characters long. The term document matrix was converted to a data frame of word frequencies. The following proper and common taxonomic terms were removed from the resulting data frame to focus the analysis on the processes studied and not the organismal systems.  ``` 'arabidopsis', 'maize', 'thaliana', 'tomato', 'rice', 'wheat',  'tobacco', 'populus', 'chlamydomonas', 'phytophthora', 'barley', 'potato', 'pseudomonas', 'salina', 'soybean', 'human', 'yeast', 'agrobacterium', 'cucumber','reinhardtii', 'drosophila', 'brassica', 'nicotiana', 'sativa', 'oryza', 'coli', 'infestans', 'elongata', 'polymorpha', 'escherichia', 'solanum', 'tumefaciens', 'rhizobium', 'indica', 'poplar', 'turnip', 'cauliflower', 'sativum', 'xanthomonas', 'pisum', 'domestica', 'physcomitrella', 'corn', 'brachypodium', 'lepidoptera', 'mouse', 'potyvirus', 'patens', 'candida', 'bacillus', 'cotton', 'mice', 'saccharomyces', 'sorghum', 'sugarcane', 'thlaspi', 'cyanobacteria', 'poaceae', 'radiata', 'carrot', 'distachyon', 'hedgehog', 'sinorhizobium', 'fusarium', 'neurospora', 'grape', 'sojae', 'strawberries', 'thuringiensis', 'vitis', 'petunia', 'caerulescens', 'cerevisiae', 'onion', 'haberlea', 'sunflower', 'cabbage', 'napus', 'vicia', 'crassa', 'cucumis', 'reinhardi', 'rhodopensis', 'alfalfa', 'aspen', 'cladosporium', 'medicago', 'panicum', 'vaccinia', 'chenopodiaceae', 'cassava', 'fulvum', 'spinach', 'triticum', 'benthamiana', 'plutellidae', 'banana', 'copi', 'switchgrass', 'elegans', 'setaria' ```  The final wordcloud was generated using the wordcloud package (5). Analysis was done using R version 3.2.2 and Python 2.7.10.  ## References 1. Scientists in Support of GMO Technology for Crop Improvement. Cornell Alliance for Science. 5 January 2016; [http://cas.nonprofitsoapbox.com/aspbsupportstatement](http://cas.nonprofitsoapbox.com/aspbsupportstatement). 2. Fahlgren N, Bart R, Herrera-Estrella L, Rell√°n-√Ålvarez R, Chitwood DH, Dinneny JR (2016). Plant scientists: GM technology is safe. Science 351: 824‚Äì824. [http://science.sciencemag.org/content/351/6275/824.1](http://science.sciencemag.org/content/351/6275/824.1). 3. Feinerer I, Hornik K, and Meyer D (2008). Text Mining Infrastructure in R. Journal of Statistical Software 25(5): 1-54. URL: [http://www.jstatsoft.org/v25/i05/](http://www.jstatsoft.org/v25/i05/).  4. Feinerer I and Hornik K (2015). tm: Text Mining Package. R package version 0.6-2. [https://CRAN.R-project.org/package=tm](https://CRAN.R-project.org/package=tm). 5. Fellows I. (2014). wordcloud: Word Clouds. R package version 2.5. [https://CRAN.R-project.org/package=wordcloud](https://CRAN.R-project.org/package=wordcloud).       "
Building Your Own Tool for Identifying DNA,Bio Technology,https://github.com/VerisimilitudeX/DNAnalyzer,"![DNAnalyzer-modified](https://user-images.githubusercontent.com/96280466/221687615-698969a1-8d39-4278-aa92-8f713625f165.png)  <div align=""center""> <h3>Democratizing AI-Powered DNA Analysis</h3> <p><i>On-device genomic insights for everyone, everywhere</i></p>  [![Copyright](https://img.shields.io/badge/copyright-2025-blue?style=for-the-badge)](https://github.com/VERISIMILITUDEX/DNAnalyzer) [![Release](https://img.shields.io/github/v/release/VERISIMILITUDEX/DNAnalyzer?style=for-the-badge&color=green)](https://github.com/VERISIMILITUDEX/DNAnalyzer/releases) [![Build Status](https://img.shields.io/github/actions/workflow/status/VerisimilitudeX/DNAnalyzer/gradle.yml?style=for-the-badge)](https://github.com/VerisimilitudeX/DNAnalyzer/actions/workflows/gradle.yml) [![DOI](https://img.shields.io/badge/DOI-10.5281%2Fzenodo.14556578-blue?style=for-the-badge)](https://zenodo.org/records/14556578)  <br>  <a href=""https://github.com/codespaces/new?hide_repo_select=true&ref=main&repo=519909104&machine=largePremiumLinux&location=WestUs&skip_quickstart=true&geo=UsWest"">     <img src=""https://github.com/codespaces/badge.svg"" alt=""Open in GitHub Codespaces"" style=""height: 35px"" /> </a>&nbsp;&nbsp; <a href=""https://huggingface.co/DNAnalyzer"">     <img src=""https://huggingface.co/datasets/huggingface/badges/resolve/main/sign-in-with-huggingface-xl-dark.svg"" alt=""Model in Hugging Face"" style=""height: 35px"" /> </a>&nbsp;&nbsp; <a href=""https://www.producthunt.com/posts/dnanalyzer?utm_source=badge-featured&utm_medium=badge&utm_souce=badge-dnanalyzer"">     <img src=""https://api.producthunt.com/widgets/embed-image/v1/featured.svg?post_id=401710&theme=dark"" alt=""DNAnalyzer on Product Hunt"" style=""height: 35px"" /> </a>  </div>  <br>  ## Table of Contents  - [About DNAnalyzer](#-about-dnanalyzer) - [Why It Matters](#-why-it-matters) - [Core Features](#-core-features) - [Quick DNA Introduction](#-quick-dna-introduction) - [Getting Started](#-getting-started) - [Roadmap](#-roadmap) - [Contributing](#-contributing) - [Citations](#-citations) - [Terms of Use](#-terms-of-use)  <br>  ## About DNAnalyzer  DNAnalyzer is a **fiscally sponsored 501(c)(3) nonprofit** (EIN: 81-2908499) revolutionizing DNA analysis by making machine learning-powered genomic insights accessible through efficient on-device computation.  Founded by [Piyush Acharya](https://github.com/VerisimilitudeX) and co-led with [@LimesKey](https://github.com/LimesKey), our platform has attracted **45 computational biologists and computer scientists** from institutions including Microsoft Research, University of Macedonia, and Northeastern University.  Our impact has been recognized by the organizers of the [AI Engineer World's Fair](https://www.ai.engineer/worldsfair) (backed by OpenAI, Microsoft, Google DeepMind, and Anthropic) and the CEO of Forem.  <br>  ## Why It Matters  <div align=""center"">  | Current Reality | DNAnalyzer's Mission | |---|---| | **$100** average cost for DNA sequencing | **Free** on-device analysis | | Up to **$600** for basic health insights | **Accessible** to underserved communities | | **78%** of companies share genetic data with third parties | **Private, secure** analysis that stays on your device | | Data breaches expose millions (23andMe: 6.9M users in 2023) | **No central database** of sensitive genetic information |  </div>  > *""Unlike a credit card number or password, stolen or misused genetic information cannot be changed.""*  <br>  ## Core Features  <table>   <tr>     <td width=""33%"" align=""center"">       <h3>Start & Stop Codons</h3>       <p>Identify protein coding regions and analyze the 20 different amino acids in polypeptide chains</p>     </td>     <td width=""33%"" align=""center"">       <h3>High Coverage Regions</h3>       <p>Detect promoter sequences with high GC-content (45-60%) that likely reveal crucial genomic information</p>     </td>     <td width=""33%"" align=""center"">       <h3>Neural Disorder Analysis</h3>       <p>Identify genetic signatures related to neurodevelopmental disorders like autism, ADHD, and schizophrenia</p>     </td>   </tr>   <tr>     <td width=""33%"" align=""center"">       <h3>Core Promoter Elements</h3>       <p>Find key promoter sequences (BRE, TATA, INR, DPE) responsible for initiating transcription</p>     </td>     <td width=""33%"" align=""center"">       <h3>FASTA Support</h3>       <p>Process multi-line and single-line FASTA database files through upload or web linking</p>     </td>     <td width=""33%"" align=""center"">       <h3>Command-Line Interface</h3>       <p>Access all core features through our powerful Methionine CLI (Met CLI) for scripting and automation</p>     </td>   </tr> </table>  > **Coming Soon:** Web-based user interface for enhanced accessibility  <br>  ## Quick DNA Introduction  **DNA: The Programming Language of Life**  DNA exists in most cells of the body and contains the blueprint for creating over 200 distinct cell types. Like a programming language exclusive to living organisms, it encodes the instructions for all biological processes.  **Databases: The Foundation of Analysis**  A DNA database is crucial for interpreting DNA sequences. By leveraging machine learning, we can make predictions on previously unseen DNA sequences, forming the foundation of modern genomic analysis.  <br>  ## Getting Started  Ready to explore your DNA? Follow our comprehensive guide to get started:  ```bash # Clone the repository git clone https://github.com/VerisimilitudeX/DNAnalyzer.git  # Navigate to project directory cd DNAnalyzer  # Install dependencies ./gradlew build ```  For detailed instructions, please refer to our [Getting Started Guide](docs/getting-started.md).  <br>  ## Roadmap  <div align=""center"">  | Upcoming Development | Description | |---|---| | **Optimized SQL Database** | High-performance vertical scaling database to store genomic data from thousands of species | | **Enhanced Neural Network** | Support for genotyped data from 3rd-party DNA testing services ($99 compatibility) | | **DIAMOND Implementation** | Combining DIAMOND's performance with BLAST's powerful algorithm |  </div>  <br>  ## Contributing  We welcome contributions from developers and researchers of all skill levels!  - [Contributing Guidelines](./docs/contributing/Contribution_Guidelines.md) - [How To Use Git](./docs/contributing/CONTRIBUTING.md)  <div align=""center"">   <a href=""https://github.com/VerisimilitudeX/DNAnalyzer/stargazers"">     <img src=""https://img.shields.io/github/stars/VerisimilitudeX/DNAnalyzer?style=for-the-badge&color=yellow"" alt=""Stars"">   </a>   <a href=""https://github.com/VerisimilitudeX/DNAnalyzer/issues"">     <img src=""https://img.shields.io/github/issues/VerisimilitudeX/DNAnalyzer?style=for-the-badge"" alt=""Issues"">   </a>   <a href=""https://github.com/VerisimilitudeX/DNAnalyzer/pulls"">     <img src=""https://img.shields.io/github/issues-pr/VerisimilitudeX/DNAnalyzer?style=for-the-badge"" alt=""Pull Requests"">   </a>   <a href=""https://discord.gg/X3YCvGf2Ug"">     <img src=""https://img.shields.io/discord/1033196198816915516?style=for-the-badge&logo=discord&logoColor=white"" alt=""Discord"">   </a> </div>  <br>  ## Citations  View our [detailed citations document](docs/citations.md) for all in-line references.  If you use DNAnalyzer in your research, please cite:  ```bibtex @software{Acharya_DNAnalyzer_ML-Powered_DNA_2022,   author = {Acharya, Piyush},   doi = {10.5281/zenodo.14556577},   month = oct,   title = {{DNAnalyzer: ML-Powered DNA Analysis Platform}},   url = {https://github.com/VerisimilitudeX/DNAnalyzer},   version = {3.5.0-beta.0},   year = {2022} } ```  <br>  ## ‚öñTerms of Use  The use of this application is entirely at your own discretion and responsibility. While the DNAnalyzer team is committed to addressing significant issues, we disclaim liability for losses, damages, or consequences arising from the use of this application.  For questions or concerns, please contact us at help@dnanalyzer.org.  **Copyright ¬© Piyush Acharya 2025.** DNAnalyzer is a fiscally sponsored 501(c)(3) nonprofit organization (EIN: 81-2908499) and is licensed under the MIT License.  <br>  <div align=""center"">   <h3>Project Growth</h3>   <a href=""https://star-history.com/#VerisimilitudeX/DNAnalyzer&Date"">     <picture>       <source media=""(prefers-color-scheme: dark)"" srcset=""https://api.star-history.com/svg?repos=VerisimilitudeX/DNAnalyzer&type=Date&theme=dark"" />       <source media=""(prefers-color-scheme: light)"" srcset=""https://api.star-history.com/svg?repos=VerisimilitudeX/DNAnalyzer&type=Date"" />       <img alt=""Star History Chart"" src=""https://api.star-history.com/svg?repos=VerisimilitudeX/DNAnalyzer&type=Date"" />     </picture>   </a> </div>  <br>  <div align=""center"">   <h3>Support DNAnalyzer</h3>   <p>Every referral helps fund our nonprofit mission</p>    <table>     <tr>       <td align=""center"">         <h4>23andMe</h4>         <p>Get <strong>10% off</strong> your order<br>DNAnalyzer earns <strong>$20</strong> per referral</p>         <a href=""https://refer.23andme.com/s/ruxd4"" target=""_blank"">           <img src=""https://img.shields.io/badge/Get_10%25_Off-23andMe-4285F4?style=for-the-badge"" alt=""23andMe Referral"">         </a>       </td>       <td align=""center"">         <h4>Ancestry¬Æ Membership</h4>         <p>Get up to <strong>24% off</strong> membership<br>DNAnalyzer earns <strong>$10</strong> per referral</p>         <a href=""https://refer.ancestry.com/verisimilitude11!6699046cdf!a"" target=""_blank"">           <img src=""https://img.shields.io/badge/Get_24%25_Off-Ancestry-83C36D?style=for-the-badge"" alt=""Ancestry Referral"">         </a>       </td>     </tr>   </table> </div>"
Chatbot,Data Science,https://github.com/assistant-ui/assistant-ui,"<a href=""https://www.assistant-ui.com"">   <img src=""https://raw.githubusercontent.com/assistant-ui/assistant-ui/main/.github/assets/header.svg"" alt=""assistant-ui Header"" width=""100%"" style=""width: 1000px"" /> </a>  <p align=""center"">   <a href=""https://www.assistant-ui.com"">Product</a> ¬∑   <a href=""https://www.assistant-ui.com/docs/getting-started"">Documentation</a> ¬∑   <a href=""https://www.assistant-ui.com/examples"">Examples</a> ¬∑   <a href=""https://discord.gg/S9dwgCNEFs"">Discord Community</a> ¬∑   <a href=""https://cal.com/simon-farshid/assistant-ui"">Contact Sales</a> </p>  [![Weave Badge](https://img.shields.io/endpoint?url=https%3A%2F%2Fapp.workweave.ai%2Fapi%2Frepository%2Fbadge%2Forg_GhSIrtWo37b5B3Mv0At3wQ1Q%2F722184017&cacheSeconds=3600)](https://app.workweave.ai/reports/repository/org_GhSIrtWo37b5B3Mv0At3wQ1Q/722184017) ![Backed by Y Combinator](https://img.shields.io/badge/Backed_by-Y_Combinator-orange)  - [‚≠êÔ∏è Star us on GitHub](https://github.com/assistant-ui/assistant-ui)  ## The UX of ChatGPT in your React app üí¨üöÄ  **assistant-ui** is an open source Typescript/React library for AI chat.  The library handles essential chat features such as auto-scrolling, accessibility, and real-time updates, while providing easy integration with LangGraph, AI SDK and custom backends.  The API of assistant-ui is inspired by libraries like shadcn/ui and cmdk. Instead of a single monolithic chat component, developers get primitive components that can be fully customized.  We have wide model provider support (OpenAI, Anthropic, Mistral, Perplexity, AWS Bedrock, Azure, Google Gemini, Hugging Face, Fireworks, Cohere, Replicate, Ollama) out of the box and the ability to integrate custom APIs.  ## Getting Started  You can get started by running `npx assistant-ui create` (new project) or `npx assistant-ui init` (existing project) in your terminal.  [![assistant-ui starter template](https://raw.githubusercontent.com/assistant-ui/assistant-ui/main/.github/assets/assistant-ui-starter.gif)](https://youtu.be/k6Dc8URmLjk)  ## Features  - shadcn/ui   - Radix UI-inspired primitives for AI Chat   - Beautiful shadcn/ui theme to get you started - Chat UI   - Streaming, Auto-scrolling, Markdown, Code Highlighting, File Attachments, and more - Keyboard shortcuts and accessibility features - Generative UI   - Map LLM tool calls and JSONs to custom UI components - Frontend tool calls   - Let LLMs take action in your frontend application - Human tool calls   - Human approvals and input collection - Chat history and analytics   - Sign up for assistant-cloud and configure by simply setting an environment variable  ## Choose your backend  - AI SDK   - First class integration into AI SDK by Vercel. Connect to any LLM provider supported by AI SDK. - LangGraph   - First class integration into LangGraph and LangGraph Cloud. Connect to any LLM provider supported by LangChain. - Custom   - Use assistant-ui as the visualization layer on top your own backend/streaming protocols.  ## Customization  The API of assistant-ui is inspired by libraries like Radix UI and cmdk. Instead of a single monolithic chat component, we give you composable primitives and a great starter configuration. You have full control over the look and feel of every pixel while leaving auto-scrolling, LLM streaming and accessibility to us.  ![Overview of components](https://raw.githubusercontent.com/assistant-ui/assistant-ui/main/.github/assets/components.png)  Sample customization to make a perplexity lookalike:  ![Perplexity clone created with assistant-ui](https://raw.githubusercontent.com/assistant-ui/assistant-ui/main/.github/assets/perplexity.gif)  ## **Demo Video**  [![Short Demo](https://img.youtube.com/vi/ZW56UHlqTCQ/hqdefault.jpg)](https://youtu.be/ZW56UHlqTCQ)  [![Long Demo](https://img.youtube.com/vi/9eLKs9AM4tU/hqdefault.jpg)](https://youtu.be/9eLKs9AM4tU)  ## Traction  Hundreds of projects use assistant-ui to build in-app AI assistants, including companies like LangChain, AthenaIntelligence, Browser Use, and more.  With >50k+ monthly downloads, assistant-ui is the most popular UI library for AI chat.  <img src=""https://raw.githubusercontent.com/assistant-ui/assistant-ui/main/.github/assets/growth.png"" alt=""Growth"" style=""max-width: 400px;"">  ## 2025 Q1 Roadmap  - [x] Assistant Cloud - [x] Chat Persistence - [x] React 19, Tailwind v4, NextJS 19 support - [x] Improved Markdown rendering performance - [x] LangGraph `interrupt()` support - [x] Open in v0 support - [ ] Improved documentation (work in progress) - [ ] OpenAI Realtime Voice (work in progress) - [ ] Resume interrupted LLM calls (work in progress) - [ ] Native PDF attachment support - [¬†] Follow-up suggestions  ## Next Steps  - [Check out example demos](https://www.assistant-ui.com/) - [Read our docs](https://www.assistant-ui.com/docs/) - [Join our Discord](https://discord.com/invite/S9dwgCNEFs) - [Book a sales call](https://cal.com/simon-farshid/assistant-ui)"
Climate Change Impact Analyser (on Global Food Supply),Data Science,https://github.com/k3nnywilliam/climate-change-food-supply,"# Data Visualization on Effects of Climate Change on Food Supply  <a href='https://colab.research.google.com/github/k3nnywilliam/climate-change-food-supply/blob/main/world-food-supply.ipynb'> 	<img alt='CircleCI' src='https://colab.research.google.com/assets/colab-badge.svg' style=""max-height:20px;width:auto""> </a>  <a href='https://app.travis-ci.com/k3nnywilliam/climate-change-food-supply'> 	<img alt='CircleCI' src=https://app.travis-ci.com/k3nnywilliam/climate-change-food-supply.svg?branch=main style=""max-height:20px;width:auto""> </a>  ![Alt text](public/src/components/images/wheat.jpg?raw=true ""Title"")  *** ### Motivation Reproducing studies on climate change effects on food production into a web-based data visualization.  ### Purpose of study  To provide an assessment of potential climate change impacts on world crop production.  ***  ### Data source  This data was acquired from the Socioeconomic Data And Applications Center (SEDAC) portal. The study is called <a href=""https://sedac.ciesin.columbia.edu/data/set/crop-climate-effects-climate-global-food-production""> ""Effects of Climate Change on Global Food Production from SRES Emissions and Socioeconomic Scenarios, v1 (1970‚Ää‚Äì‚Ää2080)"".</a>  #### Usage of data (excerpt from SEDAC website)  ""SEDAC data and information products and services are designed to help users integrate and apply socioeconomic and Earth science data in their research, educational activities, analysis, and decision making.""   ### License  This project is GNU-style licensed, as found in the LICENSE file."
Weather Prediction,Data Science,https://github.com/AR10X/data-analysis,"<h2> :snowflake: Weather Forecasting Analysis and Visualization Project</h2>  <h3>About the Project</h3>  <p>This project is aimed at providing insights and visual representations of various weather patterns and trends, by analyzing and comparing weather data of different cities and regions across the world. The project uses various data analysis and visualization tools to help users understand the weather patterns and trends in a visually appealing and easy-to-understand manner.</p>  <hr> <h3>:books: Project Description</h3>  <p>The project makes use of various data sources to gather weather information such as temperature, precipitation, pressure, and other important meteorological data. This data is then processed and analyzed using various data analysis and visualization tools such as <b>SQL, Excel, Power BI/ Tableau and Python.</b> The results of this analysis are then presented to the users through interactive visualizations and graphs, making it easier for them to understand and analyze weather trends and patterns.</p>  <hr> <h3>:bar_chart: Power BI Dashboard - Overview</h3> <img src=""https://raw.githubusercontent.com/AR10X/data-analysis/Ash/img/Dashboard.png"" width=50%> <hr> <h3>:calling: Tech Used</h3>  <p>The following tools and technologies are used in the project:</p>  <details open><summary>SQL (MYSQL):</summary> Used for managing and storing the large amounts of weather data that is collected for the analysis.</details>  <details open><summary>Power BI/ Tableau:</summary> Used for creating interactive and visually appealing dashboards, charts and graphs that present the results of the weather data analysis.</details>  <details open><summary>Excel:</summary> Used for performing basic data analysis and manipulation tasks such as sorting, filtering, and aggregating data.</details>  <details open><summary>Python:</summary> Used for developing custom scripts and algorithms that are used to process and analyze the weather data. The project also makes use of various Python libraries such as Pandas and Matplotlib for data analysis and visualization purposes.</details> <hr>  <h3>:house: Data Wizards Members </h3>  1. [Abdul Rehaman](https://github.com/AR10X) 2. [Ashrith M R](https://github.com/ashhh-01) 3. [Danish Hasan](https://github.com/DanishHasan14321) 4. [Shiwali Verma](https://github.com/AR10X/data-analysis) <hr>  "
Google Ads Keyword Generation,Data Science,https://github.com/eslamaboushashaa/Generating-Keywords-for-Google-Ads,"# Generating Keywords for Google Ads  ***One of the most popular markting tool is Search Engine Marketing (SEM).***   ## 1.  What is SEM ? **SEM is one of the most effective ways to grow your business in an increasingly competitive marketplace. It is the process of gaining website traffic by purchasing ads on search engines. Google AdWords is one of the most popular forms of online advertising: you bid for ad placement in a search engine's sponsored links for keywords related to your business, then you pay the search engine a fee for each click**  ## 2. What I did in this project **In this project, Generate all possible keywords that I want into SEM campaigning by python, if marketers do this manually is spend a lot of time, so this is awesome to do programmatically.**  "
Traffic Signs Recognition,Data Science,https://github.com/deepak2233/Traffic-Signs-Recognition-using-CNN-Keras,"# Traffic Sign Recognition  ## Overview  This project implements a Traffic Sign Recognition system using Convolutional Neural Networks (CNN) to classify images of traffic signs. The aim is to build an automated system that accurately recognizes and classifies various traffic signs from images, contributing to the development of advanced driver-assistance systems (ADAS) and autonomous vehicles.  ### Problem Statement Traffic signs are critical for ensuring road safety as they convey essential information to drivers. An automated recognition system can help improve safety and efficiency on the roads. For example, a system should recognize a ""Stop"" sign and alert the driver to stop the vehicle, thereby preventing accidents.  ### Example Images - **Stop Sign**:     ![Stop Sign](data/Test/00111.png)  - **Yield Sign**:     ![Yield Sign](data/Test/00120.png)  - **Speed Limit Sign**:     ![Speed Limit Sign](data/Test/00122.png)  ## Datasets The dataset used for this project is the **German Traffic Sign Recognition Benchmark (GTSRB)**. It contains over 50,000 images categorized into 43 classes of traffic signs.  ### Dataset Structure ``` data/      ‚îú‚îÄ‚îÄ Train/ # Contains train images organized by class     ‚îú‚îÄ‚îÄ Test/ # Contains test images organized by class      ``` ### CSV Files - **`Train.csv`**: Contains paths and labels for the training set. - **`Test.csv`**: Contains paths and labels for the test set.  ## Project Structure  ```     traffic-sign-recognition/     ‚îÇ     ‚îú‚îÄ‚îÄ data/     ‚îÇ   ‚îú‚îÄ‚îÄ Meta/     ‚îÇ   ‚îú‚îÄ‚îÄ Test/     ‚îÇ   ‚îú‚îÄ‚îÄ Train/     ‚îÇ   ‚îî‚îÄ‚îÄ ...     ‚îú‚îÄ‚îÄ scripts/     ‚îÇ   ‚îú‚îÄ‚îÄ data_preprocessing.py     ‚îÇ   ‚îú‚îÄ‚îÄ eda.py     ‚îÇ   ‚îú‚îÄ‚îÄ model_training.py     ‚îÇ   ‚îú‚îÄ‚îÄ evaluation.py     ‚îÇ   ‚îú‚îÄ‚îÄ inference.py     ‚îÇ   ‚îî‚îÄ‚îÄ streamlit_app.py     ‚îú‚îÄ‚îÄ main.py     ‚îú‚îÄ‚îÄ requirements.txt     ‚îî‚îÄ‚îÄ README.md ```  ## Setup and Installation 1. **Clone the Repository**:    ```bash        git clone <repository-url>        cd traffic-sign-recognition         python3 -m venv env         source env/bin/activate  # On Windows use `env\Scripts\activate`         pip install -r requirements.txt  # Usage  ### Data Preprocessing  To preprocess the dataset: ```     python main.py --data --data_dir data ```  ### Exploratory Data Analysis (EDA) To generate visualizations and understand the dataset: ``` python main.py --eda --data_dir data --output_dir outputs  ```  ### Model Training To train the CNN model on the preprocessed data:  ``` python main.py --training --data_dir data --model_dir models --epochs 20 --batch_size 64 --learning_rate 0.001 ```  ### Model Evaluation To evaluate the trained model: ``` python main.py --evaluation --data_dir data --model_dir models --output_dir outputs ```  ### Inference To make predictions on new images:  ``` python main.py --inference --model_dir models --image_path path/to/image.jpg  ```  ### Streamlit Application To launch the Streamlit application for interactive traffic sign recognition:  ``` streamlit run scripts/streamlit_app.py -- --model_path models/best_model.h5 ```   ### Notes - **Image Paths**: Ensure to replace `data/Train/00000.png`, etc., with actual paths to your images. - **Repository URL**: Replace `<repository-url>` with the actual URL of your GitHub repository. - **License**: Ensure you have a LICENSE file if you include a license section.  This `README.md` provides a comprehensive overview of your project, making it easy for users to understand its purpose and how to use it effectively. Let me know if you need further adjustments or additions! "
Wine Quality Analysis,Data Science,https://github.com/mayursatav/Wine-Quality-Prediction,"# Wine-Quality-Prediction Wine certification includes physiochemical tests like determination of density, pH, alcohol quantity, fixed and volatile acidity etc. We have a large datasets having the physiochemical tests results and quality on the scale of 1 to 10 of wines of the Vinho Verde variety.Such a model can be used not only by the certification bodies but also by the wine producers to improve quality based on the physicochemical properties and by the consumers to predict the quality of wines.  # Technologies Used * Numpy * Pandas * Matplotlib * Seaborn * Tkinter "
Stock Market Prediction,Data Science,https://github.com/Vatshayan/Final-Year-Machine-Learning-Stock-Price-Prediction-Project,"# Final-Year-Machine-Learning-Stock-Price-Prediction-Project Final Year B.tech Project on Machine Learning Stock Prediction through Deep Learning  # Stock-Price-Prediction Top Class Stock Price Prediction Project through Machine Learning Algorithms for Google. Easy Understanding and Implementation.  ### Project PPT [**LINK**](https://github.com/Vatshayan/Final-Year-Machine-Learning-Stock-Price-Prediction-Project/blob/main/Stock_price%20_prediction.pptx)  ### Stock Price Prediction :  Stock (also known as equity) is a security that represents the ownership of a fraction of a corporation. This entitles the owner of the stock to a proportion of the corporation's assets and profits equal to how much stock they own. Units of stock are called ""shares.""  A stock is a general term used to describe the ownership certificates of any company. Stock prices change everyday by market forces. By this we mean that share prices change because of supply and demand. If more people want to buy a stock (demand) than sell it (supply), then the price moves up. Conversely, if more people wanted to sell a stock than buy it, there would be greater supply than demand, and the price would fall. Understanding supply and demand is easy.  So, why do stock prices change? The best answer is that nobody really knows for sure. Some believe that it isn't possible to predict how stocks will change in price while others think that by drawing charts and looking at past price movements, you can determine when to buy and sell. The only thing we do know as a certainty is that stocks are volatile and can change in price extremely rapidly.   ### Understanding the Problem Statement We‚Äôll dive into the implementation part of this Project soon, but first it‚Äôs important to establish what we‚Äôre aiming to solve. Broadly, stock market analysis is divided into two parts ‚Äì Fundamental Analysis and Technical Analysis. Fundamental Analysis involves analyzing the company‚Äôs future profitability on the basis of its current business environment and financial performance. Technical Analysis, on the other hand, includes reading the charts and using statistical figures to identify the trends in the stock market. As you might have guessed, our focus will be on the technical analysis and visualization part. We‚Äôll be using a dataset from Google stock Price test and train.     ### Implementation:  1.Using Sckiit Learning( Machine Learning model)  2.Data Preprocessing using dataset  3.Visualization of Dataset  4.Feature Scaling   5.Preparing the Datasets for training   6.Reshaping the datasets  7.Model development  8.Implementation of sequential, dense, LSTM and dropout.  9.Preprocessing the Data  10.Predicting the Output  11.Result visualization    ### Research Paper  Project is totally based on research papers as project predict output using LSTM based deep learning models:  1. https://arxiv.org/abs/2009.10819  2. https://www.aclweb.org/anthology/W19-6403.pdf  3. https://www.sciencedirect.com/science/article/pii/S1877050920304865  ### Youtube Video of this Project: https://www.youtube.com/watch?v=44u5oU9MQGg   <h1 align=""center""> ‡§®‡§Æ‡§∏‡•ç‡§§‡•á (Namaste) üôèüèª , I'm Shivam Vatshayan <img src=""https://raw.githubusercontent.com/ABSphreak/ABSphreak/master/gifs/Hi.gif"" width=""30px""> ! </h1> <h3 align=""center"">I'm a Developer from India ‚ù§</h3>  **You Can use this Beautiful Project for your college Project and get good marks too.**  Email me Now **vatshayan007@gmail.com** to get this Full Project Code, PPT, Report, Synopsis, Video Presentation and Research paper of this Project.  üíå Feel free to contact me for any kind of help on projects.   ### Need Code, Documents & Explanation video ?   ## How to Reach me :  ### Mail : vatshayan007@gmail.com   ### WhatsApp: **+91 9310631437** (Helping 24*7) **[CHAT](https://wa.me/message/CHWN2AHCPMAZK1)**   ### Website : https://www.finalproject.in/  ### 1000 Computer Science Projects : https://www.computer-science-project.in/  ### Youtube Video of this Project: https://www.youtube.com/watch?v=44u5oU9MQGg  Mail/Message me for Projects Help üôèüèª"
Fake News Detection,Data Science,https://github.com/nishitpatel01/Fake_News_Detection,"# Fake News Detection  Fake News Detection in Python  In this project, we have used various natural language processing techniques and machine learning algorithms to classify fake news articles using sci-kit libraries from python.   ## Getting Started  These instructions will get you a copy of the project up and running on your local machine for development and testing purposes. See deployment for notes on how to deploy the project on a live system.  ### Prerequisites  What things you need to install the software and how to install them:  1. Python 3.6     - This setup requires that your machine has python 3.6 installed on it. you can refer to this url https://www.python.org/downloads/ to download python. Once you have python downloaded and installed, you will need to setup PATH variables (if you want to run python program directly, detail instructions are below in *how to run software section*). To do that check this: https://www.pythoncentral.io/add-python-to-path-python-is-not-recognized-as-an-internal-or-external-command/.      - Setting up PATH variable is optional as you can also run program without it and more instruction are given below on this topic.  2. Second and easier option is to download anaconda and use its anaconda prompt to run the commands. To install anaconda check this url https://www.anaconda.com/download/ 3. You will also need to download and install below 3 packages after you install either python or anaconda from the steps above    - Sklearn (scikit-learn)    - numpy    - scipy       - if you have chosen to install python 3.6 then run below commands in command prompt/terminal to install these packages    ```    pip install -U scikit-learn    pip install numpy    pip install scipy    ```    - if you have chosen to install anaconda then run below commands in anaconda prompt to install these packages    ```    conda install -c scikit-learn    conda install -c anaconda numpy    conda install -c anaconda scipy    ```     #### Dataset used The data source used for this project is LIAR dataset which contains 3 files with .tsv format for test, train and validation. Below is some description about the data files used for this project. 	 LIAR: A BENCHMARK DATASET FOR FAKE NEWS DETECTION  William Yang Wang, ""Liar, Liar Pants on Fire"": A New Benchmark Dataset for Fake News Detection, to appear in Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017), short paper, Vancouver, BC, Canada, July 30-August 4, ACL.  the original dataset contained 13 variables/columns for train, test and validation sets as follows:  * Column 1: the ID of the statement ([ID].json). * Column 2: the label. (Label class contains: True, Mostly-true, Half-true, Barely-true, FALSE, Pants-fire) * Column 3: the statement. * Column 4: the subject(s). * Column 5: the speaker. * Column 6: the speaker's job title. * Column 7: the state info. * Column 8: the party affiliation. * Column 9-13: the total credit history count, including the current statement. * 9: barely true counts. * 10: false counts. * 11: half true counts. * 12: mostly true counts. * 13: pants on fire counts. * Column 14: the context (venue / location of the speech or statement).  To make things simple we have chosen only 2 variables from this original dataset for this classification. The other variables can be added later to add some more complexity and enhance the features.  Below are the columns used to create 3 datasets that have been in used in this project * Column 1: Statement (News headline or text). * Column 2: Label (Label class contains: True, False)   You will see that newly created dataset has only 2 classes as compared to 6 from original classes. Below is method used for reducing the number of classes.  * Original 	--	New * True	   	--	True * Mostly-true	-- 	True * Half-true	-- 	True * Barely-true	-- 	False * False		-- 	False * Pants-fire	-- 	False  The dataset used for this project were in csv format named train.csv, test.csv and valid.csv and can be found in repo. The original datasets are in ""liar"" folder in tsv format.   ### File descriptions  #### DataPrep.py This file contains all the pre processing functions needed to process all input documents and texts. First we read the train, test and validation data files then performed some pre processing like tokenizing, stemming etc. There are some exploratory data analysis is performed like response variable distribution and data quality checks like null or missing values etc.  #### FeatureSelection.py In this file we have performed feature extraction and selection methods from sci-kit learn python libraries. For feature selection, we have used methods like simple bag-of-words and n-grams and then term frequency like tf-tdf weighting. we have also used word2vec and POS tagging to extract the features, though POS tagging and word2vec has not been used at this point in the project.  #### classifier.py Here we have build all the classifiers for predicting the fake news detection. The extracted features are fed into different classifiers. We have used Naive-bayes, Logistic Regression, Linear SVM, Stochastic gradient descent and Random forest classifiers from sklearn. Each of the extracted features were used in all of the classifiers. Once fitting the model, we compared the f1 score and checked the confusion matrix. After fitting all the classifiers, 2 best performing models were selected as candidate models for fake news classification. We have performed parameter tuning by implementing GridSearchCV methods on these candidate models and chosen best performing parameters for these classifier. Finally selected model was used for fake news detection with the probability of truth. In Addition to this, We have also extracted the top 50 features from our term-frequency tfidf vectorizer to see what words are most and important in each of the classes. We have also used Precision-Recall and learning curves to see how training and test set performs when we increase the amount of data in our classifiers.  #### prediction.py Our finally selected and best performing classifier was ```Logistic Regression``` which was then saved on disk with name ```final_model.sav```. Once you close this repository, this model will be copied to user's machine and will be used by prediction.py file to classify the fake news. It takes an news article as input from user then model is used for final classification output that is shown to user along with probability of truth.  Below is the Process Flow of the project:  <p align=""center"">   <img width=""600"" height=""750"" src=""https://github.com/nishitpatel01/Fake_News_Detection/blob/master/images/ProcessFlow.PNG""> </p>  ### Performance Below is the learning curves for our candidate models.   **Logistic Regression Classifier**  <p align=""center"">   <img width=""550"" height=""450"" src=""https://github.com/nishitpatel01/Fake_News_Detection/blob/master/images/LR_LCurve.PNG""> </p>  **Random Forest Classifier**  <p align=""center"">   <img width=""550"" height=""450"" src=""https://github.com/nishitpatel01/Fake_News_Detection/blob/master/images/RF_LCurve.png""> </p>  ### Next steps As we can see that our best performing models had an f1 score in the range of 70's. This is due to less number of data that we have used for training purposes and simplicity of our models. For the future implementations, we could introduce some more feature selection methods such as POS tagging, word2vec and topic modeling. In addition, we could also increase the training data size. We will extend this project to implement these techniques in future to increase the accuracy and performance of our models.   ### Installing and steps to run the software  A step by step series of examples that tell you have to get a development env running  1. The first step would be to clone this repo in a folder in your local machine. To do that you need to run following command in command prompt or in git bash ``` $ git clone https://github.com/nishitpatel01/Fake_News_Detection.git ```  2. This will copy all the data source file, program files and model into your machine.  3.    - If you have chosen to install anaconda then follow below instructions      - After all the files are saved in a folder in your machine. If you chosen to install anaconda from the steps given in 	               ```Prerequisites``` sections then open the anaconda prompt, change the directory to the folder where this project is saved in     your machine and type below command and press enter. 	``` 	cd C:/your cloned project folder path goes here/ 	```      - Once you are inside the directory call the ```prediction.py``` file, To do this, run below command in anaconda prompt. 	``` 	python prediction.py 	```      - After hitting the enter, program will ask for an input which will be a piece of information or a news headline that you 	    	   want to verify. Once you paste or type news headline, then press enter.       - Once you hit the enter, program will take user input (news headline) and will be used by model to classify in one of  categories of ""True"" and ""False"". Along with classifying the news headline, model will also provide a probability of truth associated with it.  4.  If you have chosen to install python (and did not set up PATH variable for it) then follow below instructions:     - After you clone the project in a folder in your machine. Open command prompt and change the directory to project directory by running below command.     ```     cd C:/your cloned project folder path goes here/     ```     - Locate ```python.exe``` in your machine. you can search this in window explorer search bar.      - Once you locate the ```python.exe``` path, you need to write whole path of it and then entire path of project folder with ```prediction.py``` at the end. For example if your ```python.exe``` is located at ```c:/Python36/python.exe``` and project folder is at ```c:/users/user_name/desktop/fake_news_detection/```, then your command to run program will be as below:     ```     c:/Python36/python.exe C:/users/user_name/desktop/fake_news_detection/prediction.py     ```     - After hitting the enter, program will ask for an input which will be a piece of information or a news headline that you 	    	   want to verify. Once you paste or type news headline, then press enter.      - Once you hit the enter, program will take user input (news headline) and will be used by model to classify in one of  categories of ""True"" and ""False"". Along with classifying the news headline, model will also provide a probability of truth associated with it. It might take few seconds for model to classify the given statement so wait for it.  5.  If you have chosen to install python (and already setup PATH variable for ```python.exe```) then follow instructions:     - Open the command prompt and change the directory to project folder as mentioned in above by running below command     ```     cd C:/your cloned project folder path goes here/     ```     - run below command     ```     python.exe C:/your cloned project folder path goes here/     ```     - After hitting the enter, program will ask for an input which will be a piece of information or a news headline that you 	    	   want to verify. Once you paste or type news headline, then press enter.      - Once you hit the enter, program will take user input (news headline) and will be used by model to classify in one of  categories of ""True"" and ""False"". Along with classifying the news headline, model will also provide a probability of truth associated with it. "
Video Classification,Data Science,https://github.com/HHTseng/video-classification,"# Video Classification  The repository builds a **quick and simple** code for video classification (or action recognition) using [UCF101](http://crcv.ucf.edu/data/UCF101.php) with PyTorch. A video is viewed as a 3D image or several continuous 2D images (Fig.1). Below are two simple neural nets models:   ## Dataset  ![alt text](./fig/kayaking.gif)   [UCF101](http://crcv.ucf.edu/data/UCF101.php) has total 13,320 videos from 101 actions. Videos have various time lengths (frames) and different 2d image size; the shortest is 28 frames.   To avoid painful video preprocessing like frame extraction and conversion such as [OpenCV](https://opencv.org/) or [FFmpeg](https://www.ffmpeg.org/), here I used a preprocessed dataset from [feichtenhofer](https://github.com/feichtenhofer/twostreamfusion) directly. If you want to convert or extract video frames from scratch, here are some nice tutorials:    - https://pythonprogramming.net/loading-video-python-opencv-tutorial/   - https://www.pyimagesearch.com/2017/02/06/faster-video-file-fps-with-cv2-videocapture-and-opencv/   ## Models   ### 1. 3D CNN (train from scratch) Use several 3D kernels of size *(a,b,c)* and channels *n*,  *e.g., (a, b, c, n) = (3, 3, 3, 16)* to convolve with video input, where videos are viewed as 3D images. *Batch normalization* and *dropout* are also used.   ### 2. **CNN + RNN** (CRNN)  The CRNN model is a pair of CNN encoder and RNN decoder (see figure below):    - **[encoder]** A [CNN](https://en.wikipedia.org/wiki/Convolutional_neural_network) function encodes (meaning compressing dimension) every 2D image **x(t)** into a 1D vector **z(t)** by <img src=""./fig/f_CNN.png"" width=""140"">    - **[decoder]** A [RNN](https://en.wikipedia.org/wiki/Recurrent_neural_network) receives a sequence input vectors **z(t)** from the CNN encoder and outputs another 1D sequence **h(t)**. A final fully-connected neural net is concatenated at the end for categorical predictions.       - Here the decoder RNN uses a long short-term memory [(LSTM)](https://en.wikipedia.org/wiki/Long_short-term_memory) network and the CNN encoder can be:     1. trained from scratch     2. a pretrained model [ResNet-152](https://arxiv.org/abs/1512.03385) using image dataset [ILSVRC-2012-CLS](http://www.image-net.org/challenges/LSVRC/2012/).  <img src=""./fig/CRNN.png"" width=""650"">    ## Training & testing - For 3D CNN:    1. The videos are resized as **(t-dim, channels, x-dim, y-dim) = (28, 3, 256, 342)** since CNN requires a fixed-size input. The minimal frame number 28 is the consensus of all videos in UCF101.    2. *Batch normalization*, *dropout* are used.     - For CRNN, the videos are resized as **(t-dim, channels, x-dim, y-dim) = (28, 3, 224, 224)** since the ResNet-152 only receives RGB inputs of size (224, 224).  - Training videos = **9,990** vs. testing videos = **3,330**  - In the test phase, the models are almost the same as the training phase, except that dropout has to be removed and batchnorm layer uses moving average and variance instead of mini-batch values. These are taken care by using ""**model.eval()**"".   ## Usage  For tutorial purpose, I try to build code as simple as possible. Essentially, **only 3 files are needed to for each model**. *eg.,* for 3D-CNN model   - `UCF101_3DCNN.py`: model parameters, training/testing process.   - `function.py`: modules of 3DCNN & CRNN, data loaders, and some useful functions.   - `UCF101actions.pkl`: 101 action names (labels), e.g, *'BenchPress', 'SkyDiving' , 'Bowling', etc.*  ### 0. Prerequisites - [Python 3.6](https://www.python.org/) - [PyTorch 1.0.0](https://pytorch.org/) - [Numpy 1.15.0](http://www.numpy.org/) - [Sklearn 0.19.2](https://scikit-learn.org/stable/) - [Matplotlib](https://matplotlib.org/) - [Pandas](https://pandas.pydata.org/) - [tqdm](https://github.com/tqdm/tqdm)   ### 1. Download preprocessed UCF101 dataset For convenience, we use preprocessed UCF101 dataset already sliced into RGB images [feichtenhofer/twostreamfusion](https://github.com/feichtenhofer/twostreamfusion):   - **UCF101 RGB:** [**part1**](http://ftp.tugraz.at/pub/feichtenhofer/tsfusion/data/ucf101_jpegs_256.zip.001), [**part2**](http://ftp.tugraz.at/pub/feichtenhofer/tsfusion/data/ucf101_jpegs_256.zip.002), [**part3**](http://ftp.tugraz.at/pub/feichtenhofer/tsfusion/data/ucf101_jpegs_256.zip.003)    Put the 3 parts in same folder to unzip. The folder has default name: **jpegs_256**.    ### 2. Set parameters & path  In `UCF101_CRNN.py`, for example set  ``` data_path = ""./UCF101/jpegs_256/""         # UCF101 video path action_name_path = ""./UCF101actions.pkl"" save_model_path = ""./model_ckpt/"" ```  ### 3. Train & test model  - For **3D CNN/ CRNN/ ResNetCRNN** model, in each folder run ```bash $ python UCF101_3DCNN/CRNN/ResNetCRNN.py     ```   ### 4. Model ouputs  By default, the model outputs:  - Training & testing loss/ accuracy: `epoch_train_loss/score.npy`, `epoch_test_loss/score.npy`  - Model parameters & optimizer: eg. `CRNN_epoch8.pth`, `CRNN_optimizer_epoch8.pth`. They can be used for retraining or pretrained purpose.  To check model prediction:   - Run ``check_model_prediction.py`` to load best training model and generate all 13,320 video prediction list in [Pandas](https://pandas.pydata.org/) dataframe. File output: `UCF101_Conv3D_videos_prediction.pkl`.   - Run `check_video_predictions.ipynb` with [Jupyter Notebook](http://jupyter.org/) and you can see where the model gets wrong:  <img src=""./fig/wrong_pred.png"" width=""600"">   ## Version Warrning!  As of today (May 31, 2019), it is found that in Pytorch 1.1.0 **flatten_parameters()** doesn't work under [torch.no_grad and DataParallel](https://github.com/pytorch/pytorch/issues/21108) (for multiple GPUs). Early versions before Pytorch 1.0.1 still run OK. See [Issues](https://github.com/HHTseng/video-classification/issues)  Thanks to [raghavgarg97](https://github.com/raghavgarg97)'s report.   ## Device & performance   - The models detect and use multiple GPUs by themselves, where we implemented [torch.nn.DataParallel](https://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html).  - A field test using 2 GPUs (nVidia TITAN V, 12Gb mem) with my default model parameters and batch size `30~60`.  - Some **pretrained models** can be found [here](https://drive.google.com/open?id=117mRMS2r09fz4ozkdzN4cExO1sFwcMvs), thanks to the suggestion of [MinLiAmoy](https://github.com/MinLiAmoy?tab=repositories).    network                | best epoch | testing accuracy | ------------            |:-----:| :-----:| 3D CNN                  |   4  |  50.84 % |  2D CNN + LSTM           |  25  |  54.62 % |  2D ResNet152-CNN + LSTM |  53  |**85.68 %** |        <img src=""./fig/loss_3DCNN.png"" width=""650""> <img src=""./fig/loss_CRNN.png"" width=""650""> <img src=""./fig/loss_ResNetCRNN.png"" width=""650"">    <br>"
Human Action Recognition,Data Science,https://github.com/nikhilkr31/human-action-recognition,"<h1>Human Action Recognition <p style=""text-align: left;font-size:18px"">by Nikhil Kumar Ramreddy</p></h1>  Human Action Recognition (HAR) aims to understand human behavior and assign a label to each action. It has a wide range of applications, and therefore has been attracting increasing attention in the field of computer vision. Human actions can be represented using various data modalities, such as RGB, skeleton, depth, infrared, point cloud, event stream, audio, acceleration, radar, and WiFi signal, which encode different sources of useful yet distinct information and have various advantages depending on the application scenarios.  ## What is Human Action Recognition (HAR)?  - Human activity recognition, or HAR for short, is a broad field of study concerned with identifying the specific movement or action of a person based on sensor data. - Movements are often typical activities performed indoors, such as walking, talking, standing,etc.  ## Why it is important ? - Human activity recognition plays a significant role in human-to-human interaction and interpersonal relations. - Because it provides information about the identity of a person, their personality, and psychological state, it is difficult to extract. - The human ability to recognize another person‚Äôs activities is one of the main subjects of study of the scientific areas of computer vision and machine learning. As a result of this research, many applications, including video surveillance systems, human-computer interaction, and robotics for human behavior characterization, require a multiple activity recognition system.  ## Below are some practical applications of HAR:  ![man boxing](./images/punching.gif)  Here we can see that the AI is able to identify what the man in the video is doing. This might raise the question of importance of identification of the action. Let's look at another example below:  ![running in classroom](./images/har_run.gif)  Here we can see that the model is able to identify the troublesome student who is running in the classroom highlighted in red. Whereas the other kids who are walking normally are colored in green.   This is a just small example of the endless applications that can help us automate monotonous and dangerous jobs.  ## What is a CNN?  A convolutional neural network (CNN) is a type of artificial neural network used in image recognition and processing that is specifically designed to process pixel data.  CNNs are powerful image processing, artificial intelligence (AI) that use deep learning to perform both generative and descriptive tasks, often using machine vison that includes image and video recognition, along with recommender systems and natural language processing (NLP).  ![cnn](./images/cnn.jpeg)  ## VGG16  ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) is an annual event to showcase and challenge computer vision models. In the 2014 ImageNet challenge, Karen Simonyan & Andrew Zisserman from Visual Geometry Group, Department of Engineering Science, University of Oxford showcased their model in the paper titled ‚ÄúVERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION,‚Äù which won the 1st and 2nd place in object detection and classification. The original paper can be downloaded from the below link:  [1409.1556.pdf (arxiv.org)](https://arxiv.org/pdf/1409.1556.pdf)  A convolutional neural network is also known as a ConvNet, which is a kind of artificial neural network. A convolutional neural network has an input layer, an output layer, and various hidden layers. VGG16 is a type of CNN (Convolutional Neural Network) that is considered to be one of the best computer vision models to date.  VGG16 is object detection and classification algorithm which is able to classify 1000 images of 1000 different categories with 92.7% accuracy. It is one of the popular algorithms for image classification and is easy to use with transfer learning.  ![vgg16](./images/vgg16.png)  ## Transfer Learning  Transfer learning is a machine learning method where a model developed for a task is reused as the starting point for a model on a second task. It is a popular approach in deep learning where pre-trained models are used as the starting point on computer vision and natural language processing tasks given the vast compute and time resources required to develop neural network models on these problems and from the huge jumps in skill that they provide on related problems.  ![transfer learning](./images/transfer.jpeg)  ## Sample training images data  ```Python def show_img_train():     img_num = np.random.randint(0,12599)     img = cv.imread('data/train/' + train_action.filename[img_num])     plt.imshow(cv.cvtColor(img, cv.COLOR_BGR2RGB))     plt.title(train_action.label[img_num])  show_img_train() ```   ![cycling](./images/cycling.png)   ## Action Distribution  ![pie chart](./images/pie.png)  ## Model Summary  - The loss function that we are trying to minimize is Categorical Cross Entropy. This metric is used in multiclass classification. This is used alongside softmax activation function.   - Adam is an optimization algorithm that can be used instead of the classical stochastic gradient descent procedure to update network weights iterative based in training data. This algorithm is straight forward to implement and computationally efficient.  ```Python cnn_model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy']) cnn_model.summary()  Model: ""sequential"" _________________________________________________________________  Layer (type)                Output Shape              Param #    =================================================================  vgg16 (Functional)          (None, 512)               14714688                                                                      flatten (Flatten)           (None, 512)               0                                                                             dense (Dense)               (None, 512)               262656                                                                        dense_1 (Dense)             (None, 15)                7695                                                                         ================================================================= Total params: 14,985,039 Trainable params: 270,351 Non-trainable params: 14,714,688 _________________________________________________________________ ```  ## Loss function  ![loss function](./images/loss.png)  ## Accuracy  ![accuracy function](./images/accuracy.png)   ```Python from sklearn.metrics import accuracy_score, log_loss  print('Log Loss:',log_loss(np.round(y_preds),y_test)) print('Accuracy:',accuracy_score(np.round(y_preds),y_test)) ``` ```Python Log Loss: 12.417512465789333 Accuracy: 0.6317460317460317 ```  ## Next Steps and Recommendations   - In order to improve the accuracy, we can unfreeze few more layers and retrain the model. This will help us further improve the model.  - We can tune the parameters using KerasTuner.  - The model reached a good accuracy score after the 20 epochs but it has been trained for 60 epochs which leads us to believe that the model is overfit. This can be avoided with early stopping.  - The nodes in the deep layers were fully connected. Further introducing some dropout for regularization can also be done to avoid over-fitting."
Medical Report Generation using CT Scans,Data Science,https://github.com/vysakh10/Medical-Report-Generation-using-Deep-Learning,"# Medical Report Generation Using Deep Learning NOTE : Above ipython notebooks are updated. Therefore the metric values or architectures may differ slightly from the blog. # Generating Medical Reports of Chest X-Rays using Encoder-Decoder and Attention Mechanism,  # Blog_Link : https://medium.com/@vysakh_nair/image-captioning-using-deep-learning-fe0d929cf337"
Email Classification,Data Science,https://github.com/SimarjotKaur/Email-Classifier,"EMAIL-CLASSIFICATION: -------------------- This is a text classification project which is a multi-class classification. Various classifiers are trained and tested using Python. It includes the classification of emails based on their content into three categories: Normal, Spam and Fraud.  Various classifiers including Support Vector Machine (SVM), K-Nearest Neighbour, Multinomial Na√Øve Bayes, Decision Tree, Logistic Regression, SVM with Stochastic Gradient Descent classifier (SGD-SVM) and Logistic Regression with Stochastic Gradient Descent classifier (SGD-LR) are trained on features extracted using TF-IDF vectorizer. Further, ensemble classifiers including Random Forest (RF), AdaBoost, Bagging (BGC), Extra Trees and, Vote on various classifier combinations are trained in a similar manner. Also, the effect of stemming on the model performance is observed. Additionally, classifiers are trained on the features extracted using Count Vectorizer.   Finally, all the models are evaluated based on standard evaluation metrics: Accuracy, Precision, Recall, F-score and Confusion Matrix. It is observed that Vote on SVM, BGC and RF outperform all the models, followed by SGD-SVM, trained on TF-IDF features without stemming.  PREREQUISITES: -------------- <ul>    <Li> Python 3.x. </li>   <Li> Libraries: </li>   <ul>     <Li> Pandas </li>     <Li> Sklearn </li>     <Li> Nltk </li>     <Li> Numpy </li>     <Li> Matplotlib </li>     <Li> String </li>     <Li> re </li>     <Li> Random </li>   </Ul> </Ul>  CODE BRIEF: ---------- The entire coding is done in Python3.5 which was executed in Spyder which is a part of Anaconda3. There are two python files ‚ÄòExtract_email.py‚Äô and ‚ÄòEmail_Classification.py‚Äô which involves the process of Data Extraction and, Text processing and classification respectively.  I. Extract_email.py: This file involves the process of Data Extraction. In this, 1000 fraud emails from the ‚Äòfradulent_emails.txt‚Äô file containing 4075 emails are extracted and, 1000 emails for each Spam and Normal category are extracted from ‚Äòemails.csv‚Äô file that contains 5730 emails which is a combination of both Spam and Normal emails. Finally, all the extracted emails are concatenated into one csv file. This csv file contains the final dataset that contains 3000 emails with 1000 emails for each category.  NOTE: For the proper execution of the code, update the paths for: <ul>    <li>the final csv file to be created (‚Äòfinal_dataset.csv‚Äô)</li>    <li>fradulent_emails.txt file (file containing fraud emails)</li>    <li>emails.csv file (file containing spam and normal emails)</li> </ul> II. Email_Classification.py: This file involves the complete process of email processing and classification:  1. Data Preprocessing: Functions are created for the removal of punctuation and stopwords. Another function is created for stemming of the content. In order to extract the relevant features 2 vectors were used: TF-IDF vectors and Count Vectors. First, the entire process of classification is performed by the features created using TF-IDF and then the features created by Count- Vectorizer are processed and observed. Then the features are split into train and test set in the ratio 7:3 respectively.  2. Text Classification: Various classifers are trained on the features extracted above and then, their performance is observed. Before the training of the models, Parameter Tuning is performed to identify the optimum parameters for each classifier.  3. Evaluation Metrics: The classifiers are evaluated on the basis of: <ul>    <li> Accuracy</li>    <li> Confusion Matrix</li>    <li> Precision, Recall and F-Score</li> </ul>  NOTE: For the execution of the code, change the path for final_dataset.csv that was created in the previous step for the variable ‚Äòinput_dataset‚Äô."
Uber Data Analysis,Data Science,https://github.com/Unnati0104/Uber-Data-Analysis,"# Uber-Data-Analysis <hr>  ### Objective: The objective is to first explore hidden or previously unknown information by applying exploratory data analytics on the dataset and to know the effect of each field on price with every other field of the dataset. Then we apply different machine learning models to complete the analysis. After this, the results of applied machine learning models were compared and analyzed on the basis of accuracy, and then the best performing model was suggested for further predictions of the label ‚ÄòPrice‚Äô.  ### DataSet: For this project we are taken the dataset from kaggle in which there are 57 columns and approx 6 lakh rows and the file is in csv format. you can view or download the dataset through this link: https://www.kaggle.com/brllrb/uber-and-lyft-dataset-boston-ma  ### Application of this Project: We use machine learning algorithms to predict the price of Uber, so that it is easy for the company to do analysis on price based on certain features.  ### Conclusion: Before working on features first we need to know about the data insights which we get to know by EDA. Apart from that, we visualize the data by drawing various plots, due to which we understand that we don‚Äôt have any data for taxi‚Äôs price, also the price variations of other cabs and different types of weather. Other value count plots show the type and amount of data the dataset has. After this, we convert all categorical values into continuous data type and fill price Nan by the median of other values. Then the most important part of feature selection came which was done with the help of recursive feature elimination. With the help of RFE, the top 25 features were selected. Among those 25 features still, there are some features which we think are not that important to predict the price so we drop them and left with 8 important columns. We apply four different models on our remaining dataset among which Decision Tree, Random Forest, and Gradient Boosting Regressor prove best with 96%+ accuracy on training for our model. This means the predictive power of all these three algorithms in this dataset with the chosen features is very high but in the end, we go with random forest because it does not prone to overfitting and design a function with the help of the same model to predict the price.  <hr> "
Sound Classification,Data Science,https://github.com/abishek-as/Audio-Classification-Deep-Learning,"# About the Project  Kaggle Notebook link - <https://www.kaggle.com/abishekas11/audio-classification-using-deep-learning>.<br> The demo of the project is explained here - <https://youtu.be/hJvr1dyiOxM>.  ## To run this project you need  - python3 - pip  ## Steps for running this project  - git clone this project and extract it and open a terminal in that folder itself. - Create a virtual environment by running the following command   - ```python3 -m venv audio-venv``` (audio-venv is the name of the virtual environment) - Activate the virtual environment by running the following command   - ```source audio-venv/bin/activate``` - Now run the following command   - ```pip install -r requirements.txt``` - Now run the following command to start the Django server   - ```python3 manage.py run server``` - To stop the server, press Ctrl+Shift+C or Ctrl+Alt+C  ## Abstract  Audio classification or sound classification can be referred to as the process of analysing audio recordings. This amazing technique has multiple applications in the fields of AI and data science. In this project, we will explore audio classification using deep learning concepts involving algorithms like Artificial Neural Network (ANN), 1D Convolutional Neural Network (CNN1D), and CNN2D. The dataset contains 8732 labelled sound excerpts (=4s) of urban sounds from ten categories: air for audio prediction, car horns, children playing, dog barking, drilling, engine idling, gunshots, jackhammers, sirens, and street music are used for audio prediction. Before we develop models, we do some basic data preprocessing and feature extraction on audio signals. As a result, each model is compared in terms of accuracy, training time, and prediction time. This is explained by model deployment, where users are allowed to load a desired sound output for each model being deployed successfully, which will be discussed in detail.  ## Introduction  Sound classification is one of the most widely used applications in audio deep learning. It involves learning to classify sounds and predicting the category of that sound. This type of problem can be applied to many practical scenarios, e.g., classifying music clips to identify the genre of the music, or classifying short utterances by a set of speakers to identify the speaker based on the voice. Our project involves a comparison of some deep learning models and successfully deploying them using Django. For comparison and deployment, we have selected three of the most widely used deep learning models like ANN, CNN1D, and CNN2D. Before building and deploying the models, we will do some preprocessing and feature extraction. A detailed comparison study of accuracy, training and prediction time, and finding the best model amongst the models is made at the end of the document. As our aim is to help deaf people know their surroundings, we have deployed our model in use. Here people can load the audio files (.wav) and, once they submit them, the audio sound is printed as the outcome for each model, thus achieving our goal for the project.  ## Objective and goal of the project  We always thought we could develop a useful application by applying deep learning concepts that help deaf people hear what is happening around them. We are all blessed with good hearing, but some are unlucky. So, we thought, why don't we develop some software useful for those people to upload the audio files and see a report which gives a detailed classification of the audio file. This not only helps them recognise what is going on around them, but it also helps them connect with the world in the same way that we do. Our Goal is to develop a very useful application for deaf people that helps them know the voices around them and provides a summary of those voices.  ## Problem Statement  Nowadays, deep learning is an emerging topic for upcoming IT professionals. Deep learning is mostly used in audio or image processing projects. So we thought of doing audio classification using deep learning models as our project. Audio classification or sound classification can be referred to as the process of analysing audio recordings. This amazing technique has multiple applications in the field of AI and data science, such as chatbots, automated voice translators, virtual assistants, music genre identification, and text-to-speech applications. Many deaf people suffer a lot as they cannot interact with the outside world and their relationship with the environment is not like we all have. As a group of software engineering students, this problem made us think of deploying a useful solution for the above-mentioned problem. So we came up with the idea of deploying a web application allowing deaf people/users to load audio files and get to know what all the sounds they are surrounded with using deep learning models. Here, we are also doing a comparison of some deep learning models and successfully deploying them using Django. For comparison and deployment, we have selected three of the most widely used deep learning models, like ANN, CNN1D, and CNN2D. Before building and deploying the models, we will do some preprocessing and feature extraction. A detailed comparison study of accuracy, training and prediction time, and finding the best model amongst the models is made at the end of the document. With the successful deployment of the web application, deaf people can get to know their surrounding sounds, which satisfies our main goal of the project.  ## Literature Survey  1. [An Analysis of Audio Classification Techniques using Deep Learning Architectures](https://sci-hub.se/https://doi.org/10.1109/ICICT50816.2021.9358774)., - They used CNN and RNN neural network models to accomplish the classification in this study, and a comparison of the accuracy of each model is done. They began with standard audio preprocessing techniques such as normalization, rapid Fourier transformation, and short-time Fourier transformation, all of which were conceptually and thoroughly documented in the study. For training and testing, the datasets used were UrbanSound8K, ESC-50, and FSDKaggle2018. In total, these datasets contain nearly 7000 sound excerpts. During each training cycle, all of the samples (audio) in the dataset were chosen at random. They've also looked at the CF model and the CFClean model to see which produces better outcomes. 0.0001 is the chosen learning rate. And now, as previously stated, the CNN and RNN models are used, with a comparative analysis of accuracies and loss percentages for both models provided in a table format for easier analysis. Finally, CNN outperformed RNN with a maximum accuracy of 94.52 percent. 2. [Automatic Sound Classification Using Deep Learning Networks](http://dspace.lpu.in:8080/jspui/bitstream/123456789/3157/1/11602389_11_29_2017%203_44_29%20PM_Complete%20File.pdf)., - They propose a model that uses a Tensor Deep Stacking Network to classify sounds in this paper (T-DSN). To achieve the low error rate in classification, an HMM and SoftMax layer will be utilized in addition to the Tensor deep stacking network. The fundamental concept of deep learning has been explained. The Tensor Deep Stacking Network (T-DSN) is a Deep Stacking Network extension (DSN). To obtain greater precision than the preceding layer, the original input vector is also concatenated with the intermediate output of the hidden layer. A deep stacking network differs from previous architectures in that it uses the mean square error between the current module's forecast and the ultimate prediction value rather than the gradient descent method. With this new strategy, the mistake rate will be minimized. Their main goals in this research are to compare the performance of a traditional neural network to that of deep learning architecture. This goal will justify the usage of deep architectures for massive data sets. Tensor-Deep Stacking Network with Hidden Markov Model and Nonlinear Activation for Sound Classification. This goal will be accomplished by merging a classic sound classification model with a new tensor deep stacking network technique. This research report enlightens us on the T-DSN methodology and how it is used in data. 3. [Unsupervised feature learning for audio classification using convolutional deep belief networks](http://citeseerx.ist.psu.edu/viewdoc/downloaddoi=10.1.1.154.380&rep=rep1&type=pdf)., - In this paper, they have applied Convolutional Deep Belief Networks (CDBN) to evaluate several audio classification tasks. For the application purpose, we convert time-domain signals that we got into spectrograms then we apply PCA whitening to create a lower-dimensional representation, and that the data to apply CDBN contains n number of PCA components of one-dimensional vectors of length(n). Feature learning for unsupervised data (training, visualization, Phonemes, and the CDBN features application), speech data identification, and discovering the accuracy for each data type such as a speaker, phone, and so on are all processes involved. Finally, we use music data in this paper (identification, music artist classification, and accuracy analysis using the CDBN model). All of the accurate information has been meticulously tabulated. 4. [Audio Based Bird Species Identification using Deep Learning Techniques](https://infoscience.epfl.ch/record/229232/files/16090547.pdf)., - The CNN model is used in this article to predict bird species identification. Audio processing, data augmentation, bird species recognition, and acoustic classification are among the services they provide. The sound file is separated into two parts for preprocessing: a signal part with audible bird audio and a noise part with no audio. The spectrogram is divided into equal-sized chunks for both portions, with each chunk serving as a separate sample for CNN. A batch size of 16 training samples per iteration was also employed during training, which was detailed in detail using a figure that clearly describes each step for greater understanding. This paper aids us in comprehending each phase. 5. [Efficient Pre-Processing of Audio and Video signal dataset for building an efficient Automatic Speech Recognition System](https://www.acadpubl.eu/hub/2018-119-16/1/189.pdf)., - They preprocess the dataset, which includes video and audio files (.wav files), then use algorithms to create an artificial speech recognition system in this article. This article is mostly used to prepare audio files for subsequent processing. Some audio signal features are retrieved and transformed for use in the model. .wav files are read using Python modules, and a 1-d NumPy array with the sample rate is returned. The 13 most important MFCC features for an audio signal are extracted using the mfcc function in Python. To ensure that the model generalizes well to real data, we introduced background sounds to the audio files. The process is then performed twice more, with each noisy file generated being unique. 6. [CNNs for Audio Classification](https://towardsdatascience.com/cnns-for-audio-classification-6244954665ab)., - In this article, a Kaggle dataset is downloaded and used to train CNN. There are a lot of audio files in the dataset you choose. We first import all libraries, such as Libros. CNN anticipates a grayscale image and a three-channel color image (RGB). Now we can start modeling the data, normalize it, and cast it into a NumPy array. The CNN model is now generated, and the model is then evaluated. This tutorial will assist us in implementing the CNN algorithm successfully. Similarly, we obtained references for ANN, RNN, and other topics from study material websites such as geekforgeeks, tutorials point, and so on. We can successfully learn and implement the algorithms using these sources.  ## Feasibility Study  A feasibility study is an analysis that takes all of a project's important factors into account‚Äîincluding economic, technical, legal, and scheduling problems‚Äîto ascertain the likelihood of completing the project successfully. Project directors use feasibility studies to distinguish the pros and cons of undertaking a project before they invest a lot of time and money into it.  ### Technical Feasibility  #### Gathering dataset online  The first technical problem we face is gathering the dataset. Because we are using .wav files, it took us longer to collect audio files comprising various voices. Though it took longer than expected, the subsequent process was relatively simple.  #### Memory used  Because the dataset we utilized was 6.60GB in size, training and testing each model required more memory and time. It nearly took us an hour to complete the period for each model. We didn't utilize much RAM because we used Google Collab.  #### Deploying the model  Initially, we considered utilizing any model and uploading the .wav file to acquire the appropriate outcome, as our major goal is to assist deaf individuals in recording and identifying any noises present. However, because we are also performing comparative analysis, we decided to display the outcome predicted by each of the models that we employed for better performance. This work was technically achievable as well.  #### Accessibility  The hardware required for this project is straightforward and widely available. All that is required is any type of optical instrument and a computer to interpret and assess the acquired sights.  ### Economic Feasibility  The question of whether the model can be proven to be economically feasible is one of the most significant barriers and misconceptions of any new technology. The following study explains the project's financial viability. This is a low-cost project that may be used by anyone who has a touch phone or a laptop. There aren't many hardware and software costs to worry about. The only thing to consider is the cost of maintenance. Following the successful deployment of our project, we must regularly evaluate the operation of the four projects; if there are more users, we must upgrade our project so that more users can benefit.  ### Social Feasibility  Deaf and blind people will be unaware of how their surroundings treat them. They will gain completely from our project. All they have to do is document their surroundings and be aware of what is going on around them. This will make them happy, and their worry of not knowing what is going on around them will be gone.  ### Environmental Feasibility  An Environmental Feasibility Study evaluates the environmental and social viability of a planned development, identifying potential challenges and dangers to the successful completion of the proposed development. Solutions and mitigating strategies are being researched. The goal of an Environmental Due Diligence Report is to assess potential risks and liabilities related to environmental and health and safety issues such as land contamination before entering into any contractual agreements. This is critical since these risks could result in financial liabilities for the parties concerned.  #### Environment and Health  This project is extremely beneficial to deaf individuals. The recording of their surroundings is critical to the success of our initiative. There will be no harm done to the environment's health because our project is entirely technical, and its safety will not be jeopardized by any factor.  ### Political Feasibility  There are many new solutions for physically challenged persons on the horizon, and we wanted one of our efforts to benefit the public. According to our hypothesis, this project will benefit deaf individuals and bring them delight, thereby satisfying themselves as well as the project purpose. This project may take some time to complete, but it will never fail.  ## System Design  ### About the Dataset  Dataset taken from - [https://urbansounddataset.weebly.com/urbansound8k.html]  #### Description  This dataset contains 8732 labeled sound excerpts (=4s) of urban sounds from ten categories: air conditioner, car horn, children playing, dog bark, drilling, engine idling, gun shot, jackhammer, siren, and street music. The classes are based on the taxonomy of urban sounds. All excerpts are from field recordings that have been uploaded to www.freesound.org. The files are pre-sorted into ten folds (folders named fold1-fold10) to aid in reproducing and comparing the results of the automatic classification.  #### Audio Files Included  8732 WAV audio files of urban sounds (as described above).  #### Metadata Files Included  - **UrbanSound8k.csv** : This file contains meta-data for each audio file in the dataset. This includes the following: - The name of the audio file. The name takes the following format: (fsID)-(classID)-(occurrenceID)-(sliceID).wav, where: (fsID) = the Freesound ID of the recording from which this excerpt (slice) is taken, (classID) = a numeric identifier of the sound class (see description of classID below for further details), (occurrenceID) = a numeric identifier to distinguish different occurrences of the sound within the original recording, (sliceID) = a numeric identifier to distinguish different slices taken from the same occurrence - fsid : The Freesound ID of the recording from which this excerpt (slice) is taken - start : The start time of the slice in the original Freesound recording - end: The end time of slice in the original Freesound recording - salience: A (subjective) salience rating of the sound. 1 = foreground, 2 = background. - fold: The fold number (1-10) to which this file has been allocated. - classID : A numeric identifier of the sound class:  0 = air_conditioner, 1 = car_horn, 2 = children_playing, 3 = dog_bark, 4 = drilling, 5 = engine_idling, 6 = gun_shot, 7 = jackhammer, 8 = siren, 9 = street_music - class: The class name: air_conditioner, car_horn, children_playing, dog_bark, drilling, engine_idling, gun_shot, jackhammer, siren, street_music.  ### Algorithm Used  #### Artificial Neural Network (ANN)  Artificial neural networks (ANNs) are made up of node layers, each of which has an input layer, one or more hidden layers, and an output layer. Each node, or artificial neuron, is linked to another and has its own weight and threshold. If the output of any individual node exceeds the specified threshold value, that node is activated and begins sending data to the network's next layer. Otherwise, no data is passed to the next network layer.  #### Convolutional Neural Network (CNN)  A convolutional neural network (CNN, or ConvNet) is a type of artificial neural network used to interpret visual imagery in deep learning. Based on the shared-weight architecture of the convolution kernels or filters that slide along input features and give translation equivariant responses known as feature maps, they are also known as shift invariant or space invariant artificial neural networks (SIANN). Surprisingly, most convolutional neural networks are only equivariant under translation, rather than invariant. Image and video recognition, recommender systems, image classification, image segmentation, medical image analysis, natural language processing, brain-computer interfaces, and financial time series are just a few of the areas where they can be used.  ##### CNN2D  The traditional deep CNNs discussed in the preceding section are only designed to work with 2D data like photos and movies. This is why they're referred to as ""2D CNNs"" so frequently. 2D CNN, kernel moves in 2 directions. Input and output data of 2D CNN is 3 dimensional. Mostly used on Image data. This is the conventional Convolution Neural Network, which was introduced in the Lenet-5 architecture for the first time. On Image data, Conv2D is commonly used. Because the kernel slides along two dimensions on the data, it is called 2 dimensional CNN. The main benefit of employing CNN is that it can extract spatial properties from data using its kernel, which is something that other networks can't achieve. CNN, for example, can detect edges, colour distribution, and other spatial aspects in an image, making these networks particularly robust in image classification and other data with spatial qualities.  ##### CNN1D  A modified form of 2D CNNs known as 1D Convolutional Neural Networks (1D CNNs) has recently been developed as an alternative. In dealing with 1D signals, these research have proven that 1D CNNs are beneficial and consequently preferable to their 2D counterparts in some situations. 1D CNN, kernel moves in 1 direction. Input and output data of 1D CNN is 2 dimensional. Mostly used on Time-Series data. 1D CNN can perform activity recognition task from accelerometer data, such as if the person is standing, walking, jumping etc. This data has 2 dimensions. The first dimension is time-steps and other is the values of the acceleration in 3 axes. Similarly, 1D CNNs are also used on audio and text data since we can also represent the sound and texts as a time series data. Conv1D is widely applied on sensory data, and accelerometer data is one of it.  ### Architecture Diagram  #### ANN  ![Model1](/images/Model1.png)  #### CNN1D  ![Model2](/images/Model2.png)  #### CNN2D  ![Model3](/images/Model3.png)  ### Proposed Methodology  #### Importing the Dataset, Data Preprocessing  We obtained the dataset from UrbanSound8K, which contains over 8500 data files containing various audios such as a baby crying, birds sound, dog bark, and many others in the form of.wav files. It is divided into ten folders, indicating that the dataset has ten classes as described in the dataset section. Libraries that are required are added, as well as Librosa for feature extraction.  #### Feature Extraction and Database Building  We use the data we obtained by using librosa because we require data in numeric format. We use Mel-Frequency Cepstral Coefficients (MFCC) to extract independent data, which summarizes the frequency distribution across the window size, allowing us to analyze both the frequency and time characteristics of the sound. We can identify features for classification using these audio representations. We defined the features extractor function and passed in the path to the audio file as a parameter, after which we will extract the audio features using librosa. The feature_extractor function is applied to all rows, and the results are stored in a dataframe with features and class columns for further calculations. The feature_extractor function is attached in appendix below.  #### Building, Training, Compiling ANN, CNN1D \& CNN2D Models  We save the dataframe's feature and class columns to x and y arrays, respectively. The y array is then converted to categorical values using the Labelencoder() method. The data was then divided into test and training sets. Then, based on the architecture diagrams, we design, compile, and fit the ANN, CNN1D, and CNN2D models, and save the findings for future visualization.  #### Predicting the test audio on all the Models  We Defined a function which will extract the features from the audio which is given in the parameter , and the features will be converted to proper input shapes for ANN, CNN1D and CNN2D modles. The output will give the class label. The ANN_print_prediction, CNN1D_print_prediction and CNN2D_print_prediction functions is attached in appendix bellow.  #### Deployment of Models  We deploy the model in a web application using the Django web framework. Now the test data is applied to the model and the output is displayed.  ## Implementation of System  ### Homepage  ![Homepage](/images/21.png)  ### Select the sample audio for testing  ![Select the sample audio for testing](/images/22.png)  ### Uploading the sample audio file  ![Uploading the sample audio file](/images/23.png)  ### Result page  ![Result page](/images/24.png)  ## Results & Discussion  ### Comparing Accuracy of All Modules  ANN has highest accuracy of all modules. ![Comparing Accuracy of All Modules](/images/12.png)  ### Comparing Training time of All Modules  CNN1D training takes more time of all modules. ![Comparing Training time of All Modules](/images/13.png)  ### Comparing Prediction time of All Modules  CNN2D prediction takes less time of all modules. ![Comparing Prediction time of All Modules](/images/14.png)  ### Loss per Epochs and Accuracy per Epochs for ANN Model  ![Loss per Epochs](/images/15.png) ![Accuracy per Epochs](/images/16.png)  ### Loss per Epochs and Accuracy per Epochs for CNN1D Model  ![Loss per Epochs](/images/17.png) ![Accuracy per Epochs](/images/18.png)  ### Loss per Epochs and Accuracy per Epochs for CNN2D Model  ![Loss per Epochs](/images/19.png) ![Accuracy per Epochs](/images/20.png)  ## Performance Analysis  In our project, we got audio signals and sample rates for the selected audio files, which was a difficult challenge for us. We used numerous references for each algorithm and conducted a comparison analysis of three algorithms: ANN, CNN1D, and CNN2D. Using all the techniques, we tested the algorithm with various audio files and obtained the projected value as expected. We displayed all the comparisons we made, including accuracies, training time, and prediction time for each model, to provide a visual depiction. In terms of accuracy, ANN has a 94.79\% accuracy rate, CNN1D has a 93.68\% accuracy rate, and CNN2D has an 89.93\% accuracy rate, therefore we can conclude that ANN has the highest accuracy rate.  ## Conclusion & Future Work  There are so many research papers that tells us how an algorithm works and how to predict any model or algorithm. Those references helped us a lot in achieving our project goal. As a result, we have successfully done a comparative analysis for accuracy rate, training time and prediction time. We have also deployed our project using Django and was a very challenging task. In future work, we can also do comparative analysis on many models to get a better understanding of any model. Also, we can deploy an updated model that allows users to record their surroundings using a mic and get the desired output. This way, users can easily use the model deployment anywhere without any restrictions like file format not supported or large file size and so on."
Credit Card Fraud Detection,Data Science,https://github.com/shakiliitju/Credit-Card-Fraud-Detection-Using-Machine-Learning,"# Credit-Card-Fraud-Detection-Using-Machine-Learning   ## ABSTRACT Credit card fraud is a significant problem, with billions of dollars lost each year. Machine learning can be used to detect credit card fraud by identifying patterns that are indicative of fraudulent transactions. Credit card fraud refers to the physical loss of a credit card or the loss of sensitive credit card information. Many machinelearning algorithms can be used for detection. This project proposes to develop a machine-learning model to detect credit card fraud. The model will be trained on a dataset of historical credit card transactions and evaluated on a holdout dataset of unseen transactions. <br> <br> <b>Keywords:</b> Credit Card Fraud Detection, Fraud Detection, Fraudulent Transactions, K- Nearest Neighbors, Support Vector Machine, Logistic Regression, Decision Tree.  <br> <br>  ## Overview  With the increase of people using credit cards in their daily lives, credit card companies should take special care of the security and safety of the customers. According to (Credit card statistics 2021), the number of people using credit cards worldwide was 2.8 billion in 2019; also, 70those users own a single card. Reports of Credit card fraud in the U.S. rose by 44.7in 2020. There are two kinds of credit card fraud, and the first is having a credit card account opened under your name by an identity thief. Reports of this fraudulent behaviour increased 48to 2020. The second type is when an identity thief uses an existing account you created, usually by stealing the information on the credit card. Reports on this type of Fraud increased 9to 2020(Daly, 2021). Those statistics caught We‚Äôs attention as the numbers have increased drastically and rapidly throughout the years, which motivated We to resolve the issue analytically by using different machine learning methods to detect fraudulent credit card transactions within numerous transactions.  <br> <br>  ## Project goals  The main aim of this project is the detection of fraudulent credit card transactions, as it is essential to figure out the fraudulent transactions so that customers do not get charged for the purchase of products that they did not buy. Fraudulent Credit card transactions will be detected with multiple ML techniques. Then, a comparison will be made between the outcomes and results of each method to find the best and most suited model for detecting fraudulent credit card transactions; graphs and numbers will also be provided. In addition, it explores previous literature and different techniques used to distinguish Fraud within a dataset.   <br> <br>  ## Data Source  The dataset was retrieved from an open-source website, Kaggle.com. It contains data on transactions made in 2013 by European credit card users in two days only. Thedataset consists of 31 attributes and 284,808 rows. Twenty-eight attributes are numeric variables that, due to the confidentiality and privacy of the customers, have been transformed using PCA transformation; the three remaining attributes are ‚ÄùTime‚Äù, which contains the elapsed seconds between the first and other transactions of each Attribute, ‚ÄùAmount‚Äù is the amount of each transaction, and the final attribute ‚ÄúClass‚Äù which contains binary variableswhere ‚Äú1‚Äù is a case of fraudulent transaction, and ‚Äú0‚Äù is not as case of fraudulent transaction. <br> <br> <b>Dataset: </b> <a href=""https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud"">kaggle Dataset</a>  <br> <br>  ## Algorithm  1. K-Nearest Neighbor (KNN) 2. Logistic Regression (L.R.) 3. Support Vector Machine (SVM) 4. Decision Tree (D.T.)   <br> <br>  ## Future Work  There are many ways to improve the model, such as using it on different datasets with various sizes and data types or by changing the data splitting ratio and viewing it from a different algorithm perspective. An example can be merging telecom datato calculate the location of people to have better knowledge of the location of the card owner while his/her credit card is being used; this will ease the detection because if the card owner is in Dubai and a transaction of his card was made in Abu Dhabi, it will easily be detected as Fraud.  <br> <br>  ## Conclusion In conclusion, the main objective of this project was to find the most suited model for creditcard fraud detection in terms of the machine learning techniques chosen for the project. It was met by building the four models and finding the accuracies of them all; the best in terms of accuracy is KNN and Decision Tree, which scored 100 on credit card fraud and increased the customer‚Äôs satisfaction as it will provide themwith a better experience and feeling secure. "
Sign Language Recognition,Data Science,https://github.com/AvishakeAdhikary/Realtime-Sign-Language-Detection-Using-LSTM-Model,"# Realtime Sign Language Detection Using LSTM Model  ![Mediapipe Detection](https://github.com/AvhishekAdhikary/Realtime-Sign-Language-Detection-Using-LSTM-Model/assets/32614982/4d6a7542-2409-424f-9c9f-6b90a3236ea6)   > The Realtime Sign Language Detection Using LSTM Model is a deep learning-based project that aims to recognize and interpret sign language gestures in real-time. It utilizes a Long Short-Term Memory (LSTM) neural network architecture to learn and classify sign language gestures captured from a video feed. The project provides a user-friendly interface where users can perform sign language gestures in front of a camera, and the system will instantly detect and interpret the gestures. This can be used as an assistive technology for individuals with hearing impairments to communicate effectively. Key features of the project include real-time gesture detection, high accuracy in recognition, and the ability to add and train new sign language gestures. The system is built using Python, TensorFlow, OpenCV, and Numpy, making it accessible and easy to customize. With the Realtime Sign Language Detection Using LSTM Model, we aim to bridge the communication gap and empower individuals with hearing impairments   ## Table of Contents  - [About the Project](#about-the-project) - [Demo](#demo) - [Features](#features) - [Getting Started](#getting-started) - [Usage](#usage) - [Contributing](#contributing) - [License](#license) - [Contact](#contact)  ## About the Project  This section provides an overview of the Realtime Sign Language Detection Using LSTM Model project. It describes the project's purpose, which is to develop a system that can accurately detect and interpret sign language gestures in real time. It also highlights the use of LSTM (Long Short-Term Memory) models for this task and emphasizes the project's significance in improving communication accessibility for the deaf and hard of hearing community.  ## Demo  This section showcases a demonstration of the Realtime Sign Language Detection Using LSTM Model project.   https://github.com/AvhishekAdhikary/Realtime-Sign-Language-Detection-Using-LSTM-Model/assets/32614982/16bd1d47-cc3f-488c-8d0e-e400004dc716   The demo allows viewers to see how the system accurately interprets sign language gestures and provides real-time results.  ## Features  ![model h5](https://github.com/AvhishekAdhikary/Realtime-Sign-Language-Detection-Using-LSTM-Model/assets/32614982/ece8ef5e-295c-4cfd-beb5-255ea88c8b76)   - Real-time sign language detection: The system can detect and interpret sign language gestures in real time, providing immediate results. - High accuracy: The LSTM (Long Short-Term Memory) model used in the project ensures accurate recognition of a wide range of sign language gestures. - Multi-gesture support: The system can recognize and interpret various sign language gestures, allowing for effective communication. - Easy integration: The project provides code snippets and examples for seamless integration into other applications or projects. - Accessibility improvement: The Realtime Sign Language Detection Using LSTM Model project contributes to enhancing communication accessibility for the deaf and hard of hearing community. - Customization options: The system supports customization of gestures, allowing users to adapt it to their specific needs. - Language flexibility: The model can be trained to recognize sign language gestures from different languages, making it adaptable to various communication contexts. - User-friendly interface: The project includes a user-friendly interface that simplifies the interaction with the system, ensuring a smooth user experience. - Open-source: The Realtime Sign Language Detection Using LSTM Model is an open-source project, encouraging contributions and fostering collaboration in the development community.  ![Neural Network](https://github.com/AvhishekAdhikary/Realtime-Sign-Language-Detection-Using-LSTM-Model/assets/32614982/2adabb2c-db8e-47a3-a7ae-f2ce7175cc82)   ## Getting Started  To get started with the Realtime Sign Language Detection Using LSTM Model, follow these steps:  ### Prerequisites  - Python - TensorFlow - OpenCV - Numpy  ### Installation  1. Clone the repository:  ```shell git clone https://github.com/AvhishekAdhikary/Realtime-Sign-Language-Detection-Using-LSTM-Model.git ``` 2. Install Dependencies:    ```shell   pip install notebook   ``` 3. Run Jupyter Notebook:    ```shell   jupyter notebook   ```  ## Usage  Simply run all the cells inside the 'RealTimeSignLanguageDetection.ipynb' file.  ## Contributing  Contributions are welcome! If you have any ideas, suggestions, or bug fixes, please open an issue or submit a pull request.  ## License  This section states that the Realtime Sign Language Detection Using LSTM Model project is released under the MIT License. It briefly describes the terms and conditions of the license, such as the permission to use, modify, and distribute the project, with appropriate attribution. It provides a link to the full text of the MIT License for further reference.  ## Contact  For any questions or inquiries, feel free to contact me at avhishe.adhikary11@gmail.com."
Flower Class Prediction,Data Science,https://github.com/Apaulgithub/oibsip_taskno1,"# Iris Flower Classification  **Oasis Infobyte Internship Project** - [**Credentials**](https://drive.google.com/file/d/1uDjGZcWln07jb0dL60Yuz_33Ck78DQXF/view?usp=drive_link)  ![MasterHead](https://www.embedded-robotics.com/wp-content/uploads/2022/01/Iris-Dataset-Classification-1024x367.png)  <font size=""1"">Image Courtesy: https://www.embedded-robotics.com/wp-content/uploads/2022/01/Iris-Dataset-Classification-1024x367.png</font>  Click on the following link to checkout the colab file. - [Colab](https://colab.research.google.com/drive/1kqaU-CK8mRD35y0dFF4xD9z-GJ_CCQOe?usp=sharing)   ---  ## Problem Statement  The iris flower, scientifically known as Iris, is a distinctive genus of flowering plants. Within this genus, there are three primary species: Iris setosa, Iris versicolor, and Iris virginica. These species exhibit variations in their physical characteristics, particularly in the measurements of their sepal length, sepal width, petal length, and petal width.  **Objective:**  The objective of this project is to develop a machine learning model capable of learning from the measurements of iris flowers and accurately classifying them into their respective species. The model's primary goal is to automate the classification process based on the distinct characteristics of each iris species.  **Project Details:**  - **Iris Species:** The dataset consists of iris flowers, specifically from the species setosa, versicolor, and virginica. - **Key Measurements:** The essential characteristics used for classification include sepal length, sepal width, petal length, and petal width. - **Machine Learning Model:** The project involves the creation and training of a machine learning model to accurately classify iris flowers based on their measurements.  This project's significance lies in its potential to streamline and automate the classification of iris species, which can have broader applications in botany, horticulture, and environmental monitoring.  ---  ## Project Summary  **Project Description:**  The Iris Flower Classification project focuses on developing a machine learning model to classify iris flowers into their respective species based on specific measurements. Iris flowers are classified into three species: setosa, versicolor, and virginica, each of which exhibits distinct characteristics in terms of measurements.  **Objective:**  The primary goal of this project is to leverage machine learning techniques to build a classification model that can accurately identify the species of iris flowers based on their measurements. The model aims to automate the classification process, offering a practical solution for identifying iris species.  **Key Project Details:**  - Iris flowers have three species: setosa, versicolor, and virginica. - These species can be distinguished based on measurements such as sepal length, sepal width, petal length, and petal width. - The project involves training a machine learning model on a dataset that contains iris flower measurements associated with their respective species. - The trained model will classify iris flowers into one of the three species based on their measurements.  ---  ## Results  I have selected recall as the primary evaluation metric for the Iris Flower Classification model. And after removing the overfitted models which have recall, precision, f1 scores for train as 100%, we get the final list:  | Sl. No. | Classification Model      |   Recall Train (%) |   Recall Test (%) | |:--------|:--------------------------|---------------:|--------------:| |    1    | Decision Tree tuned       |       95.24  |      95.56 | |    2    | Random Forest tuned       |       97.14  |      97.78 | |    3    | Naive Bayes               |       94.28 |      97.78 | |    4    | Naive Bayes tuned         |       94.28 |      97.78 |  ## Conclusion  In the Iris flower classification project, the tuned Random Forest model has been selected as the final prediction model. The project aimed to classify Iris flowers into three distinct species: Iris-Setosa, Iris-Versicolor, and Iris-Virginica. After extensive data exploration, preprocessing, and model evaluation, the following conclusions can be drawn:  1. **Data Exploration:** Through a thorough examination of the dataset, we gained insights into the characteristics and distributions of features. We found that Iris-Setosa exhibited distinct features compared to the other two species.  2. **Data Preprocessing:** Data preprocessing steps, including handling missing values and encoding categorical variables, were performed to prepare the dataset for modeling.  3. **Model Selection:** After experimenting with various machine learning models, tuned Random Forest was chosen as the final model due to its simplicity, interpretability, and good performance in classifying Iris species.  4. **Model Training and Evaluation:** The Random Forest (tuned) model was trained on the training dataset and evaluated using appropriate metrics. The model demonstrated satisfactory accuracy and precision in classifying Iris species.  5. **Challenges and Future Work:** The project encountered challenges related to feature engineering and model fine-tuning. Future work may involve exploring more advanced modeling techniques to improve classification accuracy further.  6. **Practical Application:** The Iris flower classification model can be applied in real-world scenarios, such as botany and horticulture, to automate the identification of Iris species based on physical characteristics.  In conclusion, the Iris flower classification project successfully employed Random Forest (tuned) as the final prediction model to classify Iris species. The project's outcomes have practical implications in the field of botany and offer valuable insights into feature importance for species differentiation. Further refinements and enhancements may lead to even more accurate and reliable classification models in the future.  ---  ## Author  - [Arindam Paul](https://www.linkedin.com/in/arindam-paul-19a085187/)  ---  ## Reference#  - [Oasis Infobyte](https://oasisinfobyte.com/)"
Color Detection,Data Science,https://github.com/manishsingh7163/Color-Detection,"# Color-Detection  About:  ======= Colour detection is the process of detecting the name of any color. This project can automatically get the name of the color by clicking on them. So for this, we will have a data file that contains the color name and its values.  Then we will calculate the distance from each color and find the shortest one.   1. Color Detection in Real Time Using Webcam ---------------------------------------------  ![](Data/Real%20Time%20Color%20Detection%20gif.gif)  2. Color Detection Of Image --------------------------- ![](Data/Image%20color%20Detection%20gif.gif)  Requirements: -------------- 1. OpenCV    https://docs.opencv.org/ 2. Pandas    https://pandas.pydata.org/ 3. Python    https://www.python.org/ "
Loan Prediction,Data Science,https://github.com/Architectshwet/Loan-prediction-using-Machine-Learning-and-Python,"# Loan-prediction-using-Machine-Learning-and-Python  ## Aim  Our aim from the project is to make use of pandas, matplotlib, & seaborn libraries from python to extract insights from the data and  xgboost, & scikit-learn libraries for machine learning.  Secondly, to learn how to hypertune the parameters using grid search cross validation for the xgboost machine learning model.  And in the end, to predict whether the loan applicant can replay the loan or not using voting ensembling techniques of combining the predictions from multiple machine learning algorithms.   ## Attributes in the dataset  Loan id, Gender, Married, Dependents, Education, Self Employed, Applicant income, Coapplicant income, Loan Amount,Credit History, Property_Area, Loan_Status  ## Major observation from the data  1. Applicants who are male and married tends to have more applicant income whereas applicant who are female and married have least applicant income  2. Applicants who are male and are graduated have more applicant income over the applicants who have not graduated.  3. Again the applicants who are married and graduated have the more applicant income.  4. Applicants who are not self employed have more applicant income than the applicants who are self employed.  5. Applicants who have more dependents have least applicant income whereas applicants which have no dependents have maximum applicant income.  6. Applicants who have property in urban and have credit history have maximum applicant income  7. Applicants who are graduate and have credit history have more applicant income.  8. Loan Amount is linearly dependent on Applicant income  9. From heatmaps, applicant income and loan amount are highly positively correlated.  10. Male applicants are more than female applicants.  11. No of applicants who are married are more than no of applicants who are not married.  12. Applicants with no dependents are maximum.  13. Applicants with graduation are more than applicants whith no graduation.  14. Property area is to be find more in semi urban areas and minimum in rural areas.        "
Road Traffic Prediction,Data Science,https://github.com/atharva-hukkeri/Traffic-Prediction-using-Machine-Learning,"## Traffic-Prediction-using-Machine-Learning A research-based practice project where a model of traffic congestion prediction was constructed by using machine learning classification algorithm - random forest and Support Vector  Regression.<br>  ## About Dataset üìåDate: The Date Column contains the date on which the data were recorded in the format DD/MM/YYYY.<br> üìåDay: The Day Column contains the weekday on which the data was collected. This is done to make the dataset more usable in terms of predicting the likelihood of traffic dependent on what day of the week it is.<br> üìåCoded Day: Each day of the week is assigned a code number by the coded day. Because we are not forced to write string functions for converting the given days to codes, predicting traffic depending on the day is considerably easier. The following are the day codes: -Monday - 1 Tuesday - 2 Wednesday - 3 Thursday - 4 Friday - 5 Saturday - 6 Sunday ‚Äì 7<br> üìåZone: This column contains the zone number for which traffic data is collected. The weather in this column has been coded. This is based on a variety of typical weather conditions. The amount of traffic fluctuates depending on the weather in each zone. This covers factors such as humidity, mist, visibility, and precipitation, among others.<br> üìåTemperature: This column contains the temperature for the given zone on a given day. Temperature has a significant impact on traffic forecasting.<br> üìåTraffic: This is the column that serves as the training dataset as well as a predictor. This column's traffic is coded on a five-level scale. The following are the levels: -1 - Less than 5 cars. 2 - 5 to 15 cars. 3 - 15 to 30 cars. 4 - 30 to 50 cars. 5 - More than 50 cars.<br>  ## Performance Comparison ### üìçAccuracy using Random Forest:<br> Error = 13.42 %<br> Accuracy= 86.58 %<br> ### üìçAccuracy using Support Vector Regression: Error = 12.16 %<br> Accuracy= 87.84 %<br>  ### Reference Used https://github.com/Nupurgopali/Traffic-Prediction-using-SVR-and-RFR"
Income Classification,Data Science,https://github.com/semasuka/Income-classification,"![banner](assets/Income_classification_banner.png)  ![Python version](https://img.shields.io/badge/Python%20version-3.10%2B-lightgrey) ![GitHub last commit](https://img.shields.io/github/last-commit/semasuka/Income-classification) ![GitHub repo size](https://img.shields.io/github/repo-size/semasuka/Income-classification) ![Type of ML](https://img.shields.io/badge/Type%20of%20ML-Binary%20Classification-red) ![Licebse](https://img.shields.io/badge/License-MIT-green) [![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1wKNNC5ZIEXEWMbgiw-knBXLznTRX6xYH) [![Streamlit App](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://share.streamlit.io/semasuka/income-classification/income_class_st.py) [![Open Source Love svg1](https://badges.frapsoft.com/os/v1/open-source.svg?v=103)](https://github.com/ellerbrock/open-source-badges/)  Badge [source](https://shields.io/)  # People with the highest education level, and who are either husbands or wifes make more money   ## Authors  - [@semasuka](https://www.github.com/semasuka)  ## Table of Contents  - [Business problem](#business-problem) - [Data source](#data-source) - [Methods](#methods) - [Tech Stack](#tech-stack) - [Quick glance at the results](#quick-glance-at-the-results) - [Lessons learned and recommendation](#lessons-learned-and-recommendation) - [Limitation and what can be improved](#limitation-and-what-can-be-improved) - [Run Locally](#run-locally) - [Explore the notebook](#explore-the-notebook) - [Deployment on streamlit](#deployment-on-streamlit) - [App deployed on Streamlit](#app-deployed-on-streamlit) - [Repository structure](#repository-structure) - [Contribution](#contribution) - [License](#license)     ## Business problem  This an app to predict if someone make more or less than 50k/year using different features.  This app can be used when that information is not available or is confidential during a loan application at any financial institution or car financing application to have a better financial picture of the applicant. ## Data source  - [Kaggle Income classification](https://www.kaggle.com/lodetomasi1995/income-classification) (Main dataset) - [GDP group dataset](https://www.kaggle.com/nitishabharathi/gdp-per-capita-all-countries) (Dataset used to enriched the main dataset with the countrie's GDP grouping) ## Methods  - Exploratory data analysis - Bivariate analysis - Multivariate correlation - Feature engineering - Feature selection - S3 bucket model hosting - Model deployment ## Tech Stack  - Python (refer to requirement.txt for the packages used in this project) - Streamlit (interface for the model) - AWS S3 (model storage)   ## Quick glance at the results  Most correlated features to the target.  ![heatmap](assets/heatmap.png)  Confusion matrix of random forest (Best estimator with the best parameters)  ![Confusion matrix](assets/confusion_matrix.png)  ROC curve of random forest (Best estimator with the best parameters)  ![ROC curve](assets/roc.png)  Top 5 models after hyper parameter tuning  | Model     	        | Precision score 	| |-------------------	|------------------	| | Random Forest     	| 87% 	            | | Neural Network    	| 81% 	            | | KNN               	| 83% 	            | | Gradient Boosting 	| 90% 	            | | Bagging           	| 87% 	            |  - ***The final model used is: Random forest classifier*** - ***Metrics used: Precision (87%)*** - ***Why choosing random forest yet gradient boosting had 90%, well because Gradient boosting was overfitting*** - ***Why choose precision as metrics: Because a financial instituation would rather get make sure that the people the loan is given do actually make more than 50k. Even though that it means approving fewer applicants. Thus prioritizing precision over recall.***   ## Lessons learned and recommendation  - Based on the analysis on this project, we found out that the education level and type of relationship are the most predictive features to determine if someone makes more or less than 50K. Other features like Capital gain, hours work and age are also usefull. The least usefull features are: their occupation and the workclass they belong to. - Recommendation would be to focus more on the most predictive feature when looking at the applicant profile, and pay less attention on their occupation and workclass. ## Limitation and what can be improved  - Speed: since the model is stored on AWS S3, it can take some few seconds to load. Solution: cache the model with the Streamlit @st.experimental_singleton for faster reload. - Dataset used: the dataset used is from 1990, inflation has not been taken into consideration and the countries's economies have changed since then. Solution: retrain with a more recent dataset. - Hyperparameter tuning: I used RandomeSearchCV to save time but could be improved by couple of % with GridSearchCV.   ## Run Locally Initialize git  ```bash git init ```   Clone the project  ```bash git clone https://github.com/semasuka/Income-classification.git ```  enter the project directory  ```bash cd Income-classification ```  Create a conda virtual environment and install all the packages from the environment.yml (recommended)  ```bash conda env create --prefix <env_name> --file assets/environment.yml ```  Activate the conda environment  ```bash conda activate <env_name> ```  List all the packages installed  ```bash conda list ```  Start the streamlit server locally  ```bash streamlit run income_class_st.py ``` If you are having issue with streamlit, please follow [this tutorial on how to set up streamlit](https://docs.streamlit.io/library/get-started/installation)  ## Explore the notebook  To explore the notebook file [here](https://nbviewer.org/github/semasuka/Income-classification/blob/master/Income_Classification.ipynb)  ## Deployment on streamlit  To deploy this project on streamlit share, follow these steps:  - first, make sure you upload your files on Github, including a requirements.txt file - go to [streamlit share](https://share.streamlit.io/) - login with Github, Google, etc. - click on new app button - select the Github repo name, branch, python file with the streamlit codes - click advanced settings, select python version 3.9 and add the secret keys if your model is stored on AWS or GCP bucket - then save and deploy!  ## App deployed on Streamlit  ![Streamlit GIF](assets/gif_streamlit.gif)  Video to gif [tool](https://ezgif.com/) ## Repository structure   ```  ‚îú‚îÄ‚îÄ assets ‚îÇ   ‚îú‚îÄ‚îÄ confusion_matrix.png        <- confusion matrix image used in the README. ‚îÇ   ‚îú‚îÄ‚îÄ gif_streamlit.gif           <- gif file used in the README. ‚îÇ   ‚îú‚îÄ‚îÄ heatmap.png                 <- heatmap image used in the README. ‚îÇ   ‚îú‚îÄ‚îÄ Income_classification_banner.png   <- banner image used in the README. ‚îÇ   ‚îú‚îÄ‚îÄ environment.yml             <- list of all the dependencies with their versions(for conda environment). ‚îÇ   ‚îú‚îÄ‚îÄ roc.png                     <- ROC image used in the README. ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ datasets ‚îÇ   ‚îú‚îÄ‚îÄ GDP.csv                     <- the data used to feature engineering/enriched the original data. ‚îÇ   ‚îú‚îÄ‚îÄ test.csv                    <- the test data. ‚îÇ   ‚îú‚îÄ‚îÄ train.csv                   <- the train data. ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ pandas_profile_file ‚îÇ   ‚îú‚îÄ‚îÄ income_class_profile.html   <- exported panda profile html file. ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ .gitignore                      <- used to ignore certain folder and files that won't be commit to git. ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ Income_Classification.ipynb     <- main python notebook where all the analysis and modeling are done. ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ LICENSE                         <- license file. ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ income_class_st.py              <- file with the best model and best hyperparameter with streamlit component for rendering the interface. ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ README.md                       <- this readme file. ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ requirements.txt                <- list of all the dependencies with their versions(used for Streamlit ).  ``` ## Contribution  Pull requests are welcome! For major changes, please open an issue first to discuss what you would like to change or contribute.  ## License  MIT License  Copyright (c) 2022 Stern Semasuka  Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.  THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.  Learn more about [MIT](https://choosealicense.com/licenses/mit/) license"
Speech Emotion Recognition,Data Science,https://github.com/MiteshPuthran/Speech-Emotion-Analyzer,"# Speech Emotion Analyzer  * The idea behind creating this project was to build a machine learning model that could detect emotions from the speech we have with each other all the time. Nowadays personalization is something that is needed in all the things we experience everyday.   * So why not have a emotion detector that will guage your emotions and in the future recommend you different things based on your mood.  This can be used by multiple industries to offer different services like marketing company suggesting you to buy products based on your emotions, automotive industry can detect the persons emotions and adjust the speed of autonomous cars as required to avoid any collisions etc.  ## Analyzing audio signals ![](images/joomla_speech_prosody.png?raw=true)  [¬©Fabien_Ringeval_PhD_Thesis](https://drive.google.com/file/d/0B2V_I9XKBODhcEtZV1lRWW1fYTg/view). <br>  ### Datasets: Made use of two different datasets: 1. [RAVDESS](https://zenodo.org/record/1188976). This dataset includes around 1500 audio file input from 24 different actors. 12 male and 12 female where these actors record short audios in 8 different emotions i.e 1 = neutral, 2 = calm, 3 = happy, 4 = sad, 5 = angry, 6 = fearful, 7 = disgust, 8 = surprised.<br> Each audio file is named in such a way that the 7th character is consistent with the different emotions that they represent.  2. [SAVEE](http://kahlan.eps.surrey.ac.uk/savee/Download.html). This dataset contains around 500 audio files recorded by 4 different male actors. The first two characters of the file name correspond to the different emotions that the potray.   ## Audio files: Tested out the audio files by plotting out the waveform and a spectrogram to see the sample audio files.<br> **Waveform** ![](images/wave.png?raw=true) <br> <br> **Spectrogram**<br> ![](images/spec.png?raw=true) <br>  ## Feature Extraction The next step involves extracting the features from the audio files which will help our model learn between these audio files. For feature extraction we make use of the [**LibROSA**](https://librosa.github.io/librosa/) library in python which is one of the libraries used for audio analysis.  <br> ![](images/feature.png?raw=true) <br> * Here there are some things to note. While extracting the features, all the audio files have been timed for 3 seconds to get equal number of features.  * The sampling rate of each file is doubled keeping sampling frequency constant to get more features which will help classify the audio file when the size of dataset is small. <br>  **The extracted features looks as follows**  <br>  ![](images/feature2.png?raw=true)  <br>  These are array of values with lables appended to them.   ## Building Models  Since the project is a classification problem, **Convolution Neural Network** seems the obivious choice. We also built **Multilayer perceptrons** and **Long Short Term Memory** models but they under-performed with very low accuracies which couldn't pass the test while predicting the right emotions.  Building and tuning a model is a very time consuming process. The idea is to always start small without adding too many layers just for the sake of making it complex. After testing out with layers, the model which gave the max validation accuracy against test data was little more than 70% <br> <br> ![](images/cnn.png?raw=true) <br>  ## Predictions  After tuning the model, tested it out by predicting the emotions for the test data. For a model with the given accuracy these are a sample of the actual vs predicted values. <br> <br> ![](images/predict.png?raw=true) <br>  ## Testing out with live voices. In order to test out our model on voices that were completely different than what we have in our training and test data, we recorded our own voices with dfferent emotions and predicted the outcomes. You can see the results below: The audio contained a male voice which said **""This coffee sucks""** in a angry tone. <br> ![](images/livevoice.PNG?raw=true) <br> <br> ![](images/livevoice2.PNG?raw=true) <br>  ### As you can see that the model has predicted the male voice and emotion very accurately in the image above.  ## NOTE: If you are using the model directly and want to decode the output ranging from 0 to 9 then the following list will help you.  0 - female_angry <br> 1 - female_calm <br> 2 - female_fearful <br> 3 - female_happy <br> 4 - female_sad <br> 5 - male_angry <br> 6 - male_calm <br> 7 - male_fearful <br> 8 - male_happy <br> 9 - male_sad <br>  ## Conclusion Building the model was a challenging task as it involved lot of trail and error methods, tuning etc. The model is very well trained to distinguish between male and female voices and it distinguishes with 100% accuracy. The model was tuned to detect emotions with more than 70% accuracy. Accuracy can be increased by including more audio files for training."
Celebrity Voice Prediction,Data Science,https://github.com/ptbailey/Speaker-Recognition,"# Speaker Recognition ### Project Definition:    Identifying which celebrity is speaking through deep neural networks. Applications for this project include machines matching commands with individuals through their voices, such that in the future, the machines can anticipate personal commands.  ### Process: 1) Collected data from [VoxCeleb](http://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox1.html)[1], a database of celebrities' voices and images. 2) Extracted audio features referencing [Aaqib Saeed's code](http://aqibsaeed.github.io/2016-09-03-urban-sound-classification-part-1/)  - **MFCC**: Mel-frequency cepstral coefficients - **Melspectrogram**: Compute a Mel-scaled power spectrogram - **Chorma-stft**: Compute a chromagram from a waveform or power spectrogram. ""In music, the term chroma feature or chromagram closely relates to the twelve different pitch classes. Chroma-based features, which are also referred to as ""pitch class profiles"", are a powerful tool for analyzing music whose pitches can be meaningfully categorized and whose tuning approximates to the equal-tempered scale."" (Wikipedia) - **Spectral Contrast**: Compute spectral contrast. Spectral contrast is defined as the level difference between peaks and valleys in the spectrum - **Tonnetz**: Computes the tonal centroid features (tonnetz) 3) Store features in pandas dataframe and prepare data for modelling 4) Run models  ### Visualization examples of mel and tonnetz features: Miranda Cosgrove's Mel             |  Smokey Robinson's Mel :-------------------------:|:-------------------------: ![](https://github.com/ptbailey/Speaker-Recognition/blob/master/MC%20mel.png)  |  ![](https://github.com/ptbailey/Speaker-Recognition/blob/master/SR%20mel.png)  Miranda Cosgrove's Tonnetz             |  Smokey Robinson's Tonnetz :-------------------------:|:-------------------------: ![](https://github.com/ptbailey/Speaker-Recognition/blob/master/MC%20tonnetz.png) | ![](https://github.com/ptbailey/Speaker-Recognition/blob/master/SR%20tonnetz.png)  ### Best Model: The best model was an 18 layer - CNN model using selu activation function, yielding 0.73 F1-score.      Error and Validation Plots:      ![](https://github.com/ptbailey/Speaker-Recognition/blob/master/Error:Validation%20plot.png)  ROC Curve for all celebrities:     ![](https://github.com/ptbailey/Speaker-Recognition/blob/master/ROC%20curve.png)  ### Demo: Challenged my audience to try and guess the celebrity from an audio clipped I played. I then ran my model to try and determine the person as well. My model won with 3 more correct guesses than the audience.  ![](https://github.com/ptbailey/Speaker-Recognition/blob/master/demo.gif)    Citation:   [1] A. Nagrani, J. S. Chung, A. Zisserman   VoxCeleb: a large-scale speaker identification dataset    INTERSPEECH, 2017"
Store Sales Prediction,Data Science,https://github.com/asim5800/Retail-Sales-Prediction,"# Retail-Sales-Prediction  <p align=""center"">   <img width=""460"" height=""300"" src=""https://upload.wikimedia.org/wikipedia/commons/thumb/b/b2/Ro%C3%9Fmann-Markt_in_Berlin.jpg/1024px-Ro%C3%9Fmann-Markt_in_Berlin.jpg""> </p>  Rossmann operates over 3,000 drug stores in 7 European countries. Currently, Rossmann store managers are tasked with predicting their daily sales for up to six weeks in advance. Store sales are influenced by many factors, including promotions, competition, school and state holidays, seasonality, and locality. With thousands of individual managers predicting sales based on their unique circumstances, the accuracy of results can be quite varied. My work includes various plots and graphs , visualizations , feature engineering , ensemble techniques , different ML algorithms with their respective parameter tuning , analysis and trends . Predictions are of 6 weeks of daily sales for 1,115 stores located across Germany.  In this project, the Kaggle Rossman challenge is being taken on. The goal is to predict the Sales of a given store on a given day. Model performance is evaluated on the root mean absolute percentage error (MAPE).   The dataset consists of two csv files: store.csv and train.csv  Data Files:  train.csv holds info about each store. store.csv holds the sales info per day for each store.  The repo contains main.py that runs the main script from step one until the end.   ## 1. Business Problem. Rossmann operates over 3,000 drug stores in 7 European countries. Currently, Rossmann store managers are tasked with predicting their daily sales for up to six weeks in advance. Store sales are influenced by many factors, including promotions, competition, school and state holidays, seasonality, and locality. With thousands of individual managers predicting sales based on their unique circumstances, the accuracy of results can be quite varied.  ## 2. Solution Strategy My strategy to solve this challenge was:  Step 01: Data Description: Use statistics metrics to identify data distributions.  Step 02: Feature Engineering: Derive new attributes based on the original variables to better describe the phenomenon that will be modeled.  Step 03: Exploratory Data Analysis: Explore the data to find insights and better understand the impact of variables on model learning.  Step 04: Feature Selection: Selection of the most significant attributes for training the model.  Step 05: Machine Learning Modelling: Machine Learning model training.  Step 06: Hyperparameter Fine Tunning: hoose the best values for each of the parameters of the model selected from the previous step.  Step 07: Convert Model Performance to Business Values: Convert the performance of the Machine Learning model into a business result.    ## 3.Machine Learning Model Implementation and performance At this stage models used : *Linear Regression, *Lasso Regression, *Random Forest Regressor  	                                        Training score                Testing score  			Linear Regression	0.780750		       0.782392 			 			Lasso Regression	0.780731		       0.782369 			 			Random Forest    	0.993811             	       0.956433      ## 4. Conclusion  Acheived MAPE of 5.65% and MAE = $376 showing predictions of model is higly accurate for the sales forecast. Generated insights by EDA and feature importance provide valuable tools to decide the amount of budget and inventory for upcoming sales."
Detecting Parkinson's Disease,Data Science,https://github.com/guptaharshnavin/Parkinson-Disease-Detection,# Parkinson üß† Disease ‚öïÔ∏è Detection üîç [![forthebadge made-with-python](http://ForTheBadge.com/images/badges/made-with-python.svg)](https://www.python.org/)<br> This project aims to make use of **Machine Learning** techniques to detect instances of Parkinson's Disease. The project performs the following tasks: <br> 1Ô∏è‚É£ Data Collection <br> 2Ô∏è‚É£ Data Preprocessing <br> 3Ô∏è‚É£ Exploratory Data Analysis <br> 4Ô∏è‚É£ Dataset Balancing & Scaling <br> 5Ô∏è‚É£ Machine Learning Models Training & Evaluation ## Dataset Details **Dataset Used :** Parkinsons Disease Dataset <br> **Dataset Source :** UCI Machine Learning Repository <br> **Dataset Hosting URL :** https://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/parkinsons.data <br> ## Machine Learning Models Trained & Evaluated The following Machine Learning models were trained and evaluated: <br> 1Ô∏è‚É£ Decision Tree Classifier <br> 2Ô∏è‚É£ Random Forest Classifier <br> 3Ô∏è‚É£ Logistic Regression <br> 4Ô∏è‚É£ Support Vector Machine Classifier <br> 5Ô∏è‚É£ Naive Bayes Classifier <br> 6Ô∏è‚É£ K Nearest Neighbor Classifier <br> 7Ô∏è‚É£ XGBoost Classifier <br> ## Best Performing Machine Learning Model **Random Forest Classifier** was found to be the best performing Classifier with: * Accuracy: 0.996102 * F1 Score : 0.961538 * R2 Score : 0.862471
Air Pollution Prediction,Data Science,https://github.com/saniyaparveez/Air_pollution_prediction,"# Air_pollution_prediction Predicting air Quality for Pollution control using ML model   OBJECTIVE :    1. To predict real-time air pollution levels in urban cities.    2. To provide air quality information for the public.   3. To identify and provide different categories and levels of pollution in air  4. To develop more accurate and cost-effective air quality prediction system by using random forest algorithm and AQI.    PROPOSED SYSTEM   As we all know air pollution is one of the major problems in urban cities, where the particulate matter is the most dangerous part of air  pollution which effects human than any other substances. In our project we predict air quality by using Random forest algorithm with air Prediction of air quality for pollution control  quality index provided by the respected government agencies. Here, we make use of feature analysis for prediction process to provide more effective and general model. Our system considers various data taken for number of hours and days from Bangalore, India. This dataset will be helpful for training the system, later we compute real time datasets for predicting the pollution levels in that city, this predicted information will provide crucial information which will be helpful for pollution control and management. "
Age & Gender Detection,Data Science,https://github.com/smahesh29/Gender-and-Age-Detection,"# Gender-and-Age-Detection   <img alt=""GitHub"" src=""https://img.shields.io/github/license/smahesh29/Gender-and-Age-Detection"">   <h2>Objective :</h2> <p>To build a gender and age detector that can approximately guess the gender and age of the person (face) in a picture or through webcam.</p>  <h2>About the Project :</h2> <p>In this Python Project, I had used Deep Learning to accurately identify the gender and age of a person from a single image of a face. I used the models trained by <a href=""https://talhassner.github.io/home/projects/Adience/Adience-data.html"">Tal Hassner and Gil Levi</a>. The predicted gender may be one of ‚ÄòMale‚Äô and ‚ÄòFemale‚Äô, and the predicted age may be one of the following ranges- (0 ‚Äì 2), (4 ‚Äì 6), (8 ‚Äì 12), (15 ‚Äì 20), (25 ‚Äì 32), (38 ‚Äì 43), (48 ‚Äì 53), (60 ‚Äì 100) (8 nodes in the final softmax layer). It is very difficult to accurately guess an exact age from a single image because of factors like makeup, lighting, obstructions, and facial expressions. And so, I made this a classification problem instead of making it one of regression.</p>  <h2>Dataset :</h2> <p>For this python project, I had used the Adience dataset; the dataset is available in the public domain and you can find it <a href=""https://www.kaggle.com/ttungl/adience-benchmark-gender-and-age-classification"">here</a>. This dataset serves as a benchmark for face photos and is inclusive of various real-world imaging conditions like noise, lighting, pose, and appearance. The images have been collected from Flickr albums and distributed under the Creative Commons (CC) license. It has a total of 26,580 photos of 2,284 subjects in eight age ranges (as mentioned above) and is about 1GB in size. The models I used had been trained on this dataset.</p>  <h2>Additional Python Libraries Required :</h2> <ul>   <li>OpenCV</li>           pip install opencv-python </ul> <ul>  <li>argparse</li>           pip install argparse </ul>  <h2>The contents of this Project :</h2> <ul>   <li>opencv_face_detector.pbtxt</li>   <li>opencv_face_detector_uint8.pb</li>   <li>age_deploy.prototxt</li>   <li>age_net.caffemodel</li>   <li>gender_deploy.prototxt</li>   <li>gender_net.caffemodel</li>   <li>a few pictures to try the project on</li>   <li>detect.py</li>  </ul>  <p>For face detection, we have a .pb file- this is a protobuf file (protocol buffer); it holds the graph definition and the trained weights of the model. We can use this to run the trained model. And while a .pb file holds the protobuf in binary format, one with the .pbtxt extension holds it in text format. These are TensorFlow files. For age and gender, the .prototxt files describe the network configuration and the .caffemodel file defines the internal states of the parameters of the layers.</p>    <h2>Usage :</h2>  <ul>   <li>Download my Repository</li>   <li>Open your Command Prompt or Terminal and change directory to the folder where all the files are present.</li>   <li><b>Detecting Gender and Age of face in Image</b> Use Command :</li>          python detect.py --image <image_name> </ul>   <p><b>Note: </b>The Image should be present in same folder where all the files are present</p>  <ul>   <li><b>Detecting Gender and Age of face through webcam</b> Use Command :</li>          python detect.py </ul> <ul>   <li>Press <b>Ctrl + C</b> to stop the program execution.</li> </ul>  # Working: [![Watch the video](https://img.youtube.com/vi/ReeccRD21EU/0.jpg)](https://youtu.be/ReeccRD21EU)  <h2>Examples :</h2> <p><b>NOTE:- I downloaded the images from Google,if you have any query or problem i can remove them, i just used it for Educational purpose.</b></p>      >python detect.py --image girl1.jpg     Gender: Female     Age: 25-32 years      <img src=""Example/Detecting age and gender girl1.png"">      >python detect.py --image girl2.jpg     Gender: Female     Age: 8-12 years      <img src=""Example/Detecting age and gender girl2.png"">      >python detect.py --image kid1.jpg     Gender: Male     Age: 4-6 years          <img src=""Example/Detecting age and gender kid1.png"">      >python detect.py --image kid2.jpg     Gender: Female     Age: 4-6 years        <img src=""Example/Detecting age and gender kid2.png"">      >python detect.py --image man1.jpg     Gender: Male     Age: 38-43 years      <img src=""Example/Detecting age and gender man1.png"">      >python detect.py --image man2.jpg     Gender: Male     Age: 25-32 years      <img src=""Example/Detecting age and gender man2.png"">      >python detect.py --image woman1.jpg     Gender: Female     Age: 38-43 years      <img src=""Example/Detecting age and gender woman1.png"">               "
Optimizing Product Price,Data Science,https://github.com/sukesh-reddy/Retail-Price-Optimization-,"# Retail-Price-Optimization- In this machine learning pricing project, we implement a retail price optimization algorithm using regression trees. This is one of the first steps to building a dynamic pricing model."
Breast Cancer Prediction,Data Science,https://github.com/Eakta08/Breast-Cancer-Prediction,"   # Breast-Cancer-Prediction ![image](https://github.com/Eakta08/Breast-Cancer-Prediction/assets/131867852/76c4fef8-71c7-496f-9676-983f09b5ae44)  This project is a machine learning and deep learning based breast cancer prediction system that uses a Breast Cancer Wisconsin dataset containing various features related to breast cancer patients.The primary goal is to predict whether a given breast cancer tumor is malignant or benign, which can aid in early diagnosis and medical decision-making. It involved exploring, preprocessing, and modeling data. Multiple algorithms were compared based on key metrics like accuracy, F1 score, recall, and precision. The aim was to identify the best-performing model for accurate breast cancer prediction.  **Project Structure:**  - Importing Libraries: *Essential Python libraries are imported for data analysis and machine learning.* - Data Reading and Exploratory Data Analysis: *The dataset is loaded and explored to understand its characteristics.* - Data Preprocessing: *Data is preprocessed, including splitting into training and testing sets and scaling.* - Model Development: *Multiple machine learning algorithms are applied and trained on the dataset.* - Model Comparison: *The models' performances are compared using evaluation metrics and visualization.* - Predictive Analysis: *Finally, the models are used to predict breast cancer occurrences.*  **About the Dataset:** The project uses the Breast Cancer Wisconsin (Diagnostic) dataset, which is publicly available and commonly used in machine learning for cancer diagnosis tasks. The dataset comprises various numerical features that assist in the classification of breast masses as either benign or malignant. "
Analyze COVID Vaccination Progress Using Python,Data Science,https://github.com/Ahad-Al-Seraihi/COVID-19-World-Vaccination-Progress,"# Capstone Project: COVID-19 World Vaccination Progress  ## About  I performed basic exploratory data analysis (EDA) using Pandas, NumPy and Matplotlib libraries on a Kaggle dataset looking at COVID-19 world vaccination progress in the period (12/2020 - 04/2021). This included data cleaning, wrangling and visualization to answer some hypotheses based on the data being explored.  ##  Install  This project required Python 3.x and the following Python libraries installed:  - Pandas - NumPy - Matplotlib  ##  Data Dictionary   The dictionary from this dataset (country_vaccinations.csv) was obtained from: https://www.kaggle.com/gpreda/covid-world-vaccination-progress  1. Country: this is the country for which the vaccination information is provided; 2. ISO_code: ISO code for the country. 3. Date: date for the data entry; for some of the dates we have only the daily vaccinations, for others, only the (cumulative) total. 4. Total_vaccinations: this is the absolute number of total immunizations in the country. 5. People_vaccinated: a person, depending on the immunization scheme, will receive one or more (typically 2) vaccines; at a certain moment, the number of vaccination might be larger than the number of people. 6. People_fully_vaccinated: this is the number of people that received the entire set of immunization according to the immunization scheme (typically 2); at a certain moment in time, there might be a certain number of people that received one vaccine and another number (smaller) of people that received all vaccines in the scheme. 7. Daily_vaccinations_raw: for a certain data entry, the number of vaccination for that date/country. 8. Daily_vaccinations: for a certain data entry, the number of vaccination for that date/country. 9. Total_vaccinations_per_hundred: ratio (in percent) between vaccination number and total population up to the date in the country. 10. People_vaccinated_per_hundred: ratio (in percent) between population immunized and total population up to the date in the country. 11. People_fully_vaccinated_per_hundred: ratio (in percent) between population fully immunized and total population up to the date in the country. 12. Daily_vaccinations_per_million: ratio (in ppm) between vaccination number and total population for the current date in the country. 13. Vaccines: total number of vaccines used in the country (up to date). 14. Source_name: source of the information (national authority, international organization, local organization etc.). 15. Source_website: website of the source of information"
Financial Status Analysis For Credit Score Rating,Data Science,https://github.com/allanphil/Credit-Score-Classification,"# Credit-Score-Classification-With-Machine-Learning Welcome to my Credit Score Classification project! In this project, I demonstrate my expertise in building and evaluating machine learning models. The goal of this project is to predict the credit score of an individual with high accuracy, based on their financial history and personal information.  The project is implemented in a Jupyter Notebook, using the Python programming language and several popular machine learning libraries such as NumPy, Pandas, Matplotlib, and Seaborn. The data used in this project is a simulated dataset of individuals and their credit scores, which I use to train and evaluate several different machine learning models.  I start by performing exploratory data analysis to gain insights into the data and understand the relationships between the various features and the credit score. I then use this understanding to preprocess the data and prepare it for modeling.  Next, I train and evaluate multiple machine learning models including Random Forest. I use various evaluation metrics such as accuracy, precision, recall, and F1-score to determine the performance of each model and select the best model based on the results.  Finally, I use the selected model to make predictions on new, unseen data and evaluate its performance on this data. I also visualize the results to gain additional insights into the performance of the model.  This project showcases my ability to work with real-world data, perform exploratory data analysis, build and evaluate machine learning models, and make informed decisions based on the results. I believe this project will impress hiring managers and demonstrate my skills in the field of machine learning and data science.  Thank you for your interest in this project. I hope you find it informative and impressive. If you have any questions or comments, please do not hesitate to reach out!"
Movie Based Recommendation,Data Science,https://github.com/rudrajikadra/Movie-Recommendation-System-Using-Python-and-Pandas,# Movie-Recommendation-System-Using-Python  In this python project where using Pandas library we will find correlation and created basic Movie Recommender System with Python.   It is an extension from the project: https://github.com/krishnaik06/Movie-Recommender-in-python  The Dataset used is a subset of MovieLens Dataset.  ### Extension Have created a text input bar to add your movie whose recommendation you want. Output will give you top 4 matches that are recommended movies.  ### Results ![Screen Shot 2020-06-06 at 9 36 16 PM](https://user-images.githubusercontent.com/15246084/83949017-fdefa080-a83e-11ea-9b21-9c278a8dea45.png) ![Screen Shot 2020-06-06 at 9 36 41 PM](https://user-images.githubusercontent.com/15246084/83949019-ffb96400-a83e-11ea-9607-3d1dbf5c3769.png)
Real-time Traffic Prediction,Data Science,https://github.com/ghimohammadr/Real-time-traffic-prediction,"# Real-time traffic prediction  The code is a Python script designed for real-time series forecasting. It trains the model on a portion of the data, performs rolling window on the remaining data points to update the model without the need to update from scratch, and evaluates the model's performance using metrics like Mean Squared Error (MSE).  # requirements ``` pandas==1.3.3 numpy==1.21.2 scikit-learn==0.24.2 matplotlib==3.4.3 tensorflow==2.6.0   keras==2.6.0 ```"
Image Captioning,Data Science,https://github.com/IEEE-NITK/Image_Captioning,"# **Image Captioning Using Attention Mechanism**  ## **MENTORS**  - Chandan Kumar - Rohan Rao H J  ## **MENTEES**  1. Aakaash G.Acharya 2. Apurva S 3. Deepanshu Gupta 4. Inbasekaran.P 5. K A Gaganashree  ### **ABSTRACT**  In this project, we systematically analyze a deep neural networks based image caption generation method. With an image as the input, the method can output an English sentence describing the content in the image.We analyze three components of the method: Convolution Neural Network (CNN), Recurrent Neural Network (RNN) and sentence generation. By replacing the CNN part with three state-of-the-art architectures, we find the Inception v3 performs best according to the BLEU (Bilingual Evaluation Understudy) score. We also propose a simplified version the Gated Recurrent Units (GRU) as a new recurrent layer. The simplified GRU achieves comparable result when it is compared with the long short-term memory (LSTM) method. But it has few parameters which saves memory and is faster in training. Finally, we generate multiple sentences using Greedy Search. The experiments show that the modified method can generate captions comparable to the-state-of-the-art methods with less training memory  ### **INTRODUCTION**  Automatically describing the content of images using natural languages is a fundamental and challenging task. It has great potential impact. For example, it could help visually impaired people better understand the content of images on the web. Also,it could provide more accurate and compact information of images/videos in scenarios such as image sharing in social network or video surveillance systems. This project accomplishes this task using deep neural networks. By learning knowledge from image and caption pairs, the method can generate image captions that are usually semantically descriptive and grammatically correct. The idea is mapping the image and captions to the same space and learning a mapping from the image to the sentences.  The GRU is like a long short-term memory (LSTM) with a forget gate, but has fewer parameters than LSTM, as it lacks an output gate. GRU's performance on certain tasks of polyphonic music modeling, speech signal modeling and natural language processing was found to be similar to that of LSTM. GRUs have been shown to exhibit better performance on certain smaller and less frequent datasets. Although all the mappings are learned in an end to-end framework, we believe the benefits of better understanding of the system by analyzing different components separately.  Attention models, or attention mechanisms, are input processing techniques for neural networks that allows the network to focus on specific aspects of a complex input, one at a time until the entire dataset is categorized. The goal is to break down complicated tasks into smaller areas of attention that are processed sequentially. Similar to how the human mind solves a new problem by dividing it into simpler tasks and solving them one by one. The model has three components. The first component is a CNN which is used to understand the content of the image. Image understanding answers the typical questions in computer vision such as ‚ÄúWhat are the objects?‚Äù, ‚ÄúWhere are the objects?‚Äù and ‚ÄúHow are the objects interactive?‚Äù.The second component is a RNN which is used to generate a sentence given the visual feature.The third component is used to generate a sentence by exploring the combination of the probabilities. So major objective about the project is to learn CNN concepts and LSTM model and build a working model of Image caption generator by implementing CNN with LSTM.  ### **CONVOLUTIONAL NEURAL NETWORK AND TRANSFER LEARNING**  In this project, Convolution Neural Network (CNN) acts as an encoder. It extracts features from the input image.Every image is reshaped and converted into a vector of fixed size when it is given as an input to the CNN model. Some of the commonly used layers in CNN are convo layer, fully connected layer, pooling layer, dropout layer etc.Two important components of CNN are Rectified Linear Unit and Dropout layers. ReLU is used as activation function to bring about non linearity [(ReLU) f(x) = max(0, x)]. Dropout randomly nullifies the contribution of some neurons of a layer. So use of Dropout layers helps in preventing over fitting of training data.The last hidden state of the CNN is connected to the decoder.  #### **A. Why Transfer Learning?**  We can either train CNN from scratch or make use of a pre trained model, trained on a larger dataset and its weights. The latter method is called as transfer learning. Studies have shown that image captioning models trained using the second method perform better. Transfer learning also helps in training the model faster. So in this project we make use of Inception V3. Inception-v3 is a convolutional neural network trained on ImageNet dataset. It consists of 48 layers. Images of size 299 by 299 are fed as input into this network. Shape of output of last layer is 8 by 8 by 2048. We use the last convolutional layer because we are using attention in this project.  #### **B. Inception V3 architecture**  1. Factorized Convolutions: reduce the number of parameters involved in the network. This helps in reducing the computational efficiency. This also keeps a check on the network efficiency. 2. Smaller convolutions: replacing bigger convolutions with smaller convolutions results in faster training. 3. Asymmetric convolutions: A 3 √ó 3 convolution could be replaced by a 1 √ó 3 convolution followed by a 3 √ó 1 convolution. 4. Auxiliary classifier: An auxiliary classifier is a small CNN inserted between layers during training, and the loss incurred is added to the main network loss. In Inception v3 an auxiliary classifier acts as a regularizer. 5. Grid size reduction: Grid size reduction is usually done by pooling operations.  <p align=""center"">   <img src=""./Images/Inception1.jpg"" alt=""Inception1"" style=""width:400px;""/> </p>  **Final Architecture** All the above concepts are consolidated into the final architecture.  <p align=""center"">   <img src=""./Images/download.png"" alt=""Inception"" style=""width:400px;""/> </p>  #### **C. Why Inception V3?**  Inception v3 focuses on burning less computational power.Compared to VGGNet, Inception Networks have proved to be more computationally efficient, in terms of the number of parameters generated and the economical cost incurred (regarding memory and other resources).  ### **SEQUENTIAL MODELS: RNN'S AND GRU**  Traditional feed-forward neural networks take in a fixed amount of input data all at the same time and produce a fixed amount of output each time. On the other hand, RNNs do not consume all the input data at once. Instead, they take them in one at a time and in a sequence. At each step, the RNN does a series of calculations before producing an output. The output, known as the hidden state, is then combined with the next input in the sequence to produce another output. A Gated Recurrent Unit (GRU), as its name suggests, is a variant of the RNN architecture, and uses gating mechanisms to control and manage the flow of information between cells in the neural network. GRUs were introduced only in 2014 by Cho, et al. The structure of the GRU allows it to adaptively capture dependencies from large sequences of data without discarding information from earlier parts of the sequence. This is achieved through its gating units, which solve the vanishing/exploding gradient problem of traditional RNNs.  <p align=""center"">   <img src=""./Images/formulas.jpg"" alt=""formulas"" style=""width:400px;""/> </p>  <p align=""center"">   <img src=""./Images/gru.jpg"" alt=""gru"" style=""width:400px;""/> </p>  ### **ATTENTION MECHANISM:**  The standard seq2seq model is generally unable to accurately process long input sequences, since only the last hidden state of the encoder RNN is used as the context vector for the decoder. On the other hand, the Attention Mechanism directly addresses this issue as it retains and utilises all the hidden states of the input sequence during the decoding process. It does this by creating a unique mapping between each time step of the decoder output to all the encoder hidden states. This means that for each output that the decoder makes, it has access to the entire input sequence and can selectively pick out specific elements from that sequence to produce the output.  #### **A.Bahadanau's Attention Mechanism**  Bahdanau‚Äôs Attention, commonly referred to as Additive Attention, came from a paper by Dzmitry Bahdanau. The model aimed at improving the sequence-to-sequence model in machine translation by aligning the decoder with the relevant input sentences and thereby implementing Attention.  <p align=""center"">   <img src=""./Images/decoder.JPG"" alt=""decoder"" style=""width:400px;""/> </p>  #### **B.Steps involved in Attention Mechanism**  The entire step-by-step process involved in attention model is as follows: 1.Producing the Encoder Hidden States - Encoder produces hidden states of each element in the input sequence 2.Calculating Alignment Scores between the previous decoder hidden state and each of the encoder‚Äôs hidden states are calculated (Note: The last encoder hidden state can be used as the first hidden state in the decoder) 3.Softmaxing the Alignment Scores - the alignment scores for each encoder hidden state are combined and represented in a single vector and subsequently softmaxed 4.Calculating the Context Vector - the encoder hidden states and their respective alignment scores are multiplied to form the context vector 5.Decoding the Output - the context vector is concatenated with the previous decoder output and fed into the Decoder RNN for that time step along with the previous decoder hidden state to produce a new output 6.The process (steps 2-5) repeats itself for each time step of the decoder until an token is produced or output is past the specified maximum length  <p align=""center"">   <img src=""./Images/attention2.png"" alt=""attention2"" style=""width:400px;""/> </p>  ### **EXPERIMENTS**  In this section, we evaluate the proposed network architectures on a real image captioning dataset.  #### **A.Dataset**  There are many datasets for image captioning. We planned to use the Common Objects in Context (COCO) dataset from Microsoft, however, due to computational and algorithmic limitations, we decided to limit the scope of our project to 1)Flickr8k The dataset consists of about 8K images bifurcated as follows: **Training Set** - 6000 images **Dev Set** - 1000 images **Test Set**- 1000 images Each image contains 5 sentences of around 10-20 words.  #### **B.Metrics**  The objective of Machine learning is to generate a new sequence of most probable caption of an image based on the available corpus words of testing dataset. Thus, we need a way to quantify the average accuracy of the system on the whole dataset. There are several metrics by which to judge the quality of machine-produced text and none without criticism.\par We chose to use the Bilingual Evaluation Understudy (BLEU) metric, as it is one of the simplest and best known.BLEU is an algorithm for evaluating the quality of text which has been machine-translated from one natural language to another. Before describing the BLEU score, we will describe a simpler and better known metric, called the precision. Let x be a vector of machine-produced n-grams, and let y be a vector of ground truth n-grams. For example, x could be the words in a sentence describing a movie frame,with xi representing an individual word, and y could be words from scripts describing the same scene. We often wish to have a ground truth y representing the many possible linguistic translations of the same idea.The precision is {x belongs to y}  <p align=""center"">   <img src=""./Images/for.jpg"" alt=""for"" style=""width:150px;""/> </p>  The BLEU score is similar to the precision, except that each occurrence of an n-gram in y can account for only one occurrence in x. For example, the statement The the the the the the would receive a perfect precision if the reference translation contained the word the,but not necessarily a perfect BLEU score, as we are limited to counting only as many occurrences of the as appear in y. An individual N-gram score is the evaluation of just matching grams of a specific order, such as single words (1-gram) or word pairs (2-gram or bigram). The weights are specified as a tuple where each index refers to the gram order. To calculate the BLEU score only for 1-gram matches, you can specify a weight of 1 for 1-gram and 0 for 2, 3 and 4 (1, 0, 0, 0).  _Note:_ BLEU 1 : Unigram BLEU Score BLEU 2 : Bigram BLEU Score BLEU 3 : Trigram BLEU Score  #### **C.Results**  - The results in our experiments so far are encouraging .We trained the basic decoder model with one layer of GRU, a hidden state of length 512, and a word embedding of length 256.The maximum length of the generated sequence was 38 words  - Image features of length 2048 were extracted using a pretrained Inception v3 model.To optimize the GRU model parameters, we used the Adam descent method in batches of 100 pairs of images and captions.  - To make the classification problem feasible, the vocabulary was restricted to the 2,000 most common words from the 8K dataset. All other words were replaced with a special unknown token < unk >.  - All tokens less than 38 words in length were padded with a special unknown token, signifying that no more words are needed to describe the image. Note that the 38-word limit includes < startofseq > and < endofseq > tokens enclosing each caption.  - After thesWhile many captions are informative, some describe a scene completely different than the one in the image, while others are complete gibberish. It should be noted that most gibberish captions make use of the < unk > token. e modifications, the model was trained for 30 epochs, on 8K dataset.We could achieve an accuracy of 98.16\% on train set and an accuracy of 97.8\% on test set. Refer Fig 5 for the Loss plot  <p align=""center"">   <img src=""./Images/loss_epoch.jpg"" alt=""loss_epoch"" style=""width:400px;""/> </p>  - While many captions are informative, some describe a scene completely different than the one in the image, while others are complete gibberish. It should be noted that most gibberish captions make use of the < unk > token.  - We used a pretrained model for Inception v3 (CNN) with ImageNet weights and implemented the Decoder in TensorFlow.  - The qualitative results of this model are quite interesting. For many images, the model generates informative and relevant captions. To qualitatively assess the accuracy of our model, we report the world-level BLEU score, as discussed in section V.B on our validation set. All start, end, null tokens are stripped from both the predicted and ground truth captions prior to computing BLEU. Note that we exclude the unknown tokens when computing BLEU.  - The next page contains some captions generated by our neural network  <p align=""center"">   <img src=""./Images/Result.jpg"" alt=""Result"" style=""width:800px;""/> </p>  Finally, we show some examples of computer generated captions along with blue scores on the test dataset.  <p align=""center"">   <img src=""./Images/Picture1.png"" alt=""Picture1"" style=""width:250px;""/> </p>  Prediction Caption: four basketball player in green Real Captions:  1. player from the white and green high school team dribble down court defended by player from the other team 2. four basketball player in action 3. four men playing basketball two from each 4. two boy in green and white uniform play basketball with two boy in blue and white uniform 5. young men playing basketball in competition    BLEU Score 1-> 100.00    BLEU Score 2-> 100.00    BLEU Score 3-> 87.48  <p align=""center"">   <img src=""./Images/Picture2.png"" alt=""Picture2"" style=""width:250px;""/> </p>  Prediction Caption: boy jump over te trampoline Real Cptions:  1. boy jumping over another boy on trampoline 2. boy jump over another boy on trampoline 3. one boy jump over another boy on trampoline 4. the little boy jump over another little boy on the trampoline 5. two boy jumping on trampoline    BLEU Score 1-> 100.00    BLEU Score 2-> 86.60    BLEU Score 3-> 63.29  The above two captions are examples of very good captions, which very relevant to the situation in the image.But not all captions are relevant like these examples.  <p align=""center"">   <img src=""./Images/Picture3.png"" alt=""Picture3"" style=""width:250px;""/> </p>  Prediction Caption: dog jumping towards the side Real Captions:  1. black and tan dog roll on his back in field 2. black dog rolling in the green grass 3. dog roll on his back 4. dog roll on his back in the grass 5. dog with paw in the air lying on grass    BLEU Score 1-> 40.00    BLEU Score 2-> 0.00    BLEU Score 3-> 0.00  <p align=""center"">   <img src=""./Images/Picture4.png"" alt=""Picture4"" style=""width:250px;""/> </p>  Prediction Caption: three red **<unk>** attire dive and near Real Captions:  1. three person sky dive team in the air performing stun 2. three people wearing parachute are free falling together through the sky 3. three skydiver are in formation above the cloud 4. trio of skydiver holding hand in sky 5. two people are parachuting over white cloud    BLEU Score 1-> 28.22    BLEU Score 2-> 0.00    BLEU Score 3-> 0.00  In the above case the model is struggling to predict the captions, from Fig6 we can tell that the attention weights are not correct. This can be improved by training on a larger data set.  <p align=""center"">   <img src=""./Images/Picture5.png"" alt=""Picture5"" style=""width:650px;""/> </p>  ### **CONCLUSION**  We have developed a neural network architecture for image captioning and reported its accuracy on the Flickr dataset. We have also extended the architecture to use attention models, and showed some initial results from this investigation. The current experiments are encouraging, giving a reasonably good BLEU score with little training.  ### **ACKNOWLEDGEMENT**  The authors would like to thank IEEE for conducting envision project and Mentors for their support,friendly advice and inspiration to do this work.  ### **REFERENCES**  1. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens , and Zbigniew Wojna. Rethinking the Inception Architecture for Computer Vision. 2015.  2. Kyunghyun Cho, Bart Van Merrienboer,Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares,Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation.  3. [Gabriel Loye, Attention Mechanism. 2015.](https://blog.floydhub.com/)  4. [Understanding LSTM Models. 2015.](https://colah.github.io/)  5. [Andrej Karpathy, The Unreasonable Effectiveness of Recurrent Neural Networks. 2015.](http://karpathy.github.io/)  6. [Andrej Karpathy. 2015. Cs231n: Convolutional neural networks for visual recognition.](http://cs231n.github.io/.)  7. [Harshall Lamba. Image Captioning with Keras. 2018.](https://towardsdatascience.com/.)  8. [Tensorflow. Image captioning with visual attention.](https://www.tensorflow.org/overview.)  9. [Bharath Raj. A Simple Guide to the Versions of the Inception Network. 2018.](https://towardsdatascience.com/>)  10. [Subham Sarkar. Image Captioning using Attention Mechanism. 2020.](https://github.com/SubhamIO/)  11. [JournalDev. How to calculate BLEU score using Phython. 2020.](https://www.journaldev.com/)  12. Andrea Galassi , Marco Lippi , and Paolo Torroni.Attention in Natural Language Processing. 2020.  13. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. 2017.Attention Is All You Need.**arXiv preprint arXiv:1706.03762**  14. Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio. 2016.Neural Machine Translation by Jointly Learning to Align and Translate.**arXiv preprint arXiv:1409.0473**  15. Kishore Papineni, Salim Roukos,Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. Proceedings of the 40th annual meeting on association for computational linguistics 2002 , pages 311‚Äì318  16. Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus, and Yann LeCun. 2013. Overfeat: Integrated recognition, localization and detection using convolutional networks.**arXiv preprint arXiv:1312.6229**  17. Simonyan and Zisserman2014 Karen Simonyan and Andrew Zisserman. 2014. Very deep convolutional networks for large-scale image recognition.**arXiv preprint arXiv:1409.1556.**  18. [Harshall Lamba. 2019.A Guide to build Sequence to sequence models using LSTM.](https://towardsdatascience.com/.)  19. Szegedy et al.2015 Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. 2015. Going deeper with convolutions.  20. Karen Simonyan, Andrew Zisserman. 2014. Very Deep Convolutional Networks for Large-Scale Image Recognition.**arXiv preprint arXiv:1409.1556**  21. Justin Johnson, Andrej Karpathy, Li Fei-Fei. 2015. DenseCap: Fully Convolutional Localization Networks for Dense Captioning.**arXiv preprint arXiv:1511.07571.**  22. Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan. 2015. Show and Tell: A Neural Image Caption Generator.**arXiv preprint arXiv:1411.4555.**  23. Russakovsky et al.2015 Olga Russakovsky, Jia Deng,Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla,Michael Bernstein, et al. 2015. Imagenet large scale visual recognition challenge. IJCV, 115(3):211‚Äì252.  24. [Vihar Kurama. 2020. A Review of Popular Deep Learning Architectures: ResNet, InceptionV3, and SqueezeNet](https://blog.paperspace.com/tag/deep-learning/.)  25. [Blaine Rister,Dieterich Lawson. 2016. Image Captioning with Attention](http://cs231n.stanford.edu/reports/2016/pdfs.)"
DSA Question Predicter,Data Science,https://github.com/dprasuna/Web-Based-Dsa-Tracker,"# DSA Practice Tracker  Welcome to the DSA Practice Tracker! This project aims to help individuals prepare for technical interviews at top-tier tech companies by providing a comprehensive platform to practice Data Structures and Algorithms (DSA) questions from various levels and platforms.  ## Introduction This web-based tracker was inspired by Striver's A2Z sheet, aiming to consolidate practice questions from platforms like Geeks for Geeks (GFG) and LeetCode. It offers a curated set of questions categorized by difficulty levels (easy, medium, hard) to assist users in honing their problem-solving skills.  ## Features - **DSA Question Bank**: Access a wide range of DSA questions from GFG and LeetCode. - **Categorized Difficulty Levels**: Questions are segregated into easy, medium, and hard categories. - **Platform Flexibility**: Practice questions from multiple platforms within a single interface. - **Progress Tracking**: Keep track of your solved and pending questions for better preparation. - **Search Functionality**: Easily find specific questions using search filters. - **User-Friendly Interface**: Intuitive design for seamless navigation and usage.  ## Getting Started To get started with the DSA Practice Tracker: 1. Clone the repository to your local machine. 2. Set up the necessary environment and dependencies as specified in the `Installation` section. 3. Launch the application and start practicing DSA questions immediately.  ## Installation Ensure you have the following prerequisites installed: - [Node.js](https://nodejs.org/)  - [npm](https://www.npmjs.com/) or [Yarn](https://yarnpkg.com/)  Steps to install: 1. Clone the repository: 2. Navigate to the project directory: 3. Install dependencies: or 4. Start the application: 5. Access the application at `http://localhost:3000` in your web browser.  ## Contributing Contributions are welcome! If you'd like to contribute to this project, please follow these steps: 1. Fork the repository. 2. Create a new branch (`git checkout -b feature/awesome-feature`). 3. Make your changes and commit them (`git commit -am 'Add awesome feature'`). 4. Push to the branch (`git push origin feature/awesome-feature`). 5. Create a pull request detailing your changes.  ## License This project is licensed under the [MIT License](LICENSE).  ## Acknowledgements - [Striver's A2Z sheet](link-to-strivers-sheet) for inspiration and guidance. - Platforms like Geeks for Geeks and LeetCode for providing valuable practice questions.  ## Contact For any inquiries or feedback, feel free to reach out at [prasunadash2005@gmail.com](mailto:prasunadash2005@gmail.com).  Happy coding and good luck with your DSA preparation! üöÄ"
IoT Social Distancing & Monitoring for Queue,Internet of Things,https://github.com/sassoftware/iot-tracking-social-distancing-computer-vision,"# Tracking Social Distancing Using Computer Vision  <img src=""https://img.shields.io/badge/Category-Computer Vision-blue""> <img src=""https://img.shields.io/badge/Sub--Category-Object Detection-yellowgreen""> <img src=""https://img.shields.io/badge/Difficulty-Intermediate-yellow""> <img src=""https://img.shields.io/badge/Analytical%20Method-YOLO-brightgreen""> <img src=""https://img.shields.io/badge/Updated-May 2020-orange"">  ## Overview  Due to the current Corona crisis, several governments have decided to implement restrictions for social distance. While most people follow these guidelines, there are still people who ignore them for various reasons.  The overall goal of this project is to identify objects (i.e., people) who are following social distancing guidelines and those who are not. This is accomplished by way of the following:  *  Transform images and object coordinates into a two-dimensional map if a homography matrix was provided *  Calculate real world distances between detected objects if a homography matrix was provided. Otherwise use distances from the image directly. *  Detect crowds that exceed a specified parameter *  Visualize the results on the camera image  ### Object Detection  The first step in the process is to detect people in images. A Tiny Yolo V2 model was used for this task, but any detection model would work. The decision to use Tiny Yolo V2 was mainly driven because of its ease to use in SAS. The training process for object detection is included in this Jupyter Notebook.  ### Transforming the Image and Calculating Distance  The second step is calculate the distances between all objects. While this may sound simple, it is actually more complicated if you think about it. Normally a camera does not provide a top view. Instead it is at some angle which leads to certain perspective. This perspective can be very important when you calculate real world distances in images. The following three pictures illustrate the transformation process.  ![](images/1_qp7Qkse7hgUZ0un0LixdYg.png)  The second image shows the results after transforming the image using a Homography Matrix. A rectangle was able to be formed, but the distances are incorrect due to the camera's perspective. The third image show the results after a transformation matrix was applied. A distance of 356 pixels on the image shrinks to 149 pixels while a distance of 62 pixels in the other direction are 83 pixels in reality. For more information, refer to the [Concepts of Homography](https://docs.opencv.org/master/d9/dab/tutorial_homography.html).  ### Identifying Crowds  Now that we have detected people and transformed the distances between them we want to determine whether we have crowds in our image. In this demonstration, KD-Trees from Scipy was used to look up  nearest neighbors given a detected person and a maximum radius. The following functions were used:  * [cKDTree](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.cKDTree.html) to efficiently calculate nearest neighbors. * [query_ball_tree](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.cKDTree.query_ball_tree.html#scipy.spatial.cKDTree.query_ball_tree) to query the KDTree with a person and a given maxium radius.  ### Streaming Process  The streaming process uses [SAS Event Stream Processing](https://www.sas.com/en_us/software/event-stream-processing.html) (ESP) and can connect to any video data source. ESP lets you define the process either graphically or programmatically via a [Python Interface](https://github.com/sassoftware/python-esppy).  The following diagram illustrates the streaming process:  ![](images/1_WFBzWNoZTayNF9F7RrgWGg.png)  The top two boxes load the trained Tiny YOLO V2 model and provide the model to the scoring window. The scoring window receives images that are resized to appropriate dimensions (416x416 pixels in this case). The scoring window provides the detected persons and their corresponding x, y, width, and height values.  The last box utilizes the Python inside SAS Event Stream Processing to transform the coordinates given the homography matrix. Additionally it uses Scipy to perform crowd detection.  The final result looks as follows:  [![Social Distancing with Computer Vision](https://img.youtube.com/vi/HnE9gC_ui4E/0.jpg)](https://www.youtube.com/watch?v=HnE9gC_ui4E)  ### Code Examples  #### Training  The training portion of the project is included in Jupyter Notebook [social_distancing_yolo_training.ipynb](files/training/social_distancing_yolo_training.ipynb). It creates a Tiny YOLO V2 model named [Tiny-Yolov2.astore](files/training/Tiny-Yolov2.astore).  #### Scoring  The scoring portion of the project is included in Jupyter Notebook [social_distancing.ipynb](files/training/social_distancing.ipynb). It utilizes two input files:  * [Tiny-Yolov2.astore](files/training/Tiny-Yolov2.astore) * [social_distancing.py](files/scoring/social_distancing.py)  ### Prerequisites  * [SAS Event Stream Processing 6.2](https://go.documentation.sas.com/?cdcId=espcdc&cdcVersion=6.2&docsetId=espov&docsetTarget=home.htm&locale=en) * [ESPPy](https://github.com/sassoftware/python-esppy) * [JupyterLab](https://jupyter.org/)   ### Reference Files  The training portion of the project requires training images and the scoring protion requires videos as input. They can be found at this [GitHub](https://github.com/Mentos05/SAS_DeepLearning/tree/master/Social%20Distancing%20Demo) site.    ### Examples  [![Social Distancing with Computer Vision](https://img.youtube.com/vi/HnE9gC_ui4E/0.jpg)](https://www.youtube.com/watch?v=HnE9gC_ui4E)  ## Contributing  > We welcome your contributions! Please read [CONTRIBUTING.md](CONTRIBUTING.md) for details on how to submit contributions to this project.   ## License  > This project is licensed under the [Apache 2.0 License](LICENSE).  ## Additional Resources  * [SAS Event Stream Processing 6.2 Documentation](https://go.documentation.sas.com/?cdcId=espcdc&cdcVersion=6.2&docsetId=espov&docsetTarget=home.htm&locale=en) * [ESPPy](https://github.com/sassoftware/python-esppy) * [JupyterLab](https://jupyter.org/) "
IoT Covid Patient Health Monitor in Quarantine,Internet of Things,https://github.com/CODERdotEXE/patient-health-monitoring-IOT,"# IoT-Based Patient Health Monitoring System  ## Introduction  In the rapidly evolving landscape of healthcare technology, the Internet of Things (IoT) is playing a crucial role in transforming how patient health is monitored and managed. This project introduces a smart patient health tracking system using a Web Server, enabling the monitoring of vital parameters such as heart rate, blood oxygen level, and body temperature.   ## Components  - **ESP32 Board**: 1 - **MAX30100 Pulse Oximeter Sensor**: 1 - **DS18B20 Temperature Sensor**: 1 - **DHT11 Humidity & Temperature Sensor**: 1 - **Resistor 4.7K**: 1 - **Connecting Wires**: 10 - **Breadboard**: 1  ### MAX30100 Pulse Oximeter Sensor The MAX30100 sensor integrates pulse oximetry and heart-rate monitoring solutions, combining LEDs, a photodetector, optimized optics, and low-noise analog signal processing. It operates from 1.8V to 3.3V power supplies and can be powered down through software with minimal standby current.  ### DS18B20 Temperature Sensor This pre-wired and waterproofed sensor measures temperatures ranging from -55 to 125¬∞C (-67¬∞F to +257¬∞F). It offers digital precision with minimal signal degradation over long distances and employs the Dallas 1-Wire protocol.  ### DHT11 Humidity & Temperature Sensor The DHT11 is a low-cost digital sensor that measures temperature and humidity. It utilizes a capacitive humidity sensor and a thermistor to provide digital signals, requiring precise timing for data retrieval with a refresh rate of once every 2 seconds.  ## Circuit Diagram  ![Circuit Diagram](./screenshots/circuit_diagram.png)  ## Methodology  1. Gather the necessary components including the ESP32 development board, pulse oximeter sensor, and DHT11 temperature and humidity sensor. 2. Connect the sensors to the ESP32 development board according to the wiring diagram. 3. Write and upload the code into the ESP32 web server using the Arduino IDE. The code includes functions to read data from the sensors, display the data on a web page, and send the data to a remote server for storage and analysis. 4. Test the code on the ESP32 development board to ensure that it functions correctly and that the sensors provide accurate data. 5. Create a user-friendly web interface for the health monitoring system, featuring real-time data visualization, alerts for abnormal readings, and historical data analysis. 6. Deploy the system in a real-world setting and test it with actual users. 7. Analyze the data collected by the system to identify trends or patterns and use the information to improve the system's performance and accuracy. 8. Continuously monitor and maintain the system to ensure optimal functioning and accurate data collection.  ## Implementation  The IoT-based Patient Health Monitoring system utilizes an ESP32 Web Server to interface with the sensors. The circuit diagram showcases the connection of MAX30100, DHT11, and DS18B20 with the ESP32 board. All sensors operate at 3.3V VCC and are connected appropriately to the power and ground pins of the ESP32.  ## Results & Working  Upon uploading the code, the ESP32 connects to a network and displays the IP Address upon successful connection. This IP Address can be accessed via any web browser, showcasing real-time data such as room temperature, humidity, heart rate, blood oxygen level, and body temperature. The patient's health status can also be viewed on a mobile phone by accessing the provided IP Address.  ## Future Scope: Sensor Network Design for Medical Institutions  We aim to enhance the system's capabilities to cater to the needs of medical institutions by developing a comprehensive sensor network design. This includes:  - **Central Node for Monitoring & Control using Local Cloud**: A central node will aggregate data from multiple sensors deployed throughout the medical institution, enabling robust data collection and analysis. - **Integration of Diverse Sensors**: Additional sensors for monitoring blood pressure, ECG, respiratory rate, glucose levels, and more will be integrated to provide holistic health monitoring solutions. - **Advanced Analytics and Predictive Insights**: Using machine learning algorithms, actionable insights such as early detection of abnormalities and predictive analytics for disease management will be derived. - **Enhanced Security and Compliance**: Stringent security measures and compliance with regulatory standards like HIPAA will be implemented to safeguard patient information. - **Scalability and Interoperability**: The system will be designed for scalability and interoperability with existing hospital management systems and medical devices. - **User-Friendly Interface**: The central monitoring and control interface will be intuitive and customizable, empowering healthcare professionals to make informed decisions swiftly.  ## Commercialization and Deployment Strategy  Our commercialization strategy involves collaboration with healthcare institutions, technology partners, and regulatory bodies to ensure seamless integration and compliance. Pilot deployments will validate the efficacy and usability of the system in real-world settings, with feedback informing iterative improvements.  ## Screenshots  ![Web Interface Screenshot](./screenshots/web_interface.png) ![Real-Time Visualization](./screenshots/real_time_data.png)  ---  By harnessing the power of IoT technology, advanced analytics, and interoperability, this project aspires to revolutionize healthcare delivery and enhance patient outcomes globally."
IoT based Manhole Detection and Monitoring System,Internet of Things,https://github.com/Pooja123667/DSCWOW_Manhole-monitoring-system,# DSCWOW_Manhole-monitoring-system This is an IoT solution for monitoring the health of manholes in a real-time environment <br><br><br>  Checkout below for the video and project presentation - <br><br> Video link - https://www.youtube.com/watch?v=fV3U1dsbopk&t=7s <br> Presentation - https://drive.google.com/file/d/1DAcaU9dHUH83bZEES9JGjUNxZaMTtdTy/view?usp=sharing
IoT based Smart Energy Meter Monitoring with Theft Detection,Internet of Things,https://github.com/Abhirambs-08/IOT-Based-Smart-Energy-Meter-BEC,"# IOT-Based-Smart-Energy-Meter-BEC ## ABSRACT In this project, prevention of power theft can be achieved through an IoT based smart electricity energy meter using ESP32 and real time monitor data on the Blynk application. With existing technology, the user needs to go to the energy-meter reading room and take down the readings. Thus, monitoring and keeping track records of the electricity consumption is a tedious task. This can be automated, thus saving time and money by automating remote data collection. Efficient energy utilization plays a very vital role for the development of smart grid in power systems. Therefore, proper monitoring and controlling of energy consumption is a chief priority of the smart grid. The existing energy meter system has many problems associated to it and one of the key problems is that there is no full duplex communication. To solve this problem, a smart energy meter is proposed based on the Internet of Things (IoT). The proposed smart energy meter controls and calculates the energy consumption using ESP32, a Wi-Fi module and uploads it to the cloud from where the consumer or producer can view the reading. Therefore, energy analysis by the consumer becomes much easier and controllable.   ## Problem Statement: With a technical view, ‚Äúpower theft‚Äù is a non-ignorable crying that is highly prevalent, and at the same time, it directly affects the economy of a nation. Detecting and preventing such crimes with the help of the developing scientific field is the need of the hour.  ## INTRODUCTION The Internet of Things (IoT) describes the network of physical objects ‚Äì ‚Äúthings‚Äù ‚Äì that are embedded with sensors, software, and other technologies for the purpose of connecting and exchanging data with other devices and systems over the internet. These devices range from ordinary household objects to sophisticated industrial tools.  With more than 7 billion connected IoT devices today, it is expected that this number will grow up-to 22 billion in coming years.  An Energy meter is a device, which is used for measuring the energy utilized by the electric load. This energy is basically the total power consumed by the load at a particular interval of time. It is used in domestic and industrial AC circuits for measuring power consumption.  In this project, a smart electricity energy meter using ESP32 Wi-Fi module is made, which can monitor the energy usage anytime, and from anywhere in the world. This will help the use to detect any kind of energy loss as soon as possible and also, make the whole system more controllable.   Electricity thefts are increasing every year across domestic as well as industrial domains which affect the economic status of the country. Various wireless communication systems are available to detect the power theft, but lacks the required infrastructure needed to employ them. The project's aim is to design a system to monitor the power consumed by load and to detect and eliminate the power theft in energy meters.   ## DESCRIPTION OF PROJECT ![image](https://github.com/Abhirambs-08/IOT-Based-Smart-Energy-Meter-BEC/assets/119886477/ff3c1b06-750b-42e4-84bf-63f301ae012c)  ## Components used: - ESP32 Development Board - ZMPT101B AC Single Phase Voltage Sensor - SCT-013 Current Sensor - 10k Œ© Resistors - 100Œ© Resistor - 10ÔÅ≠F Capacitor - Bread Board - Jumper Wires - Bulb - 3-pin plug  ## Working- 1.	First, include the necessary libraries for ESP32 Board. EmonLib handles the retrieval of data from both sensors as well as the calculation for the RMS and power values. BlynkSimpleEsp32 integrates the program to the Blynk Mobile app.     3.	The EnergyMonitor object emon is created &calibration factors are defined. The Blynk timer object is then created to handle the sending of data to the Blynk mobile app. 4.	Then define the SSID & Password on our local WIFI network & insert the authentication code from the Blynk. 5.	The milli & kWh values have to be initialized. The kWh starts at 0 and will slowly go up as time goes on. 6.	The values from the sensors are being retrieved & calculated. Using emon.calcVI(20, 2000), the real power, apparent power, power factor, Vrms, and Irms are being calculated. 7.	Then use Blynk.virtualWrite to send the data to Blynk based on the virtual pins set. 8.	Under the setup function, initialize the Serial baud rate and set the current and voltage sensor analog pin as GPIO34 & GPIO35. Then set the timer to 5000L for an update time of 5 seconds. 9.	Inside the loop function run the timer and Blynk.  ## RESULTS AND DISCUSSION  - When the device is offline, the readings will be set at 0. ![image](https://github.com/Abhirambs-08/IOT-Based-Smart-Energy-Meter-BEC/assets/119886477/2a88d32a-383a-4c35-9594-87251ad69038)  - Once the device is made online, the energy meter data is uploaded to Blynk Application after the interval of every 5 seconds. The data can be observed on Serial Monitor as well as Blynk Application. ![image](https://github.com/Abhirambs-08/IOT-Based-Smart-Energy-Meter-BEC/assets/119886477/c16bd92d-cc29-4f79-8c65-cd4501976bd0)  - The data is sent to the Blynk Application in real time. ![image](https://github.com/Abhirambs-08/IOT-Based-Smart-Energy-Meter-BEC/assets/119886477/9eede34f-81b2-420e-90cb-6aa9568e142e)   Energy Monitoring using IOT is an application of internet of things developed to control home appliances remotely over the cloud from anywhere in the world. In the proposed project current sensor and voltage sensor are used to sense the current and voltage and display it on internet using IoT. The system updates the information in every 5 seconds on the internet using BLYNK app. In the present system, energy load consumption is accessed using Wi-Fi and it will help consumers to avoid unwanted use of electricity.   ![image](https://github.com/Abhirambs-08/IOT-Based-Smart-Energy-Meter-BEC/assets/119886477/1b0c2730-dfce-48f3-9472-b4eb1c39fa3f)   IoT system where a user can monitor energy consumption and pay the bill Online can be made. Also, a system where a user can receive SMS, when he/she crosses threshold of electricity usage slab can be equipped. A system can be made which can send SMS to the concerned meter reading man of that area when theft is detected at consumer end. Also using cloud analytics, future predictions of energy consumptions can be made.  ## CONCLUSION  This chapter projects the analysis and discussion of results and findings during and after the implementation of design. It describes in detail the final design perspective as well as highlighting the probable defects engulfing the project.  A preview to the entirety of this project establishes the essence and need for embedded systems towards technological advancement.  The IOT based Smart Energy Meter is the anti-theft detection system. Even if theft is caught, the victim cannot get back their valuable belongings. That is why it is clear that ‚Äòprevention is better than cure‚Äô. It is easier to stop something happening in the first place than to repair the damage after it has happened. By using this technique, crime of stealing power may be brought to an end and thereby a new bloom may be expected in the economy of our motherland and also there will be less scarcity for power utilization. "
IoT based Intelligent Gas Leakage Detector Using Arduino,Internet of Things,https://github.com/avimagar/Smart-Gas-Leak-Detector,"Project Title: Smart Gas Leakage Detector  Description: It is an IOT based project which is aimed at making a cost effective system which detects LPG/CNG gas leakage and produces alarm. Also a message is sent to owner about leakage. Also the location is shown on google maps.  Tools & Softwares: Software tools:Arduino IDE   Hardware Tools: Arduino UNO, SIM900 GSM Module, MQ6 Sensor, Buzzer  Demo Link: https://drive.google.com/file/d/0B_3FJmqe9naDY2lEMnJsd28zbE0/view?usp=sharing&resourcekey=0-mElzvzYTQUvGH4qI_TqnaA"
IoT based Smart Agriculture Monitoring System Project,Internet of Things,https://github.com/ankitsagar/Smart-Agriculture,"# IoT Based Smart Agriculture System  The project aims at making use of evolving technology i.e. IoT and smart agriculture using automation. Monitoring environmental factors are the major factor to improve the yield of efficient crops. The feature of this project includes monitoring temperature, humidity, and moisture in the agricultural field through sensors DHT11, YL69. It will turn ON/OFF motor based on soil moisture.  ## Modules:  ![Flow Diagram](modules/Diagram12.jpg)  ### Module-1:  In the first module, we are using the ESP8266 Wi-Fi module that will act as a microcontroller and it will be attached to a DHT11 to sense the current temperature and humidity. The module will publish the temperature and humidity data to the MQTT broker i.e Raspberry Pi.  #### Component Used:  **‚Ä¢ ESP8266 ESP-01:** It is a low power consuming Wi-Fi module with an integrated TCP/IP protocol stack that can give any microcontroller access to the WiFi network. It is a self-contained SOC (System On a Chip) that doesn‚Äôt necessarily need a microcontroller to manipulate inputs and outputs.  **‚Ä¢ DHT11:** It is a temperature and humidity sensor that generates calibrated digital output for temperature and humidity. It uses a humidity sensor and thermistor to measure the surrounding environment. It has a fast response and excellent quality.  ### Module-2:  In the second module, we are using Arduino as a microcontroller that will get the soil moisture through a moisture sensor and publish it to the MQTT broker by using the ESP8266 Wi-Fi module.  #### Component Used:  **‚Ä¢ Arduino Uno:** It is a microcontroller board based on the ATmega328. It has 14 digital I/O pins and 6 analog pins. It just needs to connect to a computer with a USB cable to upload the code and powered through an AC to DC adapter or a battery.  **‚Ä¢ YL-69:** It is a soil moisture sensor used to measure the water content of the soil. It can detect whether the soil is too dry or wet. This sensor uses the two probes to pass current through the soil, and then it reads that resistance to get the moisture level.  **‚Ä¢ ESP8266 ESP-01**  ### Module-3:  In this module ESP8266, WiFi Module works as a subscriber who subscribes to the moisture data from the MQTT Broker(Rasberry Pi). ESP8266 connected to relay i.e connected to the motor and it will turn the motor ON/OFF based on soil moisture.  #### Component Used:  **‚Ä¢ Relay:** A relay is an electrically operated switch. It means that it can be turned on or off, letting the current going through or not. when a relay contact is normally open (NO), there is an open contact when the relay is not energized. When a relay contact is Normally Closed (NC), there is a closed contact when the relay is energized.  **‚Ä¢ ESP8266 ESP-01**  ### Raspberry Pi as a Broker:  MQTT (Message Queue Telemetry Transport) is a lightweight messaging protocol for small devices and sensors. It is a publisher-Subscriber based model. A publisher can publish data to broker and subscribers can subscribe topic from the broker. So, the broker plays a very important role as a middle man.  In this project, we are creating raspberry pi as a broker. To make it broker, we are using a Mosquitto MQTT i.e open-source implementation of a message broker. Raspberry Pi as a broker receives Temperature and Humidity data with a specific topic from the Arduino microcontroller as described in module1 and also receives soil moisture data with topic information as mentioned in module 2.  Now subscriber as module 3 here can subscribe to data from the broker by specifying the topic name.  ## Code Details:  This project is based on two areas **North** and **South** both are using their own module 1 & 2 to publish temperature, humidity, and moisture.  **Subscriber (Module-3)**: it is using two-channel Realy that will turn on/off motors based on soil moisture of a particular area.  ### Visulization:  I am using [ThingsBoard](https://thingsboard.io/) to visualize data i.e great open-source IoT platform for data visualize real-time data. **dataSubscriber.py**: This Script is used to subscribe to data of all areas from the broker and write it into different text files.  **thingsboardNorth.py and thingsboardSouth.py**: These Scripts will scrap data from text files and upload them to ThingsBoard Server. ![](visualization/screenshot4.jpg)  [Link To Dashboard](https://demo.thingsboard.io/dashboards/d7455310-065d-11e8-83e6-1d8d2edf4f93?publicId=2ade3530-f6a2-11e7-abe9-1d8d2edf4f93)  ## Wiring:  ### Module 1  **ESP8266 ESP-01**  Vcc -------------------------------------- 3.3V   Gnd -------------------------------------- Gnd   Pin 2 -------------------------------------- DHT Data pin  **DHT11**   Vcc -------------------------------------- 5V   Gnd -------------------------------------- Gnd   Data -------------------------------------- ESP Pin 2  ### Module 2  **YL-69 Soil Sensor**&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Arduino UNO**   Vcc -------------------------------------- 5V   Gnd -------------------------------------- Gnd   A0 --------------------------------------- A0  **ESP8266 ESP-01**&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Arduino UNO**   Vcc --------------------------------------- 3.3V   Gnd -------------------------------------- Gnd   Rx --------------------------------------- Pin 7   Tx --------------------------------------- Pin 8  ### Module 3  **Relay(2 Channel)**   IN1 ------------------------ ESP pin 0   IN1 ------------------------ ESP pin 2   Vcc ------------------------ 5V   Gnd ------------------------ Gnd"
IoT based Automatic Vehicle Accident Detection and Rescue,Internet of Things,https://github.com/syedissambukhari/IoT-Based-Automatic-Accident-Detection-And-Rescue-system,"# IoT-Based Automatic Accident Detection and Rescue System  This project focuses on developing an IoT-based system to automatically detect vehicular accidents and trigger a rescue response by alerting relevant authorities with the location and details of the accident.  ## Table of Contents  - [Introduction](#introduction) - [Features](#features) - [Technologies Used](#technologies-used) - [Installation](#installation) - [Usage](#usage) - [System Architecture](#system-architecture) - [Project Structure](#project-structure) - [Contributing](#contributing) - [License](#license) - [Contact](#contact)  ## Introduction  Accidents are a significant concern on roads, and timely rescue can save lives. This IoT-based automatic accident detection and rescue system uses sensors to detect accidents and automatically sends alerts with precise location details to emergency services and contacts.  ## Features  - **Accident Detection:** Utilizes sensors to detect a collision and determine if an accident has occurred. - **Automatic Alert System:** Sends automated alerts to predefined contacts and emergency services with the accident‚Äôs location and other relevant details. - **Real-Time Monitoring:** Continuously monitors the vehicle's status and provides real-time updates. - **Location Tracking:** Uses GPS to track the exact location of the accident.  ## Technologies Used  - Arduino - GPS Module - GSM Module - Accelerometer/Gyroscope (e.g., MPU-6050) - Sensors (e.g., vibration sensor) - IoT Platforms (e.g., ThingsBoard, MQTT) - Programming Languages: C/C++, Python  ## Installation  1. **Clone the repository:**      ```bash     git clone https://github.com/syedissambukhari/IoT-Based-Automatic-Accident-Detection-And-Rescue-system.git     ```  2. **Hardware Setup:**      - Connect the Arduino, GPS module, GSM module, and sensors according to the provided circuit diagram and code.  3. **Software Setup:**      - Install the necessary Arduino libraries for GPS, GSM, and sensors.     - Open the `.ino` file in the Arduino IDE.     - Configure the GSM module with your SIM card details.     - Upload the code to the Arduino board.  ## Usage  1. **Configure Contacts:**      - Edit the Arduino code to include the phone numbers or email addresses of emergency contacts.  2. **Deploy the System:**      - Install the system in a vehicle, ensuring all connections are secure and the GPS and GSM modules have a clear signal.  3. **Monitoring and Alerts:**      - The system will automatically detect accidents and send alerts to the configured contacts with location details.    ## Project Structure  - `Arduino/`: Contains the Arduino code (`.ino` file) and necessary libraries. - `Diagrams/`: Contains circuit diagrams and system architecture images. - `README.md`: Project documentation.  ## Contributing  Contributions are welcome! Please follow these steps:  1. Fork the repository. 2. Create a new branch (`git checkout -b feature/your-feature-name`). 3. Commit your changes (`git commit -m 'Add some feature'`). 4. Push to the branch (`git push origin feature/your-feature-name`). 5. Open a pull request.  ## License  This project is licensed under the [MIT License](LICENSE).  ## Contact  If you have any questions or suggestions, feel free to contact me.  - **Syed Issam Bukhari** - [GitHub Profile](https://github.com/syedissambukhari)"
IoT based Coal Mine Safety Monitoring and Alerting System,Internet of Things,https://github.com/Alok2580/IoT-based-Smart-Coal-Mining-Safety-and-Alerting-System,"# IoT-based-Smart-Coal-Mining-Safety-and-Alerting-System Arduino sketch showcases IoT skills with ESP8266, DHT11, and MQ2 sensors using Blynk for data visualization. Displays competency in sensor integration and readiness to explore new functionalities like rain sensing. Reflects my passion for IoT and interest in mathematical biology.  The provided Arduino sketch showcases my engagement with IoT, sensor integration, and data communication through Blynk. This code orchestrates an ESP8266 module, connecting it to Wi-Fi using the ESP8266WiFi.h library. It interfaces with various sensors like the DHT11 for temperature and humidity monitoring, an LDR for light sensing, and an MQ2 gas sensor for detecting gas levels.  I've meticulously organized the code by defining pin assignments for each sensor and initializing them in the setup function. The integration with Blynk, facilitated by BlynkSimpleEsp8266.h, allows seamless transmission of sensor data to the Blynk app, ensuring real-time monitoring and analysis.  Additionally, I've included conditional statements to execute actions based on sensor readings, such as activating an LED according to the LDR readings and triggering a buzzer concerning gas sensor values, showcasing my ability to respond to sensor data in practical scenarios.  I should note that certain sections of the code, denoted by comments, are potential expansions involving rain sensing (RainSense), although they're currently inactive. While not yet fully implemented, these segments demonstrate my readiness to explore and integrate additional functionalities.  This code reflects my initiative in IoT, sensor interfacing, and utilization of Blynk for data visualization, aligning with my eagerness to further delve into such interdisciplinary realms, especially within the context of mathematical approaches to biological systems.  "
IoT based Heart Monitoring System Using ECG,Internet of Things,https://github.com/Ashithaby/IoT-based-low-cost-ECG-and-heart-monitoring-system-with-ESP32,"# IoT-based-low-cost-ECG-and-heart-monitoring-system-with-ESP32 AIM:   To provide accessible and continuous remote monitoring of a person‚Äôs cardiac health.  INTRODUCTION:   IoT-based low-cost ECG monitoring system offers a transformative solution for democratizing access to cardiac care. By leveraging the power of the Internet of Things, this system provides continuous and affordable monitoring of the heart's electrical activity.  OBJECTIVE:  *Early Detection of Cardiac Issues  *Remote Patient Monitoring  *Low-Resource Settings  *Reducing Healthcare Costs  *Education and Research  FEATURES:  *Develop a Wearable Heart Monitoring Device:  Design a compact and wearable device capable of continuous ECG monitoring using the ESP32 and AD8232.  *Real-Time ECG Signal Processing:  Implement real-time signal processing algorithms to filter noise and extract meaningful ECG data for accurate heart rate calculation.  *Wireless Data Transmission:  Enable wireless data transmission from the monitoring device to a connected device (e.g., smartphone, computer) for real-time display and analysis.  *Heart Rate Calculation Algorithm:  Develop and optimize algorithms to calculate the user's heart rate based on the processed ECG signals.  *User Alerts for Abnormalities:  Implement an alert system to notify users in real-time of irregular heartbeats or abnormal ECG patterns, promoting early detection of potential health issues.  *Data Logging and Storage:  Create a data logging feature to store ECG data over time, facilitating historical analysis and tracking of changes.  *Cloud Integration:  Integrate cloud storage for ECG data, allowing users to access their information from multiple devices and enabling remote monitoring by healthcare professionals.  *Bluetooth Connectivity:  Develop power-saving features to optimize battery life, including sleep modes and low-power states, particularly for wearable applications.  *User Authentication and Privacy:  Implement user authentication mechanisms to ensure secure access to ECG data and prioritize user privacy in compliance with relevant regulations.  *Intuitive User Interface:  Design an intuitive and user-friendly interface for displaying real-time ECG data, heart rate, and historical trends.  *Customizable Settings:  Provide users with customizable settings, such as monitoring duration, alert thresholds, and display preferences, to enhance the user experience.  *Integration with Health Platforms:  Explore integration with existing health platforms or applications, allowing users to sync ECG data with other health-related metrics for a comprehensive health profile.  *Offline Mode and Local Data Storage:  Include an offline mode that stores a certain amount of data locally, enabling users to access their ECG history even without an internet connection.  *Open-Source Collaboration:  Foster open-source collaboration by making the project's codebase accessible to the developer community, encouraging contributions and improvements.  *Educational Resources:  Provide educational resources within the application to help users understand their ECG data, promoting heart health awareness and informed decision-making.  COMPONENTS REQUIRED:  *ESP32 Board  *AD8232 ECG sensor  *ECG Electrode connector with plastic patches  *Connecting wires  *Bread board  *16X2 LCD  OVERVIEW OF MOBILE APPLICATION:  ThingSpeak:  Purpose: ThingSpeak serves as the backend platform for storing and managing ECG data.  Functionality: ECG data is collected from sensors or devices, transmitted to ThingSpeak, and stored securely in channels.  Integration: The ThingSpeak API is utilized to retrieve ECG data for visualization in the mobile app.  MIT App Inventor:  Purpose: MIT App Inventor is the platform used for developing the mobile application.  Functionality: It provides a visual programming environment for creating an intuitive user interface and implementing the logic for ECG data retrieval and display.  Integration: The app interacts with ThingSpeak through the ThingSpeak API, fetching ECG data and presenting it to users.  MINDMAP:  ![MINDMAP](https://github.com/Ashithaby/IoT-based-low-cost-ECG-and-heart-monitoring-system-with-ESP32/assets/149662500/4bb478fc-b4c6-48b4-b133-b05b3a6617ad)  WORKING:  Creating a low-cost ECG monitoring system using an ESP32 and an LCD can be a feasible project. Here's a simplified overview of how it might work:  ECG Sensor: Connect an ECG sensor to the ESP32 to capture electrical signals from the heart.  Signal Processing: Use the ESP32 to process the raw ECG signals. You may need to filter, amplify, and digitize the signals for further analysis.  Voltage analysis: Analyze the processed signals to calculate and compare the voltage values.   Display on LCD: Use the ESP32 to send the heart rate information to the LCD for real-time display. The LCD can show whether the heart condition is good or bad based on predefined thresholds or algorithms.  User Interface: Implement user interaction via buttons or a touchscreen if needed. Users may want to start/stop monitoring or view historical data.  Communication: Optionally, you can add features to send data to a remote server or a mobile app for more comprehensive monitoring.  FLOWCHART:  ![FLOWCHART](https://github.com/Ashithaby/IoT-based-low-cost-ECG-and-heart-monitoring-system-with-ESP32/assets/149662500/7326dd43-5c61-4ad0-9fa5-b2636e24eb4f)  ALGORITHM:  STEP1:Initialization  Include Libraries: Include the LiquidCrystal, WiFi, and ThingSpeak libraries.  STEP2:Create Objects  Create a LiquidCrystal object for the LCD.  Declare variables for WiFi credentials, ThingSpeak channel information, and a WiFi client.  STEP3:Setup Function:  Serial and LCD Initialization:  Begin serial communication at a baud rate of 9600.  Initialize the LCD with its specific pins.  STEP4:Leads Off Detection Setup:  Set pin 15 as INPUT for leads off detection (LO +).  Set pin 21 as INPUT for leads off detection (LO -).  STEP5:WiFi Connection:  Connect to the WiFi network using the provided SSID and password.  ThingSpeak Initialization:  Initialize ThingSpeak communication using the ThingSpeak library and the WiFi client.  STEP6:Loop Function:  Analog Sensor Reading:  Read the analog value from pin A0 using analogRead().  STEP7:LCD Display:  Clear the LCD display.  Check for leads off condition:  If leads off is detected (digitalRead on pins 15 or 21 is 1), handle the condition as needed.  If leads are on:  Display ""Analog: "" and the analog value on the LCD.  Display ""Health: Good"" if the analog value is between 500 and 3000; otherwise, display ""Health: Bad.""  STEP8:Serial Output:  Print the analog value to the Serial Monitor.  ThingSpeak Data Sending:  Set ThingSpeak fields with the analog value and timestamp.  Send the data to ThingSpeak using ThingSpeak.writeFields().  STEP9:Success/Failure Message:  If the response code from ThingSpeak is 200, print ""Data sent to ThingSpeak successfully"" to the Serial Monitor.  If the response code is not 200, print ""Failed to send data to ThingSpeak"" to the Serial Monitor.  STEP10:Delay:  Introduce a delay of 1000 milliseconds to prevent saturation of LCD and Serial data.  STEP11:Repeat:  Loop back to step 7 and repeat the process.  BLOCK DIAGRAM:  ![BLOCK](https://github.com/Ashithaby/IoT-based-low-cost-ECG-and-heart-monitoring-system-with-ESP32/assets/149662500/f74bfa40-99a2-43f1-844e-ddb7aca67852)  CIRCUIT DIAGRAM:  ![circuit](https://github.com/Ashithaby/IoT-based-low-cost-ECG-and-heart-monitoring-system-with-ESP32/assets/149662500/a4655f89-9c2d-4cad-924b-94ab283b4fb8)   CONCLUSION:  In conclusion, the development of an IoT-based low-cost ECG monitoring system using the AD8232 ECG sensor and ESP32 presents a promising solution for remote healthcare monitoring. This innovative system leverages the power of Internet of Things (IoT) technology to provide real-time ECG data acquisition and transmission, enabling continuous monitoring of patients' cardiac health in a cost-effective manner. The system's low-cost design addresses accessibility concerns, making it more viable for a broader range of users, including those in resource-constrained environments. By utilizing open-source hardware and software components, the project encourages collaboration and customization, potentially fostering further innovations in the field.  In summary, the IoT-based low-cost ECG monitoring system demonstrates the potential to revolutionize healthcare by providing an affordable and accessible solution for continuous cardiac monitoring. This project serves as a foundation for future advancements in remote patient monitoring systems, contributing to the ongoing efforts to enhance healthcare delivery through innovative technology solutions.   "
IoT based  Weather Reporting ,Internet of Things,https://github.com/Rashmika-B/IoT-based-Weather-Reporting-System,"# IoT based Weather Reporting System  This is a simple Smart Weather Reporting system using **Raspberry Pi 4B+** and weather-related **sensors**. The system monitors and reports the weather parameters such as: Temperature,Humidity,Pressure, and gives alerts whenever it rains. We have also used [ThingSpeak Cloud](https://thingspeak.com/) platform for data collection and performed advanced data analysis using MATLAB and the [IFTTT](https://ifttt.com/) service to send alerts and notification.  ## Installation  Use the package manager [sudo apt install](https://www.raspberrypi.org/documentation/) to install the necessary packages for Raspberry Pi.  ```bash sudo apt-get install python-smbus  ``` ## Sensors 1. DHT11 Temperature Sensor 2. BMP180 Barometric Pressure Sensor 3. YL-83 Rain Sensor  ## Hardware Setup <img src=""https://github.com/Rashmika-B/IoT-based-Weather-Reporting-System/blob/main/Code%20Snapshots/RaspberryPi%20Setup/Capture.JPG"" width=""600"" height=""500"">"
IoT based  Weather Reporting ,Internet of Things,https://github.com/Rashmika-B/IoT-based-Weather-Reporting-System,"# IoT based Weather Reporting System  This is a simple Smart Weather Reporting system using **Raspberry Pi 4B+** and weather-related **sensors**. The system monitors and reports the weather parameters such as: Temperature,Humidity,Pressure, and gives alerts whenever it rains. We have also used [ThingSpeak Cloud](https://thingspeak.com/) platform for data collection and performed advanced data analysis using MATLAB and the [IFTTT](https://ifttt.com/) service to send alerts and notification.  ## Installation  Use the package manager [sudo apt install](https://www.raspberrypi.org/documentation/) to install the necessary packages for Raspberry Pi.  ```bash sudo apt-get install python-smbus  ``` ## Sensors 1. DHT11 Temperature Sensor 2. BMP180 Barometric Pressure Sensor 3. YL-83 Rain Sensor  ## Hardware Setup <img src=""https://github.com/Rashmika-B/IoT-based-Weather-Reporting-System/blob/main/Code%20Snapshots/RaspberryPi%20Setup/Capture.JPG"" width=""600"" height=""500"">"
IoT Early Flood Detection & Avoidance,Internet of Things,https://github.com/roycuadra/IoT-based-flood-detection-system,"# IoT-based-flood-detection-system  ## Video Demonstration Check this out [Video Demonstration](https://www.dropbox.com/scl/fi/6d9z8aix0gbp1qhpmv21a/409693917_863510845313476_5007686960907421415_n.mp4?rlkey=ojz26zqldq43wn1byotfap7xr&st=ifp044dm&dl=0)   ## Description  - The IoT based flood detection with Blynk Integration code allows you to monitor distance measurements of water using an ultrasonic sensor connected to an ESP8266 microcontroller. - The measured distance in both centimeters and inches is displayed on the Blynk app for real-time visualization. Additionally, LEDs and a buzzer provide visual and audible alerts based on predefined distance conditions.  ## Hardwares - ESP8266 microcontroller - Ultrasonic sensor (HC-SR04) - RGB LED - Jumper Wires - Breadboard - Buzzer ## License  This project is open-source and available under the MIT License. See the [LICENSE](LICENSE) file for more details."
IoT Garbage Monitoring Using Raspberry Pi,Internet of Things,https://github.com/cloudwithtanvir/An-Android-Based-IoT-System-for-Garbage-Monitoring,# Garbage Collection System using Internet of Things  Garbage collection system built using internet of things makes smart garbage collection available on municipality area.  ### Devices  * Raspberry Pi 3 model B * Ultrasonic Sensor  ### Technology  * Python * Node.js * Socket.io * Android 
IoT Circuit Breaker Project Women Safety Night Patrolling,Internet of Things,https://github.com/rakshixh/Samraksh,"# Samraksh: IoT Based Women Safety Device  ### Abstract * Despite the fact that our country is a powerhouse with a developing economy, it nevertheless has a number of crimes against women. * To address this big issue, we offer ""Samraksh"" as a solution. * This is a safety mechanism designed exclusively for women who are in difficulties, stress, etc. * Our system contains GPS Module, ESP8266, LED, Thingspeak Cloud service, Button & Beep Buzzar. * When a person is in distress, she must hit the button on the system provided. * The location of the device is determined in terms of longitude & latitude along with date and time. Then this data is sent to the Real-Time Cloud Service. * Now trusted person/family can access the location by copy-pasting that longitude & latitude into the Google Map followed by "","" as soon as they are alerted by our system.   ### Introduction * Safety being one of the most concerning issues, it is necessary to find an efficient way to ensure the safety of the people and the society. * Women's safety is considered to be one of the most critical issues in a country like India, where the rate of crime is thought to be higher than the pace of population increase. * NCRB states that as many as 39 crimes are reported every hour in the country. This adverse crime situation has embedded fear in the mind of people. If statistics are to be seen, around 20,532 cases are registered every year. * We come across many headlines reporting cases of molestation, trafficking, ill-treatment, ragging, kidnapping, missing and etc. * After identifying a hazard, existing handheld gadgets for human protection require human safety, such as pressing a button.  ### Objectives * To build a safety device for women. * To facilitate quick action and hence reduce the harm. * To ensure the security of woman in the society and hence to promote Woman Empowerment.  ### Problem Statement * Among all of the heinous crimes that have been witnessed recently, ensuring the safety of women has become a challenge, and thus a major source of concern in society.  ### Methodology * The system comprises of two devices: Device 1 & Device 2, where Device 1 will be with the individual whose position is to be tracked, and Device 2 will be with a trustworthy person or family member, and this device will notify using a beeping buzzer. * The system includes a NEO 6M-GPS Module, as well as an LED, a push button, and a buzzer. * When the push button is pressed LED turns on for indication purposes and an email is sent to the trustworthy person or family members. The location of the device in terms of latitude & longitude is determined along with LED status. * Now, this data from GPS Module along with LED status is uploaded or updated in the cloud by ESP8266 Node MCU for further use. As soon as LED status in the cloud is set to 1 Beeping buzzer in Device 2 will start to Beep for indication purposes. That time display of the data in the cloud is accessed by the trusted person or family members so that they can take further action.  ## Block Daigram * Device 1 - Using NEO 6M GPS Module ![Swasthya Final Implementation (GPS)](https://user-images.githubusercontent.com/83587918/192152053-5e4f3843-8bb1-44d8-abe9-a2952fd82ad8.png) * Device 2 - Using Beep Buzzer ![Device 2 Samraksh](https://user-images.githubusercontent.com/83587918/192152137-7ffc645a-226e-4b61-800c-b937dc6e55bb.png)  ## Requirements  #### Software Requirements * Arduino IDE * Thingspeak Cloud Service  #### Hardware Requirements * NEO 6M GPS Module * ESP8266 Node MCU * Power Supply * Push Button * Beep Buzzer * General PCB * Resistor * LED  #### Data Used * Longitude and Latitude * LED Status (0 & 1) * Date and Time  ## Images <h3> ESP8266 Node MCU </h3>  ![NicePng_simba-png_5011059](https://user-images.githubusercontent.com/83587918/192152704-cc5d257a-1df7-4054-b8ff-9030268eae17.png)  <h3> NEO 6M GPS Module </h3>  ![2cropped](https://user-images.githubusercontent.com/83587918/192152759-978363d2-7ab4-45fb-9162-d1942d3743ea.png)  <h3> Beep Buzzer </h3>  ![2 (2)](https://user-images.githubusercontent.com/83587918/192153227-51d5709d-2509-4539-b05f-410333a294f2.png)  <h3> LED </h3>  ![ledd full](https://user-images.githubusercontent.com/83587918/192153523-6cbf3cee-493c-4d28-9390-c82958df5cf7.png)  <h3> Power Supply </h3>  ![4](https://user-images.githubusercontent.com/83587918/192153297-5bf81667-bd8a-485b-93ff-c1b557a5641c.png)  ## Project Setup  #### Step 1  #### Step 2 Install and setup Arduino IDE in your PC. Download and install all required libraries. Check and set the port in Arduino IDE and connect the NodeMCU ESP8266 using USB cable. Copy the code given above to a ESP8266 Node MCU of Device 1 and Device 2 respectively, in your Arduino IDE and make required changes in the code. Such As: * Wifi Name * Wifi Password * Channel ID * Thingspeak API Key      #### Note     Create an account in Thingspeak Cloud service then create one channel, give a name for that particular channel. When you open that channel you will get option called ""API KEYS"" here you will get the API key of your channel which you will need to copy in the code! Also make sure to create 3 fields in ""Channel Setting"" section. And set the channel access to the public.  ## Output  ![Screenshot (285)](https://user-images.githubusercontent.com/83587918/192154265-6818ed7b-6df4-4b6b-a193-fa6851711153.png)  ![Screenshot (286)](https://user-images.githubusercontent.com/83587918/192154277-cf07ccda-2511-4b30-9506-9d59a3d9bfbb.png)  ![Screenshot (287)](https://user-images.githubusercontent.com/83587918/192154281-4b4d59c7-4e50-499f-b3ab-88b158310c93.png)  ![msg1794739563-10150](https://user-images.githubusercontent.com/83587918/192154383-61b5a74a-0083-496a-9367-79b2c1905712.jpg)  ## Result and Discussion  * Once the trusted person/family member gets the output in Thingspeak (Cloud) they have to copy the Latitude & Longitude Values then paste it to the Google Map followed by "" , ""   * Example: Latitude=52.4512 & longitude=65.2165 then 52.4512,65.2165 is to be entered in Google Map. * That time in Google Map Trusted person/family member will get the exact location of that co-ordinates. * So that they can take further action.  ![Screenshot (26)](https://user-images.githubusercontent.com/83587918/192154453-a961c9af-86c6-4d5b-8996-1b0d84431fc5.png)  ![Screenshot (27)](https://user-images.githubusercontent.com/83587918/192154464-ecd129c9-d704-4e6a-85f6-d7a400b218f7.png)  ## End Customers * Common Proplr * Police Department * Government  ## Application * Girls when in suspicious environments or when in danger can share their location and hence ask for help from the known ones. * Aged people when away from home or are remote, can ask for help in case of any threats or health issues. * Children notify their guardians about the need for help when in trouble. * Getting notified about the location and the need for help facilitates quick action.   ## Conclusion * Primary goal of this project is to ensure every woman in our society to feel safe and secured. According to the survey in India 53% of working women are not feeling safe. * Implementing real time application and a device, we can solve the problems to an extent. * With furthur research and innovation, this project is used as a small wearable device.     https://user-images.githubusercontent.com/83587918/192154887-5925bb24-6981-42fc-86ea-69c0e90c0d1c.mp4    "
IoT Robot,Internet of Things,https://github.com/vatsal2210/IoT-Robot,"# IoT-Robot  ![radio](./photos/radio.png)  <!-- TOC -->  - [1. Demo](#1-demo) - [2. Description](#2-description) - [3. Software Versions](#3-software-versions) - [4. Background/Motivation](#4-backgroundmotivation) - [5. Aims & Objectives](#5-aims--objectives) - [6. Technology](#6-technology) - [7. Design Methodology](#7-design-methodology) - [8. Implemented System](#8-implemented-system) - [9. Results](#9-results) - [10. Conclusion](#10-conclusion) - [11. Future Work](#11-future-work)  <!-- /TOC -->  <a id=""markdown-1-demo"" name=""1-demo""></a> ## 1. Demo  [Source Link](https://www.youtube.com/watch?v=EQoNMig4XXY)  <a id=""markdown-2-description"" name=""2-description""></a> ## 2. Description The purpose of this project is to control robot with an interface board of the Raspberry Pi, sensors and software to full fill real time requirement. Controlling DC motors, different sensors, camera interfacing with raspberry Pi using GPIO pin. Live streaming, Command the robot easily, sends data of different sensors which works automatically or control from anywhere at any time. Design of the website and control page of Robot is done using Java tools and HTML. This system works on IoT concept which is Internet of Things, where all the physical devices will connect with digital systems. This will enable raspberry pi to be used for more robotic applications and cut down the cost for building an IoT robot.  <a id=""markdown-3-software-versions"" name=""3-software-versions""></a> ## 3. Software Versions  It required number of Programming Tools & Languages to build a project.   - Eclipse Kepler - V4.3.2 - HTML/CSS - SERVLET/JSP - 3.0.0 - Java Script - JQuery/Ajax - MySQL database - Tomcat Web Server - MobaXtreme - WinSCP - Putty - Pi4j  <a id=""markdown-4-backgroundmotivation"" name=""4-backgroundmotivation""></a> ## 4. Background/Motivation - User can see live streaming from computer device as website or phone application as camera is attached.  - Different buttons are there such as Forward, Reverse, Left, Right and Stop to control the Robot.  - Different sensors are attached with the device such as Ultrasonic sensor, IR sensors to detect obstacle and distance and generate notifications and sends data to user.  - In smart home concepts it can add value in it. Security is always important at all the time, so there is unique login ID and password to control the Robot.  - First user have to sign up and using unique ID they will able to control it from anywhere at any time.   <a id=""markdown-5-aims--objectives"" name=""5-aims--objectives""></a> ## 5. Aims & Objectives To develop an IoT technology based Robot can be controlled by a mobile devices/ Laptops over the Wi-Fi from anywhere at any time. The core objectives are: - Gather system requirements - Evaluate and study the platform required for the system - Evaluate and study suitable development language, technologies and tools - Evaluate Methods of Interface - Program Raspberry Pi - Interface board for dc motors - Program Website & Control Page - Evaluate and test the system - Maintain system	  <a id=""markdown-6-technology"" name=""6-technology""></a> ## 6. Technology - The technology used in the project is Javas the libraries available are only in pi4j.  - Also it allows creating a user interface so that the user can see and control certain movements of the robot.  - I use the wireless technology to transmit data of the raspberry pi to the users system.  <a id=""markdown-7-design-methodology"" name=""7-design-methodology""></a> ## 7. Design Methodology  - The design consists more on actual planning of hardware part than the code to be created.  - A number of software and hardware implementation techniques were used to design and develop the system. Fig. 1 shows the block diagram of system.  - Block diagram is shows as below:   <a id=""markdown-8-implemented-system"" name=""8-implemented-system""></a> ## 8. Implemented System  - The system is implemented with Laser cutting tool and 3D printing tool. Robot chassis is designed with Laser cutting and Ultrasonic sensor and Camera case is designed with 3D priming. - Laser cutting machine is shown below and the full implemented chassis of robot.   ![Laser Cutting](./photos/laser1.jpg) ![Laser Cutting](./photos/laser2.jpg) ![Laser Cutting](./photos/laser3.jpg) ![Laser Cutting](./photos/laser4.jpg) ![Laser Cutting](./photos/laser5.jpg)  - Using machine Ultimaker 2 case of Raspberry Pi camera is designed and working with it is shown below. The full body requires time around 1 hours to print it.   ![3D Printing](./photos/3D1.jpg) ![3D Printing](./photos/3D2.jpg) ![3D Printing](./photos/3D3.jpg)  <a id=""markdown-9-results"" name=""9-results""></a> ## 9. Results  - The aim of the project is to develop a Robot on IoT based concept.  - It is working as buddy or family Member because you have to command it and control from anywhere at any time.  - If a personal wants to find something he/she has to command it from live steaming can see the actual scenario at that place and easily find out that object. -  It works as to take care for children‚Äôs, pet at home, too.   ![Flow Chart](./photos/flowchart.png)  Final IoT Robot design is as shown below:   ![Final](./photos/final.jpg)  <a id=""markdown-10-conclusion"" name=""10-conclusion""></a> ## 10. Conclusion  - To get to the aim of a project there will be always a set of objectives, to achieve that objectives we need to know how where and with what resource is the step towards completing the objectives taken.  - Now in this project too to get to the aim of the project there was a set of objectives, which gradually changed as the project research was completed and then while testing a certain technology the objectives again changed due to the failure of the method. Now the first thing of the project is a good research, I had to do a wide and a strong research before I started to put my objectives as this technology was new in market.  The research for the project was done using Advanced Google search and also from the search engines available in the student portal like tutorials, pi4j, w3school and raspberry pi. - The Google advanced search is the one that was more widely used as it is a new technology and there are very less articles or journals published regarding the raspberry pi technology and IoT.  - Each stage of the project was tested after every part of it was completed and then moved on to the next one. During the course of the project I gained knowledge of Java I also gained knowledge of the raspberry pi technology and what the small computer is capable of.  - After knowing the capabilities of raspberry pi and the applications it could have in the field of robotics, and IoT it actually has made me to think of doing more research work on the raspberry pi for the robotic and IoT applications.  - The challenges that I faced during the course of the project were that of the time constrain, as I had to learn about the raspberry pi and then learn programming in Java and HTML.  - Then during the programming of the server client interfaces the problems of calling functions with a button press. One of the main challenges that No output comes when some functions are called from software side. Other than the small problem the buddy robot works fine and meets all its purpose.  - If given an opportunity to work again on the same technology i.e. the raspberry pi and IoT technology or on a project like this where the raspberry pi is used for any kind off application I would be happy to take it up.  <a id=""markdown-11-future-work"" name=""11-future-work""></a> ## 11. Future Work  In the future this raspberry pi technology can be used in various different fields of work. The buddy robot can be made autonomous with the help of more sensor, gyroscope, compass and a GPS. So that it can be set to a target or a specific area where in can monitor. The robot can also be developed into an advanced robot toy for young people. Others future works described below: -	Face recognition: All the family members face images are stored in controller when an unknown person will come at door, it will create alert and click the image and send it to user.  -	In changing the Mechanical design work using the same concept, different functions as Open the door, Turn on/off switch, bring newspaper for user, etc work can be done. -	Adding the Pneumatics design in Mechanical design robot can walk, go up and down and it will be control from anywhere at any time. "
"IoT Energy Meter with Current, Voltage and Cost Monitoring System",Internet of Things,https://github.com/h3xb1n/IoT-Based-Smart-Energy-Meter,"# IoT Based Smart Energy Meter It is a Django Web Application, in which the user can view his live power usage of electricity day-wise, month-wise etc. with the bills. We have also provided the option to download the statistics of power usage in the excel format. How this works is that the Arduino Uno is connected to the energy meter. Arduino Uno calculates the live power usage with the help of current sensors and sends it to the web application with the help of ESP8266 Wi-Fi module. The application checks the database for latest record in time and plot it on the chart in the web application."
IoT Industry Protection System Arduino,Internet of Things,https://github.com/bharathrede/IOT-Industry-Protection-using-Arduino,# IOT-Industry-Protection-using-Arduino
IoT Paralysis Patient Health Care Project,Internet of Things,https://github.com/20MH1A04H9/IOT_PROJECT," # IOT BASED AUTOMATED PARALYSIS PATIENT HEALTHCAREMONITORING SYSTEM USING ARDUINO  An IoT-based system for monitoring paralyzed patients offers real-time health tracking and improved care. It uses sensors, actuators, and communication devices to monitor vitals, movement, and even send alerts. Careful planning, including choosing the right components and designing software, is crucial. Finally, building and testing ensure the system works accurately and reliably.   ## Compounds  - [ARDUINO UNO](https://www.flipkart.com/arduino-uno-r3-board-micro-controller-electronic-hobby-kit/p/itmf7zbwgr3cffzw?pid=EHKF7XTCJQX7MBTW&lid=LSTEHKF7XTCJQX7MBTWSUMQ72&marketplace=FLIPKART&cmpid=content_electronic-hobby-kit_8965229628_gmc)  - [TEMPERATURE SENSOR](https://www.amazon.in/Generic-LM35-Temperature-sensor/dp/B0BM9GVDWM)  - [FLEX SENSORS](https://www.amazon.in/Robodo-Electronics-FSENS-Sensor-Inches/dp/B00QV9Q1SE)  - [HEARTBEAT SENSOR](https://lastminuteengineers.com/max30100-pulse-oximeter-heart-rate-sensor-arduino-tutorial/)  - [LIQUID CRYSTAL DISPLAY](https://www.amazon.in/LCD-Display-1602-16x2-interface/dp/B0CK72YTWP?source=ps-sl-shoppingads-lpcontext&ref_=fplfs&psc=1&smid=A2QXRR0CST30Y0)  - [NODEMCU](https://www.amazon.in/Lolin-NodeMCU-ESP8266-CP2102-Wireless/dp/B010O1G1ES?source=ps-sl-shoppingads-lpcontext&ref_=fplfs&psc=1&smid=AH017Z3M1ZJ3T) ## üõ† Skills C++, Python, Sensors, Embedded Systems, Computing, Networking protocols.   ## Installation  ### Getting Started with Arduino IDE 2  To install the Arduino IDE on Windows, you can do the following:  Download the latest release from the Arduino download page  Locate the downloaded file in your browser's download manager or the Downloads folder  Double-click the executable (.exe) file  Agree to the terms and conditions  Choose a destination folder  Click Install [Arduino](https://downloads.arduino.cc/arduino-ide/arduino-ide_2.3.2_Windows_64bit.msi?_gl=1*13x1alj*_ga*Mzc4NjEzODQyLjE3MTU2NDU1MzU.*_ga_NEXN8H46L5*MTcxNTY0NTUzNC4xLjEuMTcxNTY0NTY5MS4wLjAuMTcxOTY0NDM3MQ..*_fplc*VmIlMkZxYnNFNVNuQWExR1hrQlJGQzY3V3QlMkJWbTVJTThudERhN2JEM3pZdEVMdE9nZGlSa2Z2RHNGM0VieG4lMkZaNlZocHRScGdjVVMzUTN4VWFwZmZLbHRFcnYxdjlwZ1JJeVB6eHZRVDU3VnFSQWNqbkVFZEJBbnczdyUyQkgyS3clM0QlM0Q.) ## Demo  #### *<p align=""center"">[![youtube](https://t3.ftcdn.net/jpg/04/74/05/94/360_F_474059464_qldYuzxaUWEwNTtYBJ44VN89ARuFktHW.jpg)](https://youtu.be/1iEL5uyLlP4?si=mJZqtYBPXIxccThi) </p>*   ## Documentation  This is the final documentation of our [Project Report](https://github.com/20MH1A04H9/IOT_PROJECT/blob/main/report.pdf) on  IOT BASED AUTOMATED PARALYSIS PATIENT HEALTHCAREMONITORING SYSTEM USING ARDUINO    ## Authors  - [@P.PRABHAVATHI](https://github.com/PeddireddyPrabhavathi) - [@M.MANIKANTA](https://github.com/Manikanta4493) - [@V.LAKESH MOULI](https://github.com/lakeshmouli)   ## Feedback  If you have any feedback, please reach out to us at 20MH1A04H9@acoe.edu.in "
IoT Smart Mirror With News & Temperature,Internet of Things,https://github.com/aanandpant/IOT-based-smart-mirror,"# IOT based smart mirror using raspberry pi 3  ## This project was done in my final year of Bachelor's Degree.   Smart mirror is a device which shows you various information on the go like news, weather, calendar, reminders etc on the mirror. The smart mirror can be used in homes, schools, hospitals, malls and so on. The smart mirror, I have made has the ability to control electrical appliances like lights and fans using voice commands and through a web portal. The intention of this project was for personal use.  ![alt text](https://github.com/aanandpant/smart-mirror-my-final-year-computer-engineering-project/blob/master/mirror.png)  **See the mirror working (click on the picture below)**   [![IMAGE ALT smart mirror](https://img.youtube.com/vi/jIXr54udIIE/0.jpg)](https://www.youtube.com/watch?v=jIXr54udIIE)   **features:** - News - Date and time - Calender (upcoming events and holidays) - Current weather - Weather forecast - Greeting message - Voice commands to control electrical appliances - Web portal to control electrical appliances globally.   **Supported voice commands:** - lights on - lights off  - fan on - fan off  **Hardware used:** - Raspberry pi 3 model B - 2 channel relay switch - 56.7cm LED screen - Two way mirror - 4 jumper cables - Usb microphone     **Software/Programming languages used :** - Raspbian jessie with pixel os - NodeJs (main smart mirror native app) - Pocket sphinx library for voice recognition in python programming language. - Html, css, javascript, php to control electrical appliances through a web portal. - Apache web server"
IoT Garbage Monitoring With Weight Sensing,Internet of Things,https://github.com/hegdepavankumar/smart-garbage-monitoring-system-using-iot,"# Smart Garbage Monitoring System Using IOT    ![GitHub](https://img.shields.io/github/license/hegdepavankumar/smart-garbage-monitoring-system-using-iot?style=flat) ![GitHub top language](https://img.shields.io/github/languages/top/hegdepavankumar/smart-garbage-monitoring-system-using-iot?style=flat) ![GitHub last commit](https://img.shields.io/github/last-commit/hegdepavankumar/smart-garbage-monitoring-system-using-iot?style=flat) ![ViewCount](https://views.whatilearened.today/views/github/hegdepavankumar/smart-garbage-monitoring-system-using-iot.svg?cache=remove)  ## Overview  We are living in the era of Smart cities where everything is planned and systematic. The problem we are facing is the population, which is rising rapidly. In recent years, urban migration has skyrocketed. This has resulted in the rise of garbage waste everywhere. Dumping of garbage in public places creates a polluted environment in the neighborhood. It could cause several serious diseases to the people living around. This will embarrass the evaluation of the affected area. To reduce waste and maintain good hygiene, we need a systematic approach to tackle the problem.  The traditional way of manually monitoring the wastes in waste bins is a complex, cumbersome process and utilizes more human effort, time, and cost which is not compatible with the present-day technologies in any way. We propose a solution to this waste problem which manages the garbage waste smartly. This research paper proposes an IoT-based smart system based on clean waste management that assesses the level of waste on dustbins through sensory systems. In this system, the microcontroller is used as a visual connector connecting the sensor and the IoT system. This is an advanced method in which waste management is automated. This project IoT Garbage Monitoring system is a very innovative system that will help to keep the cities clean. This system monitors the garbage bins and informs about the level of garbage collected in the garbage bins via a web page. This web page also sends all information to garbage collection vehicles.  <br>  # Real-Time Implemented Images: [click here to view](https://github.com/hegdepavankumar/smart-garbage-monitoring-system-using-iot/tree/main/sample-project-images) # Project Report: [click here to download](https://github.com/user-attachments/files/15880862/Report.Content.pdf)  <br>   ## Hardware Requirements  1) ### Ulteasonic Sensor  ![image](https://user-images.githubusercontent.com/85627085/235177501-32c84273-4d46-4518-960e-3edf8aee552b.png)  <br> An ultrasonic sensor is an electronic device that measures the distance of a target object by emitting ultrasonic sound waves and converts the reflected sound into an electrical signal. Ultrasonic waves travel faster than the speed of audible sound (i.e. the sound that humans can hear). Ultrasonic sensors have two main components: the transmitter (which emits the sound using piezoelectric crystals) and the receiver (which encounters the sound after it has traveled to and from the target).  To calculate the distance between the sensor and the object, the sensor measures the time it takes between the emission of the sound by the transmitter to its contact with the receiver. The formula for this calculation is D = ¬Ω T x C (where D is the distance, T is the time, and C is the speed of sound ~ 343 meters/second). <br>  2) ### Arduino UNO R3  ![image](https://user-images.githubusercontent.com/85627085/235177985-42b9792e-5ec4-468d-8ad0-ee05d52e814b.png)  <br>  Arduino UNO is a microcontroller board based on the ATmega328P. It has 14 digital input/output pins (of which 6 can be used as PWM outputs), 6 analog inputs, a 16 MHz ceramic resonator, a USB connection, a power jack, an ICSP header, and a reset button. It contains everything needed to support the microcontroller; simply connect it to a computer with a USB cable or power it with an AC-to-DC adapter or battery to get started.  <br>  3) ### GPS Module  ![image](https://user-images.githubusercontent.com/85627085/235178820-0b356695-8667-4847-88ac-0c4257ed9e4a.png)  <br> These GPS modules are compatible with Arduino and Raspberry Pi, making it easy for you to start to try out. The Air 530 Module in Grove - GPS(Air 530) is a high-performance, highly integrated multi-mode satellite positioning and navigation module. It supports GPS / Beidou / Glonass / Galileo / QZSS / SBAS, which makes it suitable for GNSS positioning applications such as car navigation, smart wear, and drones. And Air530 module is also supports NMEA 0183 V4.1 protocol and compatible with previous versions. Meanwhile, the E-1612-UB module series of Grove - GPS Module is a family of stand-alone GPS receivers featuring the high-performance u-blox 5 positioning engine. The 50-channel u-blox 5 positioning engine boasts a Time-To-First-Fix ( TTFF ) of under 1 second. The dedicated acquisition engine, with over 1 million correlators, is capable of massive parallel time/frequency space searches, enabling it to find satellites instantly.  <br> <br>  4) ### GSM/GPRS Module <br> <br>  ![image](https://user-images.githubusercontent.com/85627085/235181834-9da83f7b-62f2-4f13-a14d-0e01c9c88718.png)    <br>   - What is GSM? <br>   GSM (Global System for Mobile Communications, originally Groupe Sp√©cial Mobile), is a standard developed by the European Telecommunications Standards Institute (ETSI). It was created to describe the protocols for second-generation (2G) digital cellular networks used by mobile phones and is now the default global standard for mobile communications ‚Äì with over 90% market share, operating in over 219 countries and territories. <br>   - What is GPRS? <br>   General Packet Radio Service (GPRS) is a packet-oriented mobile data service on the 2G and 3G cellular communication system‚Äôs global system for mobile communications (GSM). GPRS was originally standardized by the European Telecommunications Standards Institute (ETSI) in response to the earlier CDPD and i-mode packet-switched cellular technologies. It is now maintained by the 3rd Generation Partnership Project (3GPP).   <br> <br>    5) ### Buzzer  <br>  ![image](https://user-images.githubusercontent.com/85627085/235182600-a0037ef0-0a29-450b-9c12-2e72f58e3903.png)  <br>  A buzzer or beeper is an audio signaling device, which may be mechanical, electromechanical, or piezoelectric (piezo for short). Typical uses of buzzers and beepers include alarm devices, timers, training and confirmation of user input such as a mouse click or keystroke.  <br>   6) ### Connecting Wires  <br>  ![image](https://user-images.githubusercontent.com/85627085/235187067-c6bc9a3e-2112-49f5-962f-e3123f8fb0d5.png)  <br>  A connecting wire allows the electric current from one point to another point without resistivity. The resistance of the connecting wire should always be near zero. Copper wires have low resistance and are therefore suitable for low resistance.   <br>   7) ### NodeMCU(Node MicroController Unit)  <br>  ![image](https://user-images.githubusercontent.com/85627085/235187953-d709a102-3247-4f2b-bbe8-c1292dd6dff6.png)  <br>  NodeMCU is an open-source firmware for which open-source prototyping board designs are available. The name ""NodeMCU"" combines ""node"" and ""MCU"" (micro-controller unit). Strictly speaking, the term ""NodeMCU"" refers to the firmware rather than the associated development kits. NodeMCU was created shortly after the ESP8266 came out. On December 30, 2013, Espressif Systems began production of the ESP8266.NodeMCU started on 13 Oct 2014, when Hong committed the first file of nodemcu-firmware to GitHub.Two months later, the project expanded to include an open-hardware platform when developer Huang R committed the Gerber file of an ESP8266 board, named devkit v0.9.  <br>  8) ### 16x2 LCD  <br>    ![image](https://user-images.githubusercontent.com/85627085/235443559-a2a7fdfc-966e-4357-b004-9edb3c93a655.png)   <br> The Liquid Crystal library allows you to control LCDs that are compatible with the Hitachi HD44780 driver. There are many of them out there, and you can usually tell them by the 16-pin interface. The LCDs have a parallel interface, meaning that the microcontroller has to manipulate several interface pins at once to control the display.  <br>  ## Software Requirements    - Windows 7/10/11 OS with Min 4GB RAM and 250GB Hard Disk <br>   - [Arduino IDE](https://www.arduino.cc/en/software) <br>   - Local Server and web Page for Monitoring <br>  ## Implementation & Testing    - Sketch (Fritzing) <br>      ![image](https://user-images.githubusercontent.com/85627085/235192027-edc61f5f-6932-4436-9cac-e4d0db5209d3.png)      <br>      The above diagram shows a sketch of connection devices or sensors using Fritzing software. This figure shows an ultrasonic sensor connected with Node MCU and to the Cytron Uno or Arduino Uno. An ultrasonic sensor will read the distance of the garbage and compare it with the bin depth. This sketch is one of the important parts of the Garbage Monitoring System using IoT.         <br>      ## Source Code     1) ### Code for NodeMCU    <br>    ```   #include <ESP8266WiFi.h> const char* ssid = ""Pavankumar""; //ssid of your wifi  // Mavayya-5G const char* password = ""12345678""; //password of your wifi WiFiServer server(80); //////////////////////////////////// #include <Arduino_JSON.h> String inputData = """"; boolean data_complete = false; String vala; String valb; String valc; String vald; /////////////////////////////////////  void setup()  {   Serial.begin(115200);   inputData.reserve(200);   Serial.println(""Hello"");   Serial.println();   Serial.print(""Connecting to "");   Serial.println(ssid);   WiFi.begin(ssid, password); //connecting to wifi   while (WiFi.status() != WL_CONNECTED)// while wifi not connected   {     delay(500);     Serial.print("".""); //print ""....""   }   Serial.println("""");   Serial.println(""WiFi connected"");   server.begin();   Serial.println(""Server started"");   Serial.println(WiFi.localIP());  // Print the IP address } void loop() {   while(Serial.available() > 0)   {    char inChar = Serial.read();    if( inChar == '\r')    {     inputData = """";    }    else if(inChar == '\n')    {     data_complete = true;    }    else    {     inputData+=inChar;    }   }    if(data_complete)   {    data_complete = false;    Serial.println(inputData);    demoParse();   }   //int a=vala.toInt();     WiFiClient client = server.available(); // Check if a client has connected  /* if (!client)   {    return;   }   */    String s = ""HTTP/1.1 200 OK\r\nContent-Type: text/html\r\n\r\n <!DOCTYPE html> <html> <head> <title>..........</title> <style>"";   s += ""a:link {background-color: RED;text-decoration: none;}"";   s += ""table, th, td </style> </head> <body> <h1  style="";   s += ""font-size:250%;"";   s += "" ALIGN=CENTER> Dustbin data</h1>"";   s += ""<p ALIGN=CENTER style=""""font-size:200%;"""""";   s += ""> <b> Location -001</b></p> <table ALIGN=CENTER style="";   s += ""width:10%"";   s += ""> <tr> <th>Level : </th>"";   s += ""<td ALIGN=CENTER >"";   s += vala;   //s += ""</td> </tr> <tr> <th>Tds Value : </th> <td ALIGN=CENTER >"";   //s += valb;   //s += ""</td> </tr> <tr>  <th>Water Level</th> <td ALIGN=CENTER >"";   //s += valc;   //s += ""</td></tr> <tr> <th>Water intake</th> <td ALIGN=CENTER >"";   //s += vald;   s += ""</td>  </tr> </table> "";   s += ""</body> </html>"";   client.print(s); // all the values are send to the webpage   delay(100); } void demoParse()   {   Serial.println(""parse"");   Serial.println(vala);   Serial.println(""====="");   JSONVar myObject = JSON.parse(inputData);       if (JSON.typeof(myObject) == ""undefined"")    {    Serial.println(""Parsing input failed!"");    return;   }   Serial.print(""JSON.typeof(myObject) = "");   Serial.println(JSON.typeof(myObject)); // prints: object   // myObject.hasOwnProperty(key) checks if the object contains an entry for key   if (myObject.hasOwnProperty(""anloga"")) {   Serial.print(""myObject[\""anloga\""] = "");   vala = (const char*) myObject[""anloga""];//to get value in  vala   Serial.println(vala);   Serial.println((const char*) myObject[""anloga""]);  }   if (myObject.hasOwnProperty(""anlogb"")) {     Serial.print(""myObject[\""anlogb\""] = "");     valb = (const char*) myObject[""anlogb""];     Serial.println(valb);      Serial.println((const char*) myObject[""anlogb""]);   }   if (myObject.hasOwnProperty(""anlogc"")) {     Serial.print(""myObject[\""anlogc\""] = "");     valc = (const char*) myObject[""anlogc""];     Serial.println(valc);      Serial.println((const char*) myObject[""anlogc""]);   }   if (myObject.hasOwnProperty(""anlogd"")) {     Serial.print(""myObject[\""anlogd\""] = "");     vald = (const char*) myObject[""anlogd""];     Serial.println(vald);      Serial.println((const char*) myObject[""anlogd""]);   }   // JSON vars can be printed using print or println   Serial.print(""myObject = "");   Serial.println(myObject);   Serial.println(); }   ```    2) ### Code for Arduino UNO R3  <br>  ``` #include <SoftwareSerial.h> SoftwareSerial esp8266(2,3);  #include <LiquidCrystal.h> LiquidCrystal lcd(8,9,10,11,12,13);//RS,EN,D4,D5,D6,D7 #include <Servo.h> #define buzzer 4 #define trigPin1 A4  //// front #define echoPin1 A5 int lvl1=0; long duration, distance,sensor1,sensor2,sensor3; // us variable int onetime=0,onetime1=0 ; int wet=0,moisture=0,object=0,cabin2=0,c1=0,c2=0; int powers=0,powers1=0,powers2=0,powers3=0; void setup()  {  Serial.begin(115200);  esp8266.begin(9600);  lcd.begin(16, 2);//initializing LCD  lcd.setCursor(0,0);   lcd.print(""Automatic WASTE"");  delay(3000);  pinMode(buzzer,OUTPUT);  pinMode(trigPin1, OUTPUT);  pinMode(echoPin1, INPUT);  delay(3000); } void loop() {  ultrasensor(trigPin1, echoPin1);  sensor1 = distance;    delay(10);  esp8266.println(sensor1);  lvl1=(20-sensor1)*7;  esp8266.println(lvl1);  if(lvl1<0){lvl1=0;}  if(lvl1>100){lvl1=100;}  lcd.clear();  lcd.setCursor(0,0);   lcd.print(""Dustbin Level"");  lcd.setCursor(6,1);   lcd.print(lvl1);  delay(1000);  if(lvl1>70)  {         if(onetime==0)   {     lcd.clear();    lcd.setCursor(0,0);     lcd.print(""-send msg-"");    digitalWrite(buzzer,HIGH);     tracking();     digitalWrite(buzzer,LOW);    onetime=1;   }  }    else  {   onetime=0;  }  ////////////////////////////////////////////////  String data = """";  data+= ""{"";  data+= ""\""anloga\"":"";  data+= ""\""""+String(lvl1)+""\"","";  data+= ""\""anlogb\"":"";  data+= ""\""""+String(powers)+""\"","";  data+= ""\""anlogc\"":"";  data+= ""\""""+String(powers1)+""\"","";  data+= ""\""anlogd\"":"";  data+= ""\""""+String(powers2)+""\"""";  data+= ""}"";  Serial.print('\r');  Serial.print(data);  delay(10);  Serial.print('\n');  delay(200);  ///////////////////////////////////////////////    }  void init_sms()  {   esp8266.println(""AT+CMGF=1"");   delay(400);   esp8266.println(""AT+CMGS=\""+919X083X52XX\"""");   // use your 10 digit cell no. here //   delay(400);  }  void init_sms1()  {   esp8266.println(""AT+CMGF=1"");   delay(400);   esp8266.println(""AT+CMGS=\""+918XX227XX8X\"""");   // use your 10 digit cell no. here   delay(400);  }    void send_data(String message)  {   esp8266.println(message);   delay(200);  }    void send_sms()  {   esp8266.write(26);  }   void tracking()  {   init_sms();   send_data(""dustbin-001  is almost full:\n"");   send_sms();   delay(6000);   init_sms1();   send_data(""dustbin-001  is almost full:\n"");   esp8266.print("" Level in %"");    esp8266.print(lvl1);   send_sms();   delay(6000);  }  void ultrasensor(int trigPin,int echoPin)  {    digitalWrite(trigPin, LOW);  // Added this line   delayMicroseconds(2); // Added this line   digitalWrite(trigPin, HIGH);   delayMicroseconds(10); // Added this line   digitalWrite(trigPin, LOW);   duration = pulseIn(echoPin, HIGH);   distance = (duration/2) / 29.1;  }  ``` ## Instructions/Setup  - Download and install Arduino IDE - Make all the necessary connections - Compile and upload the code to the board <br>  ## Conclusion  While completing this project proposal, there are a few constraints that come up. First, the reading of sensors is less accurate, and need to be extra careful in handling the sensors. Second, notification to the mobile phone cannot be done because the cloud platform used is an open-source platform and cannot be used to send an alert or notification to the users. Development of the system needs to be done thoroughly to decrease the possibility of errors. However, added value has been added to this system to make the system more reliable. Normalization is being used to eliminate the outliers which can help to increase the accuracy of the distance reads by an ultrasonic sensor.     ## Contributing Contributions are what make the open-source community such an amazing place to learn, inspire, and create. Any contributions you make are greatly appreciated.  1. Fork the Project 2. Create your Feature Branch (git checkout -b feature/AmazingFeature) 3. Commit your changes (git commit -m 'Add some AmazingFeature') 4. Push to the Branch (git push origin feature/AmazingFeature) 5. Open a Pull Request  Pull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.   ## Creators [üîù](# smart-garbage-monitoring-system-using-iot)  This Project is Created by:-    - [Pavankumar Hegde](https://github.com/hegdepavankumar) [Team Leader]   - [Sushil Kumar Sah](https://github.com/sushilsah)   - [Safina Fathima](https://github.com/safinafab)   - [Santhosh Reddy](https://github.com/)    <br> <h3 align=""center"">Show some &nbsp;‚ù§Ô∏è&nbsp; by starring some of the repositories!</h3> <br>    <!-- Support Me -->     if you like what I do, maybe consider buying me a coffee ü•∫üëâüëà  <a href=""https://www.buymeacoffee.com/hegdepavankumar"" target=""_blank""><img src=""https://cdn.buymeacoffee.com/buttons/v2/default-red.png"" alt=""Buy Me A Coffee"" width=""150"" ></a>   "
IoT Garbage Monitoring With Weight Sensing,Internet of Things,https://github.com/hegdepavankumar/smart-garbage-monitoring-system-using-iot,"# Smart Garbage Monitoring System Using IOT    ![GitHub](https://img.shields.io/github/license/hegdepavankumar/smart-garbage-monitoring-system-using-iot?style=flat) ![GitHub top language](https://img.shields.io/github/languages/top/hegdepavankumar/smart-garbage-monitoring-system-using-iot?style=flat) ![GitHub last commit](https://img.shields.io/github/last-commit/hegdepavankumar/smart-garbage-monitoring-system-using-iot?style=flat) ![ViewCount](https://views.whatilearened.today/views/github/hegdepavankumar/smart-garbage-monitoring-system-using-iot.svg?cache=remove)  ## Overview  We are living in the era of Smart cities where everything is planned and systematic. The problem we are facing is the population, which is rising rapidly. In recent years, urban migration has skyrocketed. This has resulted in the rise of garbage waste everywhere. Dumping of garbage in public places creates a polluted environment in the neighborhood. It could cause several serious diseases to the people living around. This will embarrass the evaluation of the affected area. To reduce waste and maintain good hygiene, we need a systematic approach to tackle the problem.  The traditional way of manually monitoring the wastes in waste bins is a complex, cumbersome process and utilizes more human effort, time, and cost which is not compatible with the present-day technologies in any way. We propose a solution to this waste problem which manages the garbage waste smartly. This research paper proposes an IoT-based smart system based on clean waste management that assesses the level of waste on dustbins through sensory systems. In this system, the microcontroller is used as a visual connector connecting the sensor and the IoT system. This is an advanced method in which waste management is automated. This project IoT Garbage Monitoring system is a very innovative system that will help to keep the cities clean. This system monitors the garbage bins and informs about the level of garbage collected in the garbage bins via a web page. This web page also sends all information to garbage collection vehicles.  <br>  # Real-Time Implemented Images: [click here to view](https://github.com/hegdepavankumar/smart-garbage-monitoring-system-using-iot/tree/main/sample-project-images) # Project Report: [click here to download](https://github.com/user-attachments/files/15880862/Report.Content.pdf)  <br>   ## Hardware Requirements  1) ### Ulteasonic Sensor  ![image](https://user-images.githubusercontent.com/85627085/235177501-32c84273-4d46-4518-960e-3edf8aee552b.png)  <br> An ultrasonic sensor is an electronic device that measures the distance of a target object by emitting ultrasonic sound waves and converts the reflected sound into an electrical signal. Ultrasonic waves travel faster than the speed of audible sound (i.e. the sound that humans can hear). Ultrasonic sensors have two main components: the transmitter (which emits the sound using piezoelectric crystals) and the receiver (which encounters the sound after it has traveled to and from the target).  To calculate the distance between the sensor and the object, the sensor measures the time it takes between the emission of the sound by the transmitter to its contact with the receiver. The formula for this calculation is D = ¬Ω T x C (where D is the distance, T is the time, and C is the speed of sound ~ 343 meters/second). <br>  2) ### Arduino UNO R3  ![image](https://user-images.githubusercontent.com/85627085/235177985-42b9792e-5ec4-468d-8ad0-ee05d52e814b.png)  <br>  Arduino UNO is a microcontroller board based on the ATmega328P. It has 14 digital input/output pins (of which 6 can be used as PWM outputs), 6 analog inputs, a 16 MHz ceramic resonator, a USB connection, a power jack, an ICSP header, and a reset button. It contains everything needed to support the microcontroller; simply connect it to a computer with a USB cable or power it with an AC-to-DC adapter or battery to get started.  <br>  3) ### GPS Module  ![image](https://user-images.githubusercontent.com/85627085/235178820-0b356695-8667-4847-88ac-0c4257ed9e4a.png)  <br> These GPS modules are compatible with Arduino and Raspberry Pi, making it easy for you to start to try out. The Air 530 Module in Grove - GPS(Air 530) is a high-performance, highly integrated multi-mode satellite positioning and navigation module. It supports GPS / Beidou / Glonass / Galileo / QZSS / SBAS, which makes it suitable for GNSS positioning applications such as car navigation, smart wear, and drones. And Air530 module is also supports NMEA 0183 V4.1 protocol and compatible with previous versions. Meanwhile, the E-1612-UB module series of Grove - GPS Module is a family of stand-alone GPS receivers featuring the high-performance u-blox 5 positioning engine. The 50-channel u-blox 5 positioning engine boasts a Time-To-First-Fix ( TTFF ) of under 1 second. The dedicated acquisition engine, with over 1 million correlators, is capable of massive parallel time/frequency space searches, enabling it to find satellites instantly.  <br> <br>  4) ### GSM/GPRS Module <br> <br>  ![image](https://user-images.githubusercontent.com/85627085/235181834-9da83f7b-62f2-4f13-a14d-0e01c9c88718.png)    <br>   - What is GSM? <br>   GSM (Global System for Mobile Communications, originally Groupe Sp√©cial Mobile), is a standard developed by the European Telecommunications Standards Institute (ETSI). It was created to describe the protocols for second-generation (2G) digital cellular networks used by mobile phones and is now the default global standard for mobile communications ‚Äì with over 90% market share, operating in over 219 countries and territories. <br>   - What is GPRS? <br>   General Packet Radio Service (GPRS) is a packet-oriented mobile data service on the 2G and 3G cellular communication system‚Äôs global system for mobile communications (GSM). GPRS was originally standardized by the European Telecommunications Standards Institute (ETSI) in response to the earlier CDPD and i-mode packet-switched cellular technologies. It is now maintained by the 3rd Generation Partnership Project (3GPP).   <br> <br>    5) ### Buzzer  <br>  ![image](https://user-images.githubusercontent.com/85627085/235182600-a0037ef0-0a29-450b-9c12-2e72f58e3903.png)  <br>  A buzzer or beeper is an audio signaling device, which may be mechanical, electromechanical, or piezoelectric (piezo for short). Typical uses of buzzers and beepers include alarm devices, timers, training and confirmation of user input such as a mouse click or keystroke.  <br>   6) ### Connecting Wires  <br>  ![image](https://user-images.githubusercontent.com/85627085/235187067-c6bc9a3e-2112-49f5-962f-e3123f8fb0d5.png)  <br>  A connecting wire allows the electric current from one point to another point without resistivity. The resistance of the connecting wire should always be near zero. Copper wires have low resistance and are therefore suitable for low resistance.   <br>   7) ### NodeMCU(Node MicroController Unit)  <br>  ![image](https://user-images.githubusercontent.com/85627085/235187953-d709a102-3247-4f2b-bbe8-c1292dd6dff6.png)  <br>  NodeMCU is an open-source firmware for which open-source prototyping board designs are available. The name ""NodeMCU"" combines ""node"" and ""MCU"" (micro-controller unit). Strictly speaking, the term ""NodeMCU"" refers to the firmware rather than the associated development kits. NodeMCU was created shortly after the ESP8266 came out. On December 30, 2013, Espressif Systems began production of the ESP8266.NodeMCU started on 13 Oct 2014, when Hong committed the first file of nodemcu-firmware to GitHub.Two months later, the project expanded to include an open-hardware platform when developer Huang R committed the Gerber file of an ESP8266 board, named devkit v0.9.  <br>  8) ### 16x2 LCD  <br>    ![image](https://user-images.githubusercontent.com/85627085/235443559-a2a7fdfc-966e-4357-b004-9edb3c93a655.png)   <br> The Liquid Crystal library allows you to control LCDs that are compatible with the Hitachi HD44780 driver. There are many of them out there, and you can usually tell them by the 16-pin interface. The LCDs have a parallel interface, meaning that the microcontroller has to manipulate several interface pins at once to control the display.  <br>  ## Software Requirements    - Windows 7/10/11 OS with Min 4GB RAM and 250GB Hard Disk <br>   - [Arduino IDE](https://www.arduino.cc/en/software) <br>   - Local Server and web Page for Monitoring <br>  ## Implementation & Testing    - Sketch (Fritzing) <br>      ![image](https://user-images.githubusercontent.com/85627085/235192027-edc61f5f-6932-4436-9cac-e4d0db5209d3.png)      <br>      The above diagram shows a sketch of connection devices or sensors using Fritzing software. This figure shows an ultrasonic sensor connected with Node MCU and to the Cytron Uno or Arduino Uno. An ultrasonic sensor will read the distance of the garbage and compare it with the bin depth. This sketch is one of the important parts of the Garbage Monitoring System using IoT.         <br>      ## Source Code     1) ### Code for NodeMCU    <br>    ```   #include <ESP8266WiFi.h> const char* ssid = ""Pavankumar""; //ssid of your wifi  // Mavayya-5G const char* password = ""12345678""; //password of your wifi WiFiServer server(80); //////////////////////////////////// #include <Arduino_JSON.h> String inputData = """"; boolean data_complete = false; String vala; String valb; String valc; String vald; /////////////////////////////////////  void setup()  {   Serial.begin(115200);   inputData.reserve(200);   Serial.println(""Hello"");   Serial.println();   Serial.print(""Connecting to "");   Serial.println(ssid);   WiFi.begin(ssid, password); //connecting to wifi   while (WiFi.status() != WL_CONNECTED)// while wifi not connected   {     delay(500);     Serial.print("".""); //print ""....""   }   Serial.println("""");   Serial.println(""WiFi connected"");   server.begin();   Serial.println(""Server started"");   Serial.println(WiFi.localIP());  // Print the IP address } void loop() {   while(Serial.available() > 0)   {    char inChar = Serial.read();    if( inChar == '\r')    {     inputData = """";    }    else if(inChar == '\n')    {     data_complete = true;    }    else    {     inputData+=inChar;    }   }    if(data_complete)   {    data_complete = false;    Serial.println(inputData);    demoParse();   }   //int a=vala.toInt();     WiFiClient client = server.available(); // Check if a client has connected  /* if (!client)   {    return;   }   */    String s = ""HTTP/1.1 200 OK\r\nContent-Type: text/html\r\n\r\n <!DOCTYPE html> <html> <head> <title>..........</title> <style>"";   s += ""a:link {background-color: RED;text-decoration: none;}"";   s += ""table, th, td </style> </head> <body> <h1  style="";   s += ""font-size:250%;"";   s += "" ALIGN=CENTER> Dustbin data</h1>"";   s += ""<p ALIGN=CENTER style=""""font-size:200%;"""""";   s += ""> <b> Location -001</b></p> <table ALIGN=CENTER style="";   s += ""width:10%"";   s += ""> <tr> <th>Level : </th>"";   s += ""<td ALIGN=CENTER >"";   s += vala;   //s += ""</td> </tr> <tr> <th>Tds Value : </th> <td ALIGN=CENTER >"";   //s += valb;   //s += ""</td> </tr> <tr>  <th>Water Level</th> <td ALIGN=CENTER >"";   //s += valc;   //s += ""</td></tr> <tr> <th>Water intake</th> <td ALIGN=CENTER >"";   //s += vald;   s += ""</td>  </tr> </table> "";   s += ""</body> </html>"";   client.print(s); // all the values are send to the webpage   delay(100); } void demoParse()   {   Serial.println(""parse"");   Serial.println(vala);   Serial.println(""====="");   JSONVar myObject = JSON.parse(inputData);       if (JSON.typeof(myObject) == ""undefined"")    {    Serial.println(""Parsing input failed!"");    return;   }   Serial.print(""JSON.typeof(myObject) = "");   Serial.println(JSON.typeof(myObject)); // prints: object   // myObject.hasOwnProperty(key) checks if the object contains an entry for key   if (myObject.hasOwnProperty(""anloga"")) {   Serial.print(""myObject[\""anloga\""] = "");   vala = (const char*) myObject[""anloga""];//to get value in  vala   Serial.println(vala);   Serial.println((const char*) myObject[""anloga""]);  }   if (myObject.hasOwnProperty(""anlogb"")) {     Serial.print(""myObject[\""anlogb\""] = "");     valb = (const char*) myObject[""anlogb""];     Serial.println(valb);      Serial.println((const char*) myObject[""anlogb""]);   }   if (myObject.hasOwnProperty(""anlogc"")) {     Serial.print(""myObject[\""anlogc\""] = "");     valc = (const char*) myObject[""anlogc""];     Serial.println(valc);      Serial.println((const char*) myObject[""anlogc""]);   }   if (myObject.hasOwnProperty(""anlogd"")) {     Serial.print(""myObject[\""anlogd\""] = "");     vald = (const char*) myObject[""anlogd""];     Serial.println(vald);      Serial.println((const char*) myObject[""anlogd""]);   }   // JSON vars can be printed using print or println   Serial.print(""myObject = "");   Serial.println(myObject);   Serial.println(); }   ```    2) ### Code for Arduino UNO R3  <br>  ``` #include <SoftwareSerial.h> SoftwareSerial esp8266(2,3);  #include <LiquidCrystal.h> LiquidCrystal lcd(8,9,10,11,12,13);//RS,EN,D4,D5,D6,D7 #include <Servo.h> #define buzzer 4 #define trigPin1 A4  //// front #define echoPin1 A5 int lvl1=0; long duration, distance,sensor1,sensor2,sensor3; // us variable int onetime=0,onetime1=0 ; int wet=0,moisture=0,object=0,cabin2=0,c1=0,c2=0; int powers=0,powers1=0,powers2=0,powers3=0; void setup()  {  Serial.begin(115200);  esp8266.begin(9600);  lcd.begin(16, 2);//initializing LCD  lcd.setCursor(0,0);   lcd.print(""Automatic WASTE"");  delay(3000);  pinMode(buzzer,OUTPUT);  pinMode(trigPin1, OUTPUT);  pinMode(echoPin1, INPUT);  delay(3000); } void loop() {  ultrasensor(trigPin1, echoPin1);  sensor1 = distance;    delay(10);  esp8266.println(sensor1);  lvl1=(20-sensor1)*7;  esp8266.println(lvl1);  if(lvl1<0){lvl1=0;}  if(lvl1>100){lvl1=100;}  lcd.clear();  lcd.setCursor(0,0);   lcd.print(""Dustbin Level"");  lcd.setCursor(6,1);   lcd.print(lvl1);  delay(1000);  if(lvl1>70)  {         if(onetime==0)   {     lcd.clear();    lcd.setCursor(0,0);     lcd.print(""-send msg-"");    digitalWrite(buzzer,HIGH);     tracking();     digitalWrite(buzzer,LOW);    onetime=1;   }  }    else  {   onetime=0;  }  ////////////////////////////////////////////////  String data = """";  data+= ""{"";  data+= ""\""anloga\"":"";  data+= ""\""""+String(lvl1)+""\"","";  data+= ""\""anlogb\"":"";  data+= ""\""""+String(powers)+""\"","";  data+= ""\""anlogc\"":"";  data+= ""\""""+String(powers1)+""\"","";  data+= ""\""anlogd\"":"";  data+= ""\""""+String(powers2)+""\"""";  data+= ""}"";  Serial.print('\r');  Serial.print(data);  delay(10);  Serial.print('\n');  delay(200);  ///////////////////////////////////////////////    }  void init_sms()  {   esp8266.println(""AT+CMGF=1"");   delay(400);   esp8266.println(""AT+CMGS=\""+919X083X52XX\"""");   // use your 10 digit cell no. here //   delay(400);  }  void init_sms1()  {   esp8266.println(""AT+CMGF=1"");   delay(400);   esp8266.println(""AT+CMGS=\""+918XX227XX8X\"""");   // use your 10 digit cell no. here   delay(400);  }    void send_data(String message)  {   esp8266.println(message);   delay(200);  }    void send_sms()  {   esp8266.write(26);  }   void tracking()  {   init_sms();   send_data(""dustbin-001  is almost full:\n"");   send_sms();   delay(6000);   init_sms1();   send_data(""dustbin-001  is almost full:\n"");   esp8266.print("" Level in %"");    esp8266.print(lvl1);   send_sms();   delay(6000);  }  void ultrasensor(int trigPin,int echoPin)  {    digitalWrite(trigPin, LOW);  // Added this line   delayMicroseconds(2); // Added this line   digitalWrite(trigPin, HIGH);   delayMicroseconds(10); // Added this line   digitalWrite(trigPin, LOW);   duration = pulseIn(echoPin, HIGH);   distance = (duration/2) / 29.1;  }  ``` ## Instructions/Setup  - Download and install Arduino IDE - Make all the necessary connections - Compile and upload the code to the board <br>  ## Conclusion  While completing this project proposal, there are a few constraints that come up. First, the reading of sensors is less accurate, and need to be extra careful in handling the sensors. Second, notification to the mobile phone cannot be done because the cloud platform used is an open-source platform and cannot be used to send an alert or notification to the users. Development of the system needs to be done thoroughly to decrease the possibility of errors. However, added value has been added to this system to make the system more reliable. Normalization is being used to eliminate the outliers which can help to increase the accuracy of the distance reads by an ultrasonic sensor.     ## Contributing Contributions are what make the open-source community such an amazing place to learn, inspire, and create. Any contributions you make are greatly appreciated.  1. Fork the Project 2. Create your Feature Branch (git checkout -b feature/AmazingFeature) 3. Commit your changes (git commit -m 'Add some AmazingFeature') 4. Push to the Branch (git push origin feature/AmazingFeature) 5. Open a Pull Request  Pull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.   ## Creators [üîù](# smart-garbage-monitoring-system-using-iot)  This Project is Created by:-    - [Pavankumar Hegde](https://github.com/hegdepavankumar) [Team Leader]   - [Sushil Kumar Sah](https://github.com/sushilsah)   - [Safina Fathima](https://github.com/safinafab)   - [Santhosh Reddy](https://github.com/)    <br> <h3 align=""center"">Show some &nbsp;‚ù§Ô∏è&nbsp; by starring some of the repositories!</h3> <br>    <!-- Support Me -->     if you like what I do, maybe consider buying me a coffee ü•∫üëâüëà  <a href=""https://www.buymeacoffee.com/hegdepavankumar"" target=""_blank""><img src=""https://cdn.buymeacoffee.com/buttons/v2/default-red.png"" alt=""Buy Me A Coffee"" width=""150"" ></a>   "
IoT Asset tracking System,Internet of Things,https://github.com/redhat-iot/iot-assettracking-demo,"Red Hat Fleet Telematics & Asset Tracking IoT Demo ================================================== This is an example IoT demo showing a realtime updating dashboard of data streaming from an IoT gateway device (based on Eclipse Kura) through an Eclipse Kapua-based instance.  It demonstrates realtime fleet telematics, package tracking, alerting, and a telemetry dashboard showing critical measurements of packages in transit, including temperature, humidity, displacement, light levels, etc.  ![Dashboard Screenshot](docs/screenshots/fleet.png ""Dashboard Screenshot"") ![Dashboard Screenshot](docs/screenshots/exec.png ""Exec Dashboard Screenshot"")  Technologies used:  - [Eclipse Kapua](http://www.eclipse.org/kapua/) - [AngularJS](http://angularjs.org) - [Patternfly](http://patternfly.org) - [JBoss Middleware](https://www.redhat.com/en/technologies/jboss-middleware) (EAP, JDG, and more to come)  Running on OpenShift --------------------  The demo deploys as an Angular.js app running on a Node.js runtime, along with JBoss Data Grid and a Data Grid proxy component that properly handles browser-based REST requests and relays to JBoss Data Grid via the Hotrod protocol.  Eclipse Kapua is also deployed and acts as the IoT cloud management layer.  Follow these steps to build and run the demo:  1. Install and have access to an [OpenShift Container Platform](https://www.openshift.com/container-platform/) 3.4 or later or [OpenShift Origin](https://www.openshift.org/) 1.4 or later. You must be able to use the `oc` command line tool.  2. Clone this repo ``` git clone https://github.com/redhat-iot/summit2017 cd summit2017 ```  3. Issue the following commands to create a new OpenShift project and deploy the demo components: ``` oc new-project redhat-iot --display-name=""Red Hat IoT Demo"" oc policy add-role-to-user view system:serviceaccount:$(oc project -q):default -n $(oc project -q) ./openshift-deploy.sh ```  You can monitor the build with `oc status` or watch the deployments using the OpenShift web console.  If you see some components with ""No Deployments"" or are not building, you may need to add imagestream definitions for ``wildfly`` and ``jboss-datagrid``. To do so, run these commands:  ``` oc login -u system:admin (Or login with any userid that has cluster-admin privileges, TODO: Explain all options here) oc create -n openshift -f https://raw.githubusercontent.com/jboss-openshift/application-templates/master/jboss-image-streams.json oc create -n openshift -f https://raw.githubusercontent.com/openshift/origin/master/examples/image-streams/image-streams-centos7.json ```  Once everything is up and running, you can access the demo using the URL of the `dashboard` route, for example `http://dashboard-redhat-iot.domain`  Confirm that all the components are running successfully:  ``` oc get pods --show-all=false ``` You should see the following pods and their status:  |NAME                 |   READY     | STATUS  | |---------------------|:-----------:|:-------:| |dashboard-1-xxx      |    1/1      | Running | |datastore-1-xxx      |    1/1      | Running | |datastore-proxy-1-xxx|    1/1      | Running | |elasticsearch-1-xxx  |    1/1      | Running | |kapua-api-1-wc1l7    |    1/1      | Running | |kapua-broker-1-xxx   |    1/1      | Running | |kapua-console-1-xxx  |    1/1      | Running | |simulator-1-xxx      |    1/1      | Running | |sql-1-xxx            |    1/1      | Running |  Eclipse Kapua API Documentation ------------------------------- Eclipse Kapua exposes a REST API which can be used to access Kapua data and invoke Kapua operations. The REST API application is running as a dedicated Java process.  For example, to use the `curl` command login to Eclipse Kapua and retrieve an authentication token:  ``` curl -X POST --header 'Content-Type: application/json' --header 'Accept: application/json' -d '{""password"": [""your_password""], ""username"": ""your_username""}' 'http://api-redhat-iot.domain/v1/authentication/user' ```  Once logged in, the retrieved token can be passed for future API calls, e.g.  ``` curl -X GET --header 'Content-Type: application/json' --header 'Accept: application/json' --header 'Authorization: Bearer <AUTH_TOKEN_HERE>' http://api-redhat-iot.domain/v1/my_scope/devices ```  The complete API documentation can accessed using the URL of the `api` route, for example `http://api-redhat-iot.domain/doc`. More information on the REST API can be found in the [Eclipse Kapua user guide](http://download.eclipse.org/kapua/docs/develop/user-manual/en/rest.html).   Add template to ""Add to project"" -------------------------------- The following command will add the template and the options to the ""Add to project"" screen in the  ""Other"" section. The template will deploy with defaults the same as it does using the scripts above. ``` oc create -f iot-demo.yml ```  Options ------- The template contains various optional parameters that can be specified when deploying the components:  ``` oc process -f openshift-template.yaml OPTION1=value OPTION2=value | oc create -f - ```  * `MAVEN_MIRROR_URL` - To speed up Maven-based builds * `GOOGLE_MAPS_API_KEY` - for your personal Google Maps API key * `GIT_URI` and `GIT_REF` - overrides where source code is pulled (e.g. using your own personal fork) * `IMAGE_VERSION` - Docker image tag to use when pulling Kapua (default `latest`) * `DOCKER_ACCOUNT` - Name of docker account to use when pulling Kapua (default: `redhatiot`)  There are other options in the template that can be overridden if you know what you are doing!  Uninstalling and cleaning up project ------------------------------------ ``` oc delete all --all -n redhat-iot && oc delete configmap hawkular-openshift-agent-kapua data-simulator-config -n redhat-iot ``` This will delete everything but the project ""Red Hat IoT"". This is suitable for testing new scripts, template, etc."
IoT Based ICU Patient Monitoring System Biometric Attendance System Over IOT,Internet of Things,https://github.com/YAGNESHPALLERLA/IoT-Based-ICU-Patient-Monitor-System,"# IoT Based ICU Patient Monitoring System #  ## Overview ##  The IoT Based ICU Patient Monitoring System is designed to continuously monitor critical health parameters of ICU patients, such as blood pressure, heart rate, and body temperature. This data is transmitted in real-time to a web interface, allowing doctors to monitor patients remotely.  **Components**: - Raspberry Pi: The main processing unit. - Blood Pressure Sensor: Measures the patient's blood pressure. - Heart Rate Sensor: Monitors the heart rate. - Temperature Sensor: Records body temperature. - LCD Display: Displays the readings. - WiFi Module: Transmits data to the web interface.  **Features**: - Real-time Monitoring: Continuously tracks vital health parameters. - Remote Access: Allows doctors to access patient data globally through a web interface. - Alert System: Sends alerts if any parameter goes beyond the set threshold.  ## Requirements:  ## Hardware:  - Raspberry Pi - Blood Pressure Sensor - Heart Rate Sensor - Temperature Sensor - LCD Display - WiFi Module -  ## Software: - Raspbian OS - Python - Web Server (Flask or similar) ## Setup  ## Hardware Setup:  - Connect the sensors to the Raspberry Pi as per the wiring diagram. - Connect the WiFi module and LCD display to the Raspberry Pi.  ## Software Installation:  - Install Raspbian OS on the Raspberry Pi. - Install necessary Python libraries: pip install flask. ## Code Deployment:  - Clone the project repository. - Upload the sensor data reading scripts to the Raspberry Pi. - Set up the web server to display the data. ## Usage - Power on the System: Ensure all hardware components are connected and the Raspberry Pi is powered on. - Start Monitoring: Run the Python scripts to start collecting data from the sensors.    Access Web Interface: Open the web interface on your browser to monitor patient data in real-time.    ## Contributing ## Fork the repository. - Create a new branch (git checkout -b feature-branch). - Commit your changes (git commit -am 'Add new feature'). - Push to the branch (git push origin feature-branch). - Create a new Pull Request.   ## Contact For further inquiries, please contact yagnesh914@gmail.com."
IoT Gas Pipe Leakage Detector Insect Robot,Internet of Things,https://github.com/Henil24G/FYP_VIT,"# FYP_VIT Industrial IoT-Based Gas Pipe Leakage Detector Robot  Abstract: Energy resources such as gas often transported through pipes which play a significant role in cities, industries. Gas leakage in pipes prompts misfortune just as a danger since they can likewise prompt flame mishaps. The point of this work is to structure an independent robot for in-pipe investigation. The system utilized includes a focal pole whereupon a translational component is fitted which thusly is associated with three edges of connections and wheels. DC motors are joined to the wheels to accomplish the drive required. The component considers little settlement in-pipe distances across. Setting sensors on each segment of pipe is in all respects exorbitant. A pipe crawling robot prototype that can detect the leakage of Methane, Butane, LPG, Smoke in a pipe by moving horizontally and vertically to prevent mishaps in industries. The Internet of Things (IoT) module is interfaced with a robot to give a real-time analysis on the cloud platform and GPS to give the exact location of the leakage.  Goswami, H., Goyal, U., Alex, J.S.R. (2020). Industrial IoT-Based Gas Pipe Leakage Detector Robot. In: Pradhan, G., Morris, S., Nayak, N. (eds) Advances in Electrical Control and Signal Systems. Lecture Notes in Electrical Engineering, vol 665. Springer, Singapore. https://doi.org/10.1007/978-981-15-5262-5_70"
IoT Irrigation Monitoring & Controller System,Internet of Things,https://github.com/rgc99/irrigation_unlimited,"<!-- prettier-ignore -->  <!-- omit in toc --> # Irrigation Unlimited  [![GitHub Release][releases-shield]][releases] [![GitHub Activity][commits-shield]][commits] [![License][license-shield]][license]  [![hacs][hacsbadge]][hacs] [![Project Maintenance][maintenance-shield]](https://github.com/rgc99) [![BuyMeCoffee][buymecoffeebadge]][buymecoffee]  [![Community Forum][forum-shield]][forum]  <!-- TOC depthfrom:2 orderedlist:true -->  - [1. Introduction](#1-introduction) - [2. Features](#2-features) - [3. Structure](#3-structure) - [4. Installation](#4-installation)   - [4.1. Install from HACS](#41-install-from-hacs)   - [4.2. Manual installation](#42-manual-installation) - [5. Configuration](#5-configuration)   - [5.1. Controller Objects](#51-controller-objects)   - [5.2. All Zone Objects](#52-all-zone-objects)   - [5.3. Zone Objects](#53-zone-objects)   - [5.4. Zone Show Object](#54-zone-show-object)   - [5.5. Schedule Objects](#55-schedule-objects)     - [5.5.1 Sun Event](#551-sun-event)     - [5.5.2 Crontab](#552-crontab)     - [5.5.3 Every `n` Days](#553-every-n-days)   - [5.6. Sequence Objects](#56-sequence-objects)   - [5.7. Sequence Zone Objects](#57-sequence-zone-objects)   - [5.8. History Object](#58-history-object)     - [5.8.1. Long term statistics (LTS)](#581-long-term-statistics-lts)   - [5.9. Clock Object](#59-clock-object)   - [5.10. Check Back Object](#510-check-back-object)   - [5.11. User Object](#511-user-object) - [6. Configuration examples](#6-configuration-examples)   - [6.1. Minimal configuration](#61-minimal-configuration)   - [6.2. Sun event example](#62-sun-event-example)   - [6.3. Sequence example](#63-sequence-example)   - [6.4. Simple water saving / eco mode example](#64-simple-water-saving--eco-mode-example)   - [6.5. Every hour on the hour](#65-every-hour-on-the-hour)   - [6.6. Seasonal watering](#66-seasonal-watering)   - [6.7. Finish at sunrise](#67-finish-at-sunrise)   - [6.8. Tips](#68-tips) - [7. Services](#7-services)   - [7.1. Services `enable`, `disable` and `toggle`](#71-services-enable-disable-and-toggle)   - [7.2. Services `pause` and `resume`](#72-services-pause-and-resume)   - [7.3. Service `suspend`](#73-service-suspend)   - [7.4. Service `cancel`](#74-service-cancel)   - [7.5. Service `manual_run`](#75-service-manual_run)   - [7.6. Service `adjust_time`](#76-service-adjust_time)   - [7.7. Service `load_schedule`](#77-service-load_schedule)   - [7.8. Service `reload`](#78-service-reload)   - [7.9. Service call access roadmap](#79-service-call-access-roadmap) - [8. Frontend](#8-frontend)   - [8.1. Generic Cards](#81-generic-cards)   - [8.2. Timeline](#82-timeline)   - [8.3. Frontend Requirements](#83-frontend-requirements)   - [8.4. Manual run card](#84-manual-run-card)   - [8.5. Enable-disable card](#85-enable-disable-card)   - [8.6. Pause-resume button](#86-pause-resume-button) - [9. Automation](#9-automation)   - [9.1. ESPHome](#91-esphome)   - [9.2. HAsmartirrigation](#92-hasmartirrigation)   - [9.3. Overnight watering](#93-overnight-watering) - [10. Notifications](#10-notifications)   - [10.1. Events](#101-events)     - [10.1.1. irrigation\_unlimited\_start, irrigation\_unlimited\_finish](#1011-irrigation_unlimited_start-irrigation_unlimited_finish)     - [10.1.2. irrigation\_unlimited\_switch\_error, irrigation\_unlimited\_sync\_error](#1012-irrigation_unlimited_switch_error-irrigation_unlimited_sync_error) - [11. Troubleshooting](#11-troubleshooting)   - [11.1. Requirements](#111-requirements)   - [11.2. HA Configuration](#112-ha-configuration)   - [11.3. Community Forum](#113-community-forum)   - [11.4. Logging](#114-logging)   - [11.5. Last but not least](#115-last-but-not-least) - [12. Notes](#12-notes) - [13. Snake case](#13-snake-case) - [14. Parameter Types](#14-parameter-types)   - [14.1 Irrigation Unlimited Entities](#141-irrigation-unlimited-entities)   - [14.2 Duration (Time Period)](#142-duration-time-period)   - [14.3 Switch entities](#143-switch-entities)   - [14.4 Templating](#144-templating)   - [14.5 Sequence](#145-sequence)   - [14.6 Zones](#146-zones)   - [14.7 Time (Time of Day)](#147-time-time-of-day) - [15. Contributions are welcome](#15-contributions-are-welcome) - [16. Credits](#16-credits) <!-- /TOC -->  ## 1. Introduction  This integration is for irrigation systems large and small. It can offer some complex arrangements without large and messy scripts. This integration will complement many other irrigation projects.  Home Assistant makes automating switches easy with the built in tools available. So why this project? You have a system in place but now you have extended it to have a number of zones. You don't want all the zones on at once because of water pressure issues. Maybe you would like each zone to have a number of schedules say a morning and evening watering. What about water restrictions that limit irrigation systems to certain days of the week or days in the month, odd or even for example. Perhaps you would like different schedules for winter and summer. Now you would like to adjust the times based on weather conditions, past, present or future. Let's turn a zone or even a controller off for system maintenance. Starting to sound more like your system? Finally what's going on now and what's up next.  Each controller has an associated (master) sensor which shows on/off status and other attributes. The master will be on when any of its zones are on. The master sensor can have a pre and post amble period to activate or warm up the system like charge up a pump, enable WiFi or turn on a master valve. The master sensor has a number of service calls available to enable/disable all the zones it controls.  Zones also have an associated sensor which, like the master, shows on/off status and various attributes. Zones sensors have service calls that can enable/disable and provide manual runs. Also adjust run times in automation scripts using information from integrations that collect weather data like [OpenWeatherMap](https://www.home-assistant.io/integrations/openweathermap/), [BOM](https://github.com/bremor/bureau_of_meteorology), [weatherunderground](https://www.home-assistant.io/integrations/wunderground/) and many others. Go crazy with projects like [HAsmartirrigation](https://github.com/jeroenterheerdt/HAsmartirrigation). Easily integrate probes and sensors from [ESPHome](https://esphome.io) for real-time adjustments. Examples provided [below](#9-automation).  View and control your system with the Irrigation Unlimited [companion card](https://github.com/rgc99/irrigation-unlimited-card). A compact card where you can monitor upcoming schedules along with irrigation history.  ## 2. Features  1. Unlimited controllers. 2. Unlimited zones. 3. Unlimited schedules. Schedule by absolute time or sun events (sunrise/sunset). Select by days of the week (mon/tue/wed...). Select by days in the month (1/2/3.../odd/even). Select by months in the year (jan/feb/mar...). Use cron expressions. Overlapped schedules. 4. Unlimited sequences. Operate zones one at a time in a particular order with a delay in between. A 'playlist' for your zones. 5. Suitable for indoor (greenhouse, hothouse, undercover areas) and outdoor (gardens, lawns, crops). 6. Hardware independent. Use your own switches/valve controllers. 7. Software independent. Pure play python.  \*Practical limitations will depend on your hardware.  ## 3. Structure  Irrigation Unlimited is comprised of controllers, zones and schedules in a tree like formation. Each controller has one or more zones and each zone has one or more schedules. Controllers and zones will have a binary sensor associated with each one so they can be integrated with Home Assistant.  ```text ‚îî‚îÄ‚îÄ Irrigation Unlimited   ‚îî‚îÄ‚îÄ Controller 1 -> binary_sensor.irrigation_unlimited_c1_m     ‚îî‚îÄ‚îÄ Zone 1 -> binary_sensor.irrigation_unlimited_c1_z1       ‚îî‚îÄ‚îÄ Schedule 1       ‚îî‚îÄ‚îÄ Schedule 2           ...       ‚îî‚îÄ‚îÄ Schedule N     ‚îî‚îÄ‚îÄ Zone 2 -> binary_sensor.irrigation_unlimited_c1_z2         ...     ‚îî‚îÄ‚îÄ Zone N -> binary_sensor.irrigation_unlimited_c1_zN         ...     ‚îî‚îÄ‚îÄ Sequence 1       ‚îî‚îÄ‚îÄ Schedule 1       ‚îî‚îÄ‚îÄ Schedule 2         ...       ‚îî‚îÄ‚îÄ Schedule N       ‚îî‚îÄ‚îÄ Zone 1       ‚îî‚îÄ‚îÄ Zone 2         ...       ‚îî‚îÄ‚îÄ Zone N     ‚îî‚îÄ‚îÄ Sequence 2       ...     ‚îî‚îÄ‚îÄ Sequence N   ‚îî‚îÄ‚îÄ Controller 2 -> binary_sensor.irrigation_unlimited_c2_m       ...   ‚îî‚îÄ‚îÄ Controller N -> binary_sensor.irrigation_unlimited_cN_m       ... ```  Controllers and zones can specify an entity such as a switch or light, basically anything that turns on or off the system can control it. This is the irrigation valve. If this does not go far enough for your purposes then track the state of the binary sensors in an automation and do your own thing like run a script or scene.  **This component will set up the following platforms.**  | Platform | Description | | ---- | ---- | | `binary_sensor` | Show a valve `on` or `off`|  A binary sensor is associated with each controller and zone. Controller or master sensors are named `binary_sensor.irrigation_unlimited_cN_m` and zone sensors `binary_sensor.irrigation_unlimited_cN_zN`. These sensors show the state of the master or child zones. Attributes show additional information like current schedule and next run time and duration.  ![entities](./examples/entities.png)  ## 4. Installation  [HACS](https://hacs.xyz) is the recommended method for installation. If you are having difficulties then please see the _[troubleshooting guide](#11-troubleshooting)_  ### 4.1. Install from HACS  1. Just search for Irrigation Unlimited integration in [HACS][hacs] and install it. 2. Add Irrigation Unlimited to your configuration.yaml file. See _[configuration examples](#6-configuration-examples)_ below. 3. Restart Home Assistant.  ### 4.2. Manual installation  1. Using the tool of choice open the directory (folder) for your HA configuration (where you find `configuration.yaml`). 2. If you do not have a `custom_components` directory (folder) there, you need to create it. 3. In the `custom_components` directory (folder) create a new folder called `irrigation_unlimited`. 4. Download _all_ the files from the `custom_components/irrigation_unlimited/` directory (folder) in this repository. 5. Place the files you downloaded in the new directory (folder) you created. 6. Restart Home Assistant 7. In the HA UI go to ""Configuration"" -> ""Integrations"" click ""+"" and search for ""Irrigation""  Using your HA configuration directory (folder) as a starting point you should now also have this:  ```text custom_components/irrigation_unlimited/__init__.py custom_components/irrigation_unlimited/binary_sensor.py custom_components/irrigation_unlimited/const.py custom_components/irrigation_unlimited/entity.py custom_components/irrigation_unlimited/history.py custom_components/irrigation_unlimited/irrigation_unlimited.py custom_components/irrigation_unlimited/manifest.json custom_components/irrigation_unlimited/schema.py custom_components/irrigation_unlimited/service.py custom_components/irrigation_unlimited/services.yaml ```  ## 5. Configuration  Configuration is done by yaml. Note: The configuration can be reloaded without restarting HA. See [below](#78-service-reload) for details and limitations.  | Name | Type | Default | Description | | -----| ---- | ------- | ----------- | | `controllers` | list | _[Controller Objects](#51-controller-objects)_ | Controller details (Must have at least one) | | `granularity` | number | 60 | System time boundaries in seconds | | `refresh_interval` | number | 30 | Refresh interval in seconds. When a controller or zone is on this value will govern how often the count down timers will update. Decrease this number for a more responsive display. Increase this number to conserve resources | | `rename_entities` | bool | false | DANGER ZONE. Allow the sensor entity_id's to be altered. The [controller_id](#51-controller-objects) and [zone_id](#53-zone-objects) will be combined to form the new entity_id. Note: Automations, sensors, scripts, front end cards etc. may need to be updated to reflect the new entity_id's of the controllers and zones | | `history_span` | number | 7 | Deprecated. See [history](#58-history-object) `span` | | `history_refresh` | number | 120 | Deprecated. See [history](#58-history-object) `refresh_interval` | | `history` | object | _[History Object](#58-history-object)_ | History data gathering options | | `clock` | object | _[Clock Object](#59-clock-object)_ | Clock options |  ### 5.1. Controller Objects  This is the controller or master object and manages a collection of zones. There must be at least one controller in the system. The controller state reflects the state of its zones. The controller will be on if any of its zones are on and off when all zones are off.  | Name | Type | Default | Description | | ---- | ---- | ------- | ----------- | | `zones` | list | _[Zone Objects](#53-zone-objects)_ | Zone details (Must have at least one) | | `sequences` | list | _[Sequence Objects](#56-sequence-objects)_ | Sequence details | | `name` | string | Controller _N_ | Friendly name for the controller | | `controller_id` | string | _N_ | Controller reference. Used to change the default entity name (enable with [rename_entities](#5-configuration)). This must be in [snake_case](#13-snake-case) style with the exception the first character _can_ be a number | | `enabled` | bool | true | Enable/disable the controller | | `preamble` | [duration](#142-duration-time-period) | '00:00' | The time master turns on before any zone turns on. This is in effect a delay-start timer, controller will turn on before the zones. Can be negative to make the controller turn on _after_ the zone | | `postamble` | [duration](#142-duration-time-period) | '00:00' | The time master remains on after all zones are off. This is in effect a run-on timer, controller will turn off after the specified delay. Can be negative to make the controller turn off _before_ the zone - this can reduce water hammering | | `entity_id` | [switch_entity](#143-switch-entities) | | Switch entity_id(s) for example `switch.my_master_valve_1` | | `all_zones_config` | object | _[All Zones Object](#52-all-zone-objects)_ | Shorthand default for all zones | | `check_back` | object | | See _[Check Back Object](#510-check-back-object)_ | | `queue_manual` | bool | false | Manual runs should be queued or run immediately | | `user` | object | | See _[User Object](#511-user-object)_ |  ### 5.2. All Zone Objects  This object is useful when the same settings are required for each zone. It is simply a shorthand or a more concise way to specify the same settings for each zone. The parameter becomes a default which can be overridden in the actual zone.  | Name | Type | Default | Description | | ---- | ---- | ------- | ----------- | | `minimum` | [duration](#142-duration-time-period) | | The minimum run time | | `maximum` | [duration](#142-duration-time-period) | | The maximum run time | | `duration` | [duration](#142-duration-time-period) | | The default run time | | `future_span` | number | 3 | Run queue look ahead in days | | `allow_manual` | bool | false | Allow manual run even when disabled | | `show` | object | | See _[Zone Show Object](#54-zone-show-object)_ | | `check_back` | object | | See _[Check Back Object](#510-check-back-object)_ | | `user` | object | | See _[User Object](#511-user-object)_ |  ### 5.3. Zone Objects  The zone object manages a collection of schedules. There must be at least one zone for each controller.  | Name | Type | Default | Description | | ---- | ---- | ------- | ----------- | | `schedules` | list | _[Schedule Objects](#55-schedule-objects)_ | Schedule details (Optional) | | `zone_id` | string | _N_ | Zone reference. Used for sequencing and to rename the entity. This must be in [snake_case](#13-snake-case) style with the exception the first character _can_ be a number. Set [rename_entities](#5-configuration) to alter the entity _id of the sensor | | `name` | string | Zone _N_ | Friendly name for the zone | | `enabled` | bool | true | Enable/disable the zone | | `minimum` | [duration](#142-duration-time-period) | '00:01' | The minimum run time | | `maximum` | [duration](#142-duration-time-period) | | The maximum run time | | `duration` | [duration](#142-duration-time-period) | | The default run time. Used when no `time` is provided for a manual run | | `future_span` | number | 3 | Number of days to look ahead | | `allow_manual` | bool | false | Allow manual run even when disabled | | `entity_id` | [switch_entity](#143-switch-entities) | | Switch entity_id(s) for example `switch.my_zone_valve_1` | | `show` | object | | See _[Zone Show Object](#54-zone-show-object)_ | | `check_back` | object | | See _[Check Back Object](#510-check-back-object)_ | | `user` | object | | See _[User Object](#511-user-object)_ |  ### 5.4. Zone Show Object  These are various options to reveal attributes on the zone entity (only one for now).  | Name | Type | Default | Description | | ---- | ---- | ------- | ----------- | | `timeline` | bool | false | Show the zone timeline. This will expose an attribute called `timeline` on the zone entity | | `config` | bool | false | Show the zone configuration. This will expose an attribute called `configuration` on the zone entity with JSON encoded configuration objects |  ### 5.5. Schedule Objects  Schedules are future events, _not_ dates for example Mondays at sunrise.  The schedule can have the commencement or completion fixed to a time or event with the `anchor` parameter. Any adjustments to the duration will alter the start time if `finish` is specified or the completion time if `start` is specified. Note: If anchoring to `finish` and the schedule can not complete before the specified time then the run will defer to the following day. This is an important consideration if adjusting run times dynamically as it may lead to a 'skipping' situation. Ensure there is sufficient time to complete the run when making adjustments. See _[here](#76-service-adjust_time)_ for more information on adjusting runs times.  The parameters `weekday`, `day`, `month` and `from/until` are date filters. If not specified then all dates qualify.  | Name | Type | Default | Description | | ---- | ---- | ------- | ----------- | | `time` | [time](#147-time-time-of-day)/_[Sun Event](#551-sun-event)_/_[Crontab](#552-crontab)_ | **Required** | The start time. Either a time (07:30), sun event or cron expression | | `anchor` | string | start | `start` or `finish`. Sets the schedule to commence or complete at the specified time | | `duration` | [duration](#142-duration-time-period) | | The length of time to run. Required for zones and optional for sequences | | `name` | string | Schedule _N_ | Friendly name for the schedule | | `weekday` | list | | The days of week to run [mon, tue...sun] | | `day` | list/string/_[Every `n` days](#553-every-n-days)_ | | Days of month to run [1, 2...31]/odd/even/_[Every `n` days](#553-every-n-days)_ | | `month` | list | | Months of year to run [jan, feb...dec] | | `enabled` | bool | true | Enable/disable the schedule | | `schedule_id` | string | | A unique identifier across all schedules. This must be in [snake_case](#13-snake-case) style | | `from` | string | see below* | Start date in the year. Format is `dd mmm` for example `15 Mar` | | `until` | string | see below*| Last date in the year. Format is `dd mmm` for example `15 Sep` |  \* `from` and `until` are mutually inclusive.  #### 5.5.1 Sun Event  Leave the time value in the _[Schedule Objects](#55-schedule-objects)_ blank and add the following object. An optional `before` or `after` time can be specified.  | Name | Type | Default | Description | | ---- | ---- | ------- | ----------- | | `sun` | string | **Required** | `sunrise` or `sunset` | | `before` | [duration](#142-duration-time-period) | '00:00' | Time before the event | | `after` | [duration](#142-duration-time-period) | '00:00' | Time after the event |  #### 5.5.2 Crontab  Leave the time value in the _[Schedule Objects](#55-schedule-objects)_ blank and add the following object.  | Name | Type | Default | Description | | ---- | ---- | ------- | ----------- | | `cron` | string | **Required** | A valid cron expression. Details can be found [here](https://github.com/josiahcarlson/parse-crontab) |  #### 5.5.3 Every `n` Days  Set the day value in the _[Schedule Objects](#55-schedule-objects)_ to a dictionary with the following keys.  | Name | Type | Default | Description | | ---- | ---- | ------- | ----------- | | `every_n_days` | number | **Required** | The interval between runs. | | `start_n_days` | date | **Required** | The start date for the interval, you can alternate multiple schdules by offseting this by `every_n_days`. |  ### 5.6. Sequence Objects  Sequences allow zones to run one at a time in a particular order with a delay in between. This is a type of watering 'playlist'. If a delay is specified and a pump or master valve is operated by the controller then consider the postamble setting in the _[Controller Object](#51-controller-objects)_. Set this to the largest delay to prevent pump on/off operations.  Sequences directly descend from a controller and are loosely connected to a zone entity via the `zone_id` parameter. The `zone_id` may point to one or many (a list) zone entities. A zone may be referenced more than once in a sequence.  ```text ‚îî‚îÄ‚îÄ Irrigation Unlimited       ‚îî‚îÄ‚îÄ> Controller             ‚îú‚îÄ‚îÄ> Zones             ‚îÇ     ‚îú‚îÄ‚îÄ> Zone 1 <‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îÇ     ‚îú‚îÄ‚îÄ> Zone 2 <‚îÄ‚îÄ‚îÄ‚îÄ‚î§             ‚îÇ     ‚îÇ     ...        ‚îÇ             ‚îÇ     ‚îî‚îÄ‚îÄ> Zone N <‚îÄ‚îÄ‚îÄ‚îÄ‚î§             ‚îî‚îÄ‚îÄ> Sequence          ‚îÇ                   ‚îú‚îÄ‚îÄ> zone_id >‚îÄ‚îÄ‚îÄ‚î§                   ‚îú‚îÄ‚îÄ> zone_id >‚îÄ‚îÄ‚îÄ‚î§                   ‚îÇ      ...       ‚îÇ                   ‚îî‚îÄ‚îÄ> zone_id >‚îÄ‚îÄ‚îÄ‚îò ```  | Name | Type | Default | Description | | ---- | ---- | ------- | ----------- | | `schedules` | list | _[Schedule Objects](#55-schedule-objects)_ | Schedule details (Optional). Note: `duration` if specified is the total run time for the sequence, see below for more details | | `zones` | list | _[Sequence Zone Objects](#57-sequence-zone-objects)_ | Zone details (Must have at least one) | | `delay` | [duration](#142-duration-time-period) | | Delay between zones. This value is a default for all _[Sequence Zone Objects](#57-sequence-zone-objects)_. Can be negative to make the next zone on _before_ the current zone has finished | | `duration` | [duration](#142-duration-time-period) | | The length of time to run each zone. This value is a default for all _[Sequence Zone Objects](#57-sequence-zone-objects)_ | | `repeat` | number | 1 | Number of times to repeat the sequence | | `name` | string | Run _N_ | Friendly name for the sequence | | `sequence_id` | string | _N_ | Sequence reference. This must be in [snake_case](#13-snake-case) style with the exception the first character _can_ be a number | | `enabled` | bool | true | Enable/disable the sequence |  ### 5.7. Sequence Zone Objects  The sequence zone is a reference to the actual zone defined in the _[Zone Objects](#53-zone-objects)_. Ensure the `zone_id`'s match between this object and the zone object. The zone may appear more than once in the case of a split run.  | Name | Type | Default | Description | | ---- | ---- | ------- | ----------- | | `zone_id` | string/list | **Required** | Zone reference. This must match the `zone_id` in the _[Zone Objects](#53-zone-objects)_ | | `delay` | [duration](#142-duration-time-period) | | Delay between zones. This value will override the `delay` setting in the _[Sequence Objects](#56-sequence-objects)_ | | `duration` | [duration](#142-duration-time-period) | | The length of time to run. This value will override the `duration` setting in the _[Sequence Objects](#56-sequence-objects)_. Can be negative to make the next zone on _before_ the current zone has finished. | | `repeat` | number | 1 | Number of times to repeat this zone | | `enabled` | bool | true | Enable/disable the sequence zone |  Special note for [schedules](#55-schedule-objects) and the `duration` parameter contained within when used with sequences. Each zone in the sequence will be proportionally adjusted to fit the specified duration. For example, if 3 zones were to each run for 10, 20 and 30 minutes respectively (total 1 hour) and the `schedule.duration` parameter specified 30 minutes then each zone would be adjusted to 5, 10 and 15 minutes. Likewise if `schedule.duration` specified 1.5 hours then the zones would be 15, 30 and 45 minutes. Some variation may occur due to rounding of the times to the system boundaries (granularity). This parameter influences the durations specified in the sequence and sequence zone objects.  ### 5.8. History Object  The `timeline` and `total_today` attributes use history information. This information is read and cached by the history module.  | Name | Type | Default | Description | | ---- | ---- | ------- | ----------- | | `enabled` | bool | true | Enable/disable history | | `span` | number | 7 | Number of days of history data to fetch | | `refresh_interval` | number | 120 | History refresh interval in seconds | | `read_delay` | number | 0 | Delay before reading history data in seconds |  #### 5.8.1. Long term statistics (LTS)  History is typically purged after 10 days. If you wish to retain the `total_today` data beyond this period then setup a Long-Term Statistic sensor. See [here](./packages/irrigation_unlimited_lts.yaml) for an example. For more information see [Long-Term Statistics](https://data.home-assistant.io/docs/statistics/)  ### 5.9. Clock Object  This object controls the internal clock mode.  | Name | Type | Default | Description | | ---- | ---- | ------- | ----------- | | `mode` | string | seer | `fixed` or `seer`. Set the clock to fixed (game loop) or seer (event loop) | | `show_log` | bool | false | Expose the clock ticks via `next_tick` and `tick_log` attributes in the coordinator entity | | `max_log_entries` | number | 50 | Set the number of entries in the tick log history |  ### 5.10. Check Back Object  This is used to check the state of the physical switch concurs with the state of the controller or zone. An out of sync can occur due to transmission or communications problems especially with protcols like WiFi, Zigbee or ZWave. The check back will report discrepancies and attempt to resync the switch. Should the resync fail a message will be logged and an [event](#1012-irrigation_unlimited_switch_error-irrigation_unlimited_sync_error) fired. The event can be use for notifications such as an email or phone alert. For more information see [Notifications](#10-notifications)  | Name | Type | Default | Description | | ---- | ---- | ------- | ----------- | | `state` | string | all | One of `none`, `all`, `on` or `off` | | `delay` | number | 30 | Seconds to wait after switch is turned on or off | | `retries` | number | 3 | Number of times to recheck the switch | | `resync` | bool | true | Attempt to resync the switch | | `toggle` | bool | false | Toggle the switch on/off or off/on instead of attempting to just set the state | | `state_on` | string | `on` | The value that represents the `on` state of the switch | | `state_off` | string | `off` | The value that repesents the `off` state of the switch | | `entity_id` | string | | Optional, use this when switch entity is write only and state is read from another entity. If switch entity is R/W then ignore this parameter |  ### 5.11. User Object  The user object is available on the `controller`, `all_zone` and `zone` objects. It is basically a pass through of arbitrary static user defined data. Elements are prefixed and presented as attributes in the entities.  | Name | Type | Default | Description | | ---- | ---- | ------- | ----------- | | `name` | string/number/bool | | | | `name_2` | string/number/bool | | |  \* `name` should be in [snake_case](#13-snake-case) style.   Here is an example:   ```yaml  controllers:     - name: ""Test controller 1""       user:         area: My Farm         picture: /my_pic.jpg       all_zones_config:         user:           actuator: KNX 6.1       zones:         - name: ""Zone 1""           user:             area: Eastern Pastures             flow_rate_gallon_per_minute: 25             picture: /my_pic.jpg             gps: 42.746635,-75.770045         - name: ""Zone 2"" ```  Thus, the controller and zone present the following attributes:  ```yaml #Controller 'Test controller 1': binary_sensor.irrigation_unlimited_c1_m.user_area = 'My Farm' binary_sensor.irrigation_unlimited_c1_m.user_picture = '/my_pic.jpg'  #Zone 1: binary_sensor.irrigation_unlimited_c1_z1.user_actuator = 'KNX 6.1' #this is inherited from all_zones_config binary_sensor.irrigation_unlimited_c1_z1.user_area = 'Eastern Pastures' binary_sensor.irrigation_unlimited_c1_z1.user_flow_rate_gallon_per_minute = '25' binary_sensor.irrigation_unlimited_c1_z1.user_picture = '/my_pic.jpg' binary_sensor.irrigation_unlimited_c1_z1.user_gps = '42.746635,-75.770045'  #Zone 2: binary_sensor.irrigation_unlimited_c1_z2.user_actuator = 'KNX 6.1' #this is inherited from all_zones_config  ```  The user defined static data available as attribute may help to customize cards or to present additional data on cards, in particular via the functionality within [entity-multiple-row](type: custom:multiple-entity-row). This [feature](https://github.com/rgc99/irrigation_unlimited/issues/143) maybe further developed and extended to the ```sequence: object``` over time.  ## 6. Configuration examples  ### 6.1. Minimal configuration  ```yaml # Example configuration.yaml entry irrigation_unlimited:   controllers:     zones:       entity_id: ""switch.my_switch""       schedules:         - time: ""06:00""           duration: ""00:20"" ```  ### 6.2. Sun event example  ```yaml # Example configuration.yaml entry # Run 20 minutes before sunrise for 30 minutes irrigation_unlimited:   controllers:     zones:       entity_id: ""switch.my_switch_1""       schedules:         - name: ""Before sunrise""           time:             sun: ""sunrise""             before: ""00:20""           duration: ""00:30"" ```  ### 6.3. Sequence example  ```yaml # Example configuration.yaml entry irrigation_unlimited:   controllers:     zones:       - name: ""Front lawn""         entity_id: ""switch.my_switch_1""       - name: ""Vege patch""         entity_id: ""switch.my_switch_2""       - name: ""Flower bed""         entity_id: ""switch.my_switch_3""     sequences:       - delay: ""00:01""         schedules:           - name: ""Sunrise""             time:               sun: ""sunrise""           - name: ""After sunset""             time:               sun: ""sunset""               after: ""00:30""         zones:           - zone_id: 1             duration: ""00:10""           - zone_id: 2             duration: ""00:02""           - zone_id: 3             duration: ""00:01"" ```  ### 6.4. Simple water saving / eco mode example  ```yaml # Example water saver. Run for 5 min on 2 off repeat 3 times irrigation_unlimited:   controllers:     zones:       - entity_id: ""switch.my_switch_1""     sequences:       - duration: ""00:05""         delay: ""00:02""         repeat: 3         schedules:           - time: ""05:00""         zones:           - zone_id: 1 ```  ### 6.5. Every hour on the hour  ```yaml # Example to run for 5 min every hour on the hour from 5am to 5pm irrigation_unlimited:   controllers:     zones:       - entity_id: ""switch.my_switch_1""     sequences:       - name: ""On the hour from 5am to 5pm""         duration: ""00:05""         delay: ""00:55""         repeat: 12         schedules:           - time: ""05:00""         zones:           - zone_id: 1 ```  Similar to above but using the cron scheduler.  ```yaml # Example to run for 5 min every hour on the hour from 5am to 5pm irrigation_unlimited:   controllers:     zones:       - entity_id: ""switch.my_switch_1""     sequences:       - name: ""On the hour from 5am to 5pm""         duration: ""00:05""         schedules:           - time:               cron: ""0 5-17 * * *""         zones:           - zone_id: 1 ```  ### 6.6. Seasonal watering  ```yaml # Run 15 min 3 times a week in summer, 10 min once a week in winter and twice a week in spring/autumn irrigation_unlimited:   controllers:     zones:       - entity_id: ""switch.my_switch_1""         schedules:           - time: ""05:30""             duration: ""00:15""             weekday: [mon, wed, fri]             month: [dec, jan, feb]           - time: ""05:30""             duration: ""00:10""             weekday: [sun]             month: [jun, jul, aug]           - time: ""05:30""             duration: ""00:12""             weekday: [mon, thu]             month: [mar, apr, may, sep, oct, nov] ```  This is similar to the above but using sequences in a 3 zone system. Each zone runs for 12 minutes for a total of 36 min (plus delays). In Summer the total duration is extended to 45 minutes and winter reduced to 30 minutes. When using the `duration` parameter in the _[Schedule](#55-schedule-objects)_ it relates to the total duration of the sequence, each zone is adjusted accordingly.  ```yaml irrigation_unlimited:   controllers:     zones:       - entity_id: ""switch.my_switch_1""       - entity_id: ""switch.my_switch_2""       - entity_id: ""switch.my_switch_3""     sequences:       - name: ""Run 1""         duration: ""00:12""         delay: ""00:01""         schedules:           - name: ""Summer""             time: ""05:30""             weekday: [mon, wed, fri]             month: [dec, jan, feb]             duration: ""00:45""           - name: ""Winter""             time: ""05:30""             weekday: [sun]             month: [jun, jul, aug]             duration: ""00:30""           - name: ""Spring and Autumn""             time: ""05:30""             weekday: [mon, thu]             month: [mar, apr, may, sep, oct, nov]         zones:           - zone_id: 1           - zone_id: 2           - zone_id: 3 ```  Just in case this does not go far enough then create three sequences with one schedule each. This will allow _complete_ control over _all_ aspects of the sequence including which zones to run, order, durations, delays, repeats etc. Still want more then create a sequence for each month of the year. This example reverses the order in Spring/Autumn for no good reason and excludes a zone in Winter.  ```yaml irrigation_unlimited:   controllers:     zones:       - entity_id: ""switch.my_switch_1""       - entity_id: ""switch.my_switch_2""       - entity_id: ""switch.my_switch_3""     sequences:       - name: ""Summer""         duration: ""00:15""         delay: ""00:01""         schedules:           - time: ""05:30""             weekday: [mon, wed, fri]             month: [dec, jan, feb]         zones:           - zone_id: 1           - zone_id: 2           - zone_id: 3       - name: ""Winter""         duration: ""00:10""         delay: ""00:01""         schedules:           - time: ""07:30""             weekday: [sun]             month: [jun, jul, aug]         zones:           - zone_id: 1           - zone_id: 3       - name: ""Spring and Autumn""         duration: ""00:12""         delay: ""00:01""         schedules:           - time: ""06:30""             weekday: [mon, thu]             month: [mar, apr, may, sep, oct, nov]         zones:           - zone_id: 3           - zone_id: 2           - zone_id: 1 ```  ### 6.7. Finish at sunrise  ```yaml # Finish a watering run 10 minutes before sunrise irrigation_unlimited:   controllers:     zones:       - entity_id: ""switch.my_switch_1""       - entity_id: ""switch.my_switch_2""       - entity_id: ""switch.my_switch_3""       - entity_id: ""switch.my_switch_4""     sequences:       - name: ""My watering run""         duration: ""00:30""         delay: ""00:01""         schedules:           - name: ""Before dawn""             time:               sun: ""sunrise""               before: ""00:10""             anchor: finish         zones:           - zone_id: 1           - zone_id: 2           - zone_id: 3           - zone_id: 4 ```  For a more comprehensive example refer to [here](./examples/all_the_bells_and_whistles.yaml).  ### 6.8. Tips  1. Schedules can not only have a day of week (mon, wed, fri) but also a month of year (jan, feb, mar). This allows the setup of seasonal watering schedules. For example run every day in summer and twice a week in winter. Setup a different schedule for each month of the year using this filter.  2. Use sequences to setup a water saving or eco mode. Eco mode uses small cycles with a delay to allow the water to soak in and minimise run off. Run all the zones for half the time and then repeat.  3. No need to restart HA after changing the configuration.yaml file. Go to Configuration -> Server Controls -> YAML configuration and reloading and press 'RELOAD IRRIGATION UNLIMITED'.  4. After setting up configuration.yaml, the operation can be controlled via service calls as shown _[below](#7-services)_. Perform manual runs, adjust watering times, cancel running schedules and enable/disable zones from a _[frontend](#8-frontend)_  ## 7. Services  The binary sensor associated with each controller and zone provide several services. These sensors offer the following services:  - `enable` - `disable` - `toggle` - `suspend` - `cancel` - `manual_run` - `adjust_time` - `load_schedule`  If a controller sensor is targetted then it will effect all its children zones.  ### 7.1. Services `enable`, `disable` and `toggle`  Enables/disables/toggles the controller, zone, sequence or sequence zone respectively.  | Service data attribute | Type | Required | Description | | ---------------------- | ---- | -------- | ----------- | | `entity_id` | [string/list](#141-irrigation-unlimited-entities) | yes | Controller or zone to enable/disable/toggle. | | `sequence_id` | [number/list](#145-sequence) | no | Sequences to enable/disable/toggle. Entity must be a controller. | | `zones` | [number/list](#146-zones) | no | Sequence zones to enable/disable/toggle. |  ### 7.2. Services `pause` and `resume`  Pauses/resumes a sequence. This service call ""stops the clock"" when `paused` so to speak and ""continues to run it"" upon `resume`.  This is particularly helpful in a use case scenario where a main water supply is used for both irrigation and ie filling a domestic cold water tank at the same time. If the water pressure is only sufficient for either irrigaton or tank filling, this service call allows to `pause` the irrigation whilst a tank is filled, and then `resumes` irrigation without interrupting the time allocated to the sequences or zones thereof.  | Service data attribute | Type | Required | Description | | ---------------------- | ---- | -------- | ----------- | | `entity_id` | [string/list](#141-irrigation-unlimited-entities) | yes | Entity_id of a controller or sequence to pause/resume. If a controller entity is selected it will target the sequence or all sequences (see next parameter). If more than one entity_id are to be targeted a group integration helper may be used. | | `sequence_id` | [number/list](#145-sequence) | only if entity_id represents a controller | Sequence to pause/resume. The sequence_id is only used when the entity_id is the controller. If sequence_id is set to 0 then all sequences of the controller will be effected. |  There is an example for a [pause-resume button](#86-pause-resume-button) that targets all sequences within all controllers creating a globla `pause` and `resume` button.  ### 7.3. Service `suspend`  Suspend operation of a controller, zone, sequence or sequence zone for a period of time. This is like a temporary `disable` that will automatically reset.  | Service data attribute | Type | Required | Description | | ---------------------- | ---- | -------- | ----------- | | `entity_id` | [string/list](#141-irrigation-unlimited-entities) | yes | Controller or zone to run. | | `sequence_id` | [number/list](#145-sequence) | no | Sequences to suspend. Entity must be a controller. | | `zones` | [number/list](#146-zones) | no | Sequence zones to suspend. | | `for` | [duration](#142-duration-time-period) | see below* | Suspend for a period of time. Supports [templating](#144-templating). | | `until` | string | see below* | Suspend until a point in time. Format is `%Y-%m-%d %H:%M:%S` for example `2023-08-01 07:30:00`. | | `reset` | none | see below* | Reset or cancel the current suspension. |  \* Must have one and only one of `for`, `until` or `reset`.  ### 7.4. Service `cancel`  Cancels the current running schedule.  | Service data attribute | Type | Required | Description | | ---------------------- | ---- | -------- | ----------- | | `entity_id` | [string/list](#141-irrigation-unlimited-entities) | yes | Controller or zone to cancel. |  ### 7.5. Service `manual_run`  Turn on the controller or zone for a period of time. When a sequence is specified each zone's duration will be auto adjusted as a proportion of the original sequence. Zone times are calculated and rounded to the nearest time boundary. This means the total run time may vary from the specified time.  | Service data attribute | Type | Required | Description | | ---------------------- | ---- | -------- | ----------- | | `entity_id` | [string/list](#141-irrigation-unlimited-entities) | yes | Controller or zone to run. | | `time` | [duration](#142-duration-time-period) | no | Total time to run. Supports [templating](#144-templating). If not provided or is ""0:00:00"" then adjusted defaults will be applied | | `delay` | [duration](#142-duration-time-period) | no | Delay between runs when queued | | `queue` | boolean | no | Queue or run immediately. | | `sequence_id` | [number/list](#145-sequence) | no | Sequences to run. Each zone duration will be adjusted to fit the allocated time, delays are not effected. Note: The time parameter _includes_ inter zone delays. If the total delays are greater than the specified time then the sequence will not run. Entity must be a controller. |  ### 7.6. Service `adjust_time`  Adjust the run times. Calling this service will override any previous adjustment i.e. it will _not_ make adjustments on adjustments. For example, if the scheduled duration is 30 minutes calling percent: 150 will make it 45 minutes then calling percent 200 will make it 60 minutes. When a sequence is specified each zone's duration will be auto adjusted as a proportion of the original sequence.  A schedule anchored to a start time will alter the completion time. Likewise a schedule anchored to a finish time will change the commencement time. In this situation ensure there is enough time in the current day for the schedule to complete or it will be deferred to the following day. Adjustments must be made _before_ the scheduled start time. Running schedules will be not affected.  Tip: Use forecast and observation data collected by weather integrations in automations to adjust the run times. See [below](#9-automation) for more information.  | Service data attribute | Type | Required | Description | | ---------------------- | ---- | -------- | ----------- | | `entity_id` | [string/list](#141-irrigation-unlimited-entities) | yes | Controller or zone to run. | | `actual` | [duration](#142-duration-time-period) | see below* | Specify a new run time. This will replace the existing duration. Supports [templating](#144-templating). | | `percentage` | float | see below* | Adjust time by a percentage. Values less than 100 will decrease the run time while values greater than 100 will increase the run time. Supports [templating](#144-templating). | | `increase` | [duration](#142-duration-time-period) | see below* | Increase the run time by the specified time. A value of '00:10' will increase the duration by 10 minutes. Value will be capped by the `maximum` setting. Supports [templating](#144-templating). | | `decrease` | [duration](#142-duration-time-period) | see below* | Decrease the run time by the specified time. A value of '00:05' will decrease the run time by 5 minutes. Value will be limited by the `minimum` setting. Supports [templating](#144-templating). | | `reset` | none | see below* | Reset adjustment back to the original schedule time (Does not effect minimum or maximum settings). | | `minimum` | [duration](#142-duration-time-period) | no | Set the minimum run time. Supports [templating](#144-templating). | | `maximum` | [duration](#142-duration-time-period) | no | Set the maximum run time. Note: The default is no limit. Supports [templating](#144-templating). | | `sequence_id` | [number/list](#145-sequence) | no | Sequences to adjust. Entity must be a controller. | | `zones` | [number/list](#146-zones) | no | Zones to adjust. |  \* Must have one and only one of `actual`, `percentage`, `increase`, `decrease` or `reset`.  ### 7.7. Service `load_schedule`  Reload a schedule. This will allow an edit to an existing schedule. All fields are optional except the `schedule_id`. If a field is specified then it is overwritten otherwise it is left untouched. This service does NOT save the new schedule in the event of a reload or HA restart, it will revert to the original configuration.  | Name | Type | Default | Description | | ---- | ---- | ------- | ----------- | | `schedule_id` | string | **Required** | The unique schedule identifier. The target [schedule](#55-schedule-objects) must have the unique `schedule_id` set | | `time` | [time](#147-time-time-of-day)/_[Sun Event](#551-sun-event)_/_[Crontab](#552-crontab)_ | | The start time. Either a time (07:30), sun event or cron expression | | `anchor` | string | | `start` or `finish`. Sets the schedule to commence or complete at the specified time | | `duration` | [duration](#142-duration-time-period) | | The length of time to run. Required for zones and optional for sequences | | `name` | string | | Friendly name for the schedule | | `weekday` | list | | The days of week to run [mon, tue...sun] | | `day` | list/string/_[Every `n` days](#553-every-n-days)_ | | Days of month to run [1, 2...31]/odd/even/_[Every `n` days](#553-every-n-days)_ | | `month` | list | | Months of year to run [jan, feb...dec] | | `enabled` | bool | | Enable/disable the schedule |  ### 7.8. Service `reload`  Reload the YAML configuration file. Do not add or delete controllers or zones, they will not work because of the associated entities which are created on startup. This may be addressed in a future release, however, suggested work around is to set enabled to false to effectively disable/delete. All other settings can be changed including schedules. You will find the control in Configuration -> Server Controls -> YAML configuration reloading. Note: since version 2021.10.0 all settings can be changed including new controllers and zones.  ### 7.9. Service call access roadmap  A reminder that sequences directly descend from a controller. Therefore service calls that manipulate a sequence should address the parent controller. An entity_id of a zone when trying to adjust a sequence will most likely not have the desired effect.  The combination of three key parameters `entity_id`, `sequence_id` and `zones` will target the various sections of the configuration.  - `entity_id:` This will be either the controller or zone entity. - `sequence_id:` This is the position number of the sequence under the controller. `sequence_id: 1` is the first, 2 is the second and so on. As a shortcut, `sequence_id` will alter _all_ sequences. - `zones:` This is the position number of the zone reference under the sequence. `zones: 1` is the first, 2 is the second and so on. As a shortcut, `zones: 0` will alter _all_ zone references in the sequence. May also take a list `zones: [1,3,5]`  The following is a valid irrigation unlimited configuration. It shows how various points can be changed using the service calls above. Example numbers have the nomenclature C.Z.S.R = Controller.Zone.Sequence.zoneReference. If Z is zero then the `entity_id` must be the controller/master i.e. binary_sensor.irrigation_unlimited_cN_m. If Z is not zero then then entity_id is the zone i.e. binary_sensor.irrigation_unlimited_cN_zN.  ```yaml irrigation_unlimited:   controllers:     - name: ""Controller 1""       enabled: true # <= See example 1.0       zones:         - name: ""Controller 1, Zone 1""           enabled: true # <= See example 1.1           - schedules:               - time: ""04:00""                 duration: ""00:10"" # <= See example 1.1.1         - name: ""Controller 1, Zone 2""           enabled: true # <= See example 1.2           - schedules:               - time: ""05:00""                 duration: ""00:10"" # <= See example 1.2.1       sequences:         - name: ""Controller 1, Sequence 1""           enabled: true # <= See example 1.0.1e           schedules:             - time: ""06:00""               duration: ""01:00"" # <= See example 1.0.1           zones:             - zone_id: [1, 2] # This is controller 1, sequence 1, zone reference 1               enabled: true # <= See example 1.0.1.1e               duration: ""00:10"" # <= See example 1.0.1.1             - zone_id: 2  # This is controller 1, sequence 1, zone reference 2               enabled: true # <= See example 1.0.1.2e               duration: ""00:10"" # <= See example 1.0.1.2         - name: ""Controller 1, Sequence 2""           enabled: true # <= See example 1.0.2e           schedules:             - time: ""07:00""               duration: ""01:00"" # <= See example 1.0.2           zones:             - zone_id: 1 # This is controller 1, sequence 2, zone reference 1               enabled: true # <= See example 1.0.2.1e               duration: ""00:10"" # <= See example 1.0.2.1     - name: ""Controller 2""       enabled: true # <= See example 2.0       zones:         - name: ""Controller 2, Zone 1""           enabled: true # <= See example 2.1         - name: ""Controller 2, Zone 2""           enabled: true # <= See example 2.2       sequences:         - name: ""Controller 2, Sequence 1""           enabled: true # <= See example 2.0.1e           schedules:             - time: ""09:00""               duration: ""01:00"" # <= See example 2.0.1           zones:             - zone_id: 1 # This is controller 2, sequence 1, zone reference 1               enabled: true # <= See example 2.0.1.1e               duration: ""00:10"" # <= See example 2.0.1.1             - zone_id: 2 # This is controller 2, sequence 1, zone reference 2               enabled: true # <= See example 2.0.1.2e               duration: ""00:10"" # <= See example 2.0.1.2         - name: ""Controller 2, Sequence 2""           enabled: true # <= See example 2.0.2e           schedules:             - time: ""09:00""               duration: ""01:00"" # <= See example 2.0.2           zones:             - zone_id: 1 # This is controller 2, sequence 2, zone reference 1               enabled: true # <= See example 2.0.2.1e               duration: ""00:10"" # <= See example 2.0.2.1             - zone_id: 2 # This is controller 2, sequence 2, zone reference 2               enabled: true # <= See example 2.0.2.2e               duration: ""00:10"" # <= See example 2.0.2.2 ```  Notes:  1. The `adjust_time` service call examples show the adjustment method of `actual`. This is shown for simplicity however all methods are available as described _[above](#76-service-adjust_time)_. 2. The `enable` service call can also be `disable` or `toggle`.  ```yaml # Example 1.0 -> controller 1 -> enabled. This will alter the enabled status for the controller. - service: irrigation_unlimited.enable   data:     entity_id: binary_sensor.irrigation_unlimited_c1_m  # Example 1.1 -> controller 1 -> zone 1 -> enabled. This will alter the enabled status for zone 1. - service: irrigation_unlimited.enable   data:     entity_id: binary_sensor.irrigation_unlimited_c1_z1  # Example 1.1.1 -> controller 1 -> zone 1 -> duration. This will alter the duration for zone 1. - service: irrigation_unlimited.adjust_time   data:     entity_id: binary_sensor.irrigation_unlimited_c1_z1     actual: ""00:20""  # Example 1.2 -> controller 1 -> zone 2 -> enabled. This will alter the enabled status of zone 2. - service: irrigation_unlimited.enable   data:     entity_id: binary_sensor.irrigation_unlimited_c1_z2  # Example 1.2.1 -> controller 1 -> zone 1 -> duration. This will alter the duration for zone 2. - service: irrigation_unlimited.adjust_time   data:     entity_id: binary_sensor.irrigation_unlimited_c1_z2     actual: ""00:20""  # Example 1.0.1e -> controller 1 -> sequence 1 -> enabled. This will alter the enabled status of sequence 1. - service: irrigation_unlimited.enable   data:     entity_id: binary_sensor.irrigation_unlimited_c1_m     sequence_id: 1  # Example 1.0.1 -> controller 1 -> sequence 1 -> duration. This will proportionally alter the duration # for all zone references in the first sequence. - service: irrigation_unlimited.adjust_time   data:     entity_id: binary_sensor.irrigation_unlimited_c1_m     sequence_id: 1     actual: ""00:20""  # Example 1.0.1.1e -> controller 1 -> sequence 1 -> zone reference 1 -> enabled. This will alter the enabled # status of the first zone reference in the first sequence. - service: irrigation_unlimited.enable   data:     entity_id: binary_sensor.irrigation_unlimited_c1_m     sequence_id: 1     zones: 1  # Example 1.0.1.1 -> controller 1 -> sequence 1 -> zone reference 1 -> duration. This will alter the duration # for the first zone reference in the first sequence. - service: irrigation_unlimited.adjust_time   data:     entity_id: binary_sensor.irrigation_unlimited_c1_m     sequence_id: 1     zones: 1     actual: ""00:20""  # Example 1.0.1.2 - controller 1 -> sequence 1 -> zone reference 2 -> enabled. This will alter the enabled # status of the second zone reference in the first sequence. - service: irrigation_unlimited.enable   data:     entity_id: binary_sensor.irrigation_unlimited_c1_m     sequence_id: 1     zones: 2  # Example 1.0.1.2 - controller 1 -> sequence 1 -> zone reference 2 -> duration. This will alter the duration # for the second zone reference in the first sequence. - service: irrigation_unlimited.adjust_time   data:     entity_id: binary_sensor.irrigation_unlimited_c1_m     sequence_id: 1     zones: 2     actual: ""00:20""  # Example 1.0.2e - controller 1 -> sequence 2 -> enabled. This will alter the enabled # status of the second sequence. - service: irrigation_unlimited.enabled   data:     entity_id: binary_sensor.irrigation_unlimited_c1_m     sequence_id: 2  # Example 1.0.2 - controller 1 -> sequence 2 -> duration. This will proportionally alter the duration # for all zone references in the second sequence. - service: irrigation_unlimited.adjust_time   data:     entity_id: binary_sensor.irrigation_unlimited_c1_m     sequence_id: 2     actual: ""00:20""  # Example 1.0.2.1e - controller 1 -> sequence 2 -> zone reference 1 -> enabled - service: irrigation_unlimited.enable   data:     entity_id: binary_sensor.irrigation_unlimited_c1_m     sequence_id: 2     zones: 1  # Example 1.0.2.1 - controller 1 -> sequence 2 -> zone reference 1 -> duration - service: irrigation_unlimited.adjust_time   data:     entity_id: binary_sensor.irrigation_unlimited_c1_m     sequence_id: 2     zones: 1     actual: ""00:20""  # Example 2.0 -> controller 2 -> enabled. This will alter the enabled status for the controller. - service: irrigation_unlimited.enable   data:     entity_id: binary_sensor.irrigation_unlimited_c2_m  # Example 2.1 -> controller 2 -> zone 1 -> enabled. This will alter the enabled status for zone 1. - service: irrigation_unlimited.enable   data:     entity_id: binary_sensor.irrigation_unlimited_c2_z1  # Example 2.2 -> controller 2 -> zone 2 -> enabled. This will alter the enabled status of zone 2. - service: irrigation_unlimited.enable   data:     entity_id: binary_sensor.irrigation_unlimited_c2_z2  # Example 2.0.1e - controller 2 -> sequence 1 -> enabled. This will alter the enabled status # for the first sequence. - service: irrigation_unlimited.enable   data:     entity_id: binary_sensor.irrigation_unlimited_c2_m     sequence_id: 1  # Example 2.0.1 - controller 2 -> sequence 1 -> duration. This will proportionally alter the duration # for all zone references in the first sequence. - service: irrigation_unlimited.adjust_time   data:     entity_id: binary_sensor.irrigation_unlimited_c2_m     sequence_id: 1     actual: ""00:20""  # Example 2.0.1.1e - controller 2 -> sequence 1 -> zone reference 1 -> enabled. This will alter the enabled # status for the first zone reference in the first sequence. - service: irrigation_unlimited.enable   data:     entity_id: binary_sensor.irrigation_unlimited_c2_m     sequence_id: 1     zones: 1  # Example 2.0.1.1 - controller 2 -> sequence 1 -> zone reference 1 -> duration. This will alter the duration # for the first zone reference in the first sequence. - service: irrigation_unlimited.adjust_time   data:     entity_id: binary_sensor.irrigation_unlimited_c2_m     sequence_id: 1     zones: 1     actual: ""00:20""  # Example 2.0.1.2e - controller 2 -> sequence 1 -> zone reference 2 -> enabled. This will alter the enabled # status for the second zone reference in the first sequence. - service: irrigation_unlimited.enable   data:     entity_id: binary_sensor.irrigation_unlimited_c2_m     sequence_id: 1     zones: 2  # Example 2.0.1.2 - controller 2 -> sequence 1 -> zone reference 2 -> duration. This will alter the duration # for the second zone reference in the first sequence. - service: irrigation_unlimited.adjust_time   data:     entity_id: binary_sensor.irrigation_unlimited_c2_m     sequence_id: 1     zones: 2     actual: ""00:20""  # Example 2.0.2e - controller 2 -> sequence 2 -> enabled - service: irrigation_unlimited.enable   data:     entity_id: binary_sensor.irrigation_unlimited_c2_m     sequence_id: 2  # Example 2.0.2 - controller 2 -> sequence 2 -> duration - service: irrigation_unlimited.adjust_time   data:     entity_id: binary_sensor.irrigation_unlimited_c2_m     sequence_id: 2     actual: ""00:20""  # Example 2.0.2.1e - controller 2 -> sequence 2 -> zone reference 1 -> enabled - service: irrigation_unlimited.enable   data:     entity_id: binary_sensor.irrigation_unlimited_c2_m     sequence_id: 2     zones: 1  # Example 2.0.2.2 - controller 2 -> sequence 2 -> zone reference 1 -> duration - service: irrigation_unlimited.adjust_time   data:     entity_id: binary_sensor.irrigation_unlimited_c2_m     sequence_id: 2     zones: 1     actual: ""00:20""  # Example 2.0.2.2e - controller 2 -> sequence 2 -> zone reference 2 -> enabled - service: irrigation_unlimited.enable   data:     entity_id: binary_sensor.irrigation_unlimited_c2_m     sequence_id: 2     zones: 2  # Example 2.0.2.2 - controller 2 -> sequence 2 -> zone reference 2 -> duration - service: irrigation_unlimited.adjust_time   data:     entity_id: binary_sensor.irrigation_unlimited_c2_m     sequence_id: 2     zones: 2     actual: ""00:20"" ```  ## 8. Frontend  From release 2022.4.0 a [companion card](https://github.com/rgc99/irrigation-unlimited-card) is available.  ![companion card collapsed](./examples/companion_card_collapsed.png)  and expands to  ![companion card expanded](./examples/companion_card_expanded.png)  ### 8.1. Generic Cards  Most of the following will require installation of further [lovelace cards](https://www.home-assistant.io/lovelace/). For some inspiration and a compact card try [this](./lovelace/card.yaml).  ![Collapsed](./examples/card_collapsed.png)  and it expands to:  ![Expanded](./examples/card_expanded.png)  Note: This card uses some custom cards [multiple-entity-row](https://github.com/benct/lovelace-multiple-entity-row), [fold-entity-row](https://github.com/thomasloven/lovelace-fold-entity-row), [logbook-card](https://github.com/royto/logbook-card) and at the moment [card-mod](https://github.com/thomasloven/lovelace-card-mod) for styles.  For watering history information here is a [sample card](./lovelace/watering_history_card.yaml).  ![watering_history_card](./examples/watering_history_card.png).  Note: At time of writing this requires a pre-released version of [mini-graph-card](https://github.com/kalkih/mini-graph-card/releases/tag/v0.11.0-dev.3). Note: If you get ""NaN"" displayed instead of the actual value then clear out your browsers cache and make sure the development release is installed.  Although not really part of the integration but to get you started quickly here is a [temperature card](./lovelace/temperature_card.yaml).  ![temperature_card](./examples/temperature_card.png).  And a [rainfall card](./lovelace/rainfall_card.yaml). Note how the watering times reduced as rainfall started. More on this below in [Automation](#9-automation).  ![rainfall_card](./examples/rainfall_card.png)  Finally, a system event [log](./lovelace/system_history_card.yaml)  ![system_history_card](./examples/system_history_card.png)  Putting it all together, here is the [complete picture](./lovelace/my_dashboard.yaml)  ![my_dashboard.png](./examples/my_dashboard.png)  This configuration is three vertical stacks and works well on mobile devices.  ### 8.2. Timeline  Minimum version 2021.12.0 of Irrigation Unlimited is required for this feature. First up, enable the timeline in the [zone show object](#54-zone-show-object).  ```yaml irrigation_unlimited:   controllers:     all_zones_config: # <= Add these three lines <‚îÄ‚îê       show: # <= to the configuration            <‚îÄ‚î§         timeline: true # <= for all zones        <‚îÄ‚îò     zones:       entity_id: ""switch.my_switch""       show: # <= Add these two lines to the                    <‚îÄ‚îê         timeline: true # <= configuration for individual zones <‚îÄ‚îò       schedules:         - time: ""06:00""           duration: ""00:20"" ```  Like the watering history card above it also shows the upcoming schedule for a complete overview of your irrigation. Find the code [here](./lovelace/timeline_chart.yaml). Requires [apexcharts-card](https://github.com/RomRider/apexcharts-card).  ![timeline_chart](./examples/timeline_chart.png)  If you prefer something akin to a airport departure board then try [this](./lovelace/timeline_card.yaml). Uses Markdown card which is built into Home Assistant so will work straight out of the box.  ![timeline_card](./examples/timeline_card.png)  ### 8.3. Frontend Requirements  The [manual_run](#84-manual-run-card) and [enable/disable](#85-enable-disable-card) cards require additional support files. Minimum version 2021.6.3 of Irrigation Unlimited is required. There is a support file [packages/irrigation_unlimited_controls.yaml](./packages/irrigation_unlimited_controls.yaml) which should go in the config/packages directory. Also required is a [pyscript](./pyscript/irrigation_unlimited_service_shim.py) which is called from the above automation to populate the input_select with all the irrigation unlimited controllers and zones. The script should go in the **config/pyscript directory**. If you don't have a packages and a pyscript folder then create them and add the following to your configuration.yaml.  ```yaml homeassistant:   packages: !include_dir_named packages ```  Using your HA configuration directory (folder) as a starting point you should now also have this:  ```text pyscript/irrigation_unlimited_service_shim.py packages/irrigation_unlimited_controls.yaml ```  More information on packages can be found [here](https://www.home-assistant.io/docs/configuration/packages) and pyscript can be found [here](https://github.com/custom-components/pyscript), don't worry about the Jupyter kernel unless you are really keen. Hint: A pyscript is used instead of Jinja2 as it produces a list which Jinja2 is not capable of, many have tried... The pyscript is a small piece of code that convert for example ‚Äò1.1 Zone1‚Äô inside an input_select control into ‚Äòbinary_sensor.irrigation_unlimited_c1_z1‚Äô and then call the actual service. They are just helpers sitting between the lovelace card and the integration. It's a great way to add some additional capabilities to lovelace cards.  ### 8.4. Manual run card  Here is a card for manual runs, see [requirements](#83-frontend-requirements) above. You can find the code [here](./lovelace/card_manual_run.yaml). Note: This card uses [paper-buttons-row](https://github.com/jcwillox/lovelace-paper-buttons-row) and [time-picker-card](https://github.com/GeorgeSG/lovelace-time-picker-card).  ![manual_run_card](./examples/card_manual_run.png)  ### 8.5. Enable-disable card  This card will enable or disable a zone from a dropdown list, see [requirements](#83-frontend-requirements) above. The code is [here](./lovelace/card_enable_disable.yaml). Like the manual run card it requires [paper-buttons-row](https://github.com/jcwillox/lovelace-paper-buttons-row).  ![enable_disable_card](./examples/card_enable_disable.png)  ### 8.6. Pause-resume button  The following yaml script can be attached to a front end button to [`pause` and `resume`](#72-services-pause-and-resume) all zones of all sequences of all UI controllers.  ![pause_resume_button](./examples/pause-resume-button.png)  Please note that a group helper for all UI controllers is required for this code sample to work.  ```yaml group:   irrigation_controllers:   name: Irrigation Controllers   entities:     #Add at least one UI controller entity here     - binary_sensor.irrigation_unlimited_c1_m     - binary_sensor.irrigation_unlimited_c2_m     - binary_sensor.irrigation_unlimited_c3_m     - binary_sensor.irrigation_unlimited_c4_m     - binary_sensor.irrigation_unlimited_c5_m     - binary_sensor.irrigation_unlimited_c6_m     #Add as many controller entities as you have configured  script:   toggle_irrigation:       alias: ""Irrigation: Pause/Resume all irrigation unlimited controllers""       sequence:         - choose:             - conditions:                 - condition: template                   value_template: >-                     {% set paused_sensors = states.binary_sensor                         | selectattr('entity_id', 'match', '^binary_sensor\.irrigation_unlimited_c\d+_s\d+$')                         | selectattr('attributes.status', 'equalto', 'paused')                         | map(attribute='entity_id')                         | list %}                     {{ paused_sensors | length > 0 }}               sequence:                 - service: irrigation_unlimited.resume                   data:                     entity_id: group.irrigation_controllers                     sequence_id: 0             - conditions:                 - condition: template                   value_template: >-                     {% set paused_sensors = states.binary_sensor                         | selectattr('entity_id', 'match', '^binary_sensor\.irrigation_unlimited_c\d+_s\d+$')                         | selectattr('attributes.status', 'equalto', 'paused')                         | map(attribute='entity_id')                         | list %}                     {{ paused_sensors | length == 0 and states('group.irrigation_controllers') == 'on' }}               sequence:                 - service: irrigation_unlimited.pause                   data:                     entity_id: group.irrigation_controllers                     sequence_id: 0       icon: mdi:play-pause ```  [Issue 142](https://github.com/rgc99/irrigation_unlimited/issues/142) has a detailed discussion relating the [`pause` and `resume` service call](#72-services-pause-and-resume) and its various use cases.  ## 9. Automation  Due to the many weather integrations available and their relevance to your situation, there is realistically no way to provide a built in 'auto-adjustment' feature. Therefore, no attempt has been made to include a solution and this also makes the integration more independent and flexible. Run time adjustment is achieved by setting up sensor(s) that consume weather information such as rainfall and temperature but could factor in wind speed, solar radiation etc. to determine if more or less watering time is required. You might also consider using forecast information... A service call is then made to irrigation unlimited to adjust the run times. This does mean some knowledge of creating automations is required.  On a personal note, I use the national weather service [BOM](http://www.bom.gov.au) for my forecast information but find their observation data not relevant due to the extreme regional variations in my situation. There are many micro climates (mountains) and a few kilometres in any direction makes a lot of difference, down pour to a few drops. To this end I have a Personal Weather Station (PWS) that feeds [Weather Underground](https://www.wunderground.com) where I use the [WUnderground](https://www.home-assistant.io/integrations/wunderground) integration to retrieve the data.  You will find my adjustment automation [here](./packages/irrigation_unlimited_adjustment.yaml) which feeds off the temperature and rainfall observation data. There is a card [here](./lovelace/observations_card.yaml) which displays this information (uses [multiple-entity-row](https://github.com/benct/lovelace-multiple-entity-row)). Some ideas were gleaned from [kloggy's](https://github.com/kloggy/HA-Irrigation-Version2) work.  ### 9.1. ESPHome  This example uses the data from a soil moisture probe created in [ESPHome](https://esphome.io/) to adjust the run times.  ```yaml automation:   - alias: ESPHome soil moisture adjustment     trigger:       platform: state       entity_id:         - sensor.yard1_humidity     action:       service: irrigation_unlimited.adjust_time       data:         entity_id: binary_sensor.irrigation_unlimited_c1_m         percentage: >           {# Threshold variable 0-100 percent #}           {% set threshold = 40 %}            {# Sensor data #}           {% set humidity = states('sensor.yard1_humidity') | float %}            {% if humidity < threshold %}             {# Option 1 - A linear sliding scale #}             {% set multiplier = 1 - (humidity / threshold) %}             {# Option 2 - On or Off #}             {% set multiplier = 1.0 %}           {% else %}             {% set multiplier = 0.0 %} {# It's too wet, turn off #}           {% endif %}            {# Return multiplier as a percentage #}           {{ (multiplier * 100) | round(0) }} ```  ### 9.2. HAsmartirrigation  [HAsmartirrigation](https://github.com/jeroenterheerdt/HAsmartirrigation) calculates the time to run your irrigation system to compensate for moisture lost by evaporation / evapotranspiration. The following automation runs at 23:30 and takes the calculated run time from HAsmartirrigation and updates Irrigation Unlimited with the new watering time. It then calls HAsmartirrigation to reset the bucket when the irrigation has run.  The example below offers two methods for a single zone or a sequence.  ```yaml # Example automation for HAsmartirrigation integration (smart_irrigation)[https://github.com/jeroenterheerdt/HAsmartirrigation] automation:   - id: 'IU1653097957047'     alias: Smart Irrigation adjustment     description: Adjust watering times based on smart irrigation calculations     trigger:       - platform: time         at: ""23:30""     condition:       condition: and       conditions:         - ""{{ states('sensor.smart_irrigation_daily_adjusted_run_time') | float(-1) >= 0 }}""     action:       - service: irrigation_unlimited.adjust_time         data:           actual: ""{{ timedelta(seconds=states('sensor.smart_irrigation_daily_adjusted_run_time') | int(0)) }}""           # -------------------------------------------------------------------           # Please see documentation regarding the adjust_time service call.           # Choose an option below. Comment out/delete as needed. This will NOT work as is.           # 1. Adjust a single zone. Change the zone as required           # entity_id: binary_sensor.irrigation_unlimited_c1_z1           # 2. Adjust a sequence. Change the sequence_id as required           # entity_id: binary_sensor.irrigation_unlimited_c1_m           # sequence_id: 1           # -------------------------------------------------------------------     mode: single    - id: 'IU1653098247170'     alias: Smart Irrigation reset bucket     description: Resets the Smart Irrigation bucket after watering     trigger:       - platform: state         entity_id:           # Add Irrigation Unlimited sensors here           - binary_sensor.irrigation_unlimited_c1_m         from: ""on""         to: ""off""     condition:       - condition: numeric_state         above: '0'         entity_id: sensor.smart_irrigation_daily_adjusted_run_time     action:       - service: smart_irrigation.smart_irrigation_reset_bucket ```  ### 9.3. Overnight watering  Run from sunset to sunrise. This automation will run 1 hour before sunset. It uses the sun integration to calculate the duration from sunset to sunrise and then set this via the adjust_time service call. Create a schedule that starts at sunset and just put in a nominal duration. The duration will be replaced by the service call. Please take note of the comments in the automation as you must change it to suit your configuration.  ```yaml automation:   - id: 'IU1655789912900'     alias: IU Overnight     description: Run irrigation from sunset to sunrise     trigger:       - platform: sun         event: sunset         offset: -00:60:00     condition: []     action:       service: irrigation_unlimited.adjust_time       data:         # -------------------------------------------------------------------         # Please see documentation regarding the adjust_time service call.         # Choose an option below. Comment out/delete/change as needed.         # *** This will NOT work as is. ***         # 1. Adjust a single zone. Change the zone as required         # entity_id: binary_sensor.irrigation_unlimited_c1_z1         # 2. Adjust a sequence. Change the sequence_id as required         # entity_id: binary_sensor.irrigation_unlimited_c1_m         # sequence_id: 1         # -------------------------------------------------------------------         actual: >           {% set t1 = as_datetime(state_attr(""sun.sun"", ""next_setting"")).replace(microsecond=0) %}           {% set t2 = as_datetime(state_attr(""sun.sun"", ""next_rising"")).replace(microsecond=0) %}           {{ t2 - t1 }}     mode: single ```  ## 10. Notifications  This section shows how to send a notification when a sequence starts or finishes. Messages can be sent for example via email (SMTP), push notification to mobile phones, twitter and many [others](https://www.home-assistant.io/integrations/#notifications). See [here](https://www.home-assistant.io/integrations/notify/) for more information on notifications in Home Assistant. Note that it is not limited to sending notifications but many other [actions](https://www.home-assistant.io/docs/automation/action/) are available. There is quite a lot of information on using notifications in Home Assistant on the web. Try Google, YouTube etc. for some great information and tips.  ### 10.1. Events  Irrigation Unlimited fires events that can be captured in an automation using the [event platform](https://www.home-assistant.io/docs/automation/trigger/#event-trigger) as a trigger.  #### 10.1.1. irrigation_unlimited_start, irrigation_unlimited_finish  These events are fired when a sequence starts and finishes. The `trigger.event.data` contains additional information that can be used in automation scripts. Here is the list of additional fields.  | Field | Description | | ----- | ----------- | | `entity_id` | The sequence entity i.e. `binary_sensor.irrigation_unlimted_c1_s1`. | | `controller.index` | The sequential index of the controller. | | `controller.controller_id` | The unique id of the controller. | | `controller.name` | The friendly name of the controller. | | `sequence.index` | The sequential index of the sequence. | | `sequence.sequence_id` | The unique id of the sequence. | | `sequence.name` | The friendly name of the sequence. | | `schedule.index` | The sequential index of the schedule. Note: This maybe blank/empty(None) if it was a manual run - useful as a test. | | `schedule.schedule_id` | The unique id of the schedule. Note: This maybe blank/empty(None) if it was a manual run. | | `schedule.name` | The friendly name of the schedule. | | `run.duration` | The run time of the sequence. |  This example displays a [persistent notification](https://www.home-assistant.io/integrations/persistent_notification/) on the front end when a sequence completes. Note the use of [templating](https://www.home-assistant.io/docs/configuration/templating/) to construct a specific message. Although not used here, this platform also supports markdown.  ```yaml automation:   - alias: ""Irrigation Unlimited Completion""     trigger:       platform: event       event_type: irrigation_unlimited_finish     action:       - service: notify.persistent_notification         data:           title: ""Irrigation Unlimited - Completed""           message: |             Time: {{ as_local(trigger.event.time_fired).strftime('%c') }}             Controller: {{ trigger.event.data.controller.index + 1 }} {{ trigger.event.data.controller.name }}             Sequence: {{ trigger.event.data.sequence.index + 1 }} {{ trigger.event.data.sequence.name }}             Schedule: {% if trigger.event.data.schedule.index is integer %}{{ trigger.event.data.schedule.index + 1 }} {{ trigger.event.data.schedule.name }}{% else %}Manual{% endif %}             Duration: {{ timedelta(seconds=trigger.event.data.run.duration) }} ```  Here is the notification displayed in the Home Assistant web interface.  ![notification](./examples/persistent_notification.png)  #### 10.1.2. irrigation_unlimited_switch_error, irrigation_unlimited_sync_error  These events are fired during a [check back](#510-check-back-object) operation. A `irrigation_unlimited_sync_error` is fired if the physical switch is found to be out of sync. This message will repeat for each attempted resync. After the specified number of retries have been exhusted a `irrigation_unlimited_switch_error` is fired. Additional information is available that can be used in automation scripts.  | Field | Description | | ----- | ----------- | | `entity_id` | A CSV list of entities. | | `expected` | The expected state of the switch. | | `controller.index` | The sequential index of the controller. | | `controller.name` | The friendly name of the controller. | | `zone.index` | The sequential index of the zone. | | `zone.name` | The friendly name of the zone. Note: This maybe blank/empty (None) if it was the controller switch. |  Example to send an email on switch failure. There are two parts to this procedure, the first is to setup an email notification for your provider. See [here](https://www.home-assistant.io/integrations/smtp/#google-mail) for a google mail example. The next is to use the notifier.  ```yaml automation:   - id: ""IU1653340138435""     alias: ""Irrigation Unlimited Switch Error""     description: ""Email a switch syncronisation error""     trigger:       - platform: event         event_type:           # - irrigation_unlimited_sync_error           - irrigation_unlimited_switch_error     action:       - service: notify.NOTIFIER_NAME # Make sure this matches the ""NOTIFIER_NAME"" in the smtp setup         data:           title: ""Irrigation Switch Error""           message: |             Type: {{ trigger.event.event_type }}             Time: {{ as_local(trigger.event.time_fired).strftime('%c') }}             Expected: {{ trigger.event.data.expected }}             Controller: {{ trigger.event.data.controller.index + 1 }} {{ trigger.event.data.controller.name }}             {% if trigger.event.data.zone.index is not none %}               Zone: {{ trigger.event.data.zone.index + 1 }} {{ trigger.event.data.zone.name }}             {% endif %}             Entity: {{ trigger.event.data.entity_id }} ```  ## 11. Troubleshooting  There should be little trouble installing this component, please use the _[HACS](#41-install-from-hacs)_ method where possible. Binary sensors are created automatically. However, if you experience difficulties please check the following:  ### 11.1. Requirements  This integration depends on two other components; _[recorder](https://www.home-assistant.io/integrations/recorder/)_ and _[history](https://www.home-assistant.io/integrations/history/)_. Both of these components are part of the standard Home Assistant installation and enabled by default with the `default_config:` line in the configuration. If you have removed this line then a `history:` and `recorder:` section must be setup manually. If a mistake is made in either one of these configurations then they will not start and in turn, Irrigation Unlimited for which it depends on, will not start. Please check the log file for the following lines:  ```text 2021-08-03 12:12:40 INFO (MainThread) [homeassistant.setup] Setting up recorder 2021-08-03 12:12:40 INFO (MainThread) [homeassistant.setup] Setup of domain recorder took 0.1 seconds ... 2021-08-03 12:12:42 INFO (MainThread) [homeassistant.setup] Setting up history 2021-08-03 12:12:42 INFO (MainThread) [homeassistant.setup] Setup of domain history took 0.0 seconds ```  The above shows the requirements were loaded successfully. Note: The lines may not be consecutive in the log. If you do not see these lines then go back to basics and remove any `history:` and `recorder:` sections and ensure the `default_config:` line is present. Restart HA and check you have these log entries.  ### 11.2. HA Configuration  There must be a `irrigation_unlimited:` section in the configuration. If the section is missing or invalid then Irrigation Unlimited will not start. Check the log file to see it successfully started up.  ```text 2021-08-03 12:12:45 INFO (MainThread) [homeassistant.setup] Setting up irrigation_unlimited ... 2021-08-03 12:12:47 INFO (MainThread) [homeassistant.setup] Setup of domain irrigation_unlimited took n.n seconds ```  The above shows that Irrigation Unlimited loaded successfully. Note: The lines will most likely not be together so do a search. If it failed then use the minimal configuration shown _[here](#61-minimal-configuration)_. This is a good starting point to get acquainted with this integration.  ### 11.3 Community Forum  Have a question then head over to the Irrigation Unlimited [community forum](https://community.home-assistant.io/t/irrigation-unlimited-integration/325468/401). If discussing a problem it is best to post the configuration along with the [log](#114-logging).  ### 11.4. Logging  These messages are very important in helping to determine any problem. Go into Settings -> System -> Logs. In the search box put in `irrigation_unlimited` (Note the underscore between the two words) and press the **LOAD FULL LOGS** button. The log should always accompany the configuration when seeking help on the [community forum](#113-community-forum) or opening an [issue](#115-last-but-not-least).  For more detailed information set your logging for the component to debug:  ```yaml logger:   default: info   logs:     custom_components.irrigation_unlimited: debug ```  ### 11.5. Last but not least  If all else fails please open an [issue](https://github.com/rgc99/irrigation_unlimited/issues). Please be sure to post the configuration _AND_ the log.  ## 12. Notes  1. All feature requests, issues and questions are welcome.  <a href=""https://www.buymeacoffee.com/rgc99"" target=""_blank""><img src=""https://cdn.buymeacoffee.com/buttons/v2/default-yellow.png"" alt=""Buy Me A Coffee"" height=40px width=144px></a><!-- markdownlint-disable-line MD033 -->  ## 13. Snake case   The [controller_id](#51-controller-objects), [zone_id](#53-zone-objects) and [schedule_id](#55-schedule-objects) identifiers need to be in snake_case like `my_garden`, `vege_patch`, `rose_bed`, `front_lawn`, `before_dawn`. The allowable characters are lower case alphabet, numerals and the underscore. The underscore cannot be used as a leading or trailing character and not more than one together. For more information see [here](https://en.wikipedia.org/wiki/Snake_case)  ## 14. Parameter Types  ### 14.1 Irrigation Unlimited Entities  This parameter specifies one or more Irrigation Unlimited entities such as 'binary_sensor.irrigation_unlimited_c1_z1'. Multiple entities can be a CSV string or a list. Here is a code snippet to show different ways to specify the entity_ids.  ```yaml   ...   entity_id: binary_sensor.irrigation_unlimited_c1_z1   entity_id: binary_sensor.irrigation_unlimited_c1_z1,binary_sensor.irrigation_unlimited_c1_z2   entity_id:     - binary_sensor.irrigation_unlimited_c1_z1     - binary_sensor.irrigation_unlimited_c1_z2   ... ```  ### 14.2 Duration (Time Period)  The time period (duration) type is a string in the format HH:MM, HH:MM:SS, the number of seconds or a dictionary. Time type must be a positive value unless otherwise noted. The value will be rounded down to the system granularity. The default granularity is one second. This is the heart beat or system pulse. All times will be synchronised to these boundaries. Here are different ways to specify 10 minutes.  ```yaml   ...   time: '00:10' # HH:MM   time: '0:10:00' # H:MM:SS   time: '00:10:00' # HH:MM:SS   time: 600 # Seconds   time: # One or more or the following     days: 0     hours: 0     minutes: 10     seconds: 0   ... ```  ### 14.3 Switch entities  These can be any entity from the `switch`, `light`, `valve` or `cover` platforms or anything that supports the `turn_on` and `turn_off` actions. Multiple entities can be a CSV string or a list. Here is a code snippet to show different ways to specify the entity_ids.  ```yaml   ...   entity_id: switch.valve_1   entity_id: light.valve_1,light.valve_2   entity_id:     - switch.valve_1     - light.valve_2   ... ```  In case you wish to control some other device like a motorised valve that presents itself in Home Assistant as a cover then adapt the following automation.  ```yaml automation:   - alias: 'Zone 1 motorised valve'     description: Turn on/off motorised valve     trigger:       - platform: state         entity_id:           - binary_sensor.irrigation_unlimited_c1_z1     condition: []     action:       - service: |-           {% if trigger.to_state.state == 'on' %}             cover.open_cover           {% else %}             cover.close_cover           {% endif %}         entity_id: cover.my_cover_1     mode: single ```  ### 14.4 Templating  Some parameters support [templating](https://www.home-assistant.io/docs/configuration/templating). Actual support is noted in the relevant documentation.  Templating is an advanced Home Assistant scripting technique. In many cases such as scripts and automations templating is built in. In other situations like the tap action from a button card, templating is not available but the string will be passed to the service call. Irrigation Unlimited will detect the string contains a template and convert it. Here are some examples that retrieve data from an input text box and pass it to a service call.  ```yaml   ...   action:     service: irrigation_unlimited.adjust_time     service_data:       percentage: ""{{ states('input_text.adjustment') | float(100) }}""   ...   action:     service: irrigation_unlimited.manual_run     service_data:       time: ""{{ states('input_text.run_time') | default('00:00') }}"" ```  ### 14.5 Sequence  Sequence to adjust, a number (1, 2..N). This is the position number of the sequence under the controller. `sequence_id: 1` is the first, 2 is the second and so on. As a shortcut, `sequence_id: 0` will alter _all_ sequences. Within a controller, sequences are numbered by their position starting at 1. Only relevant when entity_id is a controller/master. An error message will be generated if a `sequence_id` is specified and `entity_id` is not a controller/master.  ### 14.6 Zones  Zone(s) to adjust, number/list (1, 2..N). This is the position number of the zone reference under the sequence. `zones: 1` is the first, 2 is the second and so on. As a shortcut, `zones: 0` will alter _all_ zones. Within a sequence, zones are numbered by their position starting a 1.  ### 14.7 Time (Time of Day)  The time of day type is a string in the format HH:MM or HH:MM:SS. It is assumed to be in the local time.  ## 15. Contributions are welcome  If you want to contribute to this please read the [Contribution guidelines](CONTRIBUTING.md).  ## 16. Credits  Code template was mainly taken from [@Ludeeus](https://github.com/ludeeus)'s [integration_blueprint][integration_blueprint] template.  Some inspiration was taken from [kloggy's](https://github.com/kloggy/HA-Irrigation-Version2) work.  ---  [buymecoffee]: https://www.buymeacoffee.com/rgc99 [buymecoffeebadge]: https://img.shields.io/badge/buy%20me%20a%20coffee-donate-yellow.svg?style=for-the-badge [commits-shield]: https://img.shields.io/github/commit-activity/y/rgc99/irrigation_unlimited?style=for-the-badge [commits]: https://github.com/rgc99/irrigation_unlimited/commits/master [hacs]: https://github.com/custom-components/hacs [hacsbadge]: https://img.shields.io/badge/HACS-Custom-orange.svg?style=for-the-badge [forum-shield]: https://img.shields.io/badge/community-forum-brightgreen.svg?style=for-the-badge [forum]: https://community.home-assistant.io/t/irrigation-unlimited-integration/ [maintenance-shield]: https://img.shields.io/badge/maintainer-Robert%20Cook%20%40rgc99-blue.svg?style=for-the-badge [releases-shield]: https://img.shields.io/github/release/rgc99/irrigation_unlimited.svg?style=for-the-badge [releases]: https://github.com/rgc99/irrigation_unlimited/releases [license-shield]: https://img.shields.io/github/license/rgc99/irrigation_unlimited.svg?style=for-the-badge [license]: LICENSE [integration_blueprint]: https://github.com/custom-components/integration_blueprint"
IoT Electronic Door Opener,Internet of Things,https://github.com/MicrochipTech/iot_solutions_for_smart_garage_door_opener,"# IoT Solutions for Smart Garage Door Opener <img src=""images/IoT-Made-Easy-Logo.png"" width=100>   > ""Wireless Made Easy!"" - IoT Solutions for Smart Garage Door Opener  Devices: **| WFI32E01 | WBZ451 |**<br> Features: **| Multiprotocol | Secure Cloud Connectivity | Voice Control |**   <p align=""center""> <a href=""https://youtu.be/DsprKlnv_J8"" target=""_blank""> <img src=""01_wifi_solution/images/wfi32_garage_door_thumbnail.png""  alt=""Garage Door Demo with voice commands based on WFI32 Out of the box Application developed with MPLAB X IDE and MPLAB Harmony v3."" width=""480""></a> </p>  ## ‚ö† Disclaimer  <p><span style=""color:red""><b> THE SOFTWARE ARE PROVIDED ""AS IS"" AND GIVE A PATH FOR SELF-SUPPORT AND SELF-MAINTENANCE. This repository contains example code intended to help accelerate client product development. </br>  For additional Microchip repos, see: <a href=""https://github.com/Microchip-MPLAB-Harmony"" target=""_blank"">https://github.com/Microchip-MPLAB-Harmony</a>  Checkout the <a href=""https://microchipsupport.force.com/s/"" target=""_blank"">Technical support portal</a> to access our knowledge base, community forums or submit support ticket requests. </span></p></b>  ## Description  With this Smart Garage Door Opener demo from Microchip, the user has the ability to control the garage door over different Wireless technologies. Using Microchip Bluetooth Data Smartphone App to locally control the garage door or control the garage door remotely from Voice Assistant or from Alexa App over AWS Cloud.  <p align=""center""> <img src=""images/architecture.png"" width=640> </p>  ## A la carte  1. <font size=""6"">[Control Smart Garage Door with Wi-Fi using WFI32 Module](01_wifi_solution/README.md#top)</font>    1. [Introduction](01_wifi_solution/README.md#step1)    1. [Bill of materials](01_wifi_solution/README.md#step2)    1. [Hardware Setup](01_wifi_solution/README.md#step3)    1. [Software Setup](01_wifi_solution/README.md#step4)    1. [Harmony Configuration](01_wifi_solution/README.md#step5)    1. [Run the demo](01_wifi_solution/README.md#step6) 1. <font size=""6"">[Control Smart Garage Door with BLE/Zigbee using WBZ451 Module](02_ble_zigbee_solution/README.md#top)</font>    1. [Introduction](02_ble_zigbee_solution/README.md#step1)    1. [Bill of materials](02_ble_zigbee_solution/README.md#step2)    1. [Hardware Setup](02_ble_zigbee_solution/README.md#step3)    1. [Software Setup](02_ble_zigbee_solution/README.md#step4)    1. [Harmony Configuration](02_ble_zigbee_solution/README.md#step5)    1. [Run the demo](02_ble_zigbee_solution/README.md#step6)   <a href=""#top"">Back to top</a> "
IoT Home Automation Using Raspberry Pi,Internet of Things,https://github.com/amalpoulose/Home-Automation-using-RaspberryPi-IOT,"# Home-Automation-using-RaspberryPi-IOT  ![alt text](https://github.com/amalpoulose/Home-Automation-using-RaspberryPi-IOT/blob/master/smart-home.jpg)  # Introduction  Home automation or domotics is building automation for a home, called a smart home or smart house.Home automation gives you access to control devices in your home from a mobile device anywhere in the world. The term may be used for isolated programmable devices, like thermostats and sprinkler systems, but home automation more accurately describes homes in which nearly everything -- lights, appliances, electrical outlets, heating and cooling systems -- are hooked up to a remotely controllable network. Home automation is a step toward what is referred to as the ""Internet of Things,"" in which everything has an assigned IP address, and can be monitored and accessed remotely.  Automation refers to the ability to program and schedule events for the devices on the network. The programming may include time-related commands, such as having your lights turn on or off at specific times each day. It can also include non-scheduled events, such as turning on all the lights in your home when your security system alarm is triggered.  The other main characteristic of cutting-edge home automation is remote monitoring and access.Monitoring apps can provide a wealth of information about your home, from the status of the current moment to a detailed history of what has happened up to now. You can check your security system's status, whether the lights are on, whether the doors are locked, what the current temperature of your home is and much more. With cameras as part of your home automation system, you can even pull up real-time video feeds and literally see what's going on in your home while you're away.  # prerequisites:   Software :        Python2.7           External python modules needed :                -> Pubnub                -> Twilio                -> I2c tools for smbus                -> Picamera  Try the below commands to fulfill software requirements  :         $chmod +x setup.sh       $./setup.sh  Hardware :       Raspberry Pi          sensors : IR, LDR, RTC(DS3231), Temperature Sensor(DS18b20), PIR, Picamera, Relay, Buzzer               # Setting Up DS18B20(Temperature Sensor)  Before executing this program add below lines into /boot/config.txt add below lines at the end of /boot/config.txt and reboot your raspberry pi      #To enable ds18b20	     dtoverlay=w1-gpio,gpiopin=5       Connect temperature sensor output to BCM5    Temperature data will be available in file :        vi /sys/bus/w1/devices/28-xxxxxxxxxxxxxxxxxxx/wi_slave  # Setting up I2C and Picamera  Run sudo raspi-config and choose in the menu to enable the pi camera and I2c       $sudo raspi-config      choose interfacig options      enable i2c      enable picamera       # RaspberryPi 3B   ![alt text](https://github.com/amalpoulose/Home-Automation-using-RaspberryPi-IOT/blob/master/raspberrypi.jpg)  Features:      1.CPU: Quad-core 64-bit ARM Cortex A53 clocked at 1.2 GHz     2.GPU: 400MHz VideoCore IV multimedia     3.Memory: 1GB LPDDR2-900 SDRAM (i.e. 900MHz)     4.USB ports: 4     5.Video outputs: HDMI, composite video (PAL and NTSC) via 3.5 mm jack     6.Network: 10/100Mbps Ethernet and 802.11n Wireless LAN     7.Peripherals: 17 GPIO plus specific functions, and HAT ID bus     8.Bluetooth: 4.1     9.Power source: 5 V via MicroUSB or GPIO header      10.Size: 85.60mm √ó 56.5mm     11.Weight: 45g (1.6 oz)     # Pin Connections   ![alt text](https://pinout.xyz/resources/raspberry-pi-pinout.png)      -> DS3231 :       --------           RTC   :   RaspberryPi         ------     -------------           SDA           SDA(BCM2)           SCL           SCL(BCM3)                -> MCP3204 :       ---------           ADC    :    RaspberryPi          -----       -------------           clk           BCM22           cs            BCM27           din           BCM17           dout          BCM18           -> LDR :       -----         A0  : ch0 of Mcp3204           -> IR :       ------         A0 : ch1 of Mcp3204           -> DS18b20 :       ---------           out  : BCM 5 of RaspberryPi          -> PIR :       -----         out  : BCM21 of RaspberryPi          -> Refer output pins from program      -> Connect picamera to camera slot      # Instructions to run the code        1. Run following command to install all required Libraries                   chmod +x setup.sh            ./setup.sh              2. All file should be in same folder especially i2c.py, spi.py, temp.py, mail.py       3. Create account on pubnub(https://www.pubnub.com/)        4. Use the publish and subscribe keys in Program        5. Create account on Twilio(https://www.twilio.com/)       6. Use account sid and Auth Token in program        7. Create your twilio number(+1415xxxxxx) and include it in the program       8. Add mail id and password from which you want to send also add destination mail id       9. Do steps to setup Temperature sensor DS18B20(refer above).      10. Do steps to enable I2C & Picamera(refer above).       11. Use Automation.py to run the project.While testing check your mobile and mailbox      12. Use pubnub_txt.py for controlling the system using pubnub      13. Use pubnub_rcv.py for view the messages recieved in pubnub       ![alt text](https://github.com/amalpoulose/Home-Automation-using-RaspberryPi-IOT/blob/master/circuit-breadboard-1.jpg) ![alt text](https://github.com/amalpoulose/Home-Automation-using-RaspberryPi-IOT/blob/master/circuit-breadboard-2.jpg)"
IoT Alcohol & Health Monitoring System,Internet of Things,https://github.com/asadacn/IoT_based_health_monitoring_system,"# IoT_based_health_monitoring_system This is an IoT based health monitoring system which shows heath related date (ECG, Temperature, BPM, Current location, Smoke Detection &amp; Alcohol detection) in realtime with Firebase Reat-time database."
IoT Liquid Level Monitoring System,Internet of Things,https://github.com/vinitshahdeo/Water-Monitoring-System,"# [Water Monitoring System](https://vinitshahdeo.github.io/Water-Monitoring-System/) - IoT Project --- > ## :mask: [COVID-19](http://corona-cases-india.netlify.com/) | Stay Home, Stay Safe! :house:  <br> :mag_right: Checkout [this](http://corona-cases-india.netlify.com/) web app for live CORONA updates.  > ### :incoming_envelope: Read my open letter [here](https://github.com/vinitshahdeo/Water-Monitoring-System/issues/236). :heart: > ## :bar_chart: Checkout the [COVID-19 Tracker :mask: | INDIA](https://indiafightscorona.netlify.app/) :india:  > ### :point_right: You can find the repo [here](https://github.com/vinitshahdeo/COVID19/). :star:  ---  [![first-timers-only](https://img.shields.io/badge/first--timers--only-friendly-tomato.svg?style=flat&logo=git)](https://github.com/vinitshahdeo/Water-Monitoring-System/issues?q=is%3Aissue+is%3Aopen+label%3Afirst-timers-only) [![GitHub license](https://img.shields.io/github/license/vinitshahdeo/Water-Monitoring-System.svg?logo=github)](https://github.com/vinitshahdeo/Water-Monitoring-System/blob/master/LICENSE) [![GitHub stars](https://img.shields.io/github/stars/vinitshahdeo/Water-Monitoring-System.svg?logo=github)](https://github.com/vinitshahdeo/Water-Monitoring-System/stargazers) [![GitHub forks](https://img.shields.io/github/forks/vinitshahdeo/Water-Monitoring-System.svg?logo=github&color=teal)](https://github.com/vinitshahdeo/Water-Monitoring-System/network/members) [![GitHub top language](https://img.shields.io/github/languages/top/vinitshahdeo/Water-Monitoring-System?color=yellow&logo=javascript)](https://github.com/vinitshahdeo/Water-Monitoring-System) [![Open Source Helpers](https://www.codetriage.com/vinitshahdeo/water-monitoring-system/badges/users.svg)](https://www.codetriage.com/vinitshahdeo/water-monitoring-system)  > **You can view the demo [here](https://vinitshahdeo.github.io/Water-Monitoring-System/src/home.html).**  [![Water Monitoring System](https://img.shields.io/badge/Water¬†Monitoring-System-teal.svg?colorA=teal&colorB=orange&style=for-the-badge)](https://github.com/vinitshahdeo/Water-Monitoring-System/) [![IoT](https://img.shields.io/badge/IoT-Project-teal.svg?colorA=blue&colorB=red&style=for-the-badge)](https://github.com/vinitshahdeo/Water-Monitoring-System/)  Water Monitoring System is an IoT based Liquid Level Monitoring system that has the mechanism to keep the user alerted in case of liquid overflow or when tank depletes. The water tanks can be fixed with ultrasonic sensors that is placed over the container. Ultrasonic sensor is used to measure, compare container depth and liquid level.  The status of the system could be monitored by a LCD screen or a web page that provides a brilliant graphical representation. Colours are used to depict various scenarios with respect to the amount of liquid in the tanks or containers and the buzzer buzzes when the limit exceeds the permissible quantity of fill.  ### Goals of this project :  - **_To reduce the wastage of water across the city_** - **_To provide better water supply to the people_**  ```js   ____                   __        __    _  / ___|  __ ___   _____  \ \      / /_ _| |_ ___ _ __  \___ \ / _` \ \ / / _ \  \ \ /\ / / _` | __/ _ \ '__|   ___) | (_| |\ V /  __/   \ V  V / (_| | ||  __/ |  |____/ \__,_| \_/ \___|    \_/\_/ \__,_|\__\___|_|   ____                    _     _  __  / ___|  __ ___   _____  | |   (_)/ _| ___  \___ \ / _` \ \ / / _ \ | |   | | |_ / _ \   ___) | (_| |\ V /  __/ | |___| |  _|  __/  |____/ \__,_| \_/ \___| |_____|_|_|  \___|  ```  > **Click [here](https://github.com/vinitshahdeo/Water-Monitoring-System/blob/master/docs/Water%20level%20monitoring%20system.pdf?raw=true) to download the report.**  The IoT(Hardware) components required are:  - **AVR family microcontroller** - **LCD screen** - **Wifi modem** - **A buzzer** - **12V transformer**  ## Getting Started  [![GitHub code size in bytes](https://img.shields.io/github/languages/code-size/vinitshahdeo/Water-Monitoring-System?logo=github)](https://vinitshahdeo.github.io/Water-Monitoring-System/) [![GitHub commit activity](https://img.shields.io/github/commit-activity/m/vinitshahdeo/Water-Monitoring-System?color=bluevoilet&logo=github)](https://github.com/vinitshahdeo/Water-Monitoring-System/commits/) [![GitHub repo size](https://img.shields.io/github/repo-size/vinitshahdeo/Water-Monitoring-System?logo=github)](https://vinitshahdeo.github.io/Water-Monitoring-System/)  **1.** Fork [this](https://github.com/vinitshahdeo/Water-Monitoring-System/) repository. Click on the <a href=""https://github.com/vinitshahdeo/Water-Monitoring-System/""><img src=""https://img.icons8.com/ios/24/000000/code-fork.png""></a> symbol at the top right corner.  **2.** Clone the forked repository.  ```bash git clone https://github.com/<your-github-username>/Water-Monitoring-System ```  **3.** Navigate to the project directory.  ```bash cd Water-Monitoring-System ```  **4.** Create a new branch.  ```bash git checkout -b <your_branch_name> ```  **5.** Make changes in source code.  **6.** Stage your changes and commit  ```bash #Add changes to Index git add .  #Commit to the local repo git commit -m ""<your_commit_message>"" ```  >CAUTION: Synch up your local repo with [original repo](https://github.com/vinitshahdeo/Water-Monitoring-System) (Upstream) before pushing your commits. >This avoids unnecessary conflicts during the merge.  >NOTE: You can do so by adding a [remote handler](https://www.atlassian.com/de/git/tutorials/syncing) reference to the original repo and pull the changes from the respective branch. >Resolve the [merge-conflicts](https://www.atlassian.com/de/git/tutorials/using-branches/merge-conflicts) if any.  >TIPS: [Workflow diagram](git-workflow.svg) >```bash >#Add upstream repo >git remote add upstream https://github.com/vinitshahdeo/Water-Monitoring-System.git > >#Disable accidental push to the upstream >git remote set-url --push upstream DISABLE > >#List the remote repo and fetch references >git remote -v && git fetch upstream > >#Check for any new commits in the upstream branch >git log HEAD..upstream/master #No output indicates, upstream has not moved ahead > >#See the patch difference between local and upstream branch >git diff -p HEAD..upstream/master > >```  >CAUTION: If the upstream has moved ahead, rebase your commit and resolve conflicts if any. [Skip otherwise] >```bash >git rebase upstream/master >``` >  **7.** Push your local commits to the remote repo.  ```bash git push -u origin <your_branch_name> ```  **8.** Create a [PR](https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/creating-a-pull-request) !  **9.** **Congratulations!** Sit and relax, you've made your contribution to [Water Monitoring System](https://vinitshahdeo.github.io/Water-Monitoring-System/) project.  ## Issues  [![GitHub issues](https://img.shields.io/github/issues/vinitshahdeo/Water-Monitoring-System?logo=github)](https://github.com/vinitshahdeo/Water-Monitoring-System/issues) [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat&logo=git&logoColor=white)](https://github.com/vinitshahdeo/Water-Monitoring-System/pulls) [![GitHub last commit](https://img.shields.io/github/last-commit/vinitshahdeo/Water-Monitoring-System?logo=github)](https://github.com/vinitshahdeo)  | TITLE                                                                                                          | LEVEL       | | -------------------------------------------------------------------------------------------------------------- | ----------- | | [UI Enhancements](https://github.com/vinitshahdeo/Water-Monitoring-System/issues/2)                            | `EASY`      | | [Project Documentation](https://github.com/vinitshahdeo/Water-Monitoring-System/issues/3)                      | `VERY EASY` | | [Setting up Cloud APIs to send/receive data](https://github.com/vinitshahdeo/Water-Monitoring-System/issues/4) | `MEDIUM`    | | [Admin Dashboard ](https://github.com/vinitshahdeo/Water-Monitoring-System/issues/5)                           | `HARD`      | | [Notify stakeholders](https://github.com/vinitshahdeo/Water-Monitoring-System/issues/6)                        | `ADVANCED`  |  **NOTE**: **Feel free to [open issues](https://github.com/vinitshahdeo/Water-Monitoring-System/issues/new/choose)**. Make sure you follow the Issue Template provided.  ## Contribution Guidelines  [![GitHub pull requests](https://img.shields.io/github/issues-pr-raw/vinitshahdeo/Water-Monitoring-System?logo=git&logoColor=white)](https://github.com/vinitshahdeo/Water-Monitoring-System/compare) [![GitHub contributors](https://img.shields.io/github/contributors/vinitshahdeo/Water-Monitoring-System?logo=github)](https://github.com/vinitshahdeo/Water-Monitoring-System/graphs/contributors) [![Vinit Shahdeo](https://img.shields.io/badge/Author-@vinitshahdeo-gray.svg?colorA=gray&colorB=dodgerblue&logo=github)](https://github.com/vinitshahdeo/)  - Write clear meaningful git commit messages (Do read [this](http://chris.beams.io/posts/git-commit/)).  - Make sure your PR's description contains GitHub's special keyword references that automatically close the related issue when the PR is merged. (Check [this](https://github.com/blog/1506-closing-issues-via-pull-requests) for more info)  - When you make very very minor changes to a PR of yours (like for example fixing a text in button, minor changes requested by reviewers) make sure you squash your commits afterward so that you don't have an absurd number of commits for a very small fix. (Learn how to squash at [here](https://davidwalsh.name/squash-commits-git))  - When you're submitting a PR for a UI-related issue, it would be really awesome if you add a screenshot of your change or a link to a deployment where it can be tested out along with your PR. It makes it very easy for the reviewers and you'll also get reviews quicker.  - Please follow the [PR Template](https://github.com/vinitshahdeo/Water-Monitoring-System/blob/master/.github/PULL_REQUEST_TEMPLATE.md) to create the PR.  - Always create PR to `develop` branch.  - Please read our [Code of Conduct](./CODE_OF_CONDUCT.md).  - Refer [this](https://github.com/vinitshahdeo/Water-Monitoring-System/blob/master/CONTRIBUTING.md) for more.   ## Open Source Programs  This project is part of the following Open Source programs:  - [GirlScript Summer of Code 2020](https://www.gssoc.tech/) - [Rails Girls Summer of Code 2020](http://railsgirlssummerofcode.org/) - [Student Code In](https://scodein.tech/) - [Leapcode](https://www.leapcode.io/) - [JGEC Winter of Code 2020](https://jwoc.tech/) - [Hacktoberfest 2020](https://hacktoberfest.digitalocean.com/) - [Script Winter Of Code](https://swoc.tech/)  ## Frequently Asked Questions(FAQs) for [GSSoC'20](https://www.gssoc.tech/)  > This project is accepting contributions under [GirlScript Summer of Code 2020](https://www.gssoc.tech/) - three months long Open Source program conducted by [GirlScript Foundation](https://www.gssoc.tech/).  [![GSSoC](https://raw.githubusercontent.com/GirlScriptSummerOfCode/MentorshipProgram/master/GSsoc%20Type%20Logo%20Black.png)](https://www.gssoc.tech/)  #### _1. How to start as a beginner so as to contribute to the project?_  _There are many beginner level issues to begin with, which include documentation, development and design. Also, feel free to make issues for features as well as for reporting bugs.Participants can claim an issue by commenting that they are interested in it. We are also open to new ideas and suggestions._  #### _2. Is this a hardware/IoT project?_  _Yes, it is a hardware/IoT + software project. There would be maximum tasks related to frontend, backend, design and documentation of the website and later, it would be integarted with the real-time data from the sensors._  #### _3. How to communicate with us?_  _All communication will happen on the Slack channel for this particular projects. Mentors can also be reached through GitHub. Avoid DMs and unnecessary mentions on slack and GitHub both. Checkout [this](https://github.com/vinitshahdeo/Water-Monitoring-System/issues/15) issue for more._  #### _4. What is the Tech Stack for this project?_  - **Frontend:** React  - **Backend:** Django, Node.js  #### _5. What is the deadline for any task submission?_  _Mentors can assign issues to participants and also mention a deadline before which the issue is to be resolved. They can reassign issues in case of any delay, so be active and complete your tasks within the negotiated duration._   ## Shared experiences on [GSSoC'20](https://www.gssoc.tech/) :pencil:  | Article                                               | Author                                          | |-------------------------------------------------------|---------------------------------------------------| | [GirlScript Summer of Code 2020 Mentor Experience](https://medium.com/girlscript-summer-of-code/girlscript-summer-of-code-2020-mentor-experience-28daec399b1e) | [@PragatiVerma18](https://github.com/PragatiVerma18)| | [GSSOC 2020-A journey to explore the ‚ÄúUNEXPLORED‚Äù](https://medium.com/@swara.shukla65/gssoc-2020-a-journey-to-explore-the-unexplored-f25a6ade8288) | [@Swarnimashukla](https://github.com/Swarnimashukla)            |  ## About [RGSoC'20](https://railsgirlssummerofcode.org/)   <a href=""https://github.com/vinitshahdeo/Water-Monitoring-System/issues/141"" target=""_blank""><img src=""./assets/rgsoc2020.png"" width=""30%"" height=""30%""/></a>  ```javascript  var baseURL = `https://github.com`,     projectsInRGSoC = _.filter(appliedProjects,         (project) => {             RGSoC.isSelected(project,                 new Date().getFullYear())         });  if (_.contains(projectsInRGSoC, {         name: `Water Monitoring System`,         url: `${baseURL}Water-Monitoring-System`     })) {     fork('Water Monitoring System');     // Looking forward to awesome contributions     console.log('Thank You RGSoC'); }  ```  **Glad to share that this project is shortlisted for [Rails Girls Summer Of Code 2020](https://railsgirlssummerofcode.org/)**, a global fellowship program for women and non-binary coders. Students receive a three-month scholarship to work on existing Open Source projects and expand their skill set.  - [Click here](https://railsgirlssummerofcode.org/blog/) to learn more. - **RGSoC student applications have officially started on Monday, 02 March 2020. Apply [here](https://teams.railsgirlssummerofcode.org/apply).** - Check out the project [here](https://teams.railsgirlssummerofcode.org/projects/293-water-monitoring-system). - **[Click here](https://twitter.com/Vinit_Shahdeo/status/1234936360613695489) to view the tweet**. RTs are most welcome! :heart: - Feel free to **shoot your doubts [here](https://github.com/vinitshahdeo/Water-Monitoring-System/issues/141)**.  ## Leapcode  [![leapcode-logo-full-png](https://user-images.githubusercontent.com/20594326/86490975-e5639f00-bd86-11ea-853b-1a0ecdc59201.png)](http://leapcode.io/)  Now, we're officially a part of [leapcode.io](http://leapcode.io/). They're still building and hopeful to have their platform up pretty soon. [Click here](https://www.leapcode.io/) to get an early access.  ## Hacktoberfest  <a href=""https://hacktoberfest.digitalocean.com/""><img src=""assets/Logo.svg"" width=""30%""></a>  **Register [here](https://hacktoberfest.digitalocean.com) for Hacktoberfest and make four pull requests (PRs) between October 1-31 to earn a free t-shirt.**  There are many beginner level issues to begin with, with **Hacktoberfest** as the label which include documentation, development and design. Take up any issue and send your first PR!    ## Additional Info  - If you're interested in contributing for **documentation**, please checkout `feature/documentation` branch.  - If you're interested in contributing for **backend**, please checkout the following branch according to the tech stack:    - **Django**: `feature/backend-django`   - **Node.js**: `feature/backend-node`  - If you're interested in contributing for **frontend**, please checkout `feature/frontend` branch.  > **Note:** Are you an **absolute beginner?** Please [check the issues](https://github.com/vinitshahdeo/Water-Monitoring-System/issues?q=is%3Aopen+is%3Aissue+label%3Afirst-timers-only) labeled with `first-timers-only`. These issues might _let your Open Source journey begin_ - **Welcome onboard!**  ## Mentors  [![Open Source Love](https://badges.frapsoft.com/os/v2/open-source.svg?v=103)](https://github.com/vinitshahdeo) [![GitHub pull requests](https://img.shields.io/github/issues-pr-closed-raw/vinitshahdeo/Water-Monitoring-System?logo=git&logoColor=white)](https://github.com/vinitshahdeo/Water-Monitoring-System/pulls?q=is%3Apr+is%3Aclosed)  | GitHub Usernames                                      | Domain                     | | ----------------------------------------------------- | -------------------------- | | [@vinitshahdeo](https://github.com/vinitshahdeo)      | Full Stack + Documentation | | [@ShreyaAnand](https://github.com/ShreyaAnand)        | Full Stack                 | | [@YashMeh](https://github.com/YashMeh)                | Backend                    | | [@PragatiVerma18](https://github.com/PragatiVerma18)  | Backend + Documentation    | | [@ramanaditya](https://github.com/ramanaditya)        | Backend + Hardware         | | [@ArpitKotecha](https://github.com/ArpitKotecha)      | Backend                    | | [@Manvityagi](https://github.com/Manvityagi)          | APIs + Cloud               | | [@divyabhushan](https://github.com/divyabhushan)      | Documentation              | | [@jainpawan21](https://github.com/jainpawan21)        | Frontend                   | | [@amaaniqbal](https://github.com/amaaniqbal)          | Frontend                   | | [@nainikaB](https://github.com/nainikaB)              | UI/UX                      | | [@roypratik](https://github.com/roypratik)            | IoT                        | | [@sharmishthadash](https://github.com/sharmishthadash)| Product Management         | | [@nandikajain](https://github.com/nandikajain)        | Frontend (React)           | | [@harshita19244](https://github.com/harshita19244)    | Frontend (React)           |  [![](https://sourcerer.io/fame/vinitshahdeo/vinitshahdeo/Water-Monitoring-System/images/0)](https://sourcerer.io/fame/vinitshahdeo/vinitshahdeo/Water-Monitoring-System/links/0)[![](https://sourcerer.io/fame/vinitshahdeo/vinitshahdeo/Water-Monitoring-System/images/1)](https://sourcerer.io/fame/vinitshahdeo/vinitshahdeo/Water-Monitoring-System/links/1)[![](https://sourcerer.io/fame/vinitshahdeo/vinitshahdeo/Water-Monitoring-System/images/2)](https://sourcerer.io/fame/vinitshahdeo/vinitshahdeo/Water-Monitoring-System/links/2)[![](https://sourcerer.io/fame/vinitshahdeo/vinitshahdeo/Water-Monitoring-System/images/3)](https://sourcerer.io/fame/vinitshahdeo/vinitshahdeo/Water-Monitoring-System/links/3)[![](https://sourcerer.io/fame/vinitshahdeo/vinitshahdeo/Water-Monitoring-System/images/4)](https://sourcerer.io/fame/vinitshahdeo/vinitshahdeo/Water-Monitoring-System/links/4)[![](https://sourcerer.io/fame/vinitshahdeo/vinitshahdeo/Water-Monitoring-System/images/5)](https://sourcerer.io/fame/vinitshahdeo/vinitshahdeo/Water-Monitoring-System/links/5)[![](https://sourcerer.io/fame/vinitshahdeo/vinitshahdeo/Water-Monitoring-System/images/6)](https://sourcerer.io/fame/vinitshahdeo/vinitshahdeo/Water-Monitoring-System/links/6)[![](https://sourcerer.io/fame/vinitshahdeo/vinitshahdeo/Water-Monitoring-System/images/7)]()  ## Useful Resources  - [Django Docs](https://docs.djangoproject.com/en/3.0/) - [React Docs](https://reactjs.org/docs/getting-started.html) - [Node.js Docs](https://nodejs.org/api/) - [Git and GitHub](https://www.digitalocean.com/community/tutorials/how-to-use-git-a-reference-guide)  ## Project Admin  [![Relative date](https://img.shields.io/date/1577392258?color=important&label=started&logo=github)](https://github.com/vinitshahdeo/) [![Maintenance](https://img.shields.io/maintenance/yes/2020?color=green&logo=github)](https://github.com/vinitshahdeo/)  |                                                                                         <a href=""https://fayz.in/stories/s/1522/0/?ckt_id=ZGL1ZGVk&title=story_of_vinit_shahdeo""><img src=""assets/vinit-shahdeo.jpg"" width=150px height=150px /></a>                                                                                         | | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | |                                                                                                                                        **[Vinit Shahdeo](https://www.linkedin.com/in/vinitshahdeo/)**                                                                                                                                        | | <a href=""https://twitter.com/Vinit_Shahdeo""><img src=""assets/twitter.png"" width=""32px"" height=""32px""></a> <a href=""https://www.facebook.com/vinit.shahdeo""><img src=""assets/facebook.png"" width=""32px"" height=""32px""></a> <a href=""https://www.linkedin.com/in/vinitshahdeo/""><img src=""assets/linkedin.png"" width=""32px"" height=""32px""></a> |  > **_Need help?_**  > **_Feel free to contact me @ [vinitshahdeo@gmail.com](mailto:vinitshahdeo@gmail.com?Subject=WaterMonitoringProject)_**  [![GitHub followers](https://img.shields.io/github/followers/vinitshahdeo.svg?label=Follow%20@vinitshahdeo&style=social)](https://github.com/vinitshahdeo/) [![Twitter Follow](https://img.shields.io/twitter/follow/Vinit_Shahdeo?style=social)](https://twitter.com/Vinit_Shahdeo)  ## COVID-19<sup>Stay Home, Stay Safe!</sup>  I hope you are all staying safe at your home and enjoying the contribution for your awesome open source projects.  ```javascript /**  *   * Let's fight for Corona together!  */ function stayAtHome() {   eat();   sleep();   code();   repeat(); }  while(_.isAlive(new Virus('COVID-19'))) {   // Stay home, Stay safe   stayAtHome(); }  ```  ### :wave: [DO THE FIVE](https://www.mohfw.gov.in/): Help stop coronavirus  1. **HANDS**: Wash them often 2. **ELBOW**: Cough into it 3. **FACE**: Don't touch it 4. **SPACE**: Keep safe distance 5. **HOME**: Stay if you can  ### :x: Avoid Handshakes :handshake:  ### :heavy_check_mark: Do Namastey :pray:  <br>  <sup>**PS: I know it's off-topic but **Yes**, COVID-19 everyone's talking about. Please stay safe at your home and enjoy contributing to your projects.**</sup>   _Your friend,_ <br> _**[Vinit](https://www.instagram.com/vinitshahdeo/) :)**_  ---  ```javascript /**  *  * {thisRepo.url} = https://github.com/vinitshahdeo/Water-Monitoring-System/  *  */ if (thisRepo.isAwesome()) {   thisRepo.fork(); // waiting for your awesome contributions :)   thisRepo.star(); // thanks in advance :p   thisRepo.watch(bug => {     if (!doesIssueExists(bug)) {       createNewIssue(bug); // wow, that's a nice catch     } else {       checkIssueTracker(bug); // comment there     }   });   thisRepo.share(); // sharing is caring :D } ```  ---  [![built with love](https://forthebadge.com/images/badges/built-with-love.svg)](https://github.com/vinitshahdeo/) [![powered by water](https://forthebadge.com/images/badges/powered-by-water.svg)](https://github.com/vinitshahdeo/) [![smile please](https://forthebadge.com/images/badges/makes-people-smile.svg)](https://github.com/vinitshahdeo/)  <p align=""center""><strong><sup>:hugs: <br>Check out my other projects <a href=""./PROJECTS.md"">here</a>!</sup></strong></p> "
IoT Garbage Monitoring System,Internet of Things,https://github.com/hegdepavankumar/smart-garbage-monitoring-system-using-iot,"# Smart Garbage Monitoring System Using IOT    ![GitHub](https://img.shields.io/github/license/hegdepavankumar/smart-garbage-monitoring-system-using-iot?style=flat) ![GitHub top language](https://img.shields.io/github/languages/top/hegdepavankumar/smart-garbage-monitoring-system-using-iot?style=flat) ![GitHub last commit](https://img.shields.io/github/last-commit/hegdepavankumar/smart-garbage-monitoring-system-using-iot?style=flat) ![ViewCount](https://views.whatilearened.today/views/github/hegdepavankumar/smart-garbage-monitoring-system-using-iot.svg?cache=remove)  ## Overview  We are living in the era of Smart cities where everything is planned and systematic. The problem we are facing is the population, which is rising rapidly. In recent years, urban migration has skyrocketed. This has resulted in the rise of garbage waste everywhere. Dumping of garbage in public places creates a polluted environment in the neighborhood. It could cause several serious diseases to the people living around. This will embarrass the evaluation of the affected area. To reduce waste and maintain good hygiene, we need a systematic approach to tackle the problem.  The traditional way of manually monitoring the wastes in waste bins is a complex, cumbersome process and utilizes more human effort, time, and cost which is not compatible with the present-day technologies in any way. We propose a solution to this waste problem which manages the garbage waste smartly. This research paper proposes an IoT-based smart system based on clean waste management that assesses the level of waste on dustbins through sensory systems. In this system, the microcontroller is used as a visual connector connecting the sensor and the IoT system. This is an advanced method in which waste management is automated. This project IoT Garbage Monitoring system is a very innovative system that will help to keep the cities clean. This system monitors the garbage bins and informs about the level of garbage collected in the garbage bins via a web page. This web page also sends all information to garbage collection vehicles.  <br>  # Real-Time Implemented Images: [click here to view](https://github.com/hegdepavankumar/smart-garbage-monitoring-system-using-iot/tree/main/sample-project-images) # Project Report: [click here to download](https://github.com/user-attachments/files/15880862/Report.Content.pdf)  <br>   ## Hardware Requirements  1) ### Ulteasonic Sensor  ![image](https://user-images.githubusercontent.com/85627085/235177501-32c84273-4d46-4518-960e-3edf8aee552b.png)  <br> An ultrasonic sensor is an electronic device that measures the distance of a target object by emitting ultrasonic sound waves and converts the reflected sound into an electrical signal. Ultrasonic waves travel faster than the speed of audible sound (i.e. the sound that humans can hear). Ultrasonic sensors have two main components: the transmitter (which emits the sound using piezoelectric crystals) and the receiver (which encounters the sound after it has traveled to and from the target).  To calculate the distance between the sensor and the object, the sensor measures the time it takes between the emission of the sound by the transmitter to its contact with the receiver. The formula for this calculation is D = ¬Ω T x C (where D is the distance, T is the time, and C is the speed of sound ~ 343 meters/second). <br>  2) ### Arduino UNO R3  ![image](https://user-images.githubusercontent.com/85627085/235177985-42b9792e-5ec4-468d-8ad0-ee05d52e814b.png)  <br>  Arduino UNO is a microcontroller board based on the ATmega328P. It has 14 digital input/output pins (of which 6 can be used as PWM outputs), 6 analog inputs, a 16 MHz ceramic resonator, a USB connection, a power jack, an ICSP header, and a reset button. It contains everything needed to support the microcontroller; simply connect it to a computer with a USB cable or power it with an AC-to-DC adapter or battery to get started.  <br>  3) ### GPS Module  ![image](https://user-images.githubusercontent.com/85627085/235178820-0b356695-8667-4847-88ac-0c4257ed9e4a.png)  <br> These GPS modules are compatible with Arduino and Raspberry Pi, making it easy for you to start to try out. The Air 530 Module in Grove - GPS(Air 530) is a high-performance, highly integrated multi-mode satellite positioning and navigation module. It supports GPS / Beidou / Glonass / Galileo / QZSS / SBAS, which makes it suitable for GNSS positioning applications such as car navigation, smart wear, and drones. And Air530 module is also supports NMEA 0183 V4.1 protocol and compatible with previous versions. Meanwhile, the E-1612-UB module series of Grove - GPS Module is a family of stand-alone GPS receivers featuring the high-performance u-blox 5 positioning engine. The 50-channel u-blox 5 positioning engine boasts a Time-To-First-Fix ( TTFF ) of under 1 second. The dedicated acquisition engine, with over 1 million correlators, is capable of massive parallel time/frequency space searches, enabling it to find satellites instantly.  <br> <br>  4) ### GSM/GPRS Module <br> <br>  ![image](https://user-images.githubusercontent.com/85627085/235181834-9da83f7b-62f2-4f13-a14d-0e01c9c88718.png)    <br>   - What is GSM? <br>   GSM (Global System for Mobile Communications, originally Groupe Sp√©cial Mobile), is a standard developed by the European Telecommunications Standards Institute (ETSI). It was created to describe the protocols for second-generation (2G) digital cellular networks used by mobile phones and is now the default global standard for mobile communications ‚Äì with over 90% market share, operating in over 219 countries and territories. <br>   - What is GPRS? <br>   General Packet Radio Service (GPRS) is a packet-oriented mobile data service on the 2G and 3G cellular communication system‚Äôs global system for mobile communications (GSM). GPRS was originally standardized by the European Telecommunications Standards Institute (ETSI) in response to the earlier CDPD and i-mode packet-switched cellular technologies. It is now maintained by the 3rd Generation Partnership Project (3GPP).   <br> <br>    5) ### Buzzer  <br>  ![image](https://user-images.githubusercontent.com/85627085/235182600-a0037ef0-0a29-450b-9c12-2e72f58e3903.png)  <br>  A buzzer or beeper is an audio signaling device, which may be mechanical, electromechanical, or piezoelectric (piezo for short). Typical uses of buzzers and beepers include alarm devices, timers, training and confirmation of user input such as a mouse click or keystroke.  <br>   6) ### Connecting Wires  <br>  ![image](https://user-images.githubusercontent.com/85627085/235187067-c6bc9a3e-2112-49f5-962f-e3123f8fb0d5.png)  <br>  A connecting wire allows the electric current from one point to another point without resistivity. The resistance of the connecting wire should always be near zero. Copper wires have low resistance and are therefore suitable for low resistance.   <br>   7) ### NodeMCU(Node MicroController Unit)  <br>  ![image](https://user-images.githubusercontent.com/85627085/235187953-d709a102-3247-4f2b-bbe8-c1292dd6dff6.png)  <br>  NodeMCU is an open-source firmware for which open-source prototyping board designs are available. The name ""NodeMCU"" combines ""node"" and ""MCU"" (micro-controller unit). Strictly speaking, the term ""NodeMCU"" refers to the firmware rather than the associated development kits. NodeMCU was created shortly after the ESP8266 came out. On December 30, 2013, Espressif Systems began production of the ESP8266.NodeMCU started on 13 Oct 2014, when Hong committed the first file of nodemcu-firmware to GitHub.Two months later, the project expanded to include an open-hardware platform when developer Huang R committed the Gerber file of an ESP8266 board, named devkit v0.9.  <br>  8) ### 16x2 LCD  <br>    ![image](https://user-images.githubusercontent.com/85627085/235443559-a2a7fdfc-966e-4357-b004-9edb3c93a655.png)   <br> The Liquid Crystal library allows you to control LCDs that are compatible with the Hitachi HD44780 driver. There are many of them out there, and you can usually tell them by the 16-pin interface. The LCDs have a parallel interface, meaning that the microcontroller has to manipulate several interface pins at once to control the display.  <br>  ## Software Requirements    - Windows 7/10/11 OS with Min 4GB RAM and 250GB Hard Disk <br>   - [Arduino IDE](https://www.arduino.cc/en/software) <br>   - Local Server and web Page for Monitoring <br>  ## Implementation & Testing    - Sketch (Fritzing) <br>      ![image](https://user-images.githubusercontent.com/85627085/235192027-edc61f5f-6932-4436-9cac-e4d0db5209d3.png)      <br>      The above diagram shows a sketch of connection devices or sensors using Fritzing software. This figure shows an ultrasonic sensor connected with Node MCU and to the Cytron Uno or Arduino Uno. An ultrasonic sensor will read the distance of the garbage and compare it with the bin depth. This sketch is one of the important parts of the Garbage Monitoring System using IoT.         <br>      ## Source Code     1) ### Code for NodeMCU    <br>    ```   #include <ESP8266WiFi.h> const char* ssid = ""Pavankumar""; //ssid of your wifi  // Mavayya-5G const char* password = ""12345678""; //password of your wifi WiFiServer server(80); //////////////////////////////////// #include <Arduino_JSON.h> String inputData = """"; boolean data_complete = false; String vala; String valb; String valc; String vald; /////////////////////////////////////  void setup()  {   Serial.begin(115200);   inputData.reserve(200);   Serial.println(""Hello"");   Serial.println();   Serial.print(""Connecting to "");   Serial.println(ssid);   WiFi.begin(ssid, password); //connecting to wifi   while (WiFi.status() != WL_CONNECTED)// while wifi not connected   {     delay(500);     Serial.print("".""); //print ""....""   }   Serial.println("""");   Serial.println(""WiFi connected"");   server.begin();   Serial.println(""Server started"");   Serial.println(WiFi.localIP());  // Print the IP address } void loop() {   while(Serial.available() > 0)   {    char inChar = Serial.read();    if( inChar == '\r')    {     inputData = """";    }    else if(inChar == '\n')    {     data_complete = true;    }    else    {     inputData+=inChar;    }   }    if(data_complete)   {    data_complete = false;    Serial.println(inputData);    demoParse();   }   //int a=vala.toInt();     WiFiClient client = server.available(); // Check if a client has connected  /* if (!client)   {    return;   }   */    String s = ""HTTP/1.1 200 OK\r\nContent-Type: text/html\r\n\r\n <!DOCTYPE html> <html> <head> <title>..........</title> <style>"";   s += ""a:link {background-color: RED;text-decoration: none;}"";   s += ""table, th, td </style> </head> <body> <h1  style="";   s += ""font-size:250%;"";   s += "" ALIGN=CENTER> Dustbin data</h1>"";   s += ""<p ALIGN=CENTER style=""""font-size:200%;"""""";   s += ""> <b> Location -001</b></p> <table ALIGN=CENTER style="";   s += ""width:10%"";   s += ""> <tr> <th>Level : </th>"";   s += ""<td ALIGN=CENTER >"";   s += vala;   //s += ""</td> </tr> <tr> <th>Tds Value : </th> <td ALIGN=CENTER >"";   //s += valb;   //s += ""</td> </tr> <tr>  <th>Water Level</th> <td ALIGN=CENTER >"";   //s += valc;   //s += ""</td></tr> <tr> <th>Water intake</th> <td ALIGN=CENTER >"";   //s += vald;   s += ""</td>  </tr> </table> "";   s += ""</body> </html>"";   client.print(s); // all the values are send to the webpage   delay(100); } void demoParse()   {   Serial.println(""parse"");   Serial.println(vala);   Serial.println(""====="");   JSONVar myObject = JSON.parse(inputData);       if (JSON.typeof(myObject) == ""undefined"")    {    Serial.println(""Parsing input failed!"");    return;   }   Serial.print(""JSON.typeof(myObject) = "");   Serial.println(JSON.typeof(myObject)); // prints: object   // myObject.hasOwnProperty(key) checks if the object contains an entry for key   if (myObject.hasOwnProperty(""anloga"")) {   Serial.print(""myObject[\""anloga\""] = "");   vala = (const char*) myObject[""anloga""];//to get value in  vala   Serial.println(vala);   Serial.println((const char*) myObject[""anloga""]);  }   if (myObject.hasOwnProperty(""anlogb"")) {     Serial.print(""myObject[\""anlogb\""] = "");     valb = (const char*) myObject[""anlogb""];     Serial.println(valb);      Serial.println((const char*) myObject[""anlogb""]);   }   if (myObject.hasOwnProperty(""anlogc"")) {     Serial.print(""myObject[\""anlogc\""] = "");     valc = (const char*) myObject[""anlogc""];     Serial.println(valc);      Serial.println((const char*) myObject[""anlogc""]);   }   if (myObject.hasOwnProperty(""anlogd"")) {     Serial.print(""myObject[\""anlogd\""] = "");     vald = (const char*) myObject[""anlogd""];     Serial.println(vald);      Serial.println((const char*) myObject[""anlogd""]);   }   // JSON vars can be printed using print or println   Serial.print(""myObject = "");   Serial.println(myObject);   Serial.println(); }   ```    2) ### Code for Arduino UNO R3  <br>  ``` #include <SoftwareSerial.h> SoftwareSerial esp8266(2,3);  #include <LiquidCrystal.h> LiquidCrystal lcd(8,9,10,11,12,13);//RS,EN,D4,D5,D6,D7 #include <Servo.h> #define buzzer 4 #define trigPin1 A4  //// front #define echoPin1 A5 int lvl1=0; long duration, distance,sensor1,sensor2,sensor3; // us variable int onetime=0,onetime1=0 ; int wet=0,moisture=0,object=0,cabin2=0,c1=0,c2=0; int powers=0,powers1=0,powers2=0,powers3=0; void setup()  {  Serial.begin(115200);  esp8266.begin(9600);  lcd.begin(16, 2);//initializing LCD  lcd.setCursor(0,0);   lcd.print(""Automatic WASTE"");  delay(3000);  pinMode(buzzer,OUTPUT);  pinMode(trigPin1, OUTPUT);  pinMode(echoPin1, INPUT);  delay(3000); } void loop() {  ultrasensor(trigPin1, echoPin1);  sensor1 = distance;    delay(10);  esp8266.println(sensor1);  lvl1=(20-sensor1)*7;  esp8266.println(lvl1);  if(lvl1<0){lvl1=0;}  if(lvl1>100){lvl1=100;}  lcd.clear();  lcd.setCursor(0,0);   lcd.print(""Dustbin Level"");  lcd.setCursor(6,1);   lcd.print(lvl1);  delay(1000);  if(lvl1>70)  {         if(onetime==0)   {     lcd.clear();    lcd.setCursor(0,0);     lcd.print(""-send msg-"");    digitalWrite(buzzer,HIGH);     tracking();     digitalWrite(buzzer,LOW);    onetime=1;   }  }    else  {   onetime=0;  }  ////////////////////////////////////////////////  String data = """";  data+= ""{"";  data+= ""\""anloga\"":"";  data+= ""\""""+String(lvl1)+""\"","";  data+= ""\""anlogb\"":"";  data+= ""\""""+String(powers)+""\"","";  data+= ""\""anlogc\"":"";  data+= ""\""""+String(powers1)+""\"","";  data+= ""\""anlogd\"":"";  data+= ""\""""+String(powers2)+""\"""";  data+= ""}"";  Serial.print('\r');  Serial.print(data);  delay(10);  Serial.print('\n');  delay(200);  ///////////////////////////////////////////////    }  void init_sms()  {   esp8266.println(""AT+CMGF=1"");   delay(400);   esp8266.println(""AT+CMGS=\""+919X083X52XX\"""");   // use your 10 digit cell no. here //   delay(400);  }  void init_sms1()  {   esp8266.println(""AT+CMGF=1"");   delay(400);   esp8266.println(""AT+CMGS=\""+918XX227XX8X\"""");   // use your 10 digit cell no. here   delay(400);  }    void send_data(String message)  {   esp8266.println(message);   delay(200);  }    void send_sms()  {   esp8266.write(26);  }   void tracking()  {   init_sms();   send_data(""dustbin-001  is almost full:\n"");   send_sms();   delay(6000);   init_sms1();   send_data(""dustbin-001  is almost full:\n"");   esp8266.print("" Level in %"");    esp8266.print(lvl1);   send_sms();   delay(6000);  }  void ultrasensor(int trigPin,int echoPin)  {    digitalWrite(trigPin, LOW);  // Added this line   delayMicroseconds(2); // Added this line   digitalWrite(trigPin, HIGH);   delayMicroseconds(10); // Added this line   digitalWrite(trigPin, LOW);   duration = pulseIn(echoPin, HIGH);   distance = (duration/2) / 29.1;  }  ``` ## Instructions/Setup  - Download and install Arduino IDE - Make all the necessary connections - Compile and upload the code to the board <br>  ## Conclusion  While completing this project proposal, there are a few constraints that come up. First, the reading of sensors is less accurate, and need to be extra careful in handling the sensors. Second, notification to the mobile phone cannot be done because the cloud platform used is an open-source platform and cannot be used to send an alert or notification to the users. Development of the system needs to be done thoroughly to decrease the possibility of errors. However, added value has been added to this system to make the system more reliable. Normalization is being used to eliminate the outliers which can help to increase the accuracy of the distance reads by an ultrasonic sensor.     ## Contributing Contributions are what make the open-source community such an amazing place to learn, inspire, and create. Any contributions you make are greatly appreciated.  1. Fork the Project 2. Create your Feature Branch (git checkout -b feature/AmazingFeature) 3. Commit your changes (git commit -m 'Add some AmazingFeature') 4. Push to the Branch (git push origin feature/AmazingFeature) 5. Open a Pull Request  Pull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.   ## Creators [üîù](# smart-garbage-monitoring-system-using-iot)  This Project is Created by:-    - [Pavankumar Hegde](https://github.com/hegdepavankumar) [Team Leader]   - [Sushil Kumar Sah](https://github.com/sushilsah)   - [Safina Fathima](https://github.com/safinafab)   - [Santhosh Reddy](https://github.com/)    <br> <h3 align=""center"">Show some &nbsp;‚ù§Ô∏è&nbsp; by starring some of the repositories!</h3> <br>    <!-- Support Me -->     if you like what I do, maybe consider buying me a coffee ü•∫üëâüëà  <a href=""https://www.buymeacoffee.com/hegdepavankumar"" target=""_blank""><img src=""https://cdn.buymeacoffee.com/buttons/v2/default-red.png"" alt=""Buy Me A Coffee"" width=""150"" ></a>   "
IoT Garbage Monitoring System,Internet of Things,https://github.com/hegdepavankumar/smart-garbage-monitoring-system-using-iot,"# Smart Garbage Monitoring System Using IOT    ![GitHub](https://img.shields.io/github/license/hegdepavankumar/smart-garbage-monitoring-system-using-iot?style=flat) ![GitHub top language](https://img.shields.io/github/languages/top/hegdepavankumar/smart-garbage-monitoring-system-using-iot?style=flat) ![GitHub last commit](https://img.shields.io/github/last-commit/hegdepavankumar/smart-garbage-monitoring-system-using-iot?style=flat) ![ViewCount](https://views.whatilearened.today/views/github/hegdepavankumar/smart-garbage-monitoring-system-using-iot.svg?cache=remove)  ## Overview  We are living in the era of Smart cities where everything is planned and systematic. The problem we are facing is the population, which is rising rapidly. In recent years, urban migration has skyrocketed. This has resulted in the rise of garbage waste everywhere. Dumping of garbage in public places creates a polluted environment in the neighborhood. It could cause several serious diseases to the people living around. This will embarrass the evaluation of the affected area. To reduce waste and maintain good hygiene, we need a systematic approach to tackle the problem.  The traditional way of manually monitoring the wastes in waste bins is a complex, cumbersome process and utilizes more human effort, time, and cost which is not compatible with the present-day technologies in any way. We propose a solution to this waste problem which manages the garbage waste smartly. This research paper proposes an IoT-based smart system based on clean waste management that assesses the level of waste on dustbins through sensory systems. In this system, the microcontroller is used as a visual connector connecting the sensor and the IoT system. This is an advanced method in which waste management is automated. This project IoT Garbage Monitoring system is a very innovative system that will help to keep the cities clean. This system monitors the garbage bins and informs about the level of garbage collected in the garbage bins via a web page. This web page also sends all information to garbage collection vehicles.  <br>  # Real-Time Implemented Images: [click here to view](https://github.com/hegdepavankumar/smart-garbage-monitoring-system-using-iot/tree/main/sample-project-images) # Project Report: [click here to download](https://github.com/user-attachments/files/15880862/Report.Content.pdf)  <br>   ## Hardware Requirements  1) ### Ulteasonic Sensor  ![image](https://user-images.githubusercontent.com/85627085/235177501-32c84273-4d46-4518-960e-3edf8aee552b.png)  <br> An ultrasonic sensor is an electronic device that measures the distance of a target object by emitting ultrasonic sound waves and converts the reflected sound into an electrical signal. Ultrasonic waves travel faster than the speed of audible sound (i.e. the sound that humans can hear). Ultrasonic sensors have two main components: the transmitter (which emits the sound using piezoelectric crystals) and the receiver (which encounters the sound after it has traveled to and from the target).  To calculate the distance between the sensor and the object, the sensor measures the time it takes between the emission of the sound by the transmitter to its contact with the receiver. The formula for this calculation is D = ¬Ω T x C (where D is the distance, T is the time, and C is the speed of sound ~ 343 meters/second). <br>  2) ### Arduino UNO R3  ![image](https://user-images.githubusercontent.com/85627085/235177985-42b9792e-5ec4-468d-8ad0-ee05d52e814b.png)  <br>  Arduino UNO is a microcontroller board based on the ATmega328P. It has 14 digital input/output pins (of which 6 can be used as PWM outputs), 6 analog inputs, a 16 MHz ceramic resonator, a USB connection, a power jack, an ICSP header, and a reset button. It contains everything needed to support the microcontroller; simply connect it to a computer with a USB cable or power it with an AC-to-DC adapter or battery to get started.  <br>  3) ### GPS Module  ![image](https://user-images.githubusercontent.com/85627085/235178820-0b356695-8667-4847-88ac-0c4257ed9e4a.png)  <br> These GPS modules are compatible with Arduino and Raspberry Pi, making it easy for you to start to try out. The Air 530 Module in Grove - GPS(Air 530) is a high-performance, highly integrated multi-mode satellite positioning and navigation module. It supports GPS / Beidou / Glonass / Galileo / QZSS / SBAS, which makes it suitable for GNSS positioning applications such as car navigation, smart wear, and drones. And Air530 module is also supports NMEA 0183 V4.1 protocol and compatible with previous versions. Meanwhile, the E-1612-UB module series of Grove - GPS Module is a family of stand-alone GPS receivers featuring the high-performance u-blox 5 positioning engine. The 50-channel u-blox 5 positioning engine boasts a Time-To-First-Fix ( TTFF ) of under 1 second. The dedicated acquisition engine, with over 1 million correlators, is capable of massive parallel time/frequency space searches, enabling it to find satellites instantly.  <br> <br>  4) ### GSM/GPRS Module <br> <br>  ![image](https://user-images.githubusercontent.com/85627085/235181834-9da83f7b-62f2-4f13-a14d-0e01c9c88718.png)    <br>   - What is GSM? <br>   GSM (Global System for Mobile Communications, originally Groupe Sp√©cial Mobile), is a standard developed by the European Telecommunications Standards Institute (ETSI). It was created to describe the protocols for second-generation (2G) digital cellular networks used by mobile phones and is now the default global standard for mobile communications ‚Äì with over 90% market share, operating in over 219 countries and territories. <br>   - What is GPRS? <br>   General Packet Radio Service (GPRS) is a packet-oriented mobile data service on the 2G and 3G cellular communication system‚Äôs global system for mobile communications (GSM). GPRS was originally standardized by the European Telecommunications Standards Institute (ETSI) in response to the earlier CDPD and i-mode packet-switched cellular technologies. It is now maintained by the 3rd Generation Partnership Project (3GPP).   <br> <br>    5) ### Buzzer  <br>  ![image](https://user-images.githubusercontent.com/85627085/235182600-a0037ef0-0a29-450b-9c12-2e72f58e3903.png)  <br>  A buzzer or beeper is an audio signaling device, which may be mechanical, electromechanical, or piezoelectric (piezo for short). Typical uses of buzzers and beepers include alarm devices, timers, training and confirmation of user input such as a mouse click or keystroke.  <br>   6) ### Connecting Wires  <br>  ![image](https://user-images.githubusercontent.com/85627085/235187067-c6bc9a3e-2112-49f5-962f-e3123f8fb0d5.png)  <br>  A connecting wire allows the electric current from one point to another point without resistivity. The resistance of the connecting wire should always be near zero. Copper wires have low resistance and are therefore suitable for low resistance.   <br>   7) ### NodeMCU(Node MicroController Unit)  <br>  ![image](https://user-images.githubusercontent.com/85627085/235187953-d709a102-3247-4f2b-bbe8-c1292dd6dff6.png)  <br>  NodeMCU is an open-source firmware for which open-source prototyping board designs are available. The name ""NodeMCU"" combines ""node"" and ""MCU"" (micro-controller unit). Strictly speaking, the term ""NodeMCU"" refers to the firmware rather than the associated development kits. NodeMCU was created shortly after the ESP8266 came out. On December 30, 2013, Espressif Systems began production of the ESP8266.NodeMCU started on 13 Oct 2014, when Hong committed the first file of nodemcu-firmware to GitHub.Two months later, the project expanded to include an open-hardware platform when developer Huang R committed the Gerber file of an ESP8266 board, named devkit v0.9.  <br>  8) ### 16x2 LCD  <br>    ![image](https://user-images.githubusercontent.com/85627085/235443559-a2a7fdfc-966e-4357-b004-9edb3c93a655.png)   <br> The Liquid Crystal library allows you to control LCDs that are compatible with the Hitachi HD44780 driver. There are many of them out there, and you can usually tell them by the 16-pin interface. The LCDs have a parallel interface, meaning that the microcontroller has to manipulate several interface pins at once to control the display.  <br>  ## Software Requirements    - Windows 7/10/11 OS with Min 4GB RAM and 250GB Hard Disk <br>   - [Arduino IDE](https://www.arduino.cc/en/software) <br>   - Local Server and web Page for Monitoring <br>  ## Implementation & Testing    - Sketch (Fritzing) <br>      ![image](https://user-images.githubusercontent.com/85627085/235192027-edc61f5f-6932-4436-9cac-e4d0db5209d3.png)      <br>      The above diagram shows a sketch of connection devices or sensors using Fritzing software. This figure shows an ultrasonic sensor connected with Node MCU and to the Cytron Uno or Arduino Uno. An ultrasonic sensor will read the distance of the garbage and compare it with the bin depth. This sketch is one of the important parts of the Garbage Monitoring System using IoT.         <br>      ## Source Code     1) ### Code for NodeMCU    <br>    ```   #include <ESP8266WiFi.h> const char* ssid = ""Pavankumar""; //ssid of your wifi  // Mavayya-5G const char* password = ""12345678""; //password of your wifi WiFiServer server(80); //////////////////////////////////// #include <Arduino_JSON.h> String inputData = """"; boolean data_complete = false; String vala; String valb; String valc; String vald; /////////////////////////////////////  void setup()  {   Serial.begin(115200);   inputData.reserve(200);   Serial.println(""Hello"");   Serial.println();   Serial.print(""Connecting to "");   Serial.println(ssid);   WiFi.begin(ssid, password); //connecting to wifi   while (WiFi.status() != WL_CONNECTED)// while wifi not connected   {     delay(500);     Serial.print("".""); //print ""....""   }   Serial.println("""");   Serial.println(""WiFi connected"");   server.begin();   Serial.println(""Server started"");   Serial.println(WiFi.localIP());  // Print the IP address } void loop() {   while(Serial.available() > 0)   {    char inChar = Serial.read();    if( inChar == '\r')    {     inputData = """";    }    else if(inChar == '\n')    {     data_complete = true;    }    else    {     inputData+=inChar;    }   }    if(data_complete)   {    data_complete = false;    Serial.println(inputData);    demoParse();   }   //int a=vala.toInt();     WiFiClient client = server.available(); // Check if a client has connected  /* if (!client)   {    return;   }   */    String s = ""HTTP/1.1 200 OK\r\nContent-Type: text/html\r\n\r\n <!DOCTYPE html> <html> <head> <title>..........</title> <style>"";   s += ""a:link {background-color: RED;text-decoration: none;}"";   s += ""table, th, td </style> </head> <body> <h1  style="";   s += ""font-size:250%;"";   s += "" ALIGN=CENTER> Dustbin data</h1>"";   s += ""<p ALIGN=CENTER style=""""font-size:200%;"""""";   s += ""> <b> Location -001</b></p> <table ALIGN=CENTER style="";   s += ""width:10%"";   s += ""> <tr> <th>Level : </th>"";   s += ""<td ALIGN=CENTER >"";   s += vala;   //s += ""</td> </tr> <tr> <th>Tds Value : </th> <td ALIGN=CENTER >"";   //s += valb;   //s += ""</td> </tr> <tr>  <th>Water Level</th> <td ALIGN=CENTER >"";   //s += valc;   //s += ""</td></tr> <tr> <th>Water intake</th> <td ALIGN=CENTER >"";   //s += vald;   s += ""</td>  </tr> </table> "";   s += ""</body> </html>"";   client.print(s); // all the values are send to the webpage   delay(100); } void demoParse()   {   Serial.println(""parse"");   Serial.println(vala);   Serial.println(""====="");   JSONVar myObject = JSON.parse(inputData);       if (JSON.typeof(myObject) == ""undefined"")    {    Serial.println(""Parsing input failed!"");    return;   }   Serial.print(""JSON.typeof(myObject) = "");   Serial.println(JSON.typeof(myObject)); // prints: object   // myObject.hasOwnProperty(key) checks if the object contains an entry for key   if (myObject.hasOwnProperty(""anloga"")) {   Serial.print(""myObject[\""anloga\""] = "");   vala = (const char*) myObject[""anloga""];//to get value in  vala   Serial.println(vala);   Serial.println((const char*) myObject[""anloga""]);  }   if (myObject.hasOwnProperty(""anlogb"")) {     Serial.print(""myObject[\""anlogb\""] = "");     valb = (const char*) myObject[""anlogb""];     Serial.println(valb);      Serial.println((const char*) myObject[""anlogb""]);   }   if (myObject.hasOwnProperty(""anlogc"")) {     Serial.print(""myObject[\""anlogc\""] = "");     valc = (const char*) myObject[""anlogc""];     Serial.println(valc);      Serial.println((const char*) myObject[""anlogc""]);   }   if (myObject.hasOwnProperty(""anlogd"")) {     Serial.print(""myObject[\""anlogd\""] = "");     vald = (const char*) myObject[""anlogd""];     Serial.println(vald);      Serial.println((const char*) myObject[""anlogd""]);   }   // JSON vars can be printed using print or println   Serial.print(""myObject = "");   Serial.println(myObject);   Serial.println(); }   ```    2) ### Code for Arduino UNO R3  <br>  ``` #include <SoftwareSerial.h> SoftwareSerial esp8266(2,3);  #include <LiquidCrystal.h> LiquidCrystal lcd(8,9,10,11,12,13);//RS,EN,D4,D5,D6,D7 #include <Servo.h> #define buzzer 4 #define trigPin1 A4  //// front #define echoPin1 A5 int lvl1=0; long duration, distance,sensor1,sensor2,sensor3; // us variable int onetime=0,onetime1=0 ; int wet=0,moisture=0,object=0,cabin2=0,c1=0,c2=0; int powers=0,powers1=0,powers2=0,powers3=0; void setup()  {  Serial.begin(115200);  esp8266.begin(9600);  lcd.begin(16, 2);//initializing LCD  lcd.setCursor(0,0);   lcd.print(""Automatic WASTE"");  delay(3000);  pinMode(buzzer,OUTPUT);  pinMode(trigPin1, OUTPUT);  pinMode(echoPin1, INPUT);  delay(3000); } void loop() {  ultrasensor(trigPin1, echoPin1);  sensor1 = distance;    delay(10);  esp8266.println(sensor1);  lvl1=(20-sensor1)*7;  esp8266.println(lvl1);  if(lvl1<0){lvl1=0;}  if(lvl1>100){lvl1=100;}  lcd.clear();  lcd.setCursor(0,0);   lcd.print(""Dustbin Level"");  lcd.setCursor(6,1);   lcd.print(lvl1);  delay(1000);  if(lvl1>70)  {         if(onetime==0)   {     lcd.clear();    lcd.setCursor(0,0);     lcd.print(""-send msg-"");    digitalWrite(buzzer,HIGH);     tracking();     digitalWrite(buzzer,LOW);    onetime=1;   }  }    else  {   onetime=0;  }  ////////////////////////////////////////////////  String data = """";  data+= ""{"";  data+= ""\""anloga\"":"";  data+= ""\""""+String(lvl1)+""\"","";  data+= ""\""anlogb\"":"";  data+= ""\""""+String(powers)+""\"","";  data+= ""\""anlogc\"":"";  data+= ""\""""+String(powers1)+""\"","";  data+= ""\""anlogd\"":"";  data+= ""\""""+String(powers2)+""\"""";  data+= ""}"";  Serial.print('\r');  Serial.print(data);  delay(10);  Serial.print('\n');  delay(200);  ///////////////////////////////////////////////    }  void init_sms()  {   esp8266.println(""AT+CMGF=1"");   delay(400);   esp8266.println(""AT+CMGS=\""+919X083X52XX\"""");   // use your 10 digit cell no. here //   delay(400);  }  void init_sms1()  {   esp8266.println(""AT+CMGF=1"");   delay(400);   esp8266.println(""AT+CMGS=\""+918XX227XX8X\"""");   // use your 10 digit cell no. here   delay(400);  }    void send_data(String message)  {   esp8266.println(message);   delay(200);  }    void send_sms()  {   esp8266.write(26);  }   void tracking()  {   init_sms();   send_data(""dustbin-001  is almost full:\n"");   send_sms();   delay(6000);   init_sms1();   send_data(""dustbin-001  is almost full:\n"");   esp8266.print("" Level in %"");    esp8266.print(lvl1);   send_sms();   delay(6000);  }  void ultrasensor(int trigPin,int echoPin)  {    digitalWrite(trigPin, LOW);  // Added this line   delayMicroseconds(2); // Added this line   digitalWrite(trigPin, HIGH);   delayMicroseconds(10); // Added this line   digitalWrite(trigPin, LOW);   duration = pulseIn(echoPin, HIGH);   distance = (duration/2) / 29.1;  }  ``` ## Instructions/Setup  - Download and install Arduino IDE - Make all the necessary connections - Compile and upload the code to the board <br>  ## Conclusion  While completing this project proposal, there are a few constraints that come up. First, the reading of sensors is less accurate, and need to be extra careful in handling the sensors. Second, notification to the mobile phone cannot be done because the cloud platform used is an open-source platform and cannot be used to send an alert or notification to the users. Development of the system needs to be done thoroughly to decrease the possibility of errors. However, added value has been added to this system to make the system more reliable. Normalization is being used to eliminate the outliers which can help to increase the accuracy of the distance reads by an ultrasonic sensor.     ## Contributing Contributions are what make the open-source community such an amazing place to learn, inspire, and create. Any contributions you make are greatly appreciated.  1. Fork the Project 2. Create your Feature Branch (git checkout -b feature/AmazingFeature) 3. Commit your changes (git commit -m 'Add some AmazingFeature') 4. Push to the Branch (git push origin feature/AmazingFeature) 5. Open a Pull Request  Pull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.   ## Creators [üîù](# smart-garbage-monitoring-system-using-iot)  This Project is Created by:-    - [Pavankumar Hegde](https://github.com/hegdepavankumar) [Team Leader]   - [Sushil Kumar Sah](https://github.com/sushilsah)   - [Safina Fathima](https://github.com/safinafab)   - [Santhosh Reddy](https://github.com/)    <br> <h3 align=""center"">Show some &nbsp;‚ù§Ô∏è&nbsp; by starring some of the repositories!</h3> <br>    <!-- Support Me -->     if you like what I do, maybe consider buying me a coffee ü•∫üëâüëà  <a href=""https://www.buymeacoffee.com/hegdepavankumar"" target=""_blank""><img src=""https://cdn.buymeacoffee.com/buttons/v2/default-red.png"" alt=""Buy Me A Coffee"" width=""150"" ></a>   "
IoT Based Home Automation,Internet of Things,https://github.com/abhijain2472/IoT-Smart-Home-Automation,"<div align='center'>   <img src=""images/thumb.png"" height='400'> </div>  # IoT Smart Home Automation ‚ú®  <ul>   <li>This repository contains all the files and required packages related to IoT Smart Home Automation project.</li> </ul> <br>  ## [![watch](images/youtube.png)](https://youtu.be/u44gbTO9PWA) [IoT Smart Home Automation Project Full Demo Video](https://youtu.be/u44gbTO9PWA) <br>   ## üìü Project Preview :  <div align='center'>   <img src='images/mix1.jpg' width=200> </div> <br>  üü¢ [Click Here to download APK](https://github.com/abhijain2472/IoT-Smart-Home-Automation/raw/master/APK/Home%20Automation.apk) üì≤ (PIN: 123456) <br><br> üü¢ [Click Here to go to website](https://iottest-7498a.firebaseapp.com) üñ•Ô∏è (PIN: 123456)  <br>   ## ‚ÑπÔ∏è Inroduction :  <ul>   <li>IoT Smart Home Automation is a solution that enable us to control our home appliaces through Mobile, PC/Laptop, Alexa, Google Assistant and Telegram. This project even provides user to monitor power consumption of respective devices.</li>   <li>The purpose of our IoT Smart Home Automation is to provide a technological tool for today‚Äôs techno savvy that provide various functions which are significantly useful in current world for saving and efficiently using electricity.</li> </ul>  ## üé® Features :  <ul>   <li>User can control home appliance in following ways:     <ul>       <li>Mobile</li>       <li>PC/Laptop</li>       <li>Google Assistant</li>       <li>Amazon Alexa</li>       <li>Telegram</li>       <li>FB Messenger</li>     </ul>   </li>   <br>   <li>User can keep track of power consumption of the appliances:     <ul>       <li>Show estimated bill based on previous usage.</li>       <li>Consumption data can be filtered by date.</li>     </ul>   </li>   <li>Smart light mode:     <ul>       <li>This feature is implemented using PIR Motion Sensor which detects movement of leaving objects. Thus when any person is in the range of sensor, light will be turned on otherwise turned off automatically.</li>     </ul>   </li> </ul>  <div align='center'>   <img src=""images/ways.png"" height='400'> </div>  ## üìä Circuit Diagram :  <div align='center'>   <img src=""images/iot diagram.png"" height='400'> </div> <hr> <div align='center'>   <img src=""images/flow.png"" height='400'> </div>  ## ü§ñ Technologies :  <div align='center'>   <img src='images/088.jpeg'> </div> <br>  ## ‚öôÔ∏è Hardwares used :  <ul>   <li><a target=""_blank"" href=""https://www.amazon.in/Easy-Electronics-NodeMcu-Development-Board/dp/B06XYRS6KC/ref=sr_1_1?dchild=1&keywords=s+8266&qid=1598761236&sr=8-1"">Node-MCU ESP8266</a></li>   <li><a href=""https://www.amazon.in/Generic-ACS712-Current-Sensor-Module/dp/B00NU8XD80/ref=sr_1_1?dchild=1&keywords=Current+Sensor+ACS712&qid=1598761326&sr=8-1"" target=""_blank"">Current Sensor ACS712</a></li>   <li><a href=""https://www.amazon.in/AusleseTM-Channel-Module-Raspberry-Trigger/dp/B08C7W3B41/ref=sr_1_2?dchild=1&keywords=Relay+Module&qid=1598761358&sr=8-2"" target=""_blank"">Relay Module</a></li>   <li><a href=""https://www.amazon.in/Robodo-SEN3-Pyroelectric-Infrared-Detector/dp/B073Q399F6/ref=sr_1_6?dchild=1&keywords=PIR+Motion+Sensor&qid=1598761388&sr=8-6"" target=""_blank"">PIR Motion Sensor</a></li>   <li><a href=""https://www.amazon.in/ApTechDeals-Jumper-Female-breadboard-jumper/dp/B074J9CPV3/ref=sr_1_1?dchild=1&keywords=Male-Female+wires&qid=1598761452&sr=8-1"" target=""_blank"">Male-Female Wires</a></li>   <li>100W Bulb</li>   <li>Powerbank</li> </ul>  ## üì± Some Screenshots :  <div align='center'>  <img src=""images/app_welcome.jpg"" height='300'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   <img src=""images/app_home.jpg"" height='300'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   <img src=""images/app_powerusage.jpg"" height='300'> </div> <br><br> <div align='center'>   <img src=""images/app_smartlightmode.jpg"" height='300'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   <img src=""images/app_totalusage.jpg"" height='300'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   <img src=""images/app_billestimation.jpg"" height='300'> </div> <br><br> <div align='center'>   <img src=""images/web_home.jpg"" height='200'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   <img src=""images/web_power_usage.jpg"" height='200'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </div>  <div align='center'>   <img src=""images/web_login.jpg"" height='200'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   <img src=""images/web_smart_light.jpg"" height='200'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </div>  ## ü§ùüèª Project Team :  **1.  Abhishek Jain (abhishekjain2472@gmail.com)** <br><br>&nbsp;&nbsp;&nbsp;<a href='https://instagram.com/abhijain2472?igshid=15hg7ep3vtioq' target='_blank'><img src='images/insta.png' width=70></a> <br><br>&nbsp;&nbsp;&nbsp;<a href='https://github.com/abhijain2472' target='_blank'><img src='images/github.png' width=70></a> <br><br>&nbsp;&nbsp;&nbsp;<a href='https://www.linkedin.com/in/abhijain2472' target='_blank'><img src='images/linkedin.png' width=70></a> <br><br>&nbsp;&nbsp;&nbsp;<a href='https://twitter.com/abhijain2472?s=08' target='_blank'><img src='images/twitter.png' width=70></a> <br><br><br> **2.  Dhruv Patel (dhruvap.2610@gmail.com)** <br><br>&nbsp;&nbsp;&nbsp;<a href='https://instagram.com/it_z_dhruv_?igshid=1xxujk8y9rj97' target='_blank'><img src='images/insta.png' width=70></a> <br><br>&nbsp;&nbsp;&nbsp;<a href='https://github.com/dhruv2610' target='_blank'><img src='images/github.png' width=70></a> <br><br>&nbsp;&nbsp;&nbsp;<a href='https://www.linkedin.com/in/dhruv-patel-abb962188' target='_blank'><img src='images/linkedin.png' width=70></a> <br><br>&nbsp;&nbsp;&nbsp;<a href='https://twitter.com/Dhruvpatel261?s=08' target='_blank'><img src='images/twitter.png' width=70></a> <br><br><br> **3.  Raj Sathawara (rajsathawara@gmail.com)** <br><br>&nbsp;&nbsp;&nbsp;<a href='https://instagram.com/raj.sathawara?igshid=qnagtr8q5jij' target='_blank'><img src='images/insta.png' width=70></a> <br><br>&nbsp;&nbsp;&nbsp;<a href='https://github.com/raj1234sa' target='_blank'><img src='images/github.png' width=70></a> <br><br>&nbsp;&nbsp;&nbsp;<a href='https://www.linkedin.com/in/raj-sathawara-681752117' target='_blank'><img src='images/linkedin.png' width=70></a> <br><br>&nbsp;&nbsp;&nbsp;<a href='https://twitter.com/KadiyaPratham?s=08' target='_blank'><img src='images/twitter.png' width=70></a>  ## üì∑ Graphic Designer & Video Editing :  **1.  Viraj Patel (pviraj98@gmail.com)** <br><br>&nbsp;&nbsp;&nbsp;<a href='https://www.instagram.com/callme.adesigner/' target='_blank'><img src='images/insta.png' width=70></a> <br><br>&nbsp;&nbsp;&nbsp;<a href='https://www.linkedin.com/in/virajpatel98' target='_blank'><img src='images/linkedin.png' width=70></a> <br><br>&nbsp;&nbsp;&nbsp;<a href='https://twitter.com/BhartiyaaNagrik?s=08' target='_blank'><img src='images/twitter.png' width=70></a> <br><br>&nbsp;&nbsp;&nbsp;<a href='https://www.youtube.com/channel/UCX0NM2TEIizoTqDO9hlCWZg' target='_blank'><img src='images/yt.png' width=70></a>"
IoT Based Office Automation,Internet of Things,https://github.com/automagica/automagica,"![https://automagica.com)](https://automagica.com/wp-content/uploads/2020/06/logo.png)  # Automagica   The Automagica project began in 2018 with a focus on creating open source software to ensure that Robotic Process Automation technologies were accessible to all.  With subsequent releases of Automagica, more advanced features such as Wand and the Portal required a service infrastructure to deliver more resilient robots, advanced services, and management and control.  As use of these services has increased, the costs for hosting and maintaining the service layer have correspondingly increased.   In order to drive the next phase of development for these important services, today, 13th October 2020, we are pleased to announce that Netcall plc, a leading provider of low-code, customer engagement, and contact centre software, has acquired Oakwood Technologies BV (trading as ‚ÄòAutomagica‚Äô).  Netcall will integrate Automagica‚Äôs RPA into its Liberty platform, providing a powerful combination of RPA, Low-code, and Customer Engagement solutions.  The Automagica Robot is no longer be available under the terms of the AGPL3 licence.  We‚Äôre not stopping the use of the services for robots already deployed. These will continue to operate with Wand and OCR, free of charge, for three months from today (13 Oct 20).  Existing users of the Automagica Portal will also be able to access, free of charge, for three months, during which time users will be offered options to migrate to a commercial service.  We wish to thank all contributors to the project.   ![Love Automagica Example](https://i.imgur.com/C4M6LBl.gif)  ## Components The Automagica suite consists of the following components: - __Automagica Bot__: runtime/agent responsible for performing the automated tasks. - __Automagica Flow__: a visual flow designer to build automations quickly with full support for Python code. - __Automagica Wand__: UI element picker powered by AI. - __Automagica Lab__: Notebook-style automation development environment based on Jupyter Notebooks (requires Jupyter to be installed). - __Automagica Portal__: management of bots, credentials, automations, logs, ...  ![Portal and Flow](https://i.imgur.com/ps1Uhck.png)  ## Example  Browser working with Excel:  ![Excel Example Automagica](https://automagica.com/wp-content/uploads/2020/06/browser_excel.gif)   ## Activities  An overview of all official Automagica activities:  Process | Description ------- | ----------- **Cryptography** | ‚Äå‚Äå  <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/key-solid.svg"" width=""20""> [Random key](https://automagica.readthedocs.io/activities.html#automagica.activities.generate_random_key) | Generate random Fernet key. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/lock-solid.svg"" width=""20""> [Encrypt text](https://automagica.readthedocs.io/activities.html#automagica.activities.encrypt_text_with_key) | Encrypt text with (Fernet) key, <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/lock-open-solid.svg"" width=""20""> [Decrypt text](https://automagica.readthedocs.io/activities.html#automagica.activities.decrypt_text_with_key) | Dexrypt bytes-like object to string with (Fernet) key <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/lock-solid.svg"" width=""20""> [Encrypt file](https://automagica.readthedocs.io/activities.html#automagica.activities.encrypt_file_with_key) | Encrypt file with (Fernet) key. Note that file will be unusable unless unlocked with the same key. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/lock-open-solid.svg"" width=""20""> [Decrypt file](https://automagica.readthedocs.io/activities.html#automagica.activities.decrypt_file_with_key) | Decrypts file with (Fernet) key <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/lock-solid.svg"" width=""20""> [Key from password](https://automagica.readthedocs.io/activities.html#automagica.activities.generate_key_from_password) | Generate key based on password and salt. If both password and salt are known the key can be regenerated. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/fingerprint-solid.svg"" width=""20""> [Hash from file](https://automagica.readthedocs.io/activities.html#automagica.activities.generate_hash_from_file) | Generate hash from file <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/fingerprint-solid.svg"" width=""20""> [Hash from text](https://automagica.readthedocs.io/activities.html#automagica.activities.generate_hash_from_text) | Generate hash from text. Keep in mind that MD5 is not cryptographically secure. **Random** | ‚Äå‚Äå  <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/dice-solid.svg"" width=""20""> [Random number](https://automagica.readthedocs.io/activities.html#automagica.activities.generate_random_number) | Random numbers can be integers (not a fractional number) or a float (fractional number). <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/digital-tachograph-solid.svg"" width=""20""> [Random data](https://automagica.readthedocs.io/activities.html#automagica.activities.generate_random_data) | Generates all kinds of random data. Specifying locale changes format for some options <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/coins-solid.svg"" width=""20""> [Random boolean](https://automagica.readthedocs.io/activities.html#automagica.activities.generate_random_boolean) | Generates a random boolean (True or False) <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/user-tag-solid.svg"" width=""20""> [Random name](https://automagica.readthedocs.io/activities.html#automagica.activities.generate_random_name) | Generates a random name. Adding a locale adds a more common name in the specified locale. Provides first name and last name. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/comment-solid.svg"" width=""20""> [Random words](https://automagica.readthedocs.io/activities.html#automagica.activities.generate_random_words) | Generates a random sentence. Specifying locale changes language and content based on locale. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/map-marker-solid.svg"" width=""20""> [Random address](https://automagica.readthedocs.io/activities.html#automagica.activities.generate_random_address) | Generates a random address. Specifying locale changes random locations and streetnames based on locale. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/volume-up-solid.svg"" width=""20""> [Random beep](https://automagica.readthedocs.io/activities.html#automagica.activities.generate_random_beep) | Generates a random beep, only works on Windows <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/calendar-solid.svg"" width=""20""> [Random date](https://automagica.readthedocs.io/activities.html#automagica.activities.generate_random_date) | Generates a random date. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/calendar-solid.svg"" width=""20""> [Today's date](https://automagica.readthedocs.io/activities.html#automagica.activities.generate_date_today) | Generates today's date. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/random-solid.svg"" width=""20""> [Generate unique identifier](https://automagica.readthedocs.io/activities.html#automagica.activities.generate_unique_identifier) | Generates a random UUID4 (universally unique identifier).  **Output** | ‚Äå‚Äå  <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/tv-solid.svg"" width=""20""> [Display overlay message](https://automagica.readthedocs.io/activities.html#automagica.activities.display_osd_message) | Display custom OSD (on-screen display) message. Can be used to display a message for a limited amount of time. Can be used for illustration, debugging or as OSD. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/tv-solid.svg"" width=""20""> [Print message in console](https://automagica.readthedocs.io/activities.html#automagica.activities.print_console) | Print message in console. Can be used to display data in the Automagica Flow console **Browser** | ‚Äå‚Äå  <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/chrome.svg"" width=""20""> [Chrome](https://automagica.readthedocs.io/activities.html#automagica.activities.Chrome) | Open Chrome Browser <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/images-solid.svg"" width=""20""> [Save all images](https://automagica.readthedocs.io/activities.html#automagica.activities.save_all_images) | Save all images on current page in the Browser <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/chrome.svg"" width=""20""> [Browse to URL](https://automagica.readthedocs.io/activities.html#automagica.activities.browse_to) | Browse to URL. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/align-center-solid.svg"" width=""20""> [Find elements by text](https://automagica.readthedocs.io/activities.html#automagica.activities.find_elements_by_text) | Find all elements by their text. Text does not need to match exactly, part of text is enough. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/window-restore-solid.svg"" width=""20""> [Find all links](https://automagica.readthedocs.io/activities.html#automagica.activities.find_all_links) | Find all links on a webpage in the browser <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/window-restore-solid.svg"" width=""20""> [Find first link on a webpage](https://automagica.readthedocs.io/activities.html#automagica.activities.find_first_link) | Find first link on a webpage <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/window-restore-solid.svg"" width=""20""> [Get all text on webpage](https://automagica.readthedocs.io/activities.html#automagica.activities.get_text_on_webpage) | Get all the raw body text from current webpage <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/highlighter-solid.svg"" width=""20""> [Highlight element](https://automagica.readthedocs.io/activities.html#automagica.activities.highlight) | Highlight elements in yellow in the browser <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/window-close-solid.svg"" width=""20""> [Exit the browser](https://automagica.readthedocs.io/activities.html#automagica.activities.exit) | Quit the browser by exiting gracefully. One can also use the native 'quit' function <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/times-solid.svg"" width=""20""> [Find all XPaths](https://automagica.readthedocs.io/activities.html#automagica.activities.by_xpaths) | Find all elements with specified xpath on a webpage in the the browser. Can also use native 'find_elements_by_xpath' <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/times-solid.svg"" width=""20""> [Find XPath in browser](https://automagica.readthedocs.io/activities.html#automagica.activities.by_xpath) | Find all element with specified xpath on a webpage in the the browser. Can also use native 'find_elements_by_xpath' <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/times-solid.svg"" width=""20""> [Find class in browser](https://automagica.readthedocs.io/activities.html#automagica.activities.by_class) | Find element with specified class on a webpage in the the browser. Can also use native 'find_element_by_class_name' <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/times-solid.svg"" width=""20""> [Find class in browser](https://automagica.readthedocs.io/activities.html#automagica.activities.by_classes) | Find all elements with specified class on a webpage in the the browser. Can also use native 'find_elements_by_class_name' function <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/times-solid.svg"" width=""20""> [Find element in browser based on class and text](https://automagica.readthedocs.io/activities.html#automagica.activities.by_class_and_by_text) | Find all elements with specified class and text on a webpage in the the browser. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/times-solid.svg"" width=""20""> [Find id in browser](https://automagica.readthedocs.io/activities.html#automagica.activities.by_id) | Find element with specified id on a webpage in the the browser. Can also use native 'find_element_by_id' function <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/times-solid.svg"" width=""20""> [Switch to iframe in browser](https://automagica.readthedocs.io/activities.html#automagica.activities.switch_to_iframe) | Switch to an iframe in the browser **Credential Management** | ‚Äå‚Äå  <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/key-solid.svg"" width=""20""> [Set credential](https://automagica.readthedocs.io/activities.html#automagica.activities.set_credential) | Add a credential which stores credentials locally and securely. All parameters should be Unicode text. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/key-solid.svg"" width=""20""> [Delete credential](https://automagica.readthedocs.io/activities.html#automagica.activities.delete_credential) | Delete a locally stored credential. All parameters should be Unicode text. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/key-solid.svg"" width=""20""> [Get credential](https://automagica.readthedocs.io/activities.html#automagica.activities.get_credential) | Get a locally stored redential. All parameters should be Unicode text. **FTP** | ‚Äå‚Äå  <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/folder-open-solid.svg"" width=""20""> [Create FTP connection (insecure)](https://automagica.readthedocs.io/activities.html#automagica.activities.FTP) | Can be used to automate activites for FTP <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/download-solid.svg"" width=""20""> [Download file](https://automagica.readthedocs.io/activities.html#automagica.activities.download_file) | Downloads a file from FTP server. Connection needs to be established first. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/upload-solid.svg"" width=""20""> [Upload file](https://automagica.readthedocs.io/activities.html#automagica.activities.upload_file) | Upload file to FTP server <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/list-ol-solid.svg"" width=""20""> [List FTP files](https://automagica.readthedocs.io/activities.html#automagica.activities.enumerate_files) | Generate a list of all the files in the FTP directory <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/list-ol-solid.svg"" width=""20""> [Check FTP directory](https://automagica.readthedocs.io/activities.html#automagica.activities.directory_exists) | Check if FTP directory exists <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/folder-plus-solid.svg"" width=""20""> [Create FTP directory](https://automagica.readthedocs.io/activities.html#automagica.activities.create_directory) | Create a FTP directory. Note that sufficient permissions are present **Keyboard** | ‚Äå‚Äå  <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/keyboard-solid.svg"" width=""20""> [Press key](https://automagica.readthedocs.io/activities.html#automagica.activities.press_key) | Press and release an entered key. Make sure your keyboard is on US layout (standard QWERTY).If you are using this on Mac Os you might need to grant access to your terminal application. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/keyboard-solid.svg"" width=""20""> [Press key combination](https://automagica.readthedocs.io/activities.html#automagica.activities.press_key_combination) | Press a combination of two or three keys simultaneously. Make sure your keyboard is on US layout (standard QWERTY). <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/keyboard-solid.svg"" width=""20""> [Type text](https://automagica.readthedocs.io/activities.html#automagica.activities.typing) | Simulate keystrokes. If an element ID is specified, text will be typed in a specific field or element based on the element ID (vision) by the recorder. **Mouse** | ‚Äå‚Äå  <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/mouse-solid.svg"" width=""20""> [Get mouse coordinates](https://automagica.readthedocs.io/activities.html#automagica.activities.get_mouse_position) | Get the x and y pixel coordinates of current mouse position. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/search-location-solid.svg"" width=""20""> [Display mouse position](https://automagica.readthedocs.io/activities.html#automagica.activities.display_mouse_position) | Displays mouse position in an overlay. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/mouse-pointer-solid.svg"" width=""20""> [Mouse click](https://automagica.readthedocs.io/activities.html#automagica.activities.click) | Clicks on an element based on the element ID (vision) <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/mouse-pointer-solid.svg"" width=""20""> [Mouse click coordinates](https://automagica.readthedocs.io/activities.html#automagica.activities.click_coordinates) | Clicks on an element based on pixel position determined by x and y coordinates. To find coordinates one could use display_mouse_position(). <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/mouse-pointer-solid.svg"" width=""20""> [Double mouse click coordinates](https://automagica.readthedocs.io/activities.html#automagica.activities.double_click_coordinates) | Double clicks on a pixel position determined by x and y coordinates. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/mouse-pointer-solid.svg"" width=""20""> [Double mouse click](https://automagica.readthedocs.io/activities.html#automagica.activities.double_click) | Double clicks on an element based on the element ID (vision) <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/mouse-pointer-solid.svg"" width=""20""> [Right click](https://automagica.readthedocs.io/activities.html#automagica.activities.right_click) | Right clicks on an element based on the element ID (vision) <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/mouse-pointer-solid.svg"" width=""20""> [Right click coordinates](https://automagica.readthedocs.io/activities.html#automagica.activities.right_click_coordinates) | Right clicks on an element based pixel position determined by x and y coordinates. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/arrows-alt-solid.svg"" width=""20""> [Move mouse](https://automagica.readthedocs.io/activities.html#automagica.activities.move_mouse_to) | Moves te pointer to an element based on the element ID (vision) <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/arrows-alt-solid.svg"" width=""20""> [Move mouse coordinates](https://automagica.readthedocs.io/activities.html#automagica.activities.move_mouse_to_coordinates) | Moves te pointer to an element based on the pixel position determined by x and y coordinates <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/arrows-alt-solid.svg"" width=""20""> [Move mouse relative](https://automagica.readthedocs.io/activities.html#automagica.activities.move_mouse_relative) | Moves the mouse an x- and y- distance relative to its current pixel position. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/arrows-alt-solid.svg"" width=""20""> [Drag mouse](https://automagica.readthedocs.io/activities.html#automagica.activities.drag_mouse_to_coordinates) | Drags mouse to an element based on pixel position determined by x and y coordinates <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/arrows-alt-solid.svg"" width=""20""> [Drag mouse](https://automagica.readthedocs.io/activities.html#automagica.activities.drag_mouse_to) | Drags mouse to an element based on the element ID (vision) **Image** | ‚Äå‚Äå  <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/crop-alt-solid.svg"" width=""20""> [Random screen snippet](https://automagica.readthedocs.io/activities.html#automagica.activities.random_screen_snippet) | Take a random square snippet from the current screen. Mainly for testing and/or development purposes. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/expand-solid.svg"" width=""20""> [Screenshot](https://automagica.readthedocs.io/activities.html#automagica.activities.take_screenshot) | Take a screenshot of current screen. **Folder Operations** | ‚Äå‚Äå  <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/search-solid.svg"" width=""20""> [List files in folder](https://automagica.readthedocs.io/activities.html#automagica.activities.get_files_in_folder) | List all files in a folder (and subfolders) <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/folder-plus-solid.svg"" width=""20""> [Create folder](https://automagica.readthedocs.io/activities.html#automagica.activities.create_folder) | Creates new folder at the given path. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/folder-solid.svg"" width=""20""> [Rename folder](https://automagica.readthedocs.io/activities.html#automagica.activities.rename_folder) | Rename a folder <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/folder-open-solid.svg"" width=""20""> [Open a folder](https://automagica.readthedocs.io/activities.html#automagica.activities.show_folder) | Open a folder with the default explorer. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/folder-solid.svg"" width=""20""> [Move a folder](https://automagica.readthedocs.io/activities.html#automagica.activities.move_folder) | Moves a folder from one place to another. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/folder-minus-solid.svg"" width=""20""> [Remove folder](https://automagica.readthedocs.io/activities.html#automagica.activities.remove_folder) | Remove a folder including all subfolders and files. For the function to work optimal, all files and subfolders in the main targetfolder should be closed. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/folder-minus-solid.svg"" width=""20""> [Empty folder](https://automagica.readthedocs.io/activities.html#automagica.activities.empty_folder) | Remove all contents from a folderFor the function to work optimal, all files and subfolders in the main targetfolder should be closed. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/folder-solid.svg"" width=""20""> [Checks if folder exists](https://automagica.readthedocs.io/activities.html#automagica.activities.folder_exists) | Check whether folder exists or not, regardless if folder is empty or not. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/folder-solid.svg"" width=""20""> [Copy a folder](https://automagica.readthedocs.io/activities.html#automagica.activities.copy_folder) | Copies a folder from one place to another. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/archive-solid.svg"" width=""20""> [Zip](https://automagica.readthedocs.io/activities.html#automagica.activities.zip_folder) | Zip folder and its contents. Creates a .zip file. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/archive-solid.svg"" width=""20""> [Unzip](https://automagica.readthedocs.io/activities.html#automagica.activities.unzip) | Unzips a file or folder from a .zip file. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/clock-solid.svg"" width=""20""> [Return most recent file in directory](https://automagica.readthedocs.io/activities.html#automagica.activities.most_recent_file) | Return most recent file in directory **Delay** | ‚Äå‚Äå  <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/hourglass-solid.svg"" width=""20""> [Wait](https://automagica.readthedocs.io/activities.html#automagica.activities.wait) | Make the robot wait for a specified number of seconds. Note that this activity is blocking. This means that subsequent activities will not occur until the the specified waiting time has expired. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/hourglass-solid.svg"" width=""20""> [Wait for folder](https://automagica.readthedocs.io/activities.html#automagica.activities.wait_folder_exists) | Waits until a folder exists.Note that this activity is blocking and will keep the system waiting. **Word Application** | ‚Äå‚Äå  <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-word-solid.svg"" width=""20""> [Start Word Application](https://automagica.readthedocs.io/activities.html#automagica.activities.Word) | For this activity to work, Microsoft Office Word needs to be installed on the system. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-word-solid.svg"" width=""20""> [Save](https://automagica.readthedocs.io/activities.html#automagica.activities.save) | Save active Word document <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-word-solid.svg"" width=""20""> [Save As](https://automagica.readthedocs.io/activities.html#automagica.activities.save_as) | Save active Word document to a specific location <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-word-solid.svg"" width=""20""> [Append text](https://automagica.readthedocs.io/activities.html#automagica.activities.append_text) | Append text at end of Word document. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-word-solid.svg"" width=""20""> [Replace text](https://automagica.readthedocs.io/activities.html#automagica.activities.replace_text) | Can be used for example to replace arbitrary placeholder value. For example whenusing template document, using 'XXXX' as a placeholder. Take note that all strings are case sensitive. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-word-solid.svg"" width=""20""> [Read all text](https://automagica.readthedocs.io/activities.html#automagica.activities.read_all_text) | Read all the text from a document <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-pdf-solid.svg"" width=""20""> [Export to PDF](https://automagica.readthedocs.io/activities.html#automagica.activities.export_to_pdf) | Export the document to PDF <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/html5.svg"" width=""20""> [Export to HTML](https://automagica.readthedocs.io/activities.html#automagica.activities.export_to_html) | Export to HTML <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/heading-solid.svg"" width=""20""> [Set footers](https://automagica.readthedocs.io/activities.html#automagica.activities.set_footers) | Set the footers of the document <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/subscript-solid.svg"" width=""20""> [Set headers](https://automagica.readthedocs.io/activities.html#automagica.activities.set_headers) | Set the headers of the document <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-word-solid.svg"" width=""20""> [Quit Word](https://automagica.readthedocs.io/activities.html#automagica.activities.quit) | This closes Word, make sure to use 'save' or 'save_as' if you would like to save before quitting. **Word File** | ‚Äå‚Äå  <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-word-solid.svg"" width=""20""> [Read and Write Word files](https://automagica.readthedocs.io/activities.html#automagica.activities.WordFile) | These activities can read, write and edit Word (docx) files without the need of having Word installed. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-word-solid.svg"" width=""20""> [Read all text](https://automagica.readthedocs.io/activities.html#automagica.activities.read_all_text) | Read all the text from the document <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-word-solid.svg"" width=""20""> [Append text](https://automagica.readthedocs.io/activities.html#automagica.activities.append_text) | Append text at the end of the document <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-word-solid.svg"" width=""20""> [Save](https://automagica.readthedocs.io/activities.html#automagica.activities.save) | Save document <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-word-solid.svg"" width=""20""> [Save as](https://automagica.readthedocs.io/activities.html#automagica.activities.save_as) | Save file on specified path <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-word-solid.svg"" width=""20""> [Set headers](https://automagica.readthedocs.io/activities.html#automagica.activities.set_headers) | Set headers of Word document <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-word-solid.svg"" width=""20""> [Replace all](https://automagica.readthedocs.io/activities.html#automagica.activities.replace_text) | Replaces all occurences of a placeholder text in the document with a replacement text. **Outlook Application** | ‚Äå‚Äå  <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/mail-bulk-solid.svg"" width=""20""> [Start Outlook Application](https://automagica.readthedocs.io/activities.html#automagica.activities.Outlook) | For this activity to work, Outlook needs to be installed on the system. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/mail-bulk-solid.svg"" width=""20""> [Send e-mail](https://automagica.readthedocs.io/activities.html#automagica.activities.send_mail) | Send an e-mail using Outlook <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/mail-bulk-solid.svg"" width=""20""> [Retrieve folders](https://automagica.readthedocs.io/activities.html#automagica.activities.get_folders) | Retrieve list of folders from Outlook <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/mail-bulk-solid.svg"" width=""20""> [Retrieve e-mails](https://automagica.readthedocs.io/activities.html#automagica.activities.get_mails) | Retrieve list of messages from Outlook <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/mail-bulk-solid.svg"" width=""20""> [Delete e-mails](https://automagica.readthedocs.io/activities.html#automagica.activities.delete_mails) | Deletes e-mail messages in a certain folder. Can be specified by searching on subject, body or sender e-mail. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/mail-bulk-solid.svg"" width=""20""> [Move e-mails](https://automagica.readthedocs.io/activities.html#automagica.activities.move_mails) | Move e-mail messages in a certain folder. Can be specified by searching on subject, body or sender e-mail. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/mail-bulk-solid.svg"" width=""20""> [Save attachments](https://automagica.readthedocs.io/activities.html#automagica.activities.save_attachments) | Save all attachments from certain folder <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/mail-bulk-solid.svg"" width=""20""> [Retrieve contacts](https://automagica.readthedocs.io/activities.html#automagica.activities.get_contacts) | Retrieve all contacts <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/mail-bulk-solid.svg"" width=""20""> [Add a contact](https://automagica.readthedocs.io/activities.html#automagica.activities.add_contact) | Add a contact to Outlook contacts <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/mail-bulk-solid.svg"" width=""20""> [Quit](https://automagica.readthedocs.io/activities.html#automagica.activities.quit) | Close the Outlook application **Excel Application** | ‚Äå‚Äå  <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-excel-solid.svg"" width=""20""> [Start Excel Application](https://automagica.readthedocs.io/activities.html#automagica.activities.Excel) | For this activity to work, Microsoft Office Excel needs to be installed on the system. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-excel-solid.svg"" width=""20""> [Add worksheet](https://automagica.readthedocs.io/activities.html#automagica.activities.add_worksheet) | Adds a worksheet to the current workbook <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-excel-solid.svg"" width=""20""> [Activate worksheet](https://automagica.readthedocs.io/activities.html#automagica.activities.activate_worksheet) | Activate a worksheet in the current Excel document by name <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-excel-solid.svg"" width=""20""> [Save](https://automagica.readthedocs.io/activities.html#automagica.activities.save) | Save the current workbook. Defaults to homedir <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-excel-solid.svg"" width=""20""> [Save as](https://automagica.readthedocs.io/activities.html#automagica.activities.save_as) | Save the current workbook to a specific path <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-excel-solid.svg"" width=""20""> [Write cell](https://automagica.readthedocs.io/activities.html#automagica.activities.write_cell) | Write to a specific cell in the currently active workbook and active worksheet <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-excel-solid.svg"" width=""20""> [Read cell](https://automagica.readthedocs.io/activities.html#automagica.activities.read_cell) | Read a cell from the currently active workbook and active worksheet <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-excel-solid.svg"" width=""20""> [Write range](https://automagica.readthedocs.io/activities.html#automagica.activities.write_range) | Write to a specific range in the currently active worksheet in the active workbook <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-excel-solid.svg"" width=""20""> [Read range](https://automagica.readthedocs.io/activities.html#automagica.activities.read_range) | Read a range of cells from the currently active worksheet in the active workbook <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-excel-solid.svg"" width=""20""> [Run macro](https://automagica.readthedocs.io/activities.html#automagica.activities.run_macro) | Run a macro by name from the currently active workbook <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-excel-solid.svg"" width=""20""> [Get worksheet names](https://automagica.readthedocs.io/activities.html#automagica.activities.get_worksheet_names) | Get names of all the worksheets in the currently active workbook <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-excel-solid.svg"" width=""20""> [Get table](https://automagica.readthedocs.io/activities.html#automagica.activities.get_table) | Get table data from the currently active worksheet by name of the table <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-excel-solid.svg"" width=""20""> [Activate range](https://automagica.readthedocs.io/activities.html#automagica.activities.activate_range) | Activate a particular range in the currently active workbook <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-excel-solid.svg"" width=""20""> [Activate first empty cell down](https://automagica.readthedocs.io/activities.html#automagica.activities.activate_first_empty_cell_down) | Activates the first empty cell going down <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-excel-solid.svg"" width=""20""> [Activate first empty cell right](https://automagica.readthedocs.io/activities.html#automagica.activities.activate_first_empty_cell_right) | Activates the first empty cell going right <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-excel-solid.svg"" width=""20""> [Activate first empty cell left](https://automagica.readthedocs.io/activities.html#automagica.activities.activate_first_empty_cell_left) | Activates the first empty cell going left <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-excel-solid.svg"" width=""20""> [Activate first empty cell up](https://automagica.readthedocs.io/activities.html#automagica.activities.activate_first_empty_cell_up) | Activates the first empty cell going up <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-excel-solid.svg"" width=""20""> [Write cell formula](https://automagica.readthedocs.io/activities.html#automagica.activities.write_cell_formula) | Write a formula to a particular cell <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-excel-solid.svg"" width=""20""> [Read cell formula](https://automagica.readthedocs.io/activities.html#automagica.activities.read_cell_formula) | Read the formula from a particular cell <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-excel-solid.svg"" width=""20""> [Insert empty row](https://automagica.readthedocs.io/activities.html#automagica.activities.insert_empty_row) | Inserts an empty row to the currently active worksheet <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-excel-solid.svg"" width=""20""> [Insert empty column](https://automagica.readthedocs.io/activities.html#automagica.activities.insert_empty_column) | Inserts an empty column in the currently active worksheet. Existing columns will shift to the right. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-excel-solid.svg"" width=""20""> [Delete row in Excel](https://automagica.readthedocs.io/activities.html#automagica.activities.delete_row) | Deletes a row from the currently active worksheet. Existing data will shift up. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-excel-solid.svg"" width=""20""> [Delete column](https://automagica.readthedocs.io/activities.html#automagica.activities.delete_column) | Delete a column from the currently active worksheet. Existing columns will shift to the left. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-excel-solid.svg"" width=""20""> [Export to PDF](https://automagica.readthedocs.io/activities.html#automagica.activities.export_to_pdf) | Export to PDF <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-excel-solid.svg"" width=""20""> [Insert data as table](https://automagica.readthedocs.io/activities.html#automagica.activities.insert_data_as_table) | Insert list of dictionaries as a table in Excel <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-excel-solid.svg"" width=""20""> [Read worksheet](https://automagica.readthedocs.io/activities.html#automagica.activities.read_worksheet) | Read data from a worksheet as a list of lists <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-excel-solid.svg"" width=""20""> [Quit Excel](https://automagica.readthedocs.io/activities.html#automagica.activities.quit) | This closes Excel, make sure to use 'save' or 'save_as' if you would like to save before quitting. **Excel File** | ‚Äå‚Äå  <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-excel-solid.svg"" width=""20""> [Read and Write xlsx files.](https://automagica.readthedocs.io/activities.html#automagica.activities.ExcelFile) | This activity can read, write and edit Excel (xlsx) files without the need of having Excel installed. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-excel-solid.svg"" width=""20""> [Export file to dataframe](https://automagica.readthedocs.io/activities.html#automagica.activities.to_dataframe) | Export to pandas dataframe <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-excel-solid.svg"" width=""20""> [Activate worksheet](https://automagica.readthedocs.io/activities.html#automagica.activities.activate_worksheet) | Activate a worksheet. By default the first worksheet is activated. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-excel-solid.svg"" width=""20""> [Save as](https://automagica.readthedocs.io/activities.html#automagica.activities.save_as) | Save file as <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-excel-solid.svg"" width=""20""> [Save as](https://automagica.readthedocs.io/activities.html#automagica.activities.save) | Save file <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-excel-solid.svg"" width=""20""> [Write cell](https://automagica.readthedocs.io/activities.html#automagica.activities.write_cell) | Write a cell based on column and row <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-excel-solid.svg"" width=""20""> [Read cell](https://automagica.readthedocs.io/activities.html#automagica.activities.read_cell) | Read a cell based on column and row <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-excel-solid.svg"" width=""20""> [Add worksheet](https://automagica.readthedocs.io/activities.html#automagica.activities.add_worksheet) | Add a worksheet <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-excel-solid.svg"" width=""20""> [Get worksheet names](https://automagica.readthedocs.io/activities.html#automagica.activities.get_worksheet_names) | Get worksheet names **PowerPoint Application** | ‚Äå‚Äå  <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-powerpoint-solid.svg"" width=""20""> [Start PowerPoint Application](https://automagica.readthedocs.io/activities.html#automagica.activities.PowerPoint) | For this activity to work, PowerPoint needs to be installed on the system. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-powerpoint-solid.svg"" width=""20""> [Save PowerPoint](https://automagica.readthedocs.io/activities.html#automagica.activities.save_as) | Save PowerPoint Slidedeck <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-powerpoint-solid.svg"" width=""20""> [Save PowerPoint](https://automagica.readthedocs.io/activities.html#automagica.activities.save) | Save PowerPoint Slidedeck <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-powerpoint-solid.svg"" width=""20""> [Close PowerPoint Application](https://automagica.readthedocs.io/activities.html#automagica.activities.quit) | Close PowerPoint <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-powerpoint-solid.svg"" width=""20""> [Add PowerPoint Slides](https://automagica.readthedocs.io/activities.html#automagica.activities.add_slide) | Adds slides to a presentation <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-powerpoint-solid.svg"" width=""20""> [Slide count](https://automagica.readthedocs.io/activities.html#automagica.activities.number_of_slides) | Returns the number of slides <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-powerpoint-solid.svg"" width=""20""> [Text to slide](https://automagica.readthedocs.io/activities.html#automagica.activities.add_text) | Add text to a slide <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-powerpoint-solid.svg"" width=""20""> [Delete slide](https://automagica.readthedocs.io/activities.html#automagica.activities.delete_slide) | Delete a slide <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-powerpoint-solid.svg"" width=""20""> [Replace all occurences of text in PowerPoint slides](https://automagica.readthedocs.io/activities.html#automagica.activities.replace_text) | Can be used for example to replace arbitrary placeholder value in a PowerPoint. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-powerpoint-solid.svg"" width=""20""> [PowerPoint to PDF](https://automagica.readthedocs.io/activities.html#automagica.activities.export_to_pdf) | Export PowerPoint presentation to PDF file <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-powerpoint-solid.svg"" width=""20""> [Slides to images](https://automagica.readthedocs.io/activities.html#automagica.activities.export_slides_to_images) | Export PowerPoint slides to seperate image files **Office 365** | ‚Äå‚Äå  <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/envelope-solid.svg"" width=""20""> [Send email Office Outlook 365](https://automagica.readthedocs.io/activities.html#automagica.activities.send_email_with_outlook365) | Send email Office Outlook 365 **Salesforce** | ‚Äå‚Äå  <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/salesforce.svg"" width=""20""> [Salesforce API](https://automagica.readthedocs.io/activities.html#automagica.activities.salesforce_api_call) | Activity to make calls to Salesforce REST API. **E-mail (SMTP)** | ‚Äå‚Äå  <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/mail-bulk-solid.svg"" width=""20""> [Mail with SMTP](https://automagica.readthedocs.io/activities.html#automagica.activities.send_mail_smtp) | This function lets you send emails with an e-mail address. **Windows OS** | ‚Äå‚Äå  <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/readme.svg"" width=""20""> [Find window with specific title](https://automagica.readthedocs.io/activities.html#automagica.activities.find_window_title) | Find a specific window based on the name, either a perfect match or a partial match. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/passport-solid.svg"" width=""20""> [Login to Windows Remote Desktop](https://automagica.readthedocs.io/activities.html#automagica.activities.start_remote_desktop) | Create a RDP and login to Windows Remote Desktop <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/passport-solid.svg"" width=""20""> [Stop Windows Remote Desktop](https://automagica.readthedocs.io/activities.html#automagica.activities.close_remote_desktop) | Stop Windows Remote Desktop <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/passport-solid.svg"" width=""20""> [Set Windows password](https://automagica.readthedocs.io/activities.html#automagica.activities.set_user_password) | Sets the password for a Windows user. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/passport-solid.svg"" width=""20""> [Check Windows password](https://automagica.readthedocs.io/activities.html#automagica.activities.validate_user_password) | Validates a Windows user password if it is correct <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/user-lock-solid.svg"" width=""20""> [Lock Windows](https://automagica.readthedocs.io/activities.html#automagica.activities.lock_windows) | Locks Windows requiring login to continue. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/user-solid.svg"" width=""20""> [Check if Windows logged in](https://automagica.readthedocs.io/activities.html#automagica.activities.is_logged_in) | Checks if the current user is logged in and not on the lockscreen. Most automations do not work properly when the desktop is locked. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/user-solid.svg"" width=""20""> [Check if Windows is locked](https://automagica.readthedocs.io/activities.html#automagica.activities.is_desktop_locked) | Checks if the current user is locked out and on the lockscreen. Most automations do not work properly when the desktop is locked. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/user-solid.svg"" width=""20""> [Get Windows username](https://automagica.readthedocs.io/activities.html#automagica.activities.get_username) | Get current logged in user's username <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/clipboard-check-solid.svg"" width=""20""> [Set clipboard](https://automagica.readthedocs.io/activities.html#automagica.activities.set_to_clipboard) | Set any text to the Windows clipboard. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/clipboard-list-solid.svg"" width=""20""> [Get clipboard](https://automagica.readthedocs.io/activities.html#automagica.activities.get_from_clipboard) | Get the text currently in the Windows clipboard <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/clipboard-solid.svg"" width=""20""> [Empty clipboard](https://automagica.readthedocs.io/activities.html#automagica.activities.clear_clipboard) | Empty text from clipboard. Getting clipboard data after this should return in None <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/cogs-solid.svg"" width=""20""> [Run VBSscript](https://automagica.readthedocs.io/activities.html#automagica.activities.run_vbs_script) | Run a VBScript file <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/volume-up-solid.svg"" width=""20""> [Beep](https://automagica.readthedocs.io/activities.html#automagica.activities.beep) | Make a beeping sound. Make sure your volume is up and you have hardware connected. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/ethernet-solid.svg"" width=""20""> [Get all network interface names](https://automagica.readthedocs.io/activities.html#automagica.activities.get_all_network_interface_names) | Returns a list of all network interfaces of the current machine <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/ethernet-solid.svg"" width=""20""> [Enable network interface](https://automagica.readthedocs.io/activities.html#automagica.activities.enable_network_interface) | Enables a network interface by its name. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/ethernet-solid.svg"" width=""20""> [Disable network interface](https://automagica.readthedocs.io/activities.html#automagica.activities.disable_network_interface) | Disables a network interface by its name. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/print-solid.svg"" width=""20""> [Get default printer](https://automagica.readthedocs.io/activities.html#automagica.activities.get_default_printer_name) | Returns the name of the printer selected as default <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/print-solid.svg"" width=""20""> [Set default printer](https://automagica.readthedocs.io/activities.html#automagica.activities.set_default_printer) | Set the default printer. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/print-solid.svg"" width=""20""> [Remove printer](https://automagica.readthedocs.io/activities.html#automagica.activities.remove_printer) | Removes a printer by its name <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/cog-solid.svg"" width=""20""> [Get service status](https://automagica.readthedocs.io/activities.html#automagica.activities.get_service_status) | Returns the status of a service on the machine <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/cog-solid.svg"" width=""20""> [Start a service](https://automagica.readthedocs.io/activities.html#automagica.activities.start_service) | Starts a Windows service <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/cog-solid.svg"" width=""20""> [Stop a service](https://automagica.readthedocs.io/activities.html#automagica.activities.stop_service) | Stops a Windows service <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/window-restore-solid.svg"" width=""20""> [Set window to foreground](https://automagica.readthedocs.io/activities.html#automagica.activities.set_window_to_foreground) | Sets a window to foreground by its title. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/window-restore-solid.svg"" width=""20""> [Get foreground window title](https://automagica.readthedocs.io/activities.html#automagica.activities.get_foreground_window_title) | Retrieve the title of the current foreground window <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/window-restore-solid.svg"" width=""20""> [Close window](https://automagica.readthedocs.io/activities.html#automagica.activities.close_window) | Closes a window by its title <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/window-restore-solid.svg"" width=""20""> [Maximize window](https://automagica.readthedocs.io/activities.html#automagica.activities.maximize_window) | Maximizes a window by its title <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/window-restore-solid.svg"" width=""20""> [Restore window](https://automagica.readthedocs.io/activities.html#automagica.activities.restore_window) | Restore a window by its title <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/window-restore-solid.svg"" width=""20""> [Minimize window](https://automagica.readthedocs.io/activities.html#automagica.activities.minimize_window) | Minimizes a window by its title <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/window-restore-solid.svg"" width=""20""> [Resize window](https://automagica.readthedocs.io/activities.html#automagica.activities.resize_window) | Resize a window by its title <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/window-restore-solid.svg"" width=""20""> [Hide window](https://automagica.readthedocs.io/activities.html#automagica.activities.hide_window) | Hides a window from the user desktop by using it's title **Terminal** | ‚Äå‚Äå  <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/terminal-solid.svg"" width=""20""> [Run SSH command](https://automagica.readthedocs.io/activities.html#automagica.activities.run_ssh_command) | Runs a command over SSH (Secure Shell) **SNMP** | ‚Äå‚Äå  <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/ethernet-solid.svg"" width=""20""> [SNMP Get](https://automagica.readthedocs.io/activities.html#automagica.activities.snmp_get) | Retrieves data from an SNMP agent using SNMP (Simple Network Management Protocol) **Active Directory** | ‚Äå‚Äå  <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/audio-description-solid.svg"" width=""20""> [AD interface](https://automagica.readthedocs.io/activities.html#automagica.activities.ActiveDirectory) | Interface to Windows Active Directory through ADSI. Connects to the AD domain to which the machine is joined by default. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/audio-description-solid.svg"" width=""20""> [Get AD object by name](https://automagica.readthedocs.io/activities.html#automagica.activities.get_object_by_distinguished_name) | Interface to Windows Active Directory through ADSI **Utilities** | ‚Äå‚Äå  <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/home-solid.svg"" width=""20""> [Get user home path](https://automagica.readthedocs.io/activities.html#automagica.activities.home_path) | Returns the current user's home path <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/desktop-solid.svg"" width=""20""> [Get desktop path](https://automagica.readthedocs.io/activities.html#automagica.activities.desktop_path) | Returns the current user's desktop path <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/download-solid.svg"" width=""20""> [Get downloads path](https://automagica.readthedocs.io/activities.html#automagica.activities.downloads_path) | Returns the current user's default download path <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-solid.svg"" width=""20""> [Open file](https://automagica.readthedocs.io/activities.html#automagica.activities.open_file) | Opens file with default programs <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/desktop-solid.svg"" width=""20""> [Set wallpaper](https://automagica.readthedocs.io/activities.html#automagica.activities.set_wallpaper) | Set Windows desktop wallpaper with the the specified image <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/cloud-download-alt-solid.svg"" width=""20""> [Download file from a URL](https://automagica.readthedocs.io/activities.html#automagica.activities.download_file_from_url) | Download file from a URL **System** | ‚Äå‚Äå  <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-contract-solid.svg"" width=""20""> [Rename a file](https://automagica.readthedocs.io/activities.html#automagica.activities.rename_file) | This activity will rename a file. If the the desired name already exists in the folder file will not be renamed. Make sure to add the exstention to specify filetype. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-export-solid.svg"" width=""20""> [Move a file](https://automagica.readthedocs.io/activities.html#automagica.activities.move_file) | If the new location already contains a file with the same name. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/trash-solid.svg"" width=""20""> [Remove a file](https://automagica.readthedocs.io/activities.html#automagica.activities.remove_file) | Remove a file <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/tasks-solid.svg"" width=""20""> [Check if file exists](https://automagica.readthedocs.io/activities.html#automagica.activities.file_exists) | This function checks whether the file with the given path exists. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/list-alt-solid.svg"" width=""20""> [Wait until a file exists.](https://automagica.readthedocs.io/activities.html#automagica.activities.wait_file_exists) | Note that this activity is blocking and will keep the system waiting. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/list-solid.svg"" width=""20""> [List to .txt](https://automagica.readthedocs.io/activities.html#automagica.activities.write_list_to_file) | Writes a list to a  text (.txt) file.Every element of the entered list is written on a new line of the text file. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/th-list-solid.svg"" width=""20""> [Read list from .txt file](https://automagica.readthedocs.io/activities.html#automagica.activities.read_list_from_txt) | This activity reads the content of a .txt file to a list and returns that list.Every new line from the .txt file becomes a new element of the list. The activity willnot work if the entered path is not attached to a .txt file. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/th-list-solid.svg"" width=""20""> [Read .txt file](https://automagica.readthedocs.io/activities.html#automagica.activities.read_from_txt) | This activity reads a .txt file and returns the content <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/tasks-solid.svg"" width=""20""> [Append to .txt](https://automagica.readthedocs.io/activities.html#automagica.activities.append_line) | Append a text line to a file and creates the file if it does not exist yet. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/file-alt-solid.svg"" width=""20""> [Make text file](https://automagica.readthedocs.io/activities.html#automagica.activities.make_text_file) | Initialize text file <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/copy-solid.svg"" width=""20""> [Read .txt file with newlines to list](https://automagica.readthedocs.io/activities.html#automagica.activities.read_text_file_to_list) | Read a text file to a Python list-object <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/copy-solid.svg"" width=""20""> [Copy a file](https://automagica.readthedocs.io/activities.html#automagica.activities.copy_file) | Copies a file from one place to another.If the new location already contains a file with the same name, a random 4 character uid is added to the name. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/info-solid.svg"" width=""20""> [Get file extension](https://automagica.readthedocs.io/activities.html#automagica.activities.get_file_extension) | Get extension of a file <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/print-solid.svg"" width=""20""> [Print](https://automagica.readthedocs.io/activities.html#automagica.activities.send_to_printer) | Send file to default printer to priner. This activity sends a file to the printer. Make sure to have a default printer set up. **PDF** | ‚Äå‚Äå  <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/glasses-solid.svg"" width=""20""> [Text from PDF](https://automagica.readthedocs.io/activities.html#automagica.activities.read_text_from_pdf) | Extracts the text from a PDF. This activity reads text from a pdf file. Can only read PDF files that contain a text layer. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/object-ungroup-solid.svg"" width=""20""> [Merge PDF](https://automagica.readthedocs.io/activities.html#automagica.activities.join_pdf_files) | Merges multiple PDFs into a single file <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/cut-solid.svg"" width=""20""> [Extract page from PDF](https://automagica.readthedocs.io/activities.html#automagica.activities.extract_page_range_from_pdf) | Extracts a particular range of a PDF to a separate file. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/icons-solid.svg"" width=""20""> [Extract images from PDF](https://automagica.readthedocs.io/activities.html#automagica.activities.extract_images_from_pdf) | Save a specific page from a PDF as an image <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/stamp-solid.svg"" width=""20""> [Watermark a PDF](https://automagica.readthedocs.io/activities.html#automagica.activities.apply_watermark_to_pdf) | Watermark a PDF **System Monitoring** | ‚Äå‚Äå  <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/microchip-solid.svg"" width=""20""> [CPU load](https://automagica.readthedocs.io/activities.html#automagica.activities.get_cpu_load) | Get average CPU load for all cores. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/calculator-solid.svg"" width=""20""> [Count CPU](https://automagica.readthedocs.io/activities.html#automagica.activities.get_number_of_cpu) | Get the number of CPU's in the current system. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/wave-square-solid.svg"" width=""20""> [CPU frequency](https://automagica.readthedocs.io/activities.html#automagica.activities.get_cpu_frequency) | Get frequency at which CPU currently operates. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/server-solid.svg"" width=""20""> [CPU Stats](https://automagica.readthedocs.io/activities.html#automagica.activities.get_cpu_stats) | Get CPU statistics <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/memory-solid.svg"" width=""20""> [Memory statistics](https://automagica.readthedocs.io/activities.html#automagica.activities.get_memory_stats) | Get  memory statistics <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/save-solid.svg"" width=""20""> [Disk stats](https://automagica.readthedocs.io/activities.html#automagica.activities.get_disk_stats) | Get disk statistics of main disk <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/save-solid.svg"" width=""20""> [Partition info](https://automagica.readthedocs.io/activities.html#automagica.activities.get_disk_partitions) | Get disk partition info <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/clock-solid.svg"" width=""20""> [Boot time](https://automagica.readthedocs.io/activities.html#automagica.activities.get_boot_time) | Get most recent boot time <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/clock-solid.svg"" width=""20""> [Uptime](https://automagica.readthedocs.io/activities.html#automagica.activities.get_time_since_last_boot) | Get uptime since last boot **Image Processing** | ‚Äå‚Äå  <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/images-solid.svg"" width=""20""> [Show image](https://automagica.readthedocs.io/activities.html#automagica.activities.show_image) | Displays an image specified by the path variable on the default imaging program. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/undo-solid.svg"" width=""20""> [Rotate image](https://automagica.readthedocs.io/activities.html#automagica.activities.rotate_image) | Rotate an image <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/expand-arrows-alt-solid.svg"" width=""20""> [Resize image](https://automagica.readthedocs.io/activities.html#automagica.activities.resize_image) | Resizes the image specified by the path variable. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/expand-arrows-alt-solid.svg"" width=""20""> [Get image width](https://automagica.readthedocs.io/activities.html#automagica.activities.get_image_width) | Get with of image <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/arrows-alt-v-solid.svg"" width=""20""> [Get image height](https://automagica.readthedocs.io/activities.html#automagica.activities.get_image_height) | Get height of image <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/crop-solid.svg"" width=""20""> [Crop image](https://automagica.readthedocs.io/activities.html#automagica.activities.crop_image) | Crops the image specified by path to a region determined by the box variable. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/caret-up-solid.svg"" width=""20""> [Mirror image horizontally](https://automagica.readthedocs.io/activities.html#automagica.activities.mirror_image_horizontally) | Mirrors an image with a given path horizontally from left to right. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/caret-right-solid.svg"" width=""20""> [Mirror image vertically](https://automagica.readthedocs.io/activities.html#automagica.activities.mirror_image_vertically) | Mirrors an image with a given path vertically from top to bottom. **Process** | ‚Äå‚Äå  <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/cog-solid.svg"" width=""20""> [Windows run](https://automagica.readthedocs.io/activities.html#automagica.activities.run_manual) | Use Windows Run to boot a processNote this uses keyboard inputs which means this process can be disrupted by interfering inputs <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/play-solid.svg"" width=""20""> [Run process](https://automagica.readthedocs.io/activities.html#automagica.activities.run) | Use subprocess to open a windows process <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/cogs-solid.svg"" width=""20""> [Check if process is running](https://automagica.readthedocs.io/activities.html#automagica.activities.is_process_running) | Check if process is running. Validates if given process name (name) is currently running on the system. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/list-solid.svg"" width=""20""> [Get running processes](https://automagica.readthedocs.io/activities.html#automagica.activities.get_running_processes) | Get names of unique processes currently running on the system. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/window-close-solid.svg"" width=""20""> [Kill process](https://automagica.readthedocs.io/activities.html#automagica.activities.kill_process) | Kills a process forcefully **Optical Character Recognition (OCR)** | ‚Äå‚Äå  <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/readme.svg"" width=""20""> [Get text with OCR](https://automagica.readthedocs.io/activities.html#automagica.activities.extract_text_ocr) | This activity extracts all text from the current screen or an image if a path is specified. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/glasses-solid.svg"" width=""20""> [Find text on screen with OCR](https://automagica.readthedocs.io/activities.html#automagica.activities.find_text_on_screen_ocr) | This activity finds position (coordinates) of specified text on the current screen using OCR. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/mouse-pointer-solid.svg"" width=""20""> [Click on text with OCR](https://automagica.readthedocs.io/activities.html#automagica.activities.click_on_text_ocr) | This activity clicks on position (coordinates) of specified text on the current screen using OCR. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/mouse-pointer-solid.svg"" width=""20""> [Double click on text with OCR](https://automagica.readthedocs.io/activities.html#automagica.activities.double_click_on_text_ocr) | This activity double clicks on position (coordinates) of specified text on the current screen using OCR. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/mouse-pointer-solid.svg"" width=""20""> [Right click on text with OCR](https://automagica.readthedocs.io/activities.html#automagica.activities.right_click_on_text_ocr) | This activity Right clicks on position (coordinates) of specified text on the current screen using OCR. **UiPath** | ‚Äå‚Äå  <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/robot-solid.svg"" width=""20""> [Execute a UiPath process](https://automagica.readthedocs.io/activities.html#automagica.activities.execute_uipath_process) | This activity allows you to execute a process designed with the UiPath Studio. All console output from the Write Line activity will be printed as output. **AutoIt** | ‚Äå‚Äå  <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/robot-solid.svg"" width=""20""> [Execute a AutoIt script](https://automagica.readthedocs.io/activities.html#automagica.activities.run_autoit_script) | This activity allows you to run an AutoIt script. If you use the ConsoleWrite function, the output will be presented to you. **Alternative frameworks** | ‚Äå‚Äå  <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/robot-solid.svg"" width=""20""> [Execute a Robot Framework test case](https://automagica.readthedocs.io/activities.html#automagica.activities.execute_robotframework_test) | This activity allows you to run a Robot Framework test case. Console output of the test case will be printed. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/robot-solid.svg"" width=""20""> [Run a Blue Prism process](https://automagica.readthedocs.io/activities.html#automagica.activities.run_blueprism_process) | This activity allows you to run a Blue Prism process. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/robot-solid.svg"" width=""20""> [Run an Automation Anywhere task](https://automagica.readthedocs.io/activities.html#automagica.activities.run_automationanywhere_task) | This activity allows you to run an Automation Anywhere task. **General** | ‚Äå‚Äå  <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/briefcase-solid.svg"" width=""20""> [Raise exception](https://automagica.readthedocs.io/activities.html#automagica.activities.raise_exception) | Raises an exception **SAP GUI** | ‚Äå‚Äå  <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/briefcase-solid.svg"" width=""20""> [Quit SAP GUI](https://automagica.readthedocs.io/activities.html#automagica.activities.quit) | Quits the SAP GUI completely and forcibly. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/briefcase-solid.svg"" width=""20""> [Log in to SAP GUI](https://automagica.readthedocs.io/activities.html#automagica.activities.login) | Logs in to an SAP system on SAP GUI. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/briefcase-solid.svg"" width=""20""> [Click on a SAP GUI element](https://automagica.readthedocs.io/activities.html#automagica.activities.click_sap) | Clicks on an identifier in the SAP GUI. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/briefcase-solid.svg"" width=""20""> [Get text from a SAP GUI element](https://automagica.readthedocs.io/activities.html#automagica.activities.get_text) | Retrieves the text from a SAP GUI element. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/briefcase-solid.svg"" width=""20""> [Set text of a SAP GUI element](https://automagica.readthedocs.io/activities.html#automagica.activities.set_text) | Sets the text of a SAP GUI element. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/briefcase-solid.svg"" width=""20""> [Highlights a SAP GUI element](https://automagica.readthedocs.io/activities.html#automagica.activities.highlight) | Temporarily highlights a SAP GUI element **Portal** | ‚Äå‚Äå  <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/robot-solid.svg"" width=""20""> [Create a new job in the Automagica Portal](https://automagica.readthedocs.io/activities.html#automagica.activities.create_new_job_in_portal) | This activity creates a new job in the Automagica Portal for a given process. The bot performing this activity needs to be in the same team as the process it creates a job for. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/key-solid.svg"" width=""20""> [Get a credential from the Automagica Portal](https://automagica.readthedocs.io/activities.html#automagica.activities.get_credential_from_portal) | This activity retrieves a credential from the Automagica Portal. **Vision** | ‚Äå‚Äå  <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/eye-solid.svg"" width=""20""> [Check if element is visible on screen](https://automagica.readthedocs.io/activities.html#automagica.activities.is_visible) | This activity can be used to check if a certain element is visible on the screen.Note that this uses Automagica Portal and uses some advanced an fuzzy matching algorithms for finding identical elements. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/eye-solid.svg"" width=""20""> [Wait for an element to appear](https://automagica.readthedocs.io/activities.html#automagica.activities.wait_appear) | Wait for an element that is defined the recorder <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/eye-solid.svg"" width=""20""> [Wait Vanish](https://automagica.readthedocs.io/activities.html#automagica.activities.wait_vanish) | This activity allows the bot to wait for an element to vanish. <img src=""https://cdn.jsdelivr.net/npm/line-awesome@1.3.0/svg/eye-solid.svg"" width=""20""> [Read Text with Automagica Wand](https://automagica.readthedocs.io/activities.html#automagica.activities.read_text) | This activity allows the bot to detect and read the text of an element by using the Automagica Portal API with a provided sample ID. |<img width=150/>|  ‚Äå‚Äå|      ## Licensing  ### Copyright and licensing All source code and other files in this repository, unless stated otherwise, are copyright of Netcall plc.  ### Commercial license For more information on licensing, trials and commercial use see [this page](https://automagica.com/contact/)"
IoT Based Industry Automation,Internet of Things,https://github.com/vernemq/vernemq,"# VerneMQ: A Distributed MQTT Broker  ![Build and Smoketest Status](https://github.com/vernemq/vernemq/actions/workflows/pr.yml/badge.svg) <a href=""https://docs.vernemq.com""> 		<img alt=""Documentation"" src=""https://img.shields.io/badge/documentation-yes-brightgreen.svg"" target=""_blank"" /> 	</a> 	<a href=""https://github.com/vernemq/vernemq/graphs/commit-activity""> 		<img alt=""Maintenance"" src=""https://img.shields.io/badge/maintained-yes-green.svg"" target=""_blank"" /> 	</a> <a href=""https://github.com/vernemq/vernemq/releases/latest""> <img alt=""GitHub Release Date"" src=""https://img.shields.io/github/release-date/vernemq/vernemq""></a> <a href=""https://github.com/vernemq/vernemq/commits/main""> <img alt=""GitHub last commit"" src=""https://img.shields.io/github/last-commit/vernemq/vernemq""></a> <a href=""https://twitter.com/vernemq""> 		<img 			alt=""Twitter: VerneMQ"" 			src=""https://img.shields.io/twitter/follow/vernemq.svg?style=social"" 			target=""_blank"" 		/> 	</a>  <a href=""https://fosstodon.org/@VerneMQ"">  <img alt=""VerneMQ on Fosstodon"" src=""https://img.shields.io/mastodon/follow/110683123510225618?domain=https%3A%2F%2Ffosstodon.org""></a>  [![Google group : VerneMQ Users](https://img.shields.io/badge/Google%20Group-VerneMQ%20Users-blue.svg)](https://groups.google.com/forum/#!forum/vernemq-users) Old Docker Repo | New Docker Repo ------------ | ------------- [![Docker Pulls from Old Repo](https://img.shields.io/docker/pulls/erlio/docker-vernemq.svg)](https://hub.docker.com/r/erlio/docker-vernemq/)|[![Docker Pulls from New Repo](https://img.shields.io/docker/pulls/vernemq/vernemq.svg)](https://hub.docker.com/r/vernemq/vernemq/)  New: VerneMQ can now use Github Discussions! To join the discussion on features and roadmap, and be part of the <strong>VerneMQ Community Team</strong> on Github, send us your Github username for an invite! (on Twitter, Slack etc.)  Make sure to visit the new [VerneMQ Forum](https://erlangforums.com/c/erlang-platforms/vernemq-forum/82) hosted on Erlang Forums. We're happy to discuss any of your questions and ideas around VerneMQ on the Forum too!  - - -  VerneMQ is known to be deployed and used in: :us: :canada: :brazil: :mexico: :de: :fr: :switzerland: :denmark: :netherlands: :belgium: :it: :es: :romania: :portugal: :ru: :lithuania: :czech_republic: :slovakia: :austria: :poland: :norway: :sweden: :india: :jp: :indonesia: :vietnam: :kr: :south_africa: :kenya: :serbia: :croatia: :greece: :uk: :ukraine: :australia: :new_zealand: :cn: :egypt: :finland: :hungary: :israel: :singapore: :lebanon: :philippines: :pakistan: :malaysia: :tr: :taiwan: :iran: :cloud:  --- [![VerneMQ Logo](https://i.imgur.com/bln3fK3.jpg)](https://vernemq.com)  VerneMQ is a high-performance, distributed MQTT message broker. It scales horizontally and vertically on commodity hardware to support a high number of concurrent publishers and consumers while maintaining low latency and fault tolerance. VerneMQ is the reliable message hub for your IoT platform or smart products.  VerneMQ is an Apache2 licensed distributed [MQTT](http://www.mqtt.org) broker, developed in [Erlang](http://www.erlang.org).  MQTT used to stand for MQ Telemetry Transport, but it no longer is an acronym. It is an extremely simple and lightweight publish/subscribe messaging protocol, that was invented at IBM and Arcom (now Eurotech) to connect restricted devices in low bandwidth, high-latency or unreliable networks.  VerneMQ implements the MQTT 3.1, 3.1.1 and 5.0 specifications. Currently the following features are implemented and delivered as part of VerneMQ:  * QoS 0, QoS 1, QoS 2 * Basic Authentication and Authorization * Bridge Support * $SYS Tree for monitoring and reporting * TLS (SSL) Encryption * Websockets Support * Cluster Support * Logging (Console, Files, Syslog) * Reporting to Graphite * Extensible Plugin architecture * Multiple Sessions per ClientId * Session Balancing * Shared subscriptions * Message load regulation * Message load shedding (for system protection) * Offline Message Storage (based on LevelDB) * Queue can handle messages FIFO or LIFO style. * MongoDB auth & integration * Redis auth & integration * MySQL auth & integration * PostgreSQL auth & integration * CockroachDB auth & integration * Memcached integration * HTTP Webhooks * PROXY Protocol v2 * Administration HTTP API * Real-time MQTT session tracing * Full multitenancy * Cluster status web page  The following features are also applies to MQTT 5.0 clients:  * Enhanced authentication schemes (AUTH) * Message expiration * Last Will and Testament delay * Shared subscriptions * Request/response flow * Topic aliases * Flow control * Subscription flags (Retain as Published, No Local, Retain Handling) * Subscriber identifiers * All property types are supported: user properties, reason strings, content types etc.  ## Commercial Support. Binary Packages. Documentation  Below you'll find a basic introduction to building and starting VerneMQ. For more information about the binary package installation, configuration, and administration of VerneMQ, please visit our documentation at [VerneMQ Documentation](https://docs.vernemq.com) or checkout the product page [VerneMQ](https://vernemq.com) if you require more information on the available commercial [support options](https://vernemq.com/services.html).  ## VerneMQ Release Schedule  VerneMQ releases will be annonced based on bugfixes and features.  Usually a bugfix release will be cut between minor releases or if there's an urgent bugfix pending.  Custom releases can be offered for commercial users.  ## Quick Start  This section assumes that you have a copy of the VerneMQ source tree. To get started, you need to first build VerneMQ.  ### Building VerneMQ  Note: VerneMQ requires Erlang/OTP 24-26 and `libsnappy-dev` installed in your system. You'll also need a C compiler for Eleveldb. (on Debian, you install `build-essential`, as an example).  Assuming you have a working Erlang installation, building VerneMQ should be as simple as:  ```shell $ cd $VERNEMQ $ make rel ```  ### Starting VerneMQ  Once you've successfully built VerneMQ, you can start the server with the following commands:  ```shell $ cd $VERNEMQ/_build/default/rel/vernemq $ bin/vernemq start ```  If VerneMQ is running it is possible to check the status on `http://localhost:8888/status` and it should look something like:   <img src=""https://i.imgur.com/XajYjtb.png"" width=""75%"">  Note that the `$VERNEMQ/_build/default/rel/vernemq` directory is a complete, self-contained instance of VerneMQ and Erlang. It is strongly suggested that you move this directory outside the source tree if you plan to run a production instance.  ### Important links  * [VerneMQ Documentation](https://docs.vernemq.com) * [![Google group : VerneMQ Users](https://img.shields.io/badge/Google%20Group-VerneMQ%20Users-blue.svg)](https://groups.google.com/forum/#!forum/vernemq-users) * <a href=""https://twitter.com/vernemq""> 		<img 			alt=""Twitter: VerneMQ"" 			src=""https://img.shields.io/twitter/follow/vernemq.svg?style=social"" 			target=""_blank"" 		/> 	</a>  ### Thank you to all our contributors! [![contributors](https://contributors-img.web.app/image?repo=vernemq/vernemq)](https://github.com/vernemq/vernemq/graphs/contributors)"
IoT Weather Reporting System,Internet of Things,https://github.com/Rashmika-B/IoT-based-Weather-Reporting-System,"# IoT based Weather Reporting System  This is a simple Smart Weather Reporting system using **Raspberry Pi 4B+** and weather-related **sensors**. The system monitors and reports the weather parameters such as: Temperature,Humidity,Pressure, and gives alerts whenever it rains. We have also used [ThingSpeak Cloud](https://thingspeak.com/) platform for data collection and performed advanced data analysis using MATLAB and the [IFTTT](https://ifttt.com/) service to send alerts and notification.  ## Installation  Use the package manager [sudo apt install](https://www.raspberrypi.org/documentation/) to install the necessary packages for Raspberry Pi.  ```bash sudo apt-get install python-smbus  ``` ## Sensors 1. DHT11 Temperature Sensor 2. BMP180 Barometric Pressure Sensor 3. YL-83 Rain Sensor  ## Hardware Setup <img src=""https://github.com/Rashmika-B/IoT-based-Weather-Reporting-System/blob/main/Code%20Snapshots/RaspberryPi%20Setup/Capture.JPG"" width=""600"" height=""500"">"
IoT Weather Reporting System,Internet of Things,https://github.com/Rashmika-B/IoT-based-Weather-Reporting-System,"# IoT based Weather Reporting System  This is a simple Smart Weather Reporting system using **Raspberry Pi 4B+** and weather-related **sensors**. The system monitors and reports the weather parameters such as: Temperature,Humidity,Pressure, and gives alerts whenever it rains. We have also used [ThingSpeak Cloud](https://thingspeak.com/) platform for data collection and performed advanced data analysis using MATLAB and the [IFTTT](https://ifttt.com/) service to send alerts and notification.  ## Installation  Use the package manager [sudo apt install](https://www.raspberrypi.org/documentation/) to install the necessary packages for Raspberry Pi.  ```bash sudo apt-get install python-smbus  ``` ## Sensors 1. DHT11 Temperature Sensor 2. BMP180 Barometric Pressure Sensor 3. YL-83 Rain Sensor  ## Hardware Setup <img src=""https://github.com/Rashmika-B/IoT-based-Weather-Reporting-System/blob/main/Code%20Snapshots/RaspberryPi%20Setup/Capture.JPG"" width=""600"" height=""500"">"
IoT Based Antenna Positioning System,Internet of Things,https://github.com/bhavesh-asana/Antenna-Positioning-System,"# Antenna Positioning System  # Table of Contents 1. [Overview](#abstract) 2. [Napkin Diagram](#block_diagram) 3. [Deployment](#deployment) 4. [Project Wiki](#wiki) 5. [Contributors](#contribution)  --- # Overview <a name=""abstract""></a> The idea is to develop an Internet of Things(IoT) based automated control system that is used to supervise the movement & an angle of a receiving antenna in all the possible directions. Here we use the NodeMCU and Arduino UNO micro-controllers to communicate across the sensors/actuators connected. It is a sensor-based system with a servo motor connected to a receving antenna to check its facing direction. Thus, the coordinates are sent to the end user over a secure communication channel.    If the direction of a satellite or transmitting station changes over time, the antenna direction must also be changed accordingly. The receiving antennas may be placed far apart from each other across the globe. So our system allows for antenna positioning over very long distances. The antenna positions are visible over the internet to the controlling operator on the IoT GUI. We here use IOT Gecko to develop the antenna monitoring GUI system. Our system allows for monitoring antenna direction as well as transmitting new coordinates to position the antenna and the motor appropriately position the antenna accordingly.  --- # Napkin Diagram <a name=""block_diagram""></a> ![Block Diagram](https://user-images.githubusercontent.com/62237873/172036256-357c8623-6599-4c1d-be56-0c23972c625e.png)  --- # Deployment <a name=""deployment""></a> 1. Download and install <a href=""https://www.arduino.cc/en/software"">Arduino IDE</a> 2. Setup a workspace and clone the repository.     ```bash     $ git clone https://github.com/bhavesh-asana/Antenna-Positioning-System.git     ``` 3. Connect to NodeMCU and deploy the controller code.     ```bash     $ cd Codebase/Servo\ Motor     $ open servo_motor.ino     ```     **Note:** Configure the WiFi SSID and Password before deploying the code into NodeMCU microcontroller. 4. Conenct to Arduino UNO-1 and deploy the RF reveiver code.     ```bash     $ cd Codebase/Receiver     $ open receiver.ino     ``` 5. Connect to Arduino UNO-2 and deploy the RF transmitter code.     ```bash     $ cd Codebase/Transmitter     $ open transmitter.ino     ``` ## Web Interface 1. On deploying the code into NodeMCU, open the serial monitor and set the baud rate to 15000. 2. Connect your desktop/laptop to the WiFi configured in the code. If ESP8266 is connected to the WiFi, it shows the successful connectionwith an IP address. 3. Copy the IP address and paste in a new window. 4. It shows a webpage with controlles. ## Mobile Interface  --- # Project Wiki <a name=""wiki""></a> 1. <a href=""https://github.com/bhavesh-asana/Antenna-Positioning-System/wiki"">Wiki Home</a> 2. Introduction 3. Operational Interface 4. RF Receiver 5. RF Transmitter  --- # Contributors <a name=""contribution""></a> * Bhavesh Asanabada - Software Development * Manoghnaa Reeddy Bobbala - Hardware Architecture * Aravind Boyini - Resources & Maintenance "
IoT Based Fire Department Alerting System,Internet of Things,https://github.com/SumaitaB/Arduino-IoT-based-Fire-Alert-System,"# Arduino IoT based Fire Alert System  Fires are one of the most widespread cause of deaths by accident. Instant alerting to the fire department is necessary to ensure immediate action. Every minute can save many lives in such situations. So here we propose an IOT based automatic fire department alerting system that instantly and automatically alerts the fire department and informs about the situation so that immediate action can be taken. The system efficiently detects fires and alert fire department over IOT. We use an Arduino Uno in order to check if a sensor is triggered. The Internet of Things (IoT) is essentially a network of 'things' through which with the help of sensors, electronics, software and networking, physical things can exchange data. No human intervention is needed by these systems. We can send LIVE data such as temperature, smoke value detected by a specific device to the Fire Department in this Arduino fire alarm system with temperature and smoke sensor using the IOT project. Two sensors, namely, temperature and smoke sensors, are used by the IOT Based Fire Alerting System. There is an ADC converter that converts the analog signals received to digital at the end of the sensor and then transmits them to the Arduino microcontroller. When the temperature and the smoke exceed a threshold value, the micro-controller is programmed to turn on the buzzer.  ![1](https://github.com/SumaitaB/Arduino-IoT-based-Fire-Alert-System/assets/51522304/1f899ffe-5cbd-42bd-972d-cccb816f00f8)    Fig 01: Working Specification of the Smoke Detecting System    ## Applications  A wide variety of systems include a fire alerting system. Arduino's IOT-based fire alarm system can be used in chemical plants, shopping centers, local stores, educational institutions, parking areas, businesses.   Furthermore, wifi can be used as a precautionary measure at all the locations mentioned above, which can help to alert the fire departments as soon as the buzzer clicks ON. If sufficient and immediate action is taken, it will help to prevent major fire accidents. ## Apparatus  ## Hardware Specifications * Arduino Uno Rev3 * Serial LCD Display Module * Jumper Wires * Microprocessor Socket * Bread Board * Buzzer * MQ2 Smoke Detector Sensor * Cables & Connectors    ## Software Specification  * Laptop or PC Workstation * Arduino IDE * Arduino Compiler * Programming Language C   ## Circuit Diagram  ![2](https://github.com/SumaitaB/Arduino-IoT-based-Fire-Alert-System/assets/51522304/85e6ddc1-332d-4cda-8f4a-4c500a89e6e8)  Fig 02: Circuit Diagram of the Fire Detecting System  Arduino Uno is a open-source microcontroller board based on the ATmega328p microcontroller. It has 14 digital pins (out of which 6 pins can be used as PWM outputs), 6 analog inputs, on-board voltage regulators etc. Arduino Uno has 32KB of flash memory, 2KB of SRAM and 1KB of EEPROM. It operates at a clock frequency of 16MHz. Arduino Uno supports Serial, I2C, SPI communication for communicating with other devices.  ## Result and Discussion  ![3](https://github.com/SumaitaB/Arduino-IoT-based-Fire-Alert-System/assets/51522304/812a08cb-b143-4f49-a0c5-520f350b09f6)  Fig 03: Implementation of the Fire Detecting System   The Arduino IoT based fire alert system was successfully developed and tested. The system integrates various sensors, such as smoke detectors and temperature sensors, to detect fire and send alerts to the user. The system is also capable of triggering alarms and activating sprinkler systems to help prevent the spread of fire. One of the key advantages of the system is its scalability. The modular design allows the addition of new sensors and devices as needed, making it highly customizable to fit the specific requirements of a building. Additionally, the system can be easily integrated with other IoT devices to create a comprehensive smart building solution. During testing, the system successfully detected simulated fires and sent alerts in real-time. The buzzer was activated as expected, effectively containing the fire and preventing it from spreading.  Overall, the Arduino IoT based fire alert system offers a highly effective and cost-efficient solution for fire safety in buildings. By leveraging IoT technology this system can help prevent devastating fires. The system's modular design and scalability make it highly adaptable to the specific needs of any building, making it an excellent investment for property owners and that can save lives and prevent property damage. "
IoT Solar Power Monitoring System,Internet of Things,https://github.com/CJG1410/IoT-Solar-Power-Monitoring-and-Management-System,"# IoT Solar Power Monitoring System with Street Light Control  ## Overview  This is an IoT-based solar power monitoring system that utilizes solar panels to generate current. The project focuses on an example of a street light, which is controlled based on the light intensity using a light intensity sensor. An ESP32 development board is used for the project, along with Arduino Cloud for remote monitoring and control. Additional components used include a voltage sensor and an AC current sensor to display data on an LCD screen.  ![Solar Panel Street Light](coa1.jpg)  ## Project Components  - **ESP32 Development Board**: The ESP32 is the core of the project, responsible for data collection, communication, and control.  - **Solar Panels**: Solar panels are used to generate current from sunlight.  - **Light Intensity Sensor**: A light intensity sensor measures the ambient light conditions, allowing the system to control the street light based on brightness.  - **Voltage Sensor**: Monitors the voltage generated by the solar panels.  - **AC Current Sensor**: Measures the AC current produced by the system.  - **LCD Screen**: Displays real-time voltage and current data.  ## Project Structure  - `src/` directory contains the Arduino code for the ESP32. - `coa2.jpg` contains a snapshot of the code used in the project. - `coa3.jpg` shows the connections between different components.  ## Getting Started  1. Clone this repository to your local environment.  2. Open the Arduino IDE, make sure you have the necessary libraries for ESP32 and Arduino Cloud installed.  3. Load the code from the `src/` directory onto your ESP32 board.  4. Connect the various components as shown in `coa3.jpg`.  5. Power up the system using the solar panels.  6. Monitor and control the system remotely using Arduino Cloud.  ## Demo Video  For a practical demonstration of this project, check out the demo video:  [Watch Demo Video](coavedio1.mp4)  ## Project in Action  ![Project in Action](coa1.jpg)  ## License  This project is open-source and available under the [MIT License](LICENSE).  Feel free to contribute, report issues, or suggest improvements. We welcome your feedback and collaboration!  ## Acknowledgments  Special thanks to the open-source community, Arduino, and ESP32 for their contributions to this project.  Happy tinkering!"
IoT Streetlight Controller System,Internet of Things,https://github.com/sowwnn/Smart-street-light-control-system--IoT,"## Course Project: Smart street light system  # Topic: An application to control smart lights. üí°üí°üí°  ## Group 5: Octopuses üêô  ## 1. Project details:  **We want to build a smart street ‚úåÔ∏è light system that includes the following criteria:**  1. The system will turn on the street lights at a moderate level (to save electricity ‚ö°Ô∏è) 2. The light will be more bright üåü when there is a vehicle on the road or when it rains. 3. The system also takes context information such as temperature and humidity on the road and uploads it to the cloud system. 4. Light poles üöè communicate with each other wirelessly. 5. We also use solar energy in day to reduce consumption capacity. üå±  ## 2. Circuit diagram  ### 2.1 Master  <img width=""525"" alt=""circuit"" src=""./image/master.png"">  I used a Arduino UNO to read enviroment infomation. The information include: light, rain, humidity and temperature of street.  (I must to use UNO because ESP8266 can't read many analog signa).  All infomation from UNO was transmit to ESP8266 by custom TX-RX (D5,D6) ESP8266 will reciept information in Json form. Thus I'll use that information and signal from Motion sensor to turn the light in condition.  The last, all information sended on broker acrossding MQTT method.  ### 2.2 Child <img width=""525"" alt=""Child"" src=""./image/child.png"">  All child circuit subcriber the topic on broker. So they can know messenge of Master. Similar to ESP8266 on Master they will read motion signal and turn the light in codition. (Mission of child is only control the light and don't send anything else)  ## 3. System architeture  In this project, I used MQTT method to communicate between devices and device to server. Broker i was use is HiveMQ.   ## 4. File: All my code in code folder include: Uno, Node, Child and demo.py  - Uno and Node is code of Master curcuit.  - Child for Child circuit. - Demo.py is application  ## 5. How to run? 1. First one, You need to clone my repo. ``` https://github.com/RC-Sho0/Smart-street-light-control-system--IoT.git ```  2. Push source code for each circuit. **Attention ‚õîÔ∏è**: In Node.ino and Child.ino you need to replace  ``` const char* ssid = {Your WIFI};          const char* password = ""Your WIFI password"";  ```  3. Connect the wires as shown on the diagram.  4. Plug power and run application ``` streamlit run demo.py ```   --------------- **Code by Sho0**  *Hope you guys can refer to this project*  "
IoT Traffic Signal Monitoring & Controller System,Internet of Things,https://github.com/keerthana1003/IoT-Project,"# IoT-Project **IoT-based Traffic Signal Monitoring & Controller System:** An IoT-based Traffic Signal Monitoring and Controller System uses sensors and communication devices to monitor traffic in real-time and adjust signals dynamically, enhancing urban traffic management efficiency and effectiveness. raffic light management and controlling system  **Test Case 1:** No Vehicles Present Description: Test the system when no vehicles are detected. Input: No IR sensor detects a vehicle. Expected Output: The traffic signal remains green for the current lane and switches to red after the predefined time interval.   **Test Case 2:** One Lane with Vehicles Description: Test the system when vehicles are detected in only one lane. Input: One IR sensor detects vehicles, others do not. Expected Output: The traffic signal should remain green for the lane with vehicles until the vehicles pass or after a maximum green light duration, then switch to the next lane.   **Test Case 3:** All Lanes with Vehicles Description: Test the system when vehicles are detected in all lanes. Input: All IR sensors detect vehicles. Expected Output: The traffic signal should prioritize lanes based on vehicle density, giving each lane a green light in a round-robin manner, ensuring no lane is starved.   **Test Case 4:** High Vehicle Density in One Lane Description: Test the system when one lane has a significantly higher vehicle density. Input: One IR sensor detects high density, others detect low or medium density. Expected Output: The traffic signal should allocate more green light time to the lane with higher density, while still allowing other lanes to proceed in turn.   **Test Case 5**: Equal Vehicle Density in All Lanes Description: Test the system when all lanes have equal vehicle density. Input: All IR sensors detect equal vehicle density. Expected Output: The traffic signal should switch lanes in a round-robin manner, ensuring equal green light time for all lanes."
IoT Industry Automation Using Raspberry Pi,Internet of Things,https://github.com/KarthikT23/Raspberry-Pi-Industrial-IoT,"# Raspberry-Pi-Industrial-IoT   Raspberry Pi Industrial Monitoring, Management, and Data Acquisition System   RFID Reader checks the validity of a RFID Card. If the card is valid, RPi sends an alert to the phone, and then uses sensors to capture and calculate various parameters, and plots them on ThingSpeak  # Requirements  a) Raspberry Pi 3 Model B+  b) RC522 RFID Reader and Card  c) Adafruit DHT11 sensor  d) BH1750 Ambient Light Sensor  e) BMP180 Pressure Sensor  f) LEDs and Buzzers  g) Jumper Wires  h) Pushbullet  i) ThingSpeak  # Raspberry Pi Model 3B+  ![image](https://github.com/KarthikT23/Raspberry-Pi-Industrial-IoT/assets/119528503/db4c911a-daaf-4e90-a467-8b5e8a755198)    ![image](https://github.com/KarthikT23/Raspberry-Pi-Industrial-IoT/assets/119528503/4092dc90-86d5-4d10-9c8c-6fbbcf013d2e)     # Raspbian / Raspberry Pi OS  ![image](https://github.com/KarthikT23/Raspberry-Pi-Industrial-IoT/assets/119528503/b4624d9f-89fe-4480-8aad-9529f8d5c8fb)      # Adafruit DHT11 sensor  ![image](https://github.com/KarthikT23/Raspberry-Pi-Industrial-IoT/assets/119528503/9f360156-64f9-458d-8880-83b5b8f1b8ba)   # RC522 RFID Reader  ![image](https://github.com/KarthikT23/Raspberry-Pi-Industrial-IoT/assets/119528503/88f46e4d-0024-477c-83da-bd74fd2d93e6)   # BH1750 Ambient Light Sensor ![image](https://github.com/KarthikT23/Raspberry-Pi-Industrial-IoT/assets/119528503/f14f20ca-a6e8-48fb-9e5d-7f4a16d690a1)  # BMP180 Pressure Sensor ![image](https://github.com/KarthikT23/Raspberry-Pi-Industrial-IoT/assets/119528503/ea1f5b8e-6241-418e-ae2d-67fae6e23723)    # LEDs and Buzzers  ![image](https://github.com/KarthikT23/Raspberry-Pi-Industrial-IoT/assets/119528503/c675223d-85e2-4a4c-bd76-067857b387c7)    ![image](https://github.com/KarthikT23/Raspberry-Pi-Industrial-IoT/assets/119528503/1f916543-c910-49e0-bcab-b4a2a9b16859)    # Pushbullet  https://www.pushbullet.com/   ![image](https://github.com/KarthikT23/Raspberry-Pi-Industrial-IoT/assets/119528503/f5f0b815-baee-442d-aa77-cde0cd7eb1a8)   # ThingSpeak  https://thingspeak.com/  ![image](https://github.com/KarthikT23/Raspberry-Pi-Industrial-IoT/assets/119528503/3cdc35aa-5d05-4e04-bcd0-1ead047da097)   ![image](https://github.com/KarthikT23/Raspberry-Pi-Industrial-IoT/assets/119528503/97c8cc43-10d9-4d0a-8129-c7df152ce845)    # Block Diagram  ![image](https://github.com/KarthikT23/Raspberry-Pi-Industrial-IoT/assets/119528503/d4cd5ca9-9712-4c06-8db7-8916211d1b0c)    # Flowchart  ![Flowchart](https://github.com/KarthikT23/Raspberry-Pi-Industrial-IoT/assets/119528503/f9a00c66-5700-45df-b15b-6e268d099dde)    # Circuit Diagram  Connect the Vcc pin of DHT11 Sensor to Pin 2 of Raspberry Pi, Gnd pin of DHT11 Sensor to Pin 6 of Raspberry Pi and the Out pin of DHT11 Sensor to GPIO 26 of the Raspberry Pi. Also connect the + terminal of LED to GPIO 21, + terminal of buzzer to GPIO 20 and connect the ground pins of both to Pin 6 of Raspberry Pi.  Connect the SDA and SCL pins of both BMP180 and BH1750 to Pins 3 and 5 respectively. Connect 3V3 pin to Pin 1, GND and ADDR to Pin 6.  ![image](https://github.com/KarthikT23/Raspberry-Pi-Industrial-IoT/assets/119528503/9d6679c1-562d-4579-b995-06faebf587ea)   ![image](https://github.com/KarthikT23/Raspberry-Pi-Industrial-IoT/assets/119528503/f71c01fd-c164-41c4-9efb-1b909ed6bc2e)    # Setup  ![image](https://github.com/KarthikT23/Raspberry-Pi-Industrial-IoT/assets/119528503/53f0bb53-9114-456f-ad99-a2ebe7e8e1b9)      # Results  ThingSpeak: https://thingspeak.com/channels/2304287   Program output in shell ![image](https://github.com/KarthikT23/Raspberry-Pi-Industrial-IoT/assets/119528503/3301556f-f1b9-4a46-bed4-cbee70594b48)         <br>   ![image](https://github.com/KarthikT23/Raspberry-Pi-Industrial-IoT/assets/119528503/49cbc061-4a85-4ecd-b8b9-a4bdb6bc6f21)  LED glowing, indicating invalid RFID          <br>   ![image](https://github.com/KarthikT23/Raspberry-Pi-Industrial-IoT/assets/119528503/4832ba15-a5de-4a27-bc48-8ef6c5d5b036)  Pushbullet notifications on phone       # Outputs in ThingSpeak  Temperature and Humidity values plotted against time in ThingSpeak ![image](https://github.com/KarthikT23/Raspberry-Pi-Industrial-IoT/assets/119528503/ae56cd92-c602-4cdc-b939-789a80d6b136)       Temperature and Humidity gauges in ThingSpeak ![image](https://github.com/KarthikT23/Raspberry-Pi-Industrial-IoT/assets/119528503/6065cbd7-e374-4a4c-bc31-7f648852a9f0)       Temperature and Humidity numeric displays in ThingSpeak ![image](https://github.com/KarthikT23/Raspberry-Pi-Industrial-IoT/assets/119528503/0a7a1fd3-c4da-48d2-a455-7208a86118d7)      Correlation between Temperature and Humidity plotted in ThingSpeak ![image](https://github.com/KarthikT23/Raspberry-Pi-Industrial-IoT/assets/119528503/a22fbe89-0f48-4dd7-9081-9ae0d0d3c580)   Pressure and Altitude values plotted against time in ThingSpeak ![image](https://github.com/KarthikT23/Raspberry-Pi-Industrial-IoT/assets/119528503/d13f6817-bc8a-4348-ba23-f8e5b715a5bc)   Sea Level Pressure and Ambient Light values plotted against time in ThingSpeak ![image](https://github.com/KarthikT23/Raspberry-Pi-Industrial-IoT/assets/119528503/28aaf9ca-8cc2-4ed3-9cdb-c38919ae7849)   Dew Point and Air Density values plotted against time in ThingSpeak ![image](https://github.com/KarthikT23/Raspberry-Pi-Industrial-IoT/assets/119528503/2a602405-67f2-4fd5-992e-b444eef2d737)    All sensor parameters stored in a CSV file ![image](https://github.com/KarthikT23/Raspberry-Pi-Industrial-IoT/assets/119528503/a90639b8-a680-4593-9941-3bc610da942a)    # References [1] An Internet-Based Interactive Embedded Data Acquisition System for Real-Time Applications by Ali Ziya Alkar and Mehmet Atif Karaca, IEEE Transactions on Instrumentation and Measurement, Vol. 58, No. 3, March 2009.  [2] Wireless SCADA for industrial automation using Raspberry Pi by Avinash V. Ghadage, Dr. S. S. Patil, International Journal of Advanced Research in Innovative Ideas in Education (IJARIIT), Vol. 3, Issue 4, April 2017.  [3] Low-Cost Solutions for Maintenance with a Raspberry Pi by M. V. K. Sivakumar, S. V. Krishna, M. V. N. Srinivas, International Journal of Emerging Technology in Computer Science & Electronics (IJETCSE), Vol. 10, Issue 11, November 2019.  [4] Internet of Things (IoT) based Data Acquisition system using Raspberry Pi by A. S. Patil, D. S. Patil, S. S. Patil, International Journal of Engineering Research & Technology (IJERT), Vol. 3, Issue 5, May 2014.  [5] Raspberry pi based data acquisition system using wireless communication by V. N. Patil, A. S. Patil, International Journal of Recent Engineering Science (IJRES), Vol. 3, No. 2, February 2015.  [6] https://iotdesignpro.com/projects/home-security-system-using-raspberry-pi-and-pir-sensor-with-push-notification-alert  [7] https://iotstarters.com/how-to-send-sensor-data-to-thingspeak-using-raspberry-pi/  [8] https://www.raspberrypi-spy.co.uk/2017/09/dht11-temperature-and-humidity-sensor-raspberry-pi/  [9] https://learn.adafruit.com/adafruit-io-basics-digital-output/python-setup  [10] https://github.com/alaub81/rpi_sensor_scripts/tree/main"
IoT Underground Cable Fault Detector Project,Internet of Things,https://github.com/timotheengineer/IOT_UNDERGROUND-FAULT-LOCATION-AND-DETECTION,"# IOT_UNDERGROUND-FAULT-LOCATION-AND-DETECTION <Final Year Project 2021>  <img src=""Underground_fault _detection_system.jpg"">  # Abstract Fault in power systems is one of the greatest problems that cause power disruptions. It can occur from generation, distribution, and consumer systems. This project will focus mainly on the underground line fault.Underground cables are used more often in urban areas than overhead lines. It is difficult to locate a fault when it occurs, and the process may be time-consuming and costly. The proposed system will find the exact location of the fault and facilitate timely maintenance. The concept of a potential divider network connected across the underground line will be used. IoT will allow the authority to check faults over the internet while in their respective stations. Therefore, they can alert the technicians who are near the particular location for repair. A set of resistors represents the length of the cable in kilometres, and faults are created at predefined distances using switches. In case of a fault, there will be a change in current drops, which will be sent to the microcontroller. The ADC in the microcontroller interprets the data and conveys the information to the user in terms of distance in kilometres. This is then displayed over the LCD and shared online using the IoT platform.  # Acknowledgement I wish to express my sincere gratitude to Kenyatta University for providing me with the training opportunity to fulfil my dream of becoming an Electrical Engineer. More so, providing the necessary infrastructure and services to develop my engineering skills and expertise. My special thanks to my supervisor Eng. Kore for his guidance, assistance, and effort to enable the success of this project. I would also want to acknowledge my classmates for their various assistance during various research and analysis, finally, to the Almighty God for offering His grace throughout this journey.  # System Overview  The system overview is shown below  <img src=""system_overview.png"">  # Flow Chart  It is shown below  <img src=""flowchart.png"">  # System Components  1. Arduino Uno [Arduino Rev3](https://store-usa.arduino.cc/products/arduino-uno-rev3/) 2. Current sensing circuit-Combination of resistors and switches 3. 1kilo ohm resistor 4. Push button[Component](https://components101.com/switches/push-button) 5. Esp32(Wemos Lolin32)[Component](https://artofcircuits.com/product/lolin32-esp32-dual-core-wifibluetooth-development-board-with-battery-charger) 6. Relays [Component](https://elearn.ellak.gr/mod/book/view.php?id=4568&chapterid=2440)  # Schematic Diagram  <img src=""fault_system.PNG"">  # Simulation  # Firmware and Code  ## Libraries  SoftwareSerial.h,LiquidCrystal.h were included on the atmega328p code  ``` #include <SoftwareSerial.h> //library for the serial ports #include <LiquidCrystal.h> //library for the LCD display SoftwareSerial mySerial(0,1); //defining the serial communication ports  #include <WiFi.h> //include the wifi dependancies #include <ThingSpeak.h> //include the thingSpeak dependancies #include <String.h>//library for formating string ``` ## Variables and Constants  ``` int sensorPin   = A0; // select the input pin for ldr int sensorValue = 0; // variable to store the value coming from the sensor ``` ## Setup fuction for arduino uno ``` void setup() {    pinMode(8, OUTPUT);//initialize as output for red phase   digitalWrite(8, LOW);//turn off the relay   analogWrite(6, Contrast); //specifying the LCD contrast    Serial.begin(9600); //sets serial port for communication   mySerial.begin(115200);   lcd.begin(16, 2);   lcd.clear(); //clear LCD screen   lcd.print(""UNDERGROUND CABLE"");   lcd.setCursor(0, 1); //setting the cursor position   lcd.print(""FAULT LOCATOR"");   delay(2000); } ``` ## Loop function to check the fault ``` void loop()//start of the execution process {   lcd.clear();   digitalWrite(8, HIGH);   delay(3000);    sensorValue = analogRead(sensorPin); // read the value from the sensor  // Serial.println(sensorValue);   voltage=sensorValue*(5.0/1023.0);   //dtostrf(val, 4, 6, buff);      //convert float to a string    Serial.println(voltage); //prints the line voltage from the sensor.      if ( (sensorValue >= 1000) ) //5.0V   {     lcd.setCursor(0, 0);     lcd.print(""R -  NF,        "") ;     Serial.println(""R -  NF   "") ;    }    else if ( (sensorValue >= 890) && (sensorValue <= 920)  ) //4.44V   {     Serial.println(""R - 2KM,"") ;     lcd.setCursor(0, 0);     lcd.print(""R - 2KM      "") ;   }   else if ( (sensorValue >= 870) && (sensorValue <= 880)  ) //4.29V   {     Serial.print(""R - 4KM,"") ;     lcd.setCursor(0, 0);     lcd.print(""R - 4KM       "") ;   }   else if ( (sensorValue >= 800) && (sensorValue <= 825)  ) //4.0V   {     Serial.println(""R - 6KM,"") ;     lcd.setCursor(0, 0);     lcd.print(""R - 6KM      "") ;   }   else if ( (sensorValue >= 670) && (sensorValue <= 688)  ) //3.33V   {     Serial.println(""R - 8KM,"") ;     lcd.setCursor(0, 0);     lcd.print(""R - 8KM      "") ;// print fault at 8 kilometres   }   delay(2000);   digitalWrite(8, LOW);   delay(1000);   lcd.clear();   delay(1000);    } ``` * WiFi Setting I initialised the credentials for my wifi connection ``` const char* ssid = ""FesTech-07905507""; const char* password = ""ikings001*""; ``` * Function and constants for ESP32 ``` //defining the ThingSpeak channel id and api key #define channel_id 111111 #define channel_key ""xxxxxxxdx"" const int Field_Number_1 = 1;  WiFiClient client;//initializing wifi client  #define TXD2 17 //definition of tx and rx #define RXD2 16 String volt; void initWiFi()  //function to initialise wifi {         WiFi.mode(WIFI_STA); //setting wifi to station mode   WiFi.begin(ssid, password);   Serial.print(""Connecting to WiFi .."");   while (WiFi.status() != WL_CONNECTED)    {     Serial.print('.');     delay(1000);   }   Serial.println(""Connected"");   Serial.println(WiFi.localIP()); }  ``` * Setup function ``` void setup()  {   Serial.begin(115200); //setting the baud rate for serial communication   Serial2.begin(9600, SERIAL_8N1, RXD2, TXD2); //initialise the serial port   initWiFi();   Serial.print(""RRSI: "");   Serial.println(WiFi.RSSI());   ThingSpeak.begin(client); } ``` * Loop Function ``` void loop()  {   Serial.println(""Fault received "");   volt=Serial2.readString(); //initialising serial string to volt variable   Serial.println(volt);   ThingSpeak.writeField(channel_id,1,volt,channel_key); //upload the string to the IOT platform    delay(15000);   } ``` ## ThingSpeak Platform * Results When there is no fault, the voltage is 5v, when a short circuit fault occurs, the voltage drops due to change in resistance <img src=""iot_platform.PNG"">  # Conclusion The main objective of this project was to develop a working IOT underground fault cable detection system. This objective was met successfully. Moreover, project will be of great use to the power utility companies in managing faults. This idea can even be extended to overhead power lines to facilitate real-time tracking.  # Recommendation Future implementations will ensure data is collected and stored in a database for further analysis and prediction of future fault occurrence.  **Bill of Quantities**  Component	|Quantity  |	Price(Ksh)| ----------|----------|---------- Arduino Uno(Atmega328p)	|1	|1600.00 Wi-Fi module(ESP8266)|	1	|1200.00 Resistors	|20	|100.00 Push button	|12	|60 LED	|5	|50.00 LCD	|1	|400.00 Relay and Relay driver	|4	|800.00 Connecting wires	|2set	|200 Fabrication board	|1	|100 TOTAL	|32	|4510.00"
IoT Air & Sound Pollution Monitoring System Energy Meter Monitoring Over IOT,Internet of Things,https://github.com/MohanVallivedu/Remote-monitoring-of-Air-and-sound-pollution-IOT,"# Remote-monitoring-of-Air-and-sound-pollution-IOT  ABSTRACT:  As pollution increasing in our day to day life, we need to control pollution by keep tacking of the data of our surroundings. This project was designed to implement the Internet of things to monitor the Air pollution and Sound levels in the Environment. It can be done by using different types of sensors to collect data from the environment and give those details to us through smart devices like laptops and mobile phones via Internet. This project presents a system with a design and with low cost to monitor the data. As these sensors are connected to the internet, they can monitor data from any remote location without actually visiting it. Authorities can get alert messages if pollution exceeds, so they can take actions to control pollution.  LITERATURE SURVEY:  -- In 2011, Diego Mendez, Alfredo J. Perez, Miguel A. Labrador, Juan Jose Marron proposed air pollution monitoring system using GPS technology in mobile phones. It retrieves large amount of data easily and also cost effective.  -- In 2011, Wenhu Wang, Yifeng Yuan, Zhihao Ling introduced an air quality monitoring system based on ZigBee wireless sensing technology. It uses ZigBee wireless network to send data to people. If something happen and data exceed threshold value, it gives warning to them to take certain action.  -- Vikhyat from Delhi, had come up with a proposal environmental monitoring system called ArduAir which is a small and portable measurement system which includes various gas sensors and proposed software for collecting data from the ArduAir and plotting it in real-time.  -- In 2016, Navreetinder Kaur, Rita Mahajan proposed a Wireless sensor network (WSN) system which has some specific parameters of the environment such as temperature, humidity, CO2 etc and sends data to the base station through Zigbee pro.  PROPOSED SYSTEM:  We developed this System to monitor Air pollution as well as Sound levels lively using Iot technology. The data is collected from MQ gas sensor and sound sensor and send directly to the cloud using arduino with Esp wifi module and data is visualized in the form of graphs and charts. MQ 135 sensor is used to sense gases like ammonia, benzene, CO2, Smoke etc. The objective of this system is continuous monitoring of the pollution levels which is beneficial to all. Proposed system consists of sensors and Arduino connected to them. Initially the Arduino board is provided with a 5V DC supply through adapter or USB cable. Then Sensors are connected to Arduino and ESP8266 WiFi module is connected to make it connect to the Internet.  COMPONENTS:  Arduino UNO, MQ-135 Sensor, Sound Sensor, ESP 8266 WiFi Module. Software:  Arduino IDE, ThingSpeak Cloud platform."
IoT Based Person/Wheelchair Fall Detection,Internet of Things,https://github.com/arungeekay/Iot-based-wheelchair-mounted-fall-detection-and-health-monitoring-system,"# Iot-based-wheelchair-mounted-fall-detection-and-health-monitoring-system  Around the world we see many people who are physically challenged. One among them are those who use wheel chairs. In present situation a person on a wheel should have a care taker who always looks after the person on the wheel chair. It is really difficult for the caretaker to keep an eye on the patient all the time. If in case when the patient is resting and there is no one with him/her, suddenly an incident happens it will lead to risk of life of patient. This is major drawback of the wheel chairs that are in use currently. In order to overcome this problem, we have brought in a new concept and developed a smart wheel chair which monitors patient‚Äôs bloop pressure and fall detection. This will drastically help the caretaker to have a regular check of patient‚Äôs health. We also use cloud services to send the information about the patient to the caretaker. Smart Wheelchair is known as a Power Wheelchair that is integrated into multiple sensors, assistive technology, and computers that give the user with a disability such as impairment, handicaps, and permanent injury, the required mobility to move freely and safely. These types of wheelchairs are gradually replacing the traditional wheelchairs; however, their expensive costs are preventing a large size of disabled people from having one. According to the organization of World Health (WHO), only 5 to 15% out of 70 million disabled people have access to wheelchairs. Therefore, we need to offer a cost-effective Smart that not only minimized the cost but also provides plenty of features that use the latest components and technologies. In the last years, there have been many pleasant efforts that serve this purpose. They have adopted various technologies such as artificial intelligence, where they have designed an autonomous wheelchair that used machine learning concepts to navigate, and some also used Internet of Thing technology to control the wheelchair-using voice recognition system. This report will present a cost-effective Smart Wheelchair-based Arduino Nano microcontroller and IoT technology that have several features to gain disabled people, especially poor people who cannot afford expensive Smart Wheelchair, the required help to finish daily life tasks without external help. To conclude this project will make the Smart Wheelchair affordable to a wide range of disabled people and will be based on Arduino Nano, ESP-12e module to give Wi-Fi access, MPU6050 to detect fall with Voice message notification using ThinkSpeak platform."
IoT Patient Health Monitoring Project,Internet of Things,https://github.com/HackwithArpon/Iot-Based_Patient-Health-Monitoring-System-,"# Iot-Based_Patient-Health-Monitoring-System   **ABSTRACT:**  Nowadays Health-care Environment has developed science and knowledge based on  Wireless-Sensing node Technology oriented. Patients are facing a problematic  situation of unforeseen demise due to the specific reason of heart problems and attack  which is because of nonexistence of good medical maintenance to patients at the  needed time. This is for specially monitoring the old age patients and informing  doctors and loved ones. So, we are proposing an innovative project to dodge such  sudden death rates by using Patient Health Monitoring that uses sensor technology and  uses internet to communicate to the loved ones in case of problems. This system uses  **Temperature and heartbeat sensor** for tracking patient‚Äôs health. Both the sensors are  connected to the **Arduino-uno**. To track the patient's health, the micro-controller is in turn  interfaced to an LCD display and wi-fi connection to send the data to the web-server  (wireless sensing node). In case of any abrupt changes in patient **heart-rate or body  temperature alert is sent about the patient using IoT**. This system also shows patients  temperature and heartbeat tracked live data with timestamps over the Internetwork.  Thus, Patient health monitoring system based on IoT uses the internet to effectively  monitor patient health and helps the user monitor their loved one‚Äôs drome work and  saves lives.  **Introduction:**  Internet of Things (IoT) conceptualizes the idea of remotely connecting and  monitoring real world objects (things) through the Internet.  **‚ÄúThe Internet of Things is the network of physical objects that contain embedded  technology to communicate and sense or interact with their internal states or the  external environment.‚Äù ‚Äì Gartner**  ![IoT General Performance Structure](https://github.com/HackwithArpon/Iot-Based_Patient-Health-Monitoring-System-/assets/116937463/95f6defa-71c2-4cac-8226-f2e1ff8aab1e)  Smart systems and IoT are driven by combination of three things: ‚Ä¢ Sensors & Actuators ‚Ä¢ Connectivity ‚Ä¢ People & Processes  With tons of new healthcare technology start-ups, IoT is rapidly revolutionizing the  healthcare industry. In this project, we have designed the IoT Based Patient Health Monitoring System using ESP8266 & Arduino. The IoT platform used in this project is  ThingSpeak. ThingSpeak is an open-source Internet of Things (IoT) application and API to  store and retrieve data from things using the HTTP protocol over the Internet or via a Local  Area Network. This IoT device could read the pulse rate and surrounding temperature and update them to an IoT platform. This Health Monitoring System simulates two major aspects of health and wellbeing : **i) Human Body Temperature, ii) Heart Beat Rate.** It presents the numerical value of these two aspects and maintains a data logger system where the patient‚Äôs data can be stored and received for medical history and check-up purpose collectively. Along with that the output values can be viewed through waveform chart. The circuit output values in the LED represent the current heartbeat, time in second, heartbeat per minute and body¬†temperature.  **Objectives:**  ‚óè To design a portable health monitoring system, which measures the patient‚Äôs body temperature and pulse rate.  ‚óè To provide medical assistance according to the data received from the sensors which stored in cloud server (ThingSpeak).  **Block Diagram:** ![Block Diagram](https://github.com/HackwithArpon/Iot-Based_Patient-Health-Monitoring-System-/assets/116937463/07e15f60-6c7c-4983-8915-264cb5167680) This is a simple block diagram that explains the IoT Based Patient Health Monitoring System using ESP8266 & Arduino. Pulse sensor and LM35 temperature sensor measure BPM & Environmental temperature respectively. The Arduino processes the code and displays it to 16*2 LCD Display. ESP8266 Wi-Fi module connects to Wi-Fi and sends the data to IoT device server. The IoT server used here is ThingSpeak. Finally, the data can be monitored from any part of the World by logging into the ThingSpeak channel.   **Requirements:**  Hardware Requirements-  ‚óè Arduino Uno  ‚óè ESP8266 Wi-Fi Module  ‚óè LM35 Temperature Sensor  ‚óè Pulse Sensor  ‚óè 16*2 LCD Display  ‚óè Potentiometer (10K)  ‚óè Resistor (1K & 2K)  ‚óè Breadboard  Software Requirements-  ‚óè Arduino Compiler  ‚óè ThingSpeak (Cloud Service)  **1. Arduino Uno :**  Arduino is an open-source prototyping platform based on easy-to-use hardware and software.  Arduino boards are able to read inputs - light on a sensor, a finger on a button, or a Twitter  message - and turn it into an output - activating a motor, turning on an LED, publishing  something online. One can tell the board what to do by sending a set of instructions to the  microcontroller on the board. To do so we use the Arduino programming language (based on  Wiring) , and the Arduino Software (IDE), based on Processing. Over the years Arduino has been the brain of thousands of projects, from everyday objects to  complex scientific instruments. A worldwide community of makers - students, hobbyists,  artists, programmers, and professionals - has gathered around this open-source platform, their  contributions have added up to an incredible amount of accessible knowledge that can be of  great help to novices and experts alike. Arduino was born at the Ivrea Interaction Design  Institute as an easy tool for fast prototyping, aimed at students without a background in  electronics and programming. As soon as it reached a wider community, the Arduino board  started changing to adapt to new needs and challenges, differentiating its offer from simple  8-bit boards to products for IoT applications, wearable, 3D printing, and embedded  environments. All Arduino boards are completely open-source, empowering users to build  them independently and eventually adapt them to their particular needs. The software, too, is  open-source, and it is growing through the contributions of users worldwide. Arduino also simplifies the process of working with microcontrollers, but it offers some  advantage for teachers, students, and interested amateurs over other systems:  **Inexpensive -** Arduino boards are relatively inexpensive compared to other microcontroller  platforms. The least expensive version of the Arduino module can be assembled by hand, and  even the pre-assembled Arduino modules cost less than $50  **Cross-platform -** The Arduino Software (IDE) runs on Windows, Macintosh OSX,  and Linux operating systems. Most microcontroller systems are limited to Windows.  **Simple, clear programming environment -** The Arduino Software (IDE) is easy-to-use for beginners, yet flexible enough for advanced users to take advantage of as well. For teachers,  it's conveniently based on the Processing programming environment, so students learning to  program in that environment will be familiar with how the Arduino IDE works.  **Open source and extensible software -** The Arduino software is published as open-source  tools, available for extension by experienced programmers. The language can be expanded  through C++ libraries, and people wanting to understand the technical details can make the  leap from Arduino to the AVR C programming language on which it's based. Similarly, one  can add AVR-C code directly into the Arduino programs.  **Open source and extensible hardware -** The plans of the Arduino boards are published  under a Creative Commons license, so experienced circuit designers can make their own  version of the module, extending it and improving it. Even relatively inexperienced users can  build the breadboard version of the module in order to understand how it works and save  money. Arduino is an open-source electronics prototyping platform based on flexible, easyto-use hardware and software. It‚Äôs intended for artists, designers, hobbyists, and anyone  interested in creating interactive objects or environments.  **Features of Arduino Uno :**  **Processor:** ATmega328 (16 MHz) **Flash memory:** 32 KB **RAM:** 2kb **Operating Voltage:** 5V **Input Voltage:** 7-12 V **Number of analog inputs:** 6 **Number of digital I/O:** 14 (6 of them pwn)  **2. ESP8266 Wi-Fi Module :**  The ESP8266 is a very user-friendly and low-cost device to provide internet connectivity to our projects. The module can work both as an Access point (can create hotspot) and as a station (can connect to Wi-Fi), hence it can easily fetch data and upload it to the internet making the Internet of Things as easy as possible. It can also fetch data from the internet using API‚Äôs hence our project could access any information that is available on the internet, thus making it smarter. Another exciting feature of this module is that it can be programmed using the Arduino IDE which makes it a lot more user-friendly.  The ESP8266 module works with 3.3V only, anything more than 3.7V would kill the module hence be cautious with our circuits.  Here is its pins description:  **Pin 1: Ground:** Connected to the ground of the circuit  **Pin 2: Tx/GPIO ‚Äì 1:** Connected to Rx pin of programmer/uC to upload program  **Pin 3: GPIO ‚Äì 2:** General purpose Input/output pin  **Pin 4 : CH_EN:** Chip Enable/Active high  **Pin 5: Flash/GPIO ‚Äì 0:** General purpose Input/output pin  **Pin 6 : Reset:** Resets the module  **Pin 7: RX/GPIO ‚Äì 3:** General purpose Input/output pin  **Pin 8: Vcc:** Connect to +3.3V only  **Features of ESP8266 :**  ‚Ä¢ CPU (32 bit, 26MHz-52MHz, 64KB instruction RAM, 64KB boot ROM,  96KB data ‚Ä¢ RAM), CPU clock speed can reach maximum value of 160 MHz ‚Ä¢ 802.11 b/g/n protocol. ‚Ä¢ Wi-Fi Direct (P2P), soft-AP. ‚Ä¢ Integrated TCP/IP protocol stack. ‚Ä¢ It requires 3.3V power‚Äìdo not power it with 5 volts. ‚Ä¢ Wake up and transmit packets in < 2ms. ‚Ä¢ Standby power consumption of < 1.0Mw. ‚Ä¢ Integrated low power 32-bit CPU could be used as application processor. ‚Ä¢ GPIO, UART, ADC, I2C, SPI, PWM. ‚Ä¢ Real Time Operation System (RTOS) is enabled. Currently, only 20% of  MIPS has been occupied by the Wi-Fi stack, the rest can all be used for  user application programming and development.  **3. LM35 Temperature Sensor :**  The LM35 series are precision integrated-circuit temperature devices with an output voltage linearly-proportional to the Centigrade temperature. The LM35 device has an advantage over linear temperature sensors calibrated in Kelvin, as the user is not required to subtract a large constant voltage from the output to obtain convenient Centigrade scaling. The LM35 device does not require any external calibration or trimming to provide typical accuracies of ¬±¬º¬∞C at room temperature and ¬±¬æ¬∞C over a full ‚àí55¬∞C to 150¬∞C temperature range.  **4. Pulse Sensor :**  The Pulse Sensor is a plug-and-play heart-rate sensor for Arduino. It can be used by students, artists, athletes, makers, and game & mobile developers who want to easily incorporate live heart-rate data into their projects. The essence is an integrated optical amplifying circuit and noise eliminating circuit sensor. Clip the Pulse Sensor to our earlobe or fingertip and plug it into our Arduino, we can ready to read heart rate. Also, it has an Arduino demo code that makes it easy to use.  **‚óè Pin-1(GND) :** Black Colour Wire - It is connected to the GND terminal of the system.  **‚óè Pin-2(VCC) :** Red Colour Wire - It is connected to the supply voltage (+5V otherwise +3.3V) of the system.  **‚óè Pin-3(Signal) :** Purple Colour Wire - It is connected to the pulsating o/p signal.  **5. 16*2 LCD Display :**  The features of this LCD mainly :  ‚óè The operating voltage of this LCD is 4.7V-5.3V  ‚óè It includes two rows where each row can produce 16-characters  ‚óè The utilization of current is 1mA with no backlight  ‚óè Every character can be built with a 5√ó8 pixel box  ‚óè The alphanumeric LCDs alphabets & numbers  ‚óè Is display can work on two modes like 4-bit & 8-bit  ‚óè These are obtainable in Blue & Green Backlight  ‚óè It displays a few custom generated characters  **6. ThingSpeak :**  ThingSpeak allows you to aggregate, visualize and analyze live data streams in the cloud. Some of the key capabilities of ThingSpeak include the ability to:  ‚óè Easily configure devices to send data to ThingSpeak using popular IoT protocols.  ‚óè Visualize our sensor data in real-time.  ‚óè Aggregate data on-demand from third-party sources.      ‚óè Use the power of MATLAB to make sense of our IoT data.  ‚óè Run our IoT analytics automatically based on schedules or events.  ‚óè Prototype and build IoT systems without setting up servers or developing web software.  ‚óè Automatically act on our data and communicate using third-party services like Twilio or Twitter  To learn how you can collect, analyze and act on our IoT data with ThingSpeak, explore the topics below:  **Collect :** Send sensor data privately to the cloud  **Analyze :** Analyze and visualize our data with matlab  **Act:** Trigger a reaction  **Circuit Diagram & Connections :**  **1.** Connect Pulse Sensor output pin to A0 of Arduino and other two pins to VCC and GND.  **2.** Connect LM35 Temperature Sensor output pin to A1 of Arduino and other two pins to VCC & GND.  **3.** Connect the LED to Digital pin 7 of Arduino via a 220-ohm resistor.  **4.** Connect pin 1,3,5,16 of LCD to GND.  **5.** Connect pin 2,15 of LCD to VCC.  **6.** Connect pin 4,6,11,12,13,14 of LCD to Digital pin 12,11,5,4,3,2 of Arduino.  **7.** The RX pin of ESP8266 works on 3.3V and it will not communicate with the Arduino when we will connect it directly to the Arduino. So, we will have to make a voltage divider for it which will convert the 5V into 3.3V. This can be done by connecting the 2.2K & 1K resistor. Thus the RX pin of the ESP8266 is connected to pin 10 of Arduino through the resistors.  **8.** Connect the TX pin of the ESP8266 to pin 9 of the Arduino.   ![Final Project Image](https://github.com/HackwithArpon/Iot-Based_Patient-Health-Monitoring-System-/assets/116937463/f162c68a-6d3b-4c76-8c35-0778372fda1c)  **Setting the ThingSpeak**  ThingSpeak provides a very good tool for IoT based projects. By using the  ThingSpeak site, we can monitor our data and control our system over the Internet,  using the Channels and web pages provided by ThingSpeak. Then create a new channel and set up what you want. Then create the API keys. This  key is required for programming modifications and setting your data. Then upload the  code to the Arduino UNO by assembling the circuit shown above. Open the serial  monitor and it will automatically connect to Wi-Fi and set up everything.  **Conclusion:**  ‚óè Our proposed system aims at simplifying health monitoring.  ‚óè This system of ours has been proven effective enough in aiding medical assistance.  ‚óè This approach of ours contributes towards Smart India.  **References:**  **‚ùñ Marathe, Sachi, et al. ‚ÄúA Wireless Patient Monitoring System using Integrated ECG module, Pulse Oximeter, Blood Pressure and Temperature Sensor.‚Äù 2019 International Conference on Vision Towards Emerging Trends in Communication and Networking (ViTECoN). IEEE, 2019.**  **‚ùñ Mishra, Vaibhav, Durgesh Kumar Mishra and Ass. Prof. Ankit Trivedi. ‚ÄúHealth Monitoring System using IoT using Arduino Uno Microcontroller.‚Äù Health 6.08 (2019).**    By **Arpon Kumar Chowdhury**, KGEC_CSE'25"
IoT Heart Attack Detection & Heart Rate Monitor,Internet of Things,https://github.com/NuzhatJabeen/IoT-Based-Heart-Attack-Detection-and-Heart-Rate-Monitoring-System-,# IoT-Based-Heart-Attack-Detection-and-Heart-Rate-Monitoring-System-
IoT Based Toll Booth Manager System,Internet of Things,https://github.com/fahimshahrierrasel/toll_management_iot,"# IOT Based Automated Toll Collection System using RFID Technology  Collecting toll manually is time prune work and caused jam in the high ways and in the city. If the system can be automated then time and hassle can be reduced. On automated toll collection system both times and the traffic jam can be reduced. In our project, we made a small scale automated toll collection system. This project used IOT to solve the problem with the PHP based server and android application and node js based API for android application.  For IOT we have used  * Arduino Uno * Arduino Mega * Arduino Ethernet Shield * RFID (RC522) Sensor and Cards * Arduino Camera OV7670 * 16 * 2 LCD Display * IIC/I2C Serial Interface Module * Ultrasonic Sonar Sensor * Buzzer * LED Lights  For Backend Server we used:  * PHP 7.0 * MySQL  For API:  * Node.js * Express  ## Ardino Connection with sensor and other elements  ![image21](./assets/image21.png)  ##  Full Environment  ![image23](./assets/image23.png)  ![image22](./assets/image22.jpeg)  ## Web Frontend  ![image24](./assets/image24.png)  ![image25](./assets/image25.png)  ## Android Application  ![Screenshot_20181114-190026](./assets/Screenshot_20181114-190026.png) ![](./assets/Screenshot_20181114-190040.png)  ![Screenshot_20181114-190051](./assets/Screenshot_20181114-190051.png) ![Screenshot_20181114-190104](./assets/Screenshot_20181114-190104.png)  ![Screenshot_20181114-190112](./assets/Screenshot_20181114-190112.png) "
IoT Theft Detection Using Raspberry Pi,Internet of Things,https://github.com/mostafaaboseif/IOT-Theft-Detection-using-RaspberryPi,"# IOT-Theft-Detection-using-RaspberryPi  The project consists of one multithreaded server and 2 clients that communicate using sockets.  The server assigns each client to a separate thread.  One client is connected to a camera used as a motion detector (takes 2 pictures and compares them pixel by pixel to check for change),  and the other is connected to an external ADC using SPI that interfaces with a light sensor.  Once the thief enters the room, the light sensor is triggered, it sends a tweet that a thief is detected in the house.  Once the camera detects the thief in motion, it takes his picture and sends it to the server which forwards it into a face recognition module (implemented using convolution neural networks);  if it doesn‚Äôt match one of the users, it adds the picture to the MySQL database of a web server we designed and sends a tweet contaning his picture."
IoT based Flood & Drought predictor,Internet of Things,https://github.com/karthiknbharadwaj/Flood-Detection-and-Prediction-using-IOT-Data-Analytics,"# Flood-Detection-and-Prediction-using-IOT-Data-Analytics  Flood is an unavoidable natural disaster causing heavy flow of traffic and can also cause severe damage to properties and lives. For this reason, researchers created a flood detection system to monitor rising water in residential areas. The implementation of flood alert systems near any major water area or body of water provides critical information that can protect property and save lives. we are design this system to inform the people about the upcoming flood through notification and alert messages. For that purpose we are going to use some sensors which will helpful to give information about the flood. This system provides actual implementation to organizations, communities and individuals interested in establishing and operating flood monitoring and warning systems. The Arduino Flood Detector System is developed to be one of the fastest method to monitor flood that will help motorists or road user to avoid problem when flood occurred."
IoT Based Smart Door Locking System,Internet of Things,https://github.com/durjoysarkardhrubo/Smart-Door-Locking-System-Using-IoT,"# Smart Door Locking System Using IoT  ![Smart Door Locking System](https://github.com/durjoysarkardhrubo/Smart-Door-Locking-System-Using-IoT/blob/main/images/door-lock.jpg)  ## Table of Contents - [Overview](#overview) - [Features](#features) - [Components Required](#components-required) - [System Architecture](#system-architecture) - [Installation](#installation)   - [Prerequisites](#prerequisites)   - [Clone the Repository](#clone-the-repository)   - [Install Dependencies](#install-dependencies) - [Wiring Diagram](#wiring-diagram) - [Usage](#usage) - [Security Considerations](#security-considerations) - [Contributing](#contributing) - [License](#license) - [Acknowledgments](#acknowledgments)  ## Overview  The **Smart Door Locking System** is an IoT-based solution designed to enhance the security and convenience of door access control. Utilizing a Raspberry Pi, servo motor, and a web interface, this system allows users to remotely lock and unlock their doors from anywhere with an internet connection. Additionally, it provides local control through physical buttons or a keypad.  ## Features  - **Remote Control:** Lock and unlock your door via a web interface accessible through smartphones or computers. - **Local Control:** Physical buttons or a keypad for manual locking/unlocking. - **Real-Time Monitoring:** Monitor the status of your door (locked/unlocked) in real-time. - **Basic Security:** Implement basic authentication to protect against unauthorized access.  ## Components Required  1. **Raspberry Pi** (with Raspbian OS installed) 2. **Servo Motor** (e.g., SG90) for controlling the lock mechanism 3. **Push Buttons** or **Keypad** for local input 4. **Wi-Fi Module** (if not using Raspberry Pi with built-in Wi-Fi) 5. **Jumper Wires** 6. **Breadboard** 7. **Power Supply** 8. **Resistors** (for button pull-down/up configurations)  ## System Architecture  - **Hardware Control:** The servo motor is connected to the Raspberry Pi GPIO pins to control the locking mechanism. - **Local Input:** Push buttons or a keypad allow manual locking/unlocking. - **Web Interface:** A Flask-based web server allows remote control via a smartphone or computer. - **Security:** Basic authentication can be added to secure the web interface.  ## Installation  ### Prerequisites  - **Raspberry Pi** with Raspbian OS installed. - Python 3.x installed on the Raspberry Pi. - Internet connection for the Raspberry Pi. - Necessary hardware components connected as per the system overview.  ### Clone the Repository  Open the terminal on your Raspberry Pi and clone the repository:  ```bash git clone https://github.com/durjoysarkardhrubo/Smart-Door-Locking-System-Using-IoT.git cd Smart-Door-Locking-System-Using-IoT"
IoT based temprature controlled fan,Internet of Things,https://github.com/AmruthaRajsheker/IoT-Temperature-Based-Automatic-Fan-Speed-Control-and-Monitoring-System,"# IoT Temperature Based Automatic Fan Speed Control and Monitoring System    ## Overview    This project implements an IoT-based system for automatically controlling fan speed based on temperature readings from sensors. It utilizes a DHT11 sensor for measuring temperature and humidity, along with a separate analog temperature sensor for redundancy or additional coverage. The system adjusts the fan speed according to the temperature readings, ensuring optimal cooling conditions. Real-time monitoring of temperature readings is provided through the serial monitor.      ## Features    - **Automatic Fan Speed Control**: Adjusts fan speed based on temperature readings from both DHT11 and a separate analog temperature sensor.  - **Real-time Temperature Monitoring**: Provides instant temperature readings via the serial monitor.  - **Customizable Temperature Threshold**: Allows users to set their desired temperature threshold for fan speed adjustments.  - **Redundant Temperature Sensing**: Ensures reliability by utilizing multiple temperature sensors.    ## Hardware Requirements    - Arduino Uno  - DHT11 sensor  - 12V cooling fan  - 9V battery  - Jumper wires  - USB cable     ## Software Requirements    - Arduino IDE or a platform-specific development environment    ## Architecture   <img src=""https://github.com/AmruthaRajsheker/IoT-Temperature-Based-Automatic-Fan-Speed-Control-and-Monitoring-System/assets/119475943/29b9af3b-c0ce-4e2f-ab20-4dc319212c7e"" alt=""description"" style=""width: 50%; height:¬†auto;"">    ## Experiment Overview    ### Step 1: Setup Arduino Environment    1. **Install Arduino IDE**: Download and install the Arduino IDE from the official Arduino website if not already installed.  2. **Connect Arduino Uno**: Use a USB cable to connect the Arduino Uno board to your computer, enabling power and code upload capabilities.    ### Step 2: Components Assembly    1. **DHT11 Sensor Connection**:     - **Signal Pin (DHTPIN)**: Connect to digital pin 2 (D2) on the Arduino Uno.     - **VCC**: Connect to 5V pin.     - **GND**: Connect to GND pin.    2. **Analog Temperature Sensor Connection**:     - **Signal Pin (tempPin)**: Connect to analog pin A0.     - **VCC**: Connect to 5V pin.     - **GND**: Connect to GND pin.    3. **Cooling Fan Connection**:     - **Positive (+)**: Connect to the digital pin 3 (D3) on the Arduino Uno.     - **Negative (-)**: Connect to the GND pin.         4. **Power Supply**:     - Use a USB cable connected to your computer or a 9V battery connected to the power jack or VIN pin and GND pin.    ### Step 3: Write and Upload Code    1. **Open Arduino IDE**: Launch the Arduino IDE on your computer.  2. **Write Code**: Develop the code to read temperature from both sensors and control the fan based on temperature readings.  3. **Upload Code**: Upload the code to the Arduino Uno board.    ### Step 4: Monitor Readings    1. **Open Serial Monitor**: Access the serial monitor in the Arduino IDE to monitor temperature readings and fan speed control actions.  2. **Observe Results**: Power on the system and observe the displayed temperature readings and fan speed control actions.    ### Step 5: Experiment and Adjustments    1. **Experiment with Thresholds**: Adjust temperature thresholds in the code for desired fan speed control.  2. **Fine-Tuning**: Fine-tune the system by modifying the code and hardware connections as necessary.  3. **Documentation**: Document findings, adjustments, and encountered issues for future reference.        ## Circuit diagram   <img src=""https://github.com/AmruthaRajsheker/IoT-Temperature-Based-Automatic-Fan-Speed-Control-and-Monitoring-System/assets/119475943/25b11e17-641a-4e0c-a9e9-a80584c021aa"" alt=""description"" style=""width: 50%; height:¬†auto;"">    ## Circuit Setup   <img src=""https://github.com/AmruthaRajsheker/IoT-Temperature-Based-Automatic-Fan-Speed-Control-and-Monitoring-System/assets/119475943/5812c01e-04ec-48f3-8415-74d9f1567e55"" alt=""description"" style=""width: 50%; height:¬†auto;"">    ## Output     ### prototype  <img src=""https://github.com/AmruthaRajsheker/IoT-Temperature-Based-Automatic-Fan-Speed-Control-and-Monitoring-System/assets/119475943/c1c5a9be-ef54-4b04-9252-3024c7bd2d19"" alt=""description"" style=""width: 50%; height:¬†auto;"">    ### serial monitor  <img src=""https://github.com/AmruthaRajsheker/IoT-Temperature-Based-Automatic-Fan-Speed-Control-and-Monitoring-System/assets/119475943/4ebcaa4c-f04f-4a8c-a121-f748fa7e7c37"" alt=""description"" style=""width: 50%; height:¬†auto;"">      ## License    This project is licensed under the [MIT License](link to license file). "
VS Code Clone,Electron JS,https://github.com/preeti09011999/VSCode-Clone,"# VSCode-Clone This is a desktop application which is a clone of VSCode using HTML, CSS, JavaScript, Electron. It has most of the functionalities which are in VSCode like a file-explorer, terminal, code-editor and task bar to new and save file."
Zoom Clone,Electron JS,https://github.com/milankrushna/zoom-meeting-electron-app,"# electron-quick-start  **Clone and run for a quick way to see Electron in action.**  This is a minimal Electron application based on the [Quick Start Guide](https://electronjs.org/docs/tutorial/quick-start) within the Electron documentation.  **Use this app along with the [Electron API Demos](https://electronjs.org/#get-started) app for API code examples to help you get started.**  A basic Electron application needs just these files:  - `package.json` - Points to the app's main file and lists its details and dependencies. - `main.js` - Starts the app and creates a browser window to render HTML. This is the app's **main process**. - `index.html` - A web page to render. This is the app's **renderer process**.  You can learn more about each of these components within the [Quick Start Guide](https://electronjs.org/docs/tutorial/quick-start).  ## To Use  To clone and run this repository you'll need [Git](https://git-scm.com) and [Node.js](https://nodejs.org/en/download/) (which comes with [npm](http://npmjs.com)) installed on your computer. From your command line:  ```bash # Clone this repository git clone https://github.com/electron/electron-quick-start # Go into the repository cd electron-quick-start # Install dependencies npm install # Run the app npm start ```  Note: If you're using Linux Bash for Windows, [see this guide](https://www.howtogeek.com/261575/how-to-run-graphical-linux-desktop-applications-from-windows-10s-bash-shell/) or use `node` from the command prompt.  ## Resources for Learning Electron  - [electronjs.org/docs](https://electronjs.org/docs) - all of Electron's documentation - [electronjs.org/community#boilerplates](https://electronjs.org/community#boilerplates) - sample starter apps created by the community - [electron/electron-quick-start](https://github.com/electron/electron-quick-start) - a very basic starter Electron app - [electron/simple-samples](https://github.com/electron/simple-samples) - small applications with ideas for taking them further - [electron/electron-api-demos](https://github.com/electron/electron-api-demos) - an Electron app that teaches you how to use Electron - [hokein/electron-sample-apps](https://github.com/hokein/electron-sample-apps) - small demo apps for the various Electron APIs  ## License  [CC0 1.0 (Public Domain)](LICENSE.md)"
Sorting Algorithm Vizualizer,Data Structures and Algorithms,https://github.com/dharshakch97/sort-visualizer,"# Sorting algorithm visualizer  This is a web application built using HTML, CSS, Javascript to visualize classic sorting algorithms such as bubble, insertion, selection, merge, quick   **Live-** [sorting visualizer](https://dharshakch97.github.io/sort-visualizer/)   **Code-** [sort visualizer code](https://github.com/dharshakch97/sort-visualizer)  ## Purpose  I made this sorting visualizer web application to improve my skills more in HTML, CSS, and JS. And to help the beginners in programming to gain more knowledge and understanding on sorting algorigthms by visualizing them.  ## What the Sorting algorithms inside this application  This application supports the following sorting algorithms:  1. **Bubble Sort**. 2. **Insertion Sort**. 3. **Selection Sort**. 4. **Merge Sort**. 5. **Quick Sort**."
Visualization of Shortest Path Finding Algorithm,Data Structures and Algorithms,https://github.com/khalidsaifullaah/pathfinding-visualizer," <h1 align=""center"">   <br>   <a style =""color:black; text-decoration:none;"" href=""https://khalidsaifullaah.github.io/pathfinding-visualizer/"">Path Finder</a> </h1>  <h4 align=""center"">A web app to help visualizing typical graph searching algorithms</h4>   <p align=""center"">   <a href=""#live-demo"">Live Demo</a> ‚Ä¢   <a href=""#development-stack"">Development Stack</a> ‚Ä¢   <a href=""#brief-info-about-the-algorithms"">Brief Info About The Algorithms</a> ‚Ä¢   <a href=""#credits"">Credits</a> </p>  ![DEMO](https://github.com/khalidsaifullaah/pathfinding-visualizer/blob/master/project_demo.gif?raw=true)   ## Live Demo  You can [play-around](https://khalidsaifullaah.github.io/pathfinding-visualizer/) with the live demo of the project.  ## Development Stack <h1 align=""center""> <img width=""20%"" height=""100"" src=""https://www.ad-ventures.cc/static/aca21772a37e26761da9d791044f4e45/3cb25/p5js-pink.png"" alt=""P5.js logo""> <img width=""20%"" height=""100"" src=""https://www.w3.org/html/logo/downloads/HTML5_1Color_Black.svg"" alt=""HTML logo""> <img width=""20%"" height=""100"" src=""https://getbootstrap.com/docs/4.0/assets/brand/bootstrap-social-logo.png"" alt=""Bootstrap logo""> </h1>  ## Brief Info About The Algorithms  - ### **A\* Search:**      ![screenshot](https://upload.wikimedia.org/wikipedia/commons/9/98/AstarExampleEn.gif)      A* (pronounced ""A-star"") is a graph traversal and path search algorithm, which is often used in many fields of computer science due to its completeness, optimality, and optimal efficiency.One major practical drawback is its {\displaystyle O(b^{d})}O(b^d) space complexity, as it stores all generated nodes in memory. Thus, in practical travel-routing systems, it is generally outperformed by algorithms which can pre-process the graph to attain better performance, as well as memory-bounded approaches; however, A* is still the best solution in many cases.   _-Wikipedia_  - ### **Dijkstra's Algorithm:**     ![screenshot](https://upload.wikimedia.org/wikipedia/commons/thumb/5/57/Dijkstra_Animation.gif/220px-Dijkstra_Animation.gif)          Dijkstra's algorithm (or Dijkstra's Shortest Path First algorithm, SPF algorithm) is an algorithm for finding the shortest paths between nodes in a graph, which may represent, for example, road networks. It was conceived by computer scientist Edsger W. Dijkstra in 1956 and published three years later.      The algorithm exists in many variants. Dijkstra's original algorithm found the shortest path between two given nodes,[5] but a more common variant fixes a single node as the ""source"" node and finds shortest paths from the source to all other nodes in the graph, producing a shortest-path tree.     _-Wikipedia_  - ### **Breadth First Search (BFS):**     ![screenshot](https://upload.wikimedia.org/wikipedia/commons/4/46/Animated_BFS.gif)          Breadth-first search (BFS) is an algorithm for traversing or searching tree or graph data structures. It starts at the tree root (or some arbitrary node of a graph, sometimes referred to as a 'search key'), and explores all of the neighbor nodes at the present depth prior to moving on to the nodes at the next depth level.     _-Wikipedia_  - ### **Depth First Search (DFS):**     ![screenshot](https://upload.wikimedia.org/wikipedia/commons/thumb/7/7f/Depth-First-Search.gif/220px-Depth-First-Search.gif)          Depth-first search (DFS) is an algorithm for traversing or searching tree or graph data structures. The algorithm starts at the root node (selecting some arbitrary node as the root node in the case of a graph) and explores as far as possible along each branch before backtracking.     _-Wikipedia_  - ### **Best-First Search (Greedy):**     ![screenshot](https://upload.wikimedia.org/wikipedia/commons/thumb/5/57/Dijkstra_Animation.gif/220px-Dijkstra_Animation.gif)          Best-first search is a search algorithm which explores a graph by expanding the most promising node chosen according to a specified rule.     Judea Pearl described best-first search as estimating the promise of node n by a ""heuristic evaluation function {\displaystyle f(n)}f(n) which, in general, may depend on the description of n, the description of the goal, the information gathered by the search up to that point, and most important, on any extra knowledge about the problem domain.""     Some authors have used ""best-first search"" to refer specifically to a search with a heuristic that attempts to predict how close the end of a path is to a solution, so that paths which are judged to be closer to a solution are extended first. This specific type of search is called greedy best-first search or pure heuristic search.     _-Wikipedia_  _*Note\: The GIF images are unashamedly taken from Wikipedia as well*_   ## Credits   - [Daniel Shiffman's](https://github.com/shiffman/) video resources on P5.js helped me a lot to get a clear understanding about the library. - [Cl√©ment Mihailescu's](https://github.com/clementmihailescu) similar project inspired me a lot for the UI and the project itself - [P5.js](https://p5js.org/) The library behind the viz   ## You may also like...  - [8-Puzzle Using A* Search](https://khalidsaifullaah.github.io/8-Puzzle-A-Star-Search/) - Interactive way to solve the puzzle or get it solved by the AI   ---  > Stack Overflow [@khalid-saifullah](https://stackoverflow.com/users/7610724/khalid-saifullah) &nbsp;&middot;&nbsp; > GitHub [@khalidsaifullaah](https://github.com/khalidsaifullaah) &nbsp;&middot;&nbsp; > Facebook [@ikhalidsaifullaah](https://www.facebook.com/ikhalidsaifullaah/) &nbsp;&middot;&nbsp; > Twitter [@k_saifullaah](https://twitter.com/k_saifullaah) &nbsp;&middot;&nbsp; > LinkedIn [@khalidsaifullaah](https://www.linkedin.com/in/khalidsaifullaah/) "
Maze Solver using BFS,Data Structures and Algorithms,https://github.com/YeyoM/mazeSolver,"<!-- PROJECT LOGO --> <br /> <div align=""center"">   <a href=""https://github.com/YeyoM/mazeSolver"">     <img src=""public/icon.svg"" alt=""Logo"" width=""80"" height=""80"">   </a>  <h3 align=""center"">Maze Solver</h3>    <p align=""center"">     Generate and solve a maze with Python.     <br />     <a href=""https://github.com/YeyoM/mazeSolver""><strong>Explore the code ¬ª</strong></a>     <br />     <br />     <a href=""https://github.com/YeyoM/mazeSolver/issues"">Report Bug</a>   </p> </div>  <!-- ABOUT THE PROJECT --> ## About The Project  [![Product Name Screen Shot][product-screenshot]](https://github.com/YeyoM/mazeSolver)  ### What does this project do?  This project uses various techniques to generate and solve a maze using python in an easy way, to generate the maze we follow the following steps:  - Generate a matrix full of 0's which represents obtacles - Generate a grid in the matrix with 1's which representes paths that the algorithm will be able to follow - Using DFS we ""carve"" the maze generating paths between spaces in the grid which, seen from a graph approach, the spaces in the grid are nodes, the objective is to connect this nodes.  For the solution part, there are available 4 algorithms  #### DFS and BFS  BFS, Breadth-First Search, is a vertex-based technique for finding the shortest path in the graph. It uses a Queue data structure that follows first in first out. In BFS, one vertex is selected at a time when it is visited and marked then its adjacent are visited and stored in the queue. It is slower than DFS.   DFS, Depth First Search, is an edge-based technique. It uses the Stack data structure and performs two stages, first visited vertices are pushed into the stack, and second if there are no vertices then visited vertices are popped.   #### Dijkstra  Dijkstra's algorithm allows us to find the shortest path between any two vertices of a graph. Djikstra used this property in the opposite direction i.e we overestimate the distance of each vertex from the starting vertex. Then we visit each node and its neighbors to find the shortest subpath to those neighbors.  The algorithm uses a greedy approach in the sense that we find the next best solution hoping that the end result is the best solution for the whole problem.  #### A* Star  Informally speaking, A* Search algorithms, unlike other traversal techniques, it has ‚Äúbrains‚Äù (in the code is called heuristic). What it means is that it is really a smart algorithm which separates it from the other conventional algorithms. This fact is cleared in detail in below sections.  And it is also worth mentioning that many games and web-based maps use this algorithm to find the shortest path very efficiently (approximation).   <!-- GETTING STARTED --> ## Getting Started  To get a local copy up and running follow these simple example steps.  ### Prerequisites  - Have python installed in your computer  - Install with pip the following: random, numpy and time.  - Clone the repo    ```sh     git clone https://github.com/YeyoM/mazeSolver.git    ``` - The entry point of the code is main.py  <!-- LICENSE --> ## License  Distributed under the MIT License. See LICENSE.txt for more information.  <!-- CONTACT --> ## Contact  YeyoM - [@YeyoMoreno24](https://twitter.com/YeyoMoreno24) - yeyomoreno2003@hotmail.com  Maze Solver - [https://github.com/YeyoM/mazeSolver](https://github.com/YeyoM/mazeSolver)  <!-- MARKDOWN LINKS & IMAGES --> [product-screenshot]: public/screenshot.png"
Movie Recommendation Website (ML model based Full Stack Web App),Full Stack Ideas,https://github.com/venugopalkadamba/Movie-Recommendation-System-ML-React-Flask,"<div align=""center"">  <h1>Movie Recomendation System with Sentiment Analysis</h1> <img src=""https://img.shields.io/badge/Python-3.7.3-brown"" /> <img src=""https://img.shields.io/badge/Frontend-ReactJS-orange"" /> <img src=""https://img.shields.io/badge/BackendAPI-Flask-yellow"" /> <img src=""https://img.shields.io/badge/OtherAPI-TMDB-red"" /> <img src=""https://img.shields.io/badge/Deployment-Heroku-blue"" /> </div>  ## About  <b>KVG Movie Zone</b> is an AI based web application in which you can search for any Hollywood Movie. This application will provide all the information related to that movie, does <b>sentiment analysis</b> on the movie reviews and the most interesting part, this application will provide you the top 10 <b>movie recommendations</b> based on your search.<br/>  <b>ReactJS</b> was used for frontend which was deployed using <b>firebase hosting</b> and a <b>Flask API</b> was deployed using <b>Docker</b> container on <b>Heroku</b> to serve the machine learning models to the Frontend.  This application uses <b>Content Based Movie Recommendation</b> to recommend movies to the user.<b>TMDB</b> API was used to retrieve all the information related to the movie and its cast. <b>Web Scraping</b> was done on <b>IMDB</b> website to get the reviews related to the searched movie. Sentiments analysis is done using a machine learning model trained on a sample of IMDB Dataset.<br/>  <b>Deployed Web Application Link: </b>https://kvg-movie-zone.web.app/ <br/> <b>Deployed Flask API Link: </b>https://kvgmrs-api.herokuapp.com/  ## Demo  <div align=""center""> <img src=""./readme_assets/demo.gif"" alt=""demo"" /> </div>  ## Architecture  <div align=""center""> <img src=""./readme_assets/architecture.png"" alt=""architecture"" /> </div>  ## How to generate TMDB API Key?  1. Login to you your tmdb account: https://www.themoviedb.org/ or create one if you dont have. 2. Then open https://www.themoviedb.org/settings/api link and create your api key by filing all the necessary information. 3. <b>IMPORTANT:</b> After generating the TMDB API KEY, replace ""ENTER YOUR TMDB_API_KEY"" with your generated key in the API and FRONTEND code.  ## TMDB API End Points  1. BASE URL: https://api.themoviedb.org/3 2. FOR MOVIE DATA: https://api.themoviedb.org/3/movie/{tmdb_movie_id}?api_key={TMDB_API_KEY} 3. FOR MOVIE CAST DATA: https://api.themoviedb.org/3/movie/{tmdb_movie_id}/credits?api_key={TMDB_API_KEY}    <b>NOTE: </b>Please do refer the documentation at the BASE URL for better understanding.  ## Flask API end points  1. To get recommendations: https://kvgmrs-api.herokuapp.com/recommend_movie  ``` Data to be sent in POST request: {     movie_name:""The Avengers"",     number_of_recommendations:""10"" }  Data Returned by the API in JSON format: {     input_movie:{         movie_id:TMDB_MOVIE_ID     },     recommendations:[         {             rank:1,             movie_id:TMDB_MOVIE_ID         },         {             rank:2,             movie_id:TMDB_MOVIE_ID         },         .         .         .     ] } ```  2. To get Movie Reviews with Sentiments: https://kvgmrs-api.herokuapp.com/movie_reviews_sentiment  ``` Data to be sent in POST request: {     movie_imdb_id:""MOVIE_IMDB_ID"" }  Data Returned by the API in JSON format: [     {         id: 1,         content: ""THE REVIEW"",         sentiment: ""SENTIMENT FOR THE REVIEW""     },     {         id: 2,         content: ""THE REVIEW"",         sentiment: ""SENTIMENT FOR THE REVIEW""     },     .     .     .     10 ] ```  <b>NOTE: </b>The error messages are returned in the following format:  ``` {     error:""Content of ERROR Message"" } ```  ## Steps to run the React Project  1. Clone or download the repository in your local machine. 2. Open command prompt in the following folder `FRONTEND/kvg-mrs` 3. Install all the npm packages  ``` npm install ```  4. Since the Flask API is already deployed on Heroku no need to run the Flask API in your local machine to start the React frontend. You can start the react application using the following command:  ``` npm start ```  ## Steps to run the Flask API  1. Clone or download the repository and open command prompt in `API` folder. 2. Create a virtual environemt  ``` mkvirtualenv environment_name ```  3. Install all the dependencies  ``` pip install -r requirements.txt ```  4. Run the app.py file  ``` python app.py ```  The API will be running at http://127.0.0.1:5000/  <b>NOTE: </b>You can run the Flask API and the React Frontend in parallel and can use for development by replacing the baseURL,present in `FRONTEND/kvg-mrs/src/api/recommenderapi.js`, with the Flask API running link.  ## Steps to Dockerize and Deploy the Flask API on Heroku  1. Clone or download the repository and open command prompt in `API` folder. 2. Create your docker account at https://hub.docker.com 3. Download the docker desktop based on you windows version from the official website of Dockers and login to the docker desktop. 4. Start the Docker desktop in you machine. 5. The Dockerfile for dockerinzing this Flask API is already present in the API folder. 6. Open command prompt in API folder and run the below mentioned commands: 7. Building the Image:  ``` docker build -t ENTER_YOUR_OWN_TAG_NAME . ```  It will take some time for the execution of the above command. After execution of the above command you can see the docker image details using the following command:  ``` docker images ```  8. Install Heroku CLI in your local machine. 9. Login to your account using follwing command:  ``` heroku login ```  10. Run the following commands for deplyment. Logging into heroku container:  ``` heroku container:login ```  11. Create a app in heroku:  ``` heroku create YOUR_APP_NAME ```  11. Pushing the docker image into heroku:  ``` heroku container:push web --app YOUR_APP_NAME ```  12. Releasing the web app:  ``` heroku container:release web --app YOUR_APP_NAME ```  That's it, you can see your API running at `https://YOUR_APP_NAME.herokuapp.com/`  ## Steps to Dockerize and run the Flask API in local machine  1. Clone or download the repository and open command prompt in `API` folder. 2. Create your docker account at https://hub.docker.com 3. Download the docker desktop based on you windows version from the official website of Dockers and login to the docker desktop. 4. Start the Docker desktop in you machine. 5. Replace the code present in Dockerfile with the code present in localhost_docker_code.txt. 6. Open command prompt in API folder and run the below mentioned commands: 7. Building the Image:  ``` docker build -t ENTER_YOUR_OWN_TAG_NAME . ```  8. Run the docker container:  ``` docker run -d -p 5000:5000 PREVIOUSLY_ENTERED_TAG_NAME ```  After execution of the above command you can notice the Flask API running at http://localhost:5000  ## Tech Stack Used  <div align=""center"">  <table>     <tr>         <td><img src=""./readme_assets/react.png"" width=""200px"" height=""200px"" /></td>         <td><img src=""./readme_assets/firebase.png"" width=""200px"" height=""200px"" /></td>         <td><img src=""./readme_assets/docker.png"" width=""200px"" height=""200px"" /></td>     </tr>     <tr>         <td><img src=""./readme_assets/flask.png"" width=""200px"" height=""200px"" /></td>         <td><img src=""./readme_assets/gunicorn.png"" width=""200px"" height=""200px"" /></td>         <td><img src=""./readme_assets/heroku.jpg"" width=""200px"" height=""200px"" /></td>     </tr> </table>  </div>  ## Referred Article Links  For Movie Recommendation System  1. [Article 1](https://towardsdatascience.com/how-to-build-from-scratch-a-content-based-movie-recommender-with-natural-language-processing-25ad400eb243) 2. [Article 2](https://analyticsindiamag.com/how-to-build-a-content-based-movie-recommendation-system-in-python/)  For Deployment Using Dockers  1. [Article 1](https://medium.com/analytics-vidhya/dockerize-your-python-flask-application-and-deploy-it-onto-heroku-650b7a605cc9) 2. [Article 2](https://pythonise.com/series/learning-flask/building-a-flask-app-with-docker-compose) 3. [Article 3](https://betterprogramming.pub/create-a-running-docker-container-with-gunicorn-and-flask-dcd98fddb8e0) 4. [Article 4](https://itnext.io/setup-flask-project-using-docker-and-gunicorn-4dcaaa829620) 5. [Article 5](https://philchen.com/2019/07/09/a-scalable-flask-application-using-gunicorn-on-ubuntu-18-04-in-docker)  ## Dataset Links  1. [IMDB 5000 Dataset](https://www.kaggle.com/carolzhangdc/imdb-5000-movie-dataset) 2. movies_metadata.csv and credits.csv from [Movies Dataset](https://www.kaggle.com/rounakbanik/the-movies-dataset) 3. Remaining Datasets are generated using `MovieRecommendationDatasetPreparation.ipynb` in `MovieRecommendationCodes folder` 4. [IMDB 50k Movie Reviews Dataset](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)  <div align=""center""> <b>Please do ‚≠ê this repo if you liked my work</b> </div>"
Crowdfunding Online Platform,Full Stack Ideas,https://github.com/Vivek-Kamboj/Crowd_funding,"# CROWDFUNDING PLATFORM ‚ú®  ## Brief Description  A crowdfunding platform where user can browse through different campaigns and support the cause by donating some amount of money. The payment is done via the PayTM gateway & complete transparency and anonymity of the process is ensured.  ## Features and Functionalities üòÉ  **User features**  - User can see the ongoing campaigns on the landing page.  - User can click on the campaign they want to support and then be directed to the specific campaign page where they can see the campaign details.  - User can then support the campaign by donating the amount they would like to via PayTM.  - User can also share the campaign among their friends/colleagues/relatives to increase the reach of the campagin via the share-feature.  - User can see the amount details of the transaction after the payment is done on that campaign page in the List of Donation. (Anonymity is ensured)  - User can send us their queries via the form provided in the 'Contact-Us' section.    **Admin features**     - All above features are included for the admin as well.  - Admin can start a new campaign by filling in the project details such as campaign title, required amount etc.  - Admin can deactivate an ongoing campaign when the need is fulfilled or before (due to any possible reason).  - Admin can edit the details of an ongoing campaign (not allowed to change the raised amount).  - Only an admin can add another new admin (if required, for any purpose).     **CRUD** functions can be called by users to perform different types of operations on selected data within the database.    **Note** : We have created a default admin. The route for the LOGIN section is not kept in the UI. One can access it through website_url/admin/login where they are required to   enter the login credentials. The motive behind doing this was that we thought it would seem a bit weird for the general users to be able to see the LOGIN section when there is no need of it to make a payment which is the prominient feature in our website.    ## Tech Stack üíª    **MERN** stack has been used for the development of this website.   - [React.js](https://reactjs.org/)  - [Node.js](https://nodejs.org/en/)  - [Express.js](https://expressjs.com/)  - [MongoDB Atlas](https://www.mongodb.com/cloud/atlas)  ## API :man_technologist:   - [PayTM API](https://developer.paytm.com/docs/)   ## Components include:  - API calling (calling to PayTM gateway)  - Usage of MongoDB that provides us a Schema-less Database and ensures high performance and efficiency.    ## Thought behind the project   This project is a part of the Winter Project event conducted by AASF IIITM. Our motive behind choosing this project was to build a platform that can come to practical use in real world. We saw the need of a crowd-funding webapp for our **ROTARACT CLUB** in IIITM that can provide smooth functionality as well as transparency of payments for the various campaigns/charities carried out via ROTARACT. So, we decided to come up with  this project that can provide a good user experience as well as ensure the transparency of the payments carried out. And, also serve as a platform working for a good cause.   ## Hosted on Vercel Link : https://crowd-funding.vercel.app/  ## SetUp Steps  ### Prerequisites npm installed, Paytm id, MongoDB URI, create-react-app installed, etc. <br> ### For Backend  - Go to backend folder.  - Run npm install  - Set Up Environment variable as shown in `env-sample` file.  - Run `node ./server.js`   ### For Frontend  - Go to `Frontend/crowd-funding-frontend` folder.  - Run `npm install`  - Put backend url in config.js  - Run `npm start`   **Hurray, Your app is now running on port 3000 in your browser**  ## Screenshots  ### Landing Page  ![](ScreenShots/LandingPage1.png)  On Clicking Donate Button, we can slide through the Ongoing Camapigns section  ![](ScreenShots/LandingPage2.png)  ### Campaign Page  ![](ScreenShots/Campaign1.png)  Donors List (Amount and Transaction ID) for transparency   ![](ScreenShots/Campaign2.png)  Here some part of Transaction ID is hidden for security purpose (from backend as well as frontend).  ### Instant share and donate buttons  ![](ScreenShots/Instant.png)  ### All Campaigns  ![](ScreenShots/AllCampaign.png)  ### Login Page  ![](ScreenShots/LoginPage.png)  ### Admin DashBoard  Here in bottom we can see message from the Feedback Form present in the Contact us page.  ![](ScreenShots/AdminDashBoard.png)  ### About Us Page  ![](ScreenShots/AboutUsPage.png)  ### Conatact Us Page  ![](ScreenShots/ContactUsPage.png)  ### Footer  ![](ScreenShots/Footer.png)   ## Video Demo  [Click for Demo](https://youtu.be/Xl-5Blm3Pm8)"
Amazon Clone,Generic Stack,https://github.com/shubho0908/Amazon-clone,"# Amazon Clone   This is a clone of Amazon web application, with improved UI and all major functionalities such as adding products to wishlist, browsing different product categories, adding products to cart, managing shipping details, and payment options. The project is responsive and can be used on any device.  **Demo Video**   https://user-images.githubusercontent.com/81776711/224985061-d7012611-8997-4a6b-ba7c-a4fc7f2c9954.mp4   # Technologies Used   ReactJS  Firebase Authentication  Firebase Firestore  FakeStore API   # Features   Improved UI than the original Amazon web application  User can add products to wishlist  User can browse products based on different categories  User can add products to cart  User can manage shipping details  User can make payment for the products  User profile tab for managing personal information   # Installation and Setup   Clone the repository  Navigate to the project directory and `run npm install` to install all the dependencies  Create a Firebase account and enable Firebase Authentication and Firestore  Create a new Firebase project and copy the configuration details  Rename the `.env.sample` file to .env and replace the Firebase configuration details  Run `npm start` to start the application   # Screenshots  **Authentication page**  ![image](https://user-images.githubusercontent.com/81776711/224613171-8b2c9392-5b97-4757-aaed-5e570e51f4a3.png)  ![image](https://user-images.githubusercontent.com/81776711/224613223-7ad2a01e-13f6-474a-800b-2133ba0d27a7.png)   **Home page**  ![image](https://user-images.githubusercontent.com/81776711/224613299-feb71146-bec7-4fd2-948b-e776d9701b47.png)   **Wishlist page**  ![image](https://user-images.githubusercontent.com/81776711/224613383-6fed93fe-c269-4aba-888c-d0a3cf802f0a.png)   **Product page**  ![image](https://user-images.githubusercontent.com/81776711/224615004-fe0bf691-333f-40f7-90ef-c4f26ebe5395.png)  ![image](https://user-images.githubusercontent.com/81776711/224615047-2b015e56-ebdb-44e1-a29d-6fd382b40981.png)   **Cart page**  ![image](https://user-images.githubusercontent.com/81776711/224614923-de0f913c-6305-4357-8b99-c32de24041b0.png)   **Payment page**  ![image](https://user-images.githubusercontent.com/81776711/224615321-63c60972-78a4-49b0-8df4-d49a85a6cb86.png)   **Orders page**  ![image](https://user-images.githubusercontent.com/81776711/224616126-7f3d7bff-e8d5-41dc-adb6-f9f1e6191654.png)   **Profile page**  ![image](https://user-images.githubusercontent.com/81776711/224616059-925032b3-17a6-481e-80e7-f55016b81a09.png)   # UI design inspirations  The UI of the project is completely inspired and even taken from these, all the design credits goes to them:  [Home Page](https://dribbble.com/shots/15350650-Amazon-Website-Redesign-Concept)  [Profile section](https://dribbble.com/shots/20739514-Ecommerce-Account-Page-Dashboard-UXUI)   # Contributing   Contributions to this project are always welcome. Please feel free to raise an issue or submit a pull request if you find any bug or have any feature requests."
Fashion eCommerce Website,Generic Stack,https://github.com/nimone/Fashion-Store,"# Fashion-Store  https://user-images.githubusercontent.com/30614282/161916675-86fb41ce-75cf-40f2-8c6e-db35ac1f059e.mp4  --- ## Front-end - [React](https://es.reactjs.org/) - Front-End JavaScript library - [Windi CSS](https://windicss.org/) - Next generation utility-first CSS framework - [Feather Icons](https://feathericons.com/) - Simply beautiful open source icons - [Vite](https://vitejs.dev/) - Frontend Tooling  ## Back-end  - [Node.js](https://nodejs.org/) - JavaScript runtime built on Chrome's V8 JavaScript engine. - [Express.js](https://expressjs.com/) - Minimal and flexible Node.js web application framework. - [Mongoose](https://mongoosejs.com/) - Elegant mongodb object modeling for node.js  - [Stripe](https://stripe.com/docs/js) - Payments infrastructure for the internet - [celebrate](https://github.com/arb/celebrate) - A joi validation middleware for Express. - [JsonWebToken](https://github.com/auth0/node-jsonwebtoken) - An implementation of JSON Web Tokens for Node.js  ## Database - [Monogodb](https://www.mongodb.com/) - Cross-platform document-oriented NoSQL database.  --- ## Run yourself > Make sure you have [mongodb](https://www.mongodb.com/try/download/community) & [nodejs](https://nodejs.org/) installed on your system before proceeding.  1. Clone this repo ```bash git clone https://github.com/nimone/Fashion-Store && cd Fashion-Store ``` 2. Install project dependecies ```bash cd ./api && npm install cd ./client && npm install ``` 3. Start development servers (api & client) with the provided script `rundev.sh` ```bash bash rundev.sh ``` > Or, start them manually by `npm run dev`"
Blog Content Management System,Generic Stack,https://github.com/koushil-mankali/blog-content-management-system,"Blog CMS (Blog Content Management System) Project  ############################################################################################## #											     # #	Admin-Login:  http://localhost/BlogCMS/admin/					     # #											     # #	id 	: koushil  								     # #	password: koushil 								     # #											     # #	Create Database ""blogcms"" and import database file which you can get from 	     #	 #	database folder.								     # #											     # ##############################################################################################  Front Page of BlogCMs  <img src=""BlogCMS.png"" alt=""blogcms"">  Technologies / Languages Used: ------------------------------ Frontend : ========== 1.HTML 2.CSS 3Bootstrap   (Not Much) 4.Javascript (Not Much) 5.JQuery     (Not Much)  Backend : ========= 1.PHP  Features: ========= 1.Admin can add users as admins / moderators. 2.Admin / Moderators can publish article and later can also update article. 3.Article Images can also be updated. 4.Post count will be updated on article publishing / updating / deleting. 5.Dashboard has realtime posts / users / categorys count and also latest article updates. 6.Fully functional Search Option. 7.Pagination in all pages.  Dashboard of BlogCMs  <img src=""dashboard.png"" alt=""dashboard"">  Security Features: ================== 1.Users (Admin/Moderator) passwords are hased. 2.Better Session Handling. 3.Error Page Management (:> If visitor directly enters page url / manipulates url / some code errors are managed and redirected to seperatly designed error page.) 4.Visitors cant access floders (eg. css floder / javascript floder / images floder) which are part of blog. When they try to access folders they will be redirected to error page."
Food Delivery App,Generic Stack,https://github.com/enatega/food-delivery-singlevendor,"<div align=""right""> <a target=""_blank"" href=""https://www.facebook.com/sharer/sharer.php?u=https://github.com/Ninjas-Code-official/Enatega-Food-Delivery-Solution"" style=""text-decoration:none"">   <img src=""https://img.shields.io/badge/-0d1117?logo=facebook""  width=""40"" height=""30""> </a> <a target=""_blank"" href=""https://www.linkedin.com/shareArticle?mini=true&url=https://github.com/Ninjas-Code-official/Enatega-Food-Delivery-Solution"" style=""text-decoration:none"">   <img src=""https://img.shields.io/badge/-0d1117?logo=linkedin""  width=""40"" height=""30""> </a> <a target=""_blank"" href=""https://twitter.com/intent/tweet?&url=https://github.com/Ninjas-Code-official/Enatega-Food-Delivery-Solution&via=TWITTER-HANDLE"" style=""text-decoration:none"">   <img src=""https://img.shields.io/badge/-0d1117?logo=twitter"" width=""40"" height=""30""> </a> </div>  <div align=""center"">   <h2>Enatega Single Vendor Food Delivery Solution</h2>   <i>The white label food delivery solution built for restaurants of all sizes!</i>  <br/> <br /> </div>  <div align=""center"">  [![Static Badge](https://img.shields.io/badge/License-MIT-red)](https://github.com/Ninjas-Code-official/Enatega-Food-Delivery-Solution/blob/main/LICENSE) [![Stars](https://img.shields.io/github/stars/Ninjas-Code-official/Enatega-Food-Delivery-Solution.svg)](https://github.com/Ninjas-Code-official/Enatega-Food-Delivery-Solution/stargazers) [![Forks](https://img.shields.io/github/forks/Ninjas-Code-official/Enatega-Food-Delivery-Solution.svg)](https://github.com/Ninjas-Code-official/Enatega-Food-Delivery-Solution/forks) [![GitHub contributors](https://img.shields.io/github/contributors/Ninjas-Code-official/Enatega-Food-Delivery-Solution)](https://github.com/Ninjas-Code-official/Enatega-Food-Delivery-Solution/graphs/contributors) [![Open Pull Requests](https://img.shields.io/github/issues-pr-raw/Ninjas-Code-official/Enatega-Food-Delivery-Solution.svg)](https://github.com/Ninjas-Code-official/Enatega-Food-Delivery-Solution/pulls) [![Activity](https://img.shields.io/github/last-commit/Ninjas-Code-official/Enatega-Food-Delivery-Solution.svg)](https://github.com/Ninjas-Code-official/Enatega-Food-Delivery-Solution/commits/main) [![YouTube Channel](https://img.shields.io/badge/Watch_us-Youtube-red)](https://www.youtube.com/@ninjascode509) [![Company Website](https://img.shields.io/badge/Visit_us-Website-blue)](https://enatega.com) [![Closed Issues](https://img.shields.io/github/issues-closed/Ninjas-Code-official/Enatega-Food-Delivery-Solution?color=success)](https://github.com/Ninjas-Code-official/Enatega-Food-Delivery-Solution/issues?q=is%3Aissue+is%3Aclosed) [![Closed Pull Requests](https://img.shields.io/badge/Closed%20Pull%20Requests-View%20on%20GitHub-blue.svg)](https://github.com/Ninjas-Code-official/Enatega-Food-Delivery-Solution/pulls?q=is%3Apr+is%3Aclosed)  </div>  <div align=""center"">  [![Static Badge](https://img.shields.io/badge/facebook-blue?logo=facebook&logoColor=Blue&color=%23fbfbfb)](https://www.facebook.com/enatega) [![Static Badge](https://img.shields.io/badge/Instagram-blue?logo=instagram&logoColor=D815BE&color=%23fcfcfc)](https://www.instagram.com/enatega.nb/) [![Static Badge](https://img.shields.io/badge/Twitter-blue?logo=Twitter&logoColor=blue&color=%23fcfcfc)](https://twitter.com/EnategaA) [![Static Badge](https://img.shields.io/badge/LinkedIn-blue?logo=LinkedIn&logoColor=darkblue&color=%23fcfcfc)](https://www.linkedin.com/company/14583783/)  </div>  # Enatega Single Vendor Solution  <div align=""center"">    <a href=""https://www.youtube.com/watch?v=8sE7ivnFyo0&feature=youtu.be&ab_channel=NinjasCode"">   <img src=""./assets/final.webp"" alt=""Demo video"" style=""border-radius: 6px; width: auto;"">   </a>  </div>  <br/> Enatega Single Vendor is a white label food delivery solution that allows restaurant owners to easily manage their deliveries. Enatega Single Vendor can help to intuitively and instantly automate your deliveries, and handle the logistics. Our food delivery solution provides the capability of order management, as well as separate applications for the rider and the customer. It also boasts a suite of features and can be customized to match your brand thanks to its white label capabilities.  <b>This is the full free source code of our solution, however the backend and API is proprietary and can be obtained via paid license.</b>  <!-- Add a horizontal rule for separation --> <hr/>  ## :fast_forward: Quick Links  - [:book: What is included](#heading-1) - [:rocket: Features](#heading-2) - [:wrench: Setup](#heading-3) - [:gear: Prerequisites](#heading-4) - [:computer: Technologies](#heading-5) - [:camera: Screenshots](#heading-6) - [:triangular_ruler: High Level Architecture](#heading-7) - [:page_with_curl: Documentation](#heading-8) - [:movie_camera: Demo Videos](#heading-14) - [:video_game: Demos](#heading-9) - [:busts_in_silhouette: Contributors](#heading-14) - [:warning: Disclaimer](#heading-12) - [:email: Contact Us](#heading-13)  <!-- Add a horizontal rule for separation --> <hr/>  ## :question: What is included: <a id=""heading-1""></a>  Our food delivery solution contains three separate modules for order management. These include the admin panel, the delivery app and the rider app. Below, the capabilities of all three modules have been listed:  - The admin panel receives the orders that can be placed via the customer app. It also allows managing the restaurant‚Äôs orders as well as the riders‚Äô accounts.(Run on node version 14)  - The customer app allows for customers to choose their specific selections and customize their order before placing it.  - The rider app can accept the orders and also allows for location based zoning as well as the ability to locate customer‚Äôs address via google map‚Äôs API integration.  ## :fire: Features: <a id=""heading-2""></a>  - Analytics dashboard for the mobile app - Payment integration with Paypal and Stripe - Order tracking feature - Email Integration e.g for order confirmation etc. - Ability to provide ratings and reviews - Finding address using GPS integration - Facebook and Google authentication integration - Mobile responsive dashboard - Multi-Language support using localization - Separate rider app for order management - Multiple variations of food items - Push notifications for both mobile and web  ## :repeat_one: Setup: <a id=""heading-3""></a>  As we‚Äôve mentioned above, the solution includes three separate modules. To setup these modules, follow the steps below:  To run the module, you need to have nodejs installed on your machine(Install node version 14). Once nodejs is installed, go to the directory and enter the following commands  The required credentials and keys have been set already. You can setup your own keys and credentials  The version of nodejs should be between 14.0 to 16.0  [![Guide Badge](https://img.shields.io/badge/Do_with_guided_tutorial-blue?style=for-the-badge&logo=book-reader)](https://enatega.com/single-vendor-doc/)  ## :framed_picture: Screenshots: <a id=""heading-6""></a>  |        Customer App        | | :------------------------: | | ![](./assets/customer.jpg) |  |          Rider App          | | :-------------------------: | | ![](./assets/rider-app.jpg) |  |              Dashboard               | | :----------------------------------: | | ![](./assets/dashboard-scaled-1.jpg) |  ## :wrench: High Level Architecture: <a id=""heading-7""></a>  ![High Level Architecture](./assets/HighArchitectDiagram.png)  ## :information_source: Prerequisites: <a id=""heading-4""></a>  ##### App Ids for Mobile App in app.json  - Facebook Scheme - Facebook App Id - Facebook Display Name - iOS Client Id Google - Android Id Google - Amplitude Api Key - server url  ##### Set credentials in API in file helpers/config.js and helpers/credentials.js  - Email User Name - Password For Email - Mongo User - Mongo Password - Mongo DB Name - Reset Password Link - Admin User name - Admin Password - User Id - Name  ##### Set credentials in Admin Dashboard in file src/index.js  - Firebase Api Key - Auth Domain - Database Url - Project Id - Storage Buck - Messaging Sender Id - App Id  ##### NOTE: Email provider has been only been tested for gmail accounts  ## :hammer_and_wrench: Technologies: <a id=""heading-5""></a>  |                                               Expo                                                |                                                   React-Navigation                                                   |                                                Apollo GraphQL                                                |                                               ReactJS                                                |                                                NodeJS                                                 |                                                 MongoDB                                                 |                                                   Firebase                                                   | | :-----------------------------------------------------------------------------------------------: | :------------------------------------------------------------------------------------------------------------------: | :----------------------------------------------------------------------------------------------------------: | :--------------------------------------------------------------------------------------------------: | :---------------------------------------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------------------: | :----------------------------------------------------------------------------------------------------------: | | <a href=""https://expo.dev/""><img src=""./assets/expoicon.png"" alt=""Enatega Logos"" width=""100""></a> | <a href=""https://reactnavigation.org/""><img src=""./assets/react-navigation.png"" alt=""Enatega Logos"" width=""100""></a> | <a href=""https://www.apollographql.com/""><img src=""./assets/apollo.png"" alt=""Enatega Logos"" width=""100""></a> | <a href=""https://reactjs.org/""><img src=""./assets/react-js.png"" alt=""Enatega Logos"" width=""100""></a> | <a href=""https://nodejs.org/en/""><img src=""./assets/node-js.png"" alt=""Enatega Logos"" width=""100""></a> | <a href=""https://www.mongodb.com/""><img src=""./assets/mongoDB.png"" alt=""Enatega Logos"" width=""100""></a> | <a href=""https://firebase.google.com/""><img src=""./assets/firebase.png"" alt=""Enatega Logos"" width=""100""></a> |  |                                                 React Native                                                 |                                                       React Router                                                       |                                                GraphQL                                                |                                                ExpressJS                                                 |                                                   React Strap                                                    |                                                Amplitude                                                | | :----------------------------------------------------------------------------------------------------------: | :----------------------------------------------------------------------------------------------------------------------: | :---------------------------------------------------------------------------------------------------: | :------------------------------------------------------------------------------------------------------: | :--------------------------------------------------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------------------: | | <a href=""https://reactnative.dev/""><img src=""./assets/react-native.png"" alt=""Enatega Logos"" width=""100""></a> | <a href=""https://reactrouter.com/""><img src=""./assets/react-router-svgrepo-com.png"" alt=""Enatega Logos"" width=""100""></a> | <a href=""https://graphql.org/""><img src=""./assets/graphQl-1.png"" alt=""Enatega Logos"" width=""100""></a> | <a href=""https://expressjs.com/""><img src=""./assets/express-js.png"" alt=""Enatega Logos"" width=""100""></a> | <a href=""https://reactstrap.github.io/""><img src=""./assets/React-strap.png"" alt=""Enatega Logos"" width=""100""></a> | <a href=""https://amplitude.com/""><img src=""./assets/amplitude.png"" alt=""Enatega Logos"" width=""100""></a> |  ## :iphone: Demos: <a id=""heading-9""></a>  |                                                                                                                                       Customer App                                                                                                                                       |                                                                                                                                             Rider App                                                                                                                                             |                                                     Admin Dashboard                                                     | | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | :---------------------------------------------------------------------------------------------------------------------: | |                                                                              <a href=""#heading-9"" style=""pointer-events: none;""><img src=""./assets/LOGO-CUSTOMER.png"" alt=""Enatega Logos"" width=""150""></a>                                                                               |                                                                                  <a href=""#heading-9"" style=""pointer-events: none;""><img src=""./assets/RIDER-APP-LOGO.png"" alt=""Enatega Logos"" width=""150""></a>                                                                                   | <a href=""https://singlevendor-admin.enatega.com""><img src=""./assets/worldwide.png"" alt=""Enatega Logos"" width=""100""></a> | | <a href=""https://play.google.com/store/apps/details?id=com.enatega.vendor""><img src=""./assets/android_518705.png"" alt=""Android Logo"" width=""25""></a> <a href=""https://apps.apple.com/pk/app/enatega/id1493209281""><img src=""./assets/social_10096939.png"" alt=""iOS Logo"" width=""25""></a> | <a href=""https://play.google.com/store/apps/details?id=com.enatega.rider""><img src=""./assets/android_518705.png"" alt=""Android Logo"" width=""25""></a> <a href=""https://apps.apple.com/pk/app/enatega-rider-app/id1493291047""><img src=""./assets/social_10096939.png"" alt=""iOS Logo"" width=""25""></a> |  ## :book: Documentation <a id=""heading-8""></a>  Find the link for the complete documentation of the Enatega Single Vendor Solution [here](https://enatega.com/singlevendor-documentation/).  ## :tv: Demo Videos: <a id=""heading-14""></a>  | Demo | | :--: |  | <a href=""https://www.youtube.com/watch?v=AWbdt9GX1t4""><img src=""https://img.youtube.com/vi/AWbdt9GX1t4/0.jpg"" width=""200"" alt=""Video""></a>  ## :people_holding_hands: Contributors: <a id=""heading-14""></a>  <div align=""center""> <br> <a href=""https://github.com/Ninjas-Code-official/Enatega-Food-Delivery-Solution/graphs/contributors"">   <img src=""https://contrib.rocks/image?repo=Ninjas-Code-official/Enatega-Food-Delivery-Solution"""" style=""max-width: 50%; height: auto;"" /> </a> </div>  ## :warning: Disclaimer: <a id=""heading-12""></a>  The frontend source code for our solution is completely open source. However, the API and backend is proprietary and can be accessed via a paid license. For further information, contact us on the channels provided below.  ## :mailbox_with_mail: Contact Us: <a id=""heading-13""></a>  [Check out the product page and pricing and more for Enatega Food Delivery Solution](https://enatega.com/?utm_source=github&utm_medium=referral&utm_campaign=github_guide&utm_id=12345678)."
Poll App,Generic Stack,https://github.com/DebjitPramanick/Polling-App,"# POLLING APP  Hi! I'm Debjit, a **MERN Stack Developer**. This is a clone of popular project management application **Polling App**. I have created it with **React** in frontend,  **Node and Express** for backend and **MongoDB** for storing data. <br> <br> > ### Web App Link - https://polling-app-2c1ab.web.app/   <br>  ## Features  - Sign-in/Sign-up - Create Polls - Vote Polls - Delete Poll - See Stats of Each Poll  ##  NPM Packages Used  - React-redux - Material UI - Express - Nodemon - Mongoose - JWT - Brypt.js - Axios - React Chart.js 2  ## Screenshots  <img src=""./screenshots/ss1.png"" alt=""""/> <br> <img src=""./screenshots/ss2.png"" alt=""""/> <br> <img src=""./screenshots/ss3.png"" alt=""""/> <br> <img src=""./screenshots/ss4.png"" alt=""""/> <br> <img src=""./screenshots/ss5.png"" alt=""""/> <br>"
GuestBook App,LAMP Stack,https://github.com/mah-shamim/simple-guestbook-application,"# Simple Guestbook Application A guestbook application where visitors can leave messages  **Topics**: PHP, MySQL, AJAX, CSS, Guestbook, Web Development  ### Installation Process  1. **Clone the Repository**:    ```sh    git clone https://github.com/yourusername/simple-guestbook-application.git    cd simple-guestbook-application    ```  2. **Database Setup**:     - Create a new MySQL database.     - Import the provided `guestbook.sql` file to set up the necessary table.     - Update the database configuration in `config.php`.     - Import the provided SQL file to set up the necessary tables.      ```sh      mysql -u yourusername -p todo_list < guestbook.sql      ```  4. **Configure the Database Connection:**    - Open the `config.php` file and update the database credentials.      ```php      <?php      // config.php      $servername = ""localhost"";      $username = ""yourusername"";      $password = ""yourpassword"";      $dbname = ""guestbook_db"";      ```  5. **Start a Local PHP Server:**    - Start the PHP built-in server.      ```sh      php -S localhost:8000      ```  6. **Access the Application:**    - Open your web browser and navigate to `http://localhost:8000`.  ### File Structure  Here‚Äôs a basic file structure for your to-do list application:  ``` simple-guestbook-application/ ‚îú‚îÄ‚îÄ config.php ‚îú‚îÄ‚îÄ db.php ‚îú‚îÄ‚îÄ index.php ‚îú‚îÄ‚îÄ submit.php ‚îú‚îÄ‚îÄ fetch.php ‚îú‚îÄ‚îÄ css/ ‚îÇ   ‚îî‚îÄ‚îÄ style.css ‚îú‚îÄ‚îÄ guestbook.sql ‚îî‚îÄ‚îÄ js/     ‚îî‚îÄ‚îÄ main.js ```"
ToDo App,LAMP Stack,https://github.com/Denmlak/PHP-TO-DO-List,"<h3>TO-DO List app</h3>  Simple LAMP stack PHP/MySQL To-Do list app. Full responsive in different screen sizes. <h4>Technologies Used:</h4> <ul> <li>Html5</li> <li>CSS3</li> <li>PHP 7.2.10</li> <li>MySQL 5.6 / MariaDB</li> </ul> <h4>Features:</h4> <ul>   <li>Sign Up</li>   <li>Log In/ Log Out</li>   <li>Add/Edit/Delete Task</li> </ul> <h4>How to use:</h4> <ul>   <li>Clone the repository or download the zip into your document root</li>   <li>Import todolist.sql to your database</li>   <li>Edit config.php and change host, user, password and database name</li> </ul> <h4>Preview pictures:</h4> <img src=""https://i.ibb.co/0F7k7BQ/Untitled-2.jpg"" alt=""Untitled-2"" border=""0""> <img src=""https://i.ibb.co/9GNvcJr/Untitled-1.jpg"" alt=""Untitled-1"" border=""0"">  "
Student Management System,LAMP Stack,https://github.com/amirhamza05/Student-Management-System,"# Student Management System  > This is a simple web-based open source software written in PHP and JavaScript. This was specifically written for school or educational organization. You can easily install this system in your own server.  > If you find any bug or added new feature feel free to send a pull request.  > If you have any suggestions feel free create issues.  - [Demo](#demo) - [Feature](#feature-of-student-management-system) - [Technology](#technology) - [Screen Shot](#screen-shot) - [Installation](#installation)  Demo ----------------------------- - **URL** - http://ems.tserm.com - **Username** -  github_user - **Password** - guser  Feature of Student Management System ----------------------------- - Add Student Information - Admit Student In Multiple Program - Student Payment System - Student Attendence System - Generate Student Id Card - Program Add - Exam Add - Add Exam Result Add and generate Auto Ranking - Sending Result by SMS - Sending Notice By SMS - Institute Report (Payment,Expence,Income,Profit,Attendence) - All Activity are Auto Saved and Admin or User Can See Change Previous and Present Change  Technology ----------------------- - PHP - Ajax - Java Script - Bootstrap - Html - css - mysql  Screen Shot ----------------------- ![screenshot](https://raw.githubusercontent.com/amirhamza05/Student-Management-System/master/screen_shot/login_screen.png) Login Screen | ![screenshot](https://raw.githubusercontent.com/amirhamza05/Student-Management-System/master/screen_shot/dashboard.PNG) Dashboard | |-|-|  ![screenshot](https://raw.githubusercontent.com/amirhamza05/Student-Management-System/master/screen_shot/user_profile.PNG) User Profile | ![screenshot](https://raw.githubusercontent.com/amirhamza05/Student-Management-System/master/screen_shot/student_profile.PNG) Student Profile | |-|-|  ![screenshot](https://raw.githubusercontent.com/amirhamza05/Student-Management-System/master/screen_shot/id_card.PNG) ID Card | ![screenshot](https://raw.githubusercontent.com/amirhamza05/Student-Management-System/master/screen_shot/print_id_card.PNG) Print ID Card |  |-|-|  ![screenshot](https://raw.githubusercontent.com/amirhamza05/Student-Management-System/master/screen_shot/monthly_attendence_report.PNG) Student Attendence Report | ![screenshot](https://raw.githubusercontent.com/amirhamza05/Student-Management-System/master/screen_shot/payment_dashboard.PNG) Payment Dashboard | |-|-|  ![screenshot](https://raw.githubusercontent.com/amirhamza05/Student-Management-System/master/screen_shot/payment_status_list.PNG) Payment Status List | ![screenshot](https://raw.githubusercontent.com/amirhamza05/Student-Management-System/master/screen_shot/payment_money_recept.PNG) Payment Money Recept | |-|-|  ![screenshot](https://raw.githubusercontent.com/amirhamza05/Student-Management-System/master/screen_shot/sms_dashboard.PNG) SMS Dashboard | ![screenshot](https://raw.githubusercontent.com/amirhamza05/Student-Management-System/master/screen_shot/compare_user_activity_data.PNG) Compare User Activity | |-|-|  ![screenshot](https://raw.githubusercontent.com/amirhamza05/Student-Management-System/master/screen_shot/theme_change.PNG) Multiple Themes | ![screenshot](https://raw.githubusercontent.com/amirhamza05/Student-Management-System/master/screen_shot/update_setting.png) Update Institute Information | |-|-|  Installation ----------------------- - Clone this project from https://github.com/amirhamza05/Student-Management-System - After clone you can see this installation screen ![screenshot](https://raw.githubusercontent.com/amirhamza05/Student-Management-System/master/screen_shot/install_screen.png)  - Fill this database setup form and when you install button click then if information is correct then you see install success screen ![screenshot](https://raw.githubusercontent.com/amirhamza05/Student-Management-System/master/screen_shot/install_success.png)  - After this your install is complete then you can login this system. Your Default id and password is 'admin' and 'admin'.    "
Admin Dashboard,LAMP Stack,https://github.com/satnaing/shadcn-admin,"# Shadcn Admin Dashboard  Admin Dashboard UI crafted with Shadcn and Vite. Built with responsiveness and accessibility in mind.  ![alt text](public/images/shadcn-admin.png)  I've been creating dashboard UIs at work and for my personal projects. I always wanted to make a reusable collection of dashboard UI for future projects; and here it is now. While I've created a few custom components, some of the code is directly adapted from ShadcnUI examples.  > This is not a starter project (template) though. I'll probably make one in the future.  ## Features  - Light/dark mode - Responsive - Accessible - With built-in Sidebar component - Global Search Command - 10+ pages - Extra custom components  ## Tech Stack  **UI:** [ShadcnUI](https://ui.shadcn.com) (TailwindCSS + RadixUI)  **Build Tool:** [Vite](https://vitejs.dev/)  **Routing:** [TanStack Router](https://tanstack.com/router/latest)  **Type Checking:** [TypeScript](https://www.typescriptlang.org/)  **Linting/Formatting:** [Eslint](https://eslint.org/) & [Prettier](https://prettier.io/)  **Icons:** [Tabler Icons](https://tabler.io/icons)  ## Run Locally  Clone the project  ```bash   git clone https://github.com/satnaing/shadcn-admin.git ```  Go to the project directory  ```bash   cd shadcn-admin ```  Install dependencies  ```bash   pnpm install ```  Start the server  ```bash   pnpm run dev ```  ## Author  Crafted with ü§ç by [@satnaing](https://github.com/satnaing)  ## License  Licensed under the [MIT License](https://choosealicense.com/licenses/mit/)"
User Management System,MERN Stack,https://github.com/JackWorld99/MERN-User-Management-System,"<p align=""center""> <img src=""public/banner.png"" alt=""Logo"" width=""250"" height=""250"" justify-items=""center""/> <h3 align=""center"">üë©üèª‚Äçüíª User Management System üõ°Ô∏è</h3> </p>  ---  üéâ Welcome to the ultimate **User Management System** designed with security, efficiency, and user experience in mind. This system leverages the **MERN Stack** (MongoDB, Express.js, React.js, Node.js) and utilizes **JSON Web Tokens** (JWT) for secure **Authentication** and **Authorization**. Below is an overview of the key features that make this system a robust and reliable solution for managing user accounts and protecting sensitive data.  --- ### üéà Key Features  ##### üõ°Ô∏è Authentication & Authorization ü™™  > - **Signup Option**: Allow new users to create an account easily with a secure signup process. > - **Login Option**: Enable users to log in to their accounts securely using their credentials. > - **Logout Option**: Provide a straightforward way for users to log out of their accounts. > - **Persistent Login**: Offer users the option to stay logged in for 7 days, enhancing convenience while maintaining security. > - **Weekly Login Requirement**: For added security, users are required to log in at least once a week. > - **Auto Logout**: Automatically log out users after 15 minutes of inactivity if the persistent login option is not selected. > - **Abuse Detection**: Utilize reCAPTCHA v3 to detect and prevent abusive traffic or bots. > - **Email Verification**: Ensure email authenticity with checks for one-time or fake emails, and require email verification for account activation. > - **Two-Factor Authentication**: Add an extra layer of security with two-factor authentication.  ---  ##### üñ•Ô∏è Login Demo ![](public/login.gif)  ---  ##### üíª Sign in with Google Demo ![](public/google.gif)  ---  ##### üñ•Ô∏è Sign up Demo ![](public/signup.gif)  ---  ##### üõ°Ô∏è Password and Account Security  > - **Forgot Password Handling**: Users can reset their passwords securely using an OTP (One-Time Password), ensuring both ease of use and security. > - **Account Protection**: Limit daily OTP requests and login attempts. Immediate account suspension occurs upon detecting suspicious activity to prevent unauthorized access. > - **Password Encryption**: Use bcrypt to encrypt and safeguard user passwords. > - **Secure Data Transmission**: Employ JWT to securely transmit information between parties.  ![](public/reset-password.gif)  ##### ü™™ Admin Controls and User Roles üë©üèª‚Äçüíª  > - **Immediate User Suspension**: Admins can instantly suspend users to protect company data and system integrity in urgent situations. > - **Role-Based Authorization**: Assign roles as User or Admin, with appropriate permissions for each. > - **Status Bar**: Display the current user and their assigned role, providing a clear overview of their status.  ![](public/status.gif)  ---  #### üë©üèª‚Äçüíª User Management ‚å®Ô∏è  > - **Root User Privileges**: The root user has maximum privileges within the system. > - **User Settings Access**: Only the root user and admins can access user settings. > - **Admin Privileges**: Admins cannot delete or change each other's profiles. > - **User Creation**: Root user and admins can create new users. > - **User Management**: Root user and admins can change a user's name, email, password, and roles. > - **User Search**: Provides a feature for searching user names to find out user details. > - **Account Deactivation**: Provide a permission feature that restricts user access as soon as possible if needed. > - **Account Deletion**: Provide a way to remove user access by deleting accounts. > - **User Activity Monitoring**: Root and admin users can view if a user is online and the last time they were online in real-time.  ---  #### üìù Task Management üßæ  > - **Task Assignment**: Tasks are assigned to specific users. > - **Task Viewing**: Users can only view their assigned tasks. > - **Task Management**: The root user can view, edit, and delete all tasks. > - **Task Creator Privileges**: Task creators can view, edit, update, and delete tasks they created. > - **Task Deletion**: Tasks can only be deleted by the admin or root user who created them. > - **Task Status**: Tasks can have statuses of PENDING, EXPIRED, or COMPLETED. > - **Task Details**: The status bar shows the task creator, created, and edited date-time details. > - **Task Assignment Viewing**: Admin and root users can view who has been assigned to a task via the list.  ![](public/tasks.gif)  ---  #### üìì Note Management ‚úèÔ∏è  > - **Markdown Support**: Users can create, view, edit, and delete their notes with markdown support. > - **Note Management**: Root user and admins have permission to view, edit, and delete user notes if needed. > - **Note Search**: Provides search features for titles/tags to view the contents of notes. > - **Note Tagging**: Tags are provided for tagging notes.  ---  ### ü™õ Getting Started  The quickest way to get started as shown below:  > Create .env files to manage environment variables for your startup.  ##### Server Side  ```bash cd Backend npm i && npm run dev ```  ##### Client Side  ```bash cd Frontend npm i && npm run start ```  ---"
Virtual Queue Management System,MERN Stack,https://github.com/winster/vqms,"# Virtual Queue Management System  Mobile Web App to virtual queue management.  ### Features   - Automatic Queue Management   - Chrome Web notifications   - Websocket based instant updates  ### Raspberry PI code   This is server repo. For client(IOT), please visit [Git Repo][site]  It is complete JavaScript solution. Starting from bluetooth and GSM operations in raspberry pi, JavaScript is used in nodejs and even for database integration.  > The goal of Virtual Queue Management solution is the hassle free airport experience for the traveler. > It also helps Airport authority to easily handle big crowd in security gates.  ### Version 0.0.1  ### Tech  VQMS uses a number of open source projects to work properly:  * [service worker] - enable local notification * [Websockets] - 2 way communication between client and server * [Twitter Bootstrap] - great UI boilerplate for modern web apps * [node.js] - evented I/O for the backend * [Express] - fast node.js network app framework [@tjholowaychuk] * [GCM] - Google Cloud Messaging for Chrome * [Firebase] - Database as a Service from Google  And of course Virtual Queue Management System itself is open source on GitHub.  License ----  MIT       [site]: <https://github.com/winster/vqms_iot.git>    [bootstrap]: <http://getbootstrap.com/>    [service worker]: <https://www.w3.org/TR/service-workers/>    [Websockets]: <https://html.spec.whatwg.org/multipage/comms.html#network>    [node.js]: <http://nodejs.org>    [Twitter Bootstrap]: <http://twitter.github.com/bootstrap/>    [express]: <http://expressjs.com>    [GCM]: <https://developers.google.com/cloud-messaging/>    [Firebase]: <https://www.firebase.com/>"
eCommerce Website,MERN Stack,https://github.com/ajaybor0/MERN-eCommerce,"# eCommerce Platform Project - MERN Stack  Welcome to the eCommerce Platform Project built using the MERN (MongoDB, Express.js, React, Node.js) Stack. This project provides a robust and full-featured online shopping platform with various functionalities to enhance the user experience.  **Live App Demo** : [https://mern-shop-abxs.onrender.com/](https://mern-shop-abxs.onrender.com/)</br> Note: Please be aware that Render's free tier will automatically shut down after 15 minutes of inactivity. Consequently, the first request after reactivation may experience a delay, but subsequent requests will be faster.  ## Features  - **Full-Featured Shopping Cart**: Seamless shopping cart functionality for users to add, remove, and manage products. - **Product Reviews and Ratings**: Users can leave reviews and provide ratings for products. - **Top Products Carousel**: Display a carousel of top-rated or featured products. - **Product Pagination**: Navigate through products efficiently with pagination. - **Product Search Feature**: Easily search for products based on keywords. - **User Profile with Orders**: Users can create profiles and track their order history. - **Admin Dashboard**: Comprehensive dashboard for administrators to manage admins, products, users, and orders. - **Admin Admin Management**: Manage admin accounts. - **Admin Product Management**: Add, edit, and delete products from the platform. - **Admin User Management**: Manage user accounts. - **Admin Order Details Page**: Access detailed information about each order. - **Mark Orders as Delivered Option**: Ability to update order status to ""delivered."" - **Checkout Process**: Seamless checkout with options for shipping and payment methods. - **Razorpay Integration**: Secure payment processing through Razorpay. - **Database Seeder**: Easily populate the database with sample products and users.  ## Getting Started  ### Prerequisites  1. Fork the repository to your GitHub account. 2. Clone the forked repository to your local machine  ```bash git clone https://github.com/your-username/MERN-eCommerce.git ```  ```bash cd MERN-eCommerce ```  3. Create a MongoDB database and obtain your MongoDB URI from [MongoDB Atlas](https://www.mongodb.com/cloud/atlas). 4. Create a Razorpay account and obtain your Key ID and Key Secret from [Razorpay](https://razorpay.com/). 5. Create a Brevo account and generate a new SMTP Key from [Brevo](https://www.brevo.com/)  ### Env Variables  1. Rename the `.env.example` file to `.env` and add the following environment variables:  ```dotenv NODE_ENV=development PORT=5000 JWT_SECRET=ADD_YOUR_JWT_SECRET_HERE MONGO_URI=ADD_YOUR_MONGO_URI_HERE RAZORPAY_KEY_ID=ADD_YOUT_RAZORPAY_KEY_ID RAZORPAY_KEY_SECRET=ADD_YOUR_RAZORPAY_KEY_SECRET PAGINATION_MAX_LIMIT=12 # This will show 12 products per page; you can change it. EMAIL_HOST=smtp-relay.brevo.com EMAIL_PORT=587 EMAIL_USER=ADD_YOUR_BREVO_LOGIN EMAIL_PASS=ADD_YOUR_BREVO_PASSWORD EMAIL_FROM=ADD_YOUR_BREVO_LOGIN ```  ### Install Dependencies  Run the following commands to install dependencies for both the frontend and backend:  ```bash npm install cd frontend npm install ```  ### Run  To run both the frontend and backend concurrently, use:  ```bash npm run dev ```  To run only the backend:  ```bash npm run server ```  ## Build & Deploy  To create a production build for the frontend:  ```bash cd frontend npm run build ```  ## Seed Database  Use the following commands to seed the database with sample users and products, or destroy all data:  ```bash # Import data npm run data:import  # Destroy data npm run data:destroy ```  ## Sample User Logins  - **Live Admin Dashboard Login:**: [https://mern-shop-abxs.onrender.com/admin/login](https://mern-shop-abxs.onrender.com/admin/login)    - Email: admin@admin.com   - Password: admin123  - **Live Customer Logins:**: [https://mern-shop-abxs.onrender.com/login](https://mern-shop-abxs.onrender.com/login)   - John Doe     - Email: john@email.com     - Password: john123   - Alice Smith     - Email: alice@email.com     - Password: alice123  Feel free to explore and customize this eCommerce platform for your specific needs. Happy codingü§©!  # Contributing to the eCommerce Platform Project  We welcome and appreciate contributions from the community to enhance and improve the eCommerce Platform Project. Whether you're a developer, designer, tester, or someone with valuable feedback, your input is valuable. Here's how you can contribute:  ## Getting Started  1. Fork the repository to your GitHub account.  2. Clone the forked repository to your local machine:     ```bash    git clone https://github.com/your-username/MERN-eCommerce.git    ```  3. Navigate to the project directory:     ```bash    cd MERN-eCommerce    ```  4. Create a new branch for your contributions:     ```bash    git checkout -b feature/your-feature-name    git checkout -b issues/your-issue-name    ```  ## Making Changes  1. Implement your changes and improvements.  2. Ensure that your changes adhere to the project's coding style and conventions.  3. Test your changes thoroughly to avoid introducing bugs.  4. Update the project documentation if necessary.  ## Committing Changes  1. Commit your changes with a descriptive commit message:     ```bash    git add .    git commit -m ""Add your descriptive commit message here""    ```  2. Push your changes to your forked repository:     ```bash    git push origin feature/your-feature-name    git push origin issues/your-issue-name    ```  ## Creating a Pull Request (PR)  1. Visit your forked repository on GitHub.  2. Switch to the branch containing your changes.  3. Click on the ""New Pull Request"" button.  4. Provide a clear title and description for your pull request, explaining the purpose and scope of your changes.  5. Submit the pull request.  ## Code Review  Your contribution will be reviewed by the project maintainers. Be prepared to address any feedback or suggestions to ensure the quality and compatibility of your changes.  ## Thank You!  Thank you for considering contributing to the eCommerce Platform Project. Your efforts help make this project better for everyone. If you have any questions or need assistance, feel free to reach out through the issue tracker or discussions. Happy codingü§©!"
Online Distance Education System,MERN Stack,https://github.com/Pika003/e-Learning-Platform," # Online Learning Platform  Online Learning Platform using MERN Stack  ## Objective:  Develop a comprehensive online learning platform with three user types (Student, Teacher, Admin), featuring course creation, approval process, and live video conferencing.  ## *BACKEND for the PROJECT    - Backend is developed by [Parag](https://github.com/paragkadyan).  ## Features ![Screenshot 2024-05-14 212028](https://github.com/Pika003/e-Learning-Platform/assets/104189733/e2f9ce48-764b-48d2-8af1-188ea2918e8c)   #### 1. *User Authentication:*    - Student Login    - Teacher Login    - Admin Login       ![Screenshot 2024-05-14 211251](https://github.com/Pika003/e-Learning-Platform/assets/104189733/3179ba23-ae52-4ab5-8d0a-b2891cc43e0f)  ![Screenshot 2024-05-14 211154](https://github.com/Pika003/e-Learning-Platform/assets/104189733/377d8aa0-c35b-46d0-9408-f18b6ecb1ac1)  #### 2. *Application Approval:*    - Students and teachers can submit applications for approval.    - Admin validates and approves applications.       ![Screenshot 2024-05-15 212149](https://github.com/Pika003/e-Learning-Platform/assets/104189733/6e8afdba-a8a5-47e3-977c-f5292e136c3f)   #### 3. *Dashboard:*    - Students see purchased courses, progress, and communication options.    - Teachers view created courses, student enrollments, and communication features.        ![Screenshot 2024-05-14 211938](https://github.com/Pika003/e-Learning-Platform/assets/104189733/1008e68d-b683-4e8a-bc85-6d5890946724) ![Screenshot 2024-05-14 211854](https://github.com/Pika003/e-Learning-Platform/assets/104189733/c857a214-5366-49db-8035-13d2bfb88396) #### 4. *Course Purchase:*     - Students can browse and buy courses on the platform.       ![Screenshot 2024-05-14 211813](https://github.com/Pika003/e-Learning-Platform/assets/104189733/1578ca04-b85d-4c7b-8875-12f6756f2621)  #### 5. *Live Video Conferencing:*    - Integrated video conferencing tool (similar to Google Meet) for real-time teacher-student interaction.  #### 6. *Communication:*    - In-platform messaging system for communication between teachers and students.  #### 7. *Payment Integration:*    - Integrate a secure payment gateway for course purchases.  ----   ## *Tech Stack:*  #### *UI/UX:*   - [figma](https://www.figma.com/file/6b4R8evBkii6mI53IA4vSS/Online-Learning-Platform?type=design&node-id=0-1&mode=design&t=HBUPk2hRYW3ioAUj-0)    - Dribbble  #### *Frontend:*   - React (Vite) for dynamic and responsive UI.  #### *Backend:*   - Node.js, Express and Mongoose for server-side development.  #### *Database:*   - MongoDB for storing user profiles, course details, and application data.  #### *Authentication:*   - JWT (JSON Web Tokens) for secure authentication.  #### *Video Conferencing:*   - Integrate WebRTC for real-time video communication.   - Or just using google meet link  #### *Payment Integration:*   - Stripe or PayPal or razorpay for secure and seamless payments "
Online Distance Education System,MERN Stack,https://github.com/Pika003/e-Learning-Platform," # Online Learning Platform  Online Learning Platform using MERN Stack  ## Objective:  Develop a comprehensive online learning platform with three user types (Student, Teacher, Admin), featuring course creation, approval process, and live video conferencing.  ## *BACKEND for the PROJECT    - Backend is developed by [Parag](https://github.com/paragkadyan).  ## Features ![Screenshot 2024-05-14 212028](https://github.com/Pika003/e-Learning-Platform/assets/104189733/e2f9ce48-764b-48d2-8af1-188ea2918e8c)   #### 1. *User Authentication:*    - Student Login    - Teacher Login    - Admin Login       ![Screenshot 2024-05-14 211251](https://github.com/Pika003/e-Learning-Platform/assets/104189733/3179ba23-ae52-4ab5-8d0a-b2891cc43e0f)  ![Screenshot 2024-05-14 211154](https://github.com/Pika003/e-Learning-Platform/assets/104189733/377d8aa0-c35b-46d0-9408-f18b6ecb1ac1)  #### 2. *Application Approval:*    - Students and teachers can submit applications for approval.    - Admin validates and approves applications.       ![Screenshot 2024-05-15 212149](https://github.com/Pika003/e-Learning-Platform/assets/104189733/6e8afdba-a8a5-47e3-977c-f5292e136c3f)   #### 3. *Dashboard:*    - Students see purchased courses, progress, and communication options.    - Teachers view created courses, student enrollments, and communication features.        ![Screenshot 2024-05-14 211938](https://github.com/Pika003/e-Learning-Platform/assets/104189733/1008e68d-b683-4e8a-bc85-6d5890946724) ![Screenshot 2024-05-14 211854](https://github.com/Pika003/e-Learning-Platform/assets/104189733/c857a214-5366-49db-8035-13d2bfb88396) #### 4. *Course Purchase:*     - Students can browse and buy courses on the platform.       ![Screenshot 2024-05-14 211813](https://github.com/Pika003/e-Learning-Platform/assets/104189733/1578ca04-b85d-4c7b-8875-12f6756f2621)  #### 5. *Live Video Conferencing:*    - Integrated video conferencing tool (similar to Google Meet) for real-time teacher-student interaction.  #### 6. *Communication:*    - In-platform messaging system for communication between teachers and students.  #### 7. *Payment Integration:*    - Integrate a secure payment gateway for course purchases.  ----   ## *Tech Stack:*  #### *UI/UX:*   - [figma](https://www.figma.com/file/6b4R8evBkii6mI53IA4vSS/Online-Learning-Platform?type=design&node-id=0-1&mode=design&t=HBUPk2hRYW3ioAUj-0)    - Dribbble  #### *Frontend:*   - React (Vite) for dynamic and responsive UI.  #### *Backend:*   - Node.js, Express and Mongoose for server-side development.  #### *Database:*   - MongoDB for storing user profiles, course details, and application data.  #### *Authentication:*   - JWT (JSON Web Tokens) for secure authentication.  #### *Video Conferencing:*   - Integrate WebRTC for real-time video communication.   - Or just using google meet link  #### *Payment Integration:*   - Stripe or PayPal or razorpay for secure and seamless payments "
Farming Management System,MERN Stack,https://github.com/MohdSaif-1807/Agri-Assist-Project,"# Agri-Assist-Project  Welcome to Agri-Assist Project, an innovative agricultural application designed to tackle key challenges in farming using cutting-edge technology. This project, developed with the MERN (MongoDB, Express.js, React, Node.js) stack and integrated with a Flask API, encompasses two distinct modules. These modules cater to crop-related issues and enable yield marketing through an intuitive e-commerce platform.  ## Features  ### Crop-related Issues Module  - **Plant Disease Prediction:** Employing advanced algorithms to predict and identify plant diseases for proactive management. - **Crop Selection:** Utilizes detailed soil and climate information to recommend suitable crops for cultivation.  ### Yield Marketing Module  - **E-commerce Platform:** Empowering farmers to seamlessly post and sell their crops, creating a dynamic virtual marketplace. - **User Roles:** Designed for farmers and common people for purchasing.  ## Technologies Used  - **Frontend:** Developed with React, utilizing CSS frameworks such as MUI and react-bootstrap. - **Middleware:** Leveraging Express.js for efficient communication between frontend and backend. - **Backend:** Powered by Node.js for orchestrating complex operations. - **API:** Flask, providing additional functionality and seamless integration. - **Database:** MongoDB stores user credentials, while AWS S3 bucket is used for image storage,Now Google Cloud Platform has been introduced to store images through google-drive API  ## Installation  1. Clone the repository: `git clone https://github.com/MohdSaif-1807/agri-assist-project.git` 2. Navigate to the project directory: `cd agri-assist-project` 3. Install dependencies: `npm install` (for both frontend and backend) 4. Navigate to the project directory: `cd agri-assist-project/flask-backend` 5. Install dependencies: `pip install -r requirements.txt`  ## Usage  1. Start the backend server: `npm run start:backend` 2. Start the frontend development server: `npm run start:frontend` 3. Stare the flask-backend server:`python app.py`  ## Project Execution Screenshots  ### Farmer Section  #### Home Page ![Screenshot (1894)](https://github.com/MohdSaif-1807/Agri-Assist-Project/assets/113005309/2526c974-25ee-403e-aba9-c83442c1243d)  #### Sign Up Page ![Screenshot (1893)](https://github.com/MohdSaif-1807/Agri-Assist-Project/assets/113005309/e0dcca88-21db-4916-8cc0-d9b4bcfb6ea1)  #### Login Page ![Screenshot (1895)](https://github.com/MohdSaif-1807/Agri-Assist-Project/assets/113005309/a1042ebb-eb06-47df-a36c-49910d763e1e)  ####  Farmer Feature Section ![Screenshot (1897)](https://github.com/MohdSaif-1807/Agri-Assist-Project/assets/113005309/e923469f-d228-4374-b191-b29c288399ee)  ####  Crop Related Issues Section ![Screenshot (1898)](https://github.com/MohdSaif-1807/Agri-Assist-Project/assets/113005309/9b3f2e2c-7039-4906-8a3e-051a0df01119)  ####  Plant Disease Prediction ![Screenshot (1899)](https://github.com/MohdSaif-1807/Agri-Assist-Project/assets/113005309/19a32773-de18-4390-b85f-875c457d6d9a)  ####  Plant Disease Prediction Input ![Screenshot (1900)](https://github.com/MohdSaif-1807/Agri-Assist-Project/assets/113005309/977d317a-4402-4c12-9319-f442bbd8c388)  ####  Plant Disease Prediction Output-1 ![Screenshot (1901)](https://github.com/MohdSaif-1807/Agri-Assist-Project/assets/113005309/2b129b3a-7785-468d-a6ca-64e64a7df2b1)  #### Plant Disease Prediction Output-2 ![Screenshot (1902)](https://github.com/MohdSaif-1807/Agri-Assist-Project/assets/113005309/9bc30fc1-dd76-4974-89fb-1cfc0088a987)  #### Crop Prediction Section ![Screenshot (1903)](https://github.com/MohdSaif-1807/Agri-Assist-Project/assets/113005309/12e06f1f-48e4-4fd2-9105-338137cab430)  #### Crop Prediction Section Input ![Screenshot (1904)](https://github.com/MohdSaif-1807/Agri-Assist-Project/assets/113005309/b3c40a77-3bd2-4d0e-9274-6e14449dbf56)  #### Crop Prediction Output ![Screenshot (1905)](https://github.com/MohdSaif-1807/Agri-Assist-Project/assets/113005309/b043bfa5-b1d9-4285-9669-4d5a3c5fe994)  #### Farmer's Yeild Marketing Section-1 ![Screenshot (1906)](https://github.com/MohdSaif-1807/Agri-Assist-Project/assets/113005309/40ff6e08-e110-45e9-9454-53ccda5d8a0e)  #### Farmer's Yeild Marketing Section-2 ![Screenshot (1907)](https://github.com/MohdSaif-1807/Agri-Assist-Project/assets/113005309/faeff9ed-a2cb-4d16-b622-d78b09b9014c)  #### Posting New Item-1 ![Screenshot (1908)](https://github.com/MohdSaif-1807/Agri-Assist-Project/assets/113005309/7cdccbba-304e-4df0-889b-fd33ca15f8b7)  #### Posted Item Acknowledgement ![Screenshot (1909)](https://github.com/MohdSaif-1807/Agri-Assist-Project/assets/113005309/6762f69c-adef-41b0-aa87-1ef86bfbeb89)  #### New Item Posted Successfully ![Screenshot (1910)](https://github.com/MohdSaif-1807/Agri-Assist-Project/assets/113005309/8c44c682-9686-486f-b533-080eeae2092b)  #### Deleting Item ![Screenshot (1911)](https://github.com/MohdSaif-1807/Agri-Assist-Project/assets/113005309/7c1c2c09-8fa4-40ba-81ea-e32c8222c766)  #### Item Deleted Successfully ![Screenshot (1912)](https://github.com/MohdSaif-1807/Agri-Assist-Project/assets/113005309/ab5caded-b232-4a66-91ef-504f481600dc)   #### Normal User Section   #### User's Feature Section ![Screenshot (1915)](https://github.com/MohdSaif-1807/Agri-Assist-Project/assets/113005309/e7eab3b2-14c2-4f13-968e-0a173d9a8441)  #### User's Yeild Marketing Section ![Screenshot (1916)](https://github.com/MohdSaif-1807/Agri-Assist-Project/assets/113005309/df9b5acb-5c3e-4171-833e-f0de7361a10a)  #### Adding an item to cart ![Screenshot (1917)](https://github.com/MohdSaif-1807/Agri-Assist-Project/assets/113005309/2a32b988-af84-46ac-a09f-c9a30e1fcd08)  #### Acknowledgement for addition of an item ![Screenshot (1918)](https://github.com/MohdSaif-1807/Agri-Assist-Project/assets/113005309/5c1202f7-a55f-49b7-946b-5d085d1e0b1c)  #### Contact Us Section  ![Screenshot (1913)](https://github.com/MohdSaif-1807/Agri-Assist-Project/assets/113005309/30a8c222-2708-4986-a5b4-b821d017f97f)  #### Contact Us Form Sended Acknowledgement ![Screenshot (1918)](https://github.com/MohdSaif-1807/Agri-Assist-Project/assets/113005309/f0cdb0b9-2a4a-4c30-a72d-cc15861dfebd)  #### Mail response ![Screenshot (1919)](https://github.com/MohdSaif-1807/Agri-Assist-Project/assets/113005309/0f1b6705-4851-4de4-93a8-1694ac6a54bc)    "
Contact Keeping,MERN Stack,https://github.com/nil1729/contact-keeper,"# Contact Keeping App  - #### This site is live [here](https://contact--keeper.herokuapp.com/). - #### Github Repository [link](https://github.com/nil1729/contact-keeper).  ---  ### Modules used for this Project  1. _`NodeJS`_ is used for building Backend part of this Website. 2. _`MongoDB`_ used for Database to store data. 3. _`ExpressJS`_ is used as Backend Framework. 4. _`React JS`_ is used as frontend Framework for building SPA. 5. _`Bootswatch united Theme`_ is used as CSS Framework and the website is Responsive for all Devices. 6. _`JWT`_ is used for Authentication purpose. 7. Token will be stored in `Local Storage` of Client Browser. 8. Encrypt passwords with `bcrypt` 9. Add Some Transition Effects to Contacts using `react-transition-group`.  ---  ### Website Preview  ## <img src=""./demo.png"" alt="""">  <p style=""text-align: center;"">Made With<span style=""color: red;""> &#10084; </span>by <a href=""https://github.com/nil1729"" target=""_blank""> Nilanjan Deb </a> </p>"
eLearning Portal,MERN Stack,https://github.com/Pika003/e-Learning-Platform," # Online Learning Platform  Online Learning Platform using MERN Stack  ## Objective:  Develop a comprehensive online learning platform with three user types (Student, Teacher, Admin), featuring course creation, approval process, and live video conferencing.  ## *BACKEND for the PROJECT    - Backend is developed by [Parag](https://github.com/paragkadyan).  ## Features ![Screenshot 2024-05-14 212028](https://github.com/Pika003/e-Learning-Platform/assets/104189733/e2f9ce48-764b-48d2-8af1-188ea2918e8c)   #### 1. *User Authentication:*    - Student Login    - Teacher Login    - Admin Login       ![Screenshot 2024-05-14 211251](https://github.com/Pika003/e-Learning-Platform/assets/104189733/3179ba23-ae52-4ab5-8d0a-b2891cc43e0f)  ![Screenshot 2024-05-14 211154](https://github.com/Pika003/e-Learning-Platform/assets/104189733/377d8aa0-c35b-46d0-9408-f18b6ecb1ac1)  #### 2. *Application Approval:*    - Students and teachers can submit applications for approval.    - Admin validates and approves applications.       ![Screenshot 2024-05-15 212149](https://github.com/Pika003/e-Learning-Platform/assets/104189733/6e8afdba-a8a5-47e3-977c-f5292e136c3f)   #### 3. *Dashboard:*    - Students see purchased courses, progress, and communication options.    - Teachers view created courses, student enrollments, and communication features.        ![Screenshot 2024-05-14 211938](https://github.com/Pika003/e-Learning-Platform/assets/104189733/1008e68d-b683-4e8a-bc85-6d5890946724) ![Screenshot 2024-05-14 211854](https://github.com/Pika003/e-Learning-Platform/assets/104189733/c857a214-5366-49db-8035-13d2bfb88396) #### 4. *Course Purchase:*     - Students can browse and buy courses on the platform.       ![Screenshot 2024-05-14 211813](https://github.com/Pika003/e-Learning-Platform/assets/104189733/1578ca04-b85d-4c7b-8875-12f6756f2621)  #### 5. *Live Video Conferencing:*    - Integrated video conferencing tool (similar to Google Meet) for real-time teacher-student interaction.  #### 6. *Communication:*    - In-platform messaging system for communication between teachers and students.  #### 7. *Payment Integration:*    - Integrate a secure payment gateway for course purchases.  ----   ## *Tech Stack:*  #### *UI/UX:*   - [figma](https://www.figma.com/file/6b4R8evBkii6mI53IA4vSS/Online-Learning-Platform?type=design&node-id=0-1&mode=design&t=HBUPk2hRYW3ioAUj-0)    - Dribbble  #### *Frontend:*   - React (Vite) for dynamic and responsive UI.  #### *Backend:*   - Node.js, Express and Mongoose for server-side development.  #### *Database:*   - MongoDB for storing user profiles, course details, and application data.  #### *Authentication:*   - JWT (JSON Web Tokens) for secure authentication.  #### *Video Conferencing:*   - Integrate WebRTC for real-time video communication.   - Or just using google meet link  #### *Payment Integration:*   - Stripe or PayPal or razorpay for secure and seamless payments "
eLearning Portal,MERN Stack,https://github.com/Pika003/e-Learning-Platform," # Online Learning Platform  Online Learning Platform using MERN Stack  ## Objective:  Develop a comprehensive online learning platform with three user types (Student, Teacher, Admin), featuring course creation, approval process, and live video conferencing.  ## *BACKEND for the PROJECT    - Backend is developed by [Parag](https://github.com/paragkadyan).  ## Features ![Screenshot 2024-05-14 212028](https://github.com/Pika003/e-Learning-Platform/assets/104189733/e2f9ce48-764b-48d2-8af1-188ea2918e8c)   #### 1. *User Authentication:*    - Student Login    - Teacher Login    - Admin Login       ![Screenshot 2024-05-14 211251](https://github.com/Pika003/e-Learning-Platform/assets/104189733/3179ba23-ae52-4ab5-8d0a-b2891cc43e0f)  ![Screenshot 2024-05-14 211154](https://github.com/Pika003/e-Learning-Platform/assets/104189733/377d8aa0-c35b-46d0-9408-f18b6ecb1ac1)  #### 2. *Application Approval:*    - Students and teachers can submit applications for approval.    - Admin validates and approves applications.       ![Screenshot 2024-05-15 212149](https://github.com/Pika003/e-Learning-Platform/assets/104189733/6e8afdba-a8a5-47e3-977c-f5292e136c3f)   #### 3. *Dashboard:*    - Students see purchased courses, progress, and communication options.    - Teachers view created courses, student enrollments, and communication features.        ![Screenshot 2024-05-14 211938](https://github.com/Pika003/e-Learning-Platform/assets/104189733/1008e68d-b683-4e8a-bc85-6d5890946724) ![Screenshot 2024-05-14 211854](https://github.com/Pika003/e-Learning-Platform/assets/104189733/c857a214-5366-49db-8035-13d2bfb88396) #### 4. *Course Purchase:*     - Students can browse and buy courses on the platform.       ![Screenshot 2024-05-14 211813](https://github.com/Pika003/e-Learning-Platform/assets/104189733/1578ca04-b85d-4c7b-8875-12f6756f2621)  #### 5. *Live Video Conferencing:*    - Integrated video conferencing tool (similar to Google Meet) for real-time teacher-student interaction.  #### 6. *Communication:*    - In-platform messaging system for communication between teachers and students.  #### 7. *Payment Integration:*    - Integrate a secure payment gateway for course purchases.  ----   ## *Tech Stack:*  #### *UI/UX:*   - [figma](https://www.figma.com/file/6b4R8evBkii6mI53IA4vSS/Online-Learning-Platform?type=design&node-id=0-1&mode=design&t=HBUPk2hRYW3ioAUj-0)    - Dribbble  #### *Frontend:*   - React (Vite) for dynamic and responsive UI.  #### *Backend:*   - Node.js, Express and Mongoose for server-side development.  #### *Database:*   - MongoDB for storing user profiles, course details, and application data.  #### *Authentication:*   - JWT (JSON Web Tokens) for secure authentication.  #### *Video Conferencing:*   - Integrate WebRTC for real-time video communication.   - Or just using google meet link  #### *Payment Integration:*   - Stripe or PayPal or razorpay for secure and seamless payments "
NGO Connecting Website,MERN Stack,https://github.com/akarsh-jain-790/CharityConnect," # CHARITY CONNECT - CONNECTING ALL NGO'S The aim of our project is to centralized the idea of donating to different NGO's based in different places.                   This website connects user who wants to donate to the nearby NGO's.  ### Links front-end: https://6419279426b0ac3dc54ff5b4--endearing-kulfi-b38859.netlify.app/ back-end: https://charity-connect.up.railway.app/  ## Problem statement  Non-governmental organizations (NGOs) play a vital role in addressing various social, environmental, and humanitarian issues around the world. However, many NGOs face challenges in raising funds, reaching donors, and managing their resources effectively. There is no centralized platform that connects all NGOs and facilitates the process of donation for both donors and recipients. This leads to inefficiencies, duplication of efforts, lack of transparency, and reduced impact of NGO work.  ## Relevance:  Creating a centralized platform that connects all NGOs and streamlines the donation process would benefit both donors and recipients. Donors would be able to find NGOs that match their interests and preferences, donate easily and securely, track their donations‚Äô impact, and receive feedback from recipients. Recipients would be able to access more funding opportunities, showcase their work and achievements, communicate with donors directly, and receive guidance on best practices.  ## Objectives:  The main objective of this research is to design and develop a centralized platform that connects all NGOs and simplifies the donation process for both donors and recipients. The specific objectives are:   * To conduct a needs assessment of NGOs and donors regarding their current challenges and expectations for a centralized platform.  * To identify the features and functionalities that would make a centralized platform user-friendly, accessible, secure, transparent, and impactful.  * To prototype and test a centralized platform with selected NGOs and donors using an agile development approach.  * To evaluate the usability, satisfaction, adoption rate, effectiveness, efficiency ,and impact of the centralized platform on NGO work. ## ‚öôÔ∏èTech Stack  *   For developing this website we can use the MERN stack as our tech stack.The MERN stack is a JavaScript-based web development framework that stands for ***MongoDB***, ***Express.js***, ***React.js***, and ***Node.js***. These are the four key technologies that make up the MERN stack. Using the MERN stack, we can develop a NGO centralized website that has the following features:   * A responsive and user-friendly front-end that displays information about NGOs, their projects, their achievements, their needs, and their donors using React.js components.  * A secure and scalable back-end that handles requests from the front-end using Express.js routes and middleware.  * A RESTful API that communicates with MongoDB database using Node.js drivers to perform CRUD operations on data related to NGOs, donors, donations ,and feedbacks.  * A cloud-based database that stores data in MongoDB Atlas clusters using MongoDB Atlas service.   ## Our Team  - Akarsh jain       - [GitHub](https://github.com/akarsh-jain-790)      - [Linkedin](https://linkedin.com/in/akarshjain158) - Alok Pawar       - [GitHub](https://github.com/alok8bb)      - [LinkedIn](https://linkedin.com/in/alok8bb) - Maushish Yadav       - [Github](https://github.com/maushish)      - [LinkedIn](https://www.linkedin.com/in/maushish-yadav-6a2b1a251/)  - Dev        - [GitHub](https://github.com/devbadraj)      - [Linkedin](https://linkedin.com/in/dev-badraj-51736b258/) - Deepanshu      - [Linkedin](https://www.linkedin.com/in/deepanshu-lokhande-272922257/)"
Notes App With Login system,MERN Stack,https://github.com/arunopal/react-notes-app,"# React Notes App # Introduction A web application with **CRUD** (Create, Retrieve, Update, Delete) functionality built using **MERN** stack (MongoDB, Express, React, Node.js) that allows the user to view, create, update and delete notes in the form of visual cards. # Tech stacks used  - **[MongoDB](https://www.mongodb.com/)** - For database functionality  - **[Express](https://expressjs.com/)** - For handling server side (backend) functions  - **[React](https://react.dev/)** - For frontend and client side functions  - **[Node.js](https://nodejs.org/en)** - For backend functions # Approach ## Backend ### General  - Express routers are used to handle user and note functions.  - [Mongoose](https://mongoosejs.com/) is used to handle MongoDB connection.  ### Authentication  - HTTP **POST** requests are used for both sign up and log in.  - Signing up requires creating a username, entering email and password.  - Uses [bcrypt](https://www.npmjs.com/package/bcrypt) to encrypt passwords given by users before storing them in database.  - Before sign up, the API checks DB to ensure email does not already exist.  - For logging in, DB is checked to make sure user exists.   - Then a [JWT](https://jwt.io/) token is created after signing the payload (the user ID in the database) with a secret key and given an expiration time of 1 day.  - After successful login, this token is returned.  - For every operation involving notes, this token is sent in the appropriate HTTP request's header.  - If token expires, user is requested to login again. ### Database Models/Schema  - Two models - user and note  - User model contains username, email, and password (encrypted), also auto-generated user ID.  - Note model contains title, body and user ID of the user who created it, also auto-generated note ID. ### Creating Notes  - An HTTP **POST** request is sent with the aforementioned **token** in its header.  - Notes are in JSON form containing title and body.  - The user for which note is created is identified from token.  - Uses `save` method from Mongoose. ### Retrieving Notes  - An HTTP **GET** request is sent with the token in its header.  - The user ID is identified from token and the DB is searched for notes having user ID of current user.  - Uses `find` method from Mongoose.  - The notes are then sent back in JSON form. ### Updating Notes  - An HTTP **PATCH** request is sent to the API whose header contains the token as well as the ID of the required note.  - The new title and body is sent in JSON form.  - It uses `findByIdAndUpdate` method from Mongoose. ### Deleting Notes  - An HTTP **DELETE** request is sent to the API with the token and required note ID in its header.  - It uses `findByIdAndDelete` method from Mongoose.  All methods send appropriate status messages (successful/unsuccessful) upon completion.  ## Frontend ### Components  - The look and feel of the frontend as well as several components like navigation bar, note cards, buttons, dialog boxes and avatars have been derived from [Chakra UI](https://chakra-ui.com/).  - It has 4 pages: Home, Login, Sign up and Notes.  - Components used - Navigation bar, Note card, Modal, Avatar, Menu, Button. #### Home page  Contains links to log in and sign up #### Sign up page  - User has to fill up the required fields and click on Sign Up button  - `useState()` function is used to set default values for username, email, password.  - The input given by user is then passed on to the API by Axios.  - After successful sign up, user is taken to login page. #### Login page  - User has to fill up the required fields and click on Log In button  - `useState()` function is used to set default values for email and password.  - The input given by user is then passed on to the API by Axios.  - After successful login, user is taken to notes page. #### Notes page  - Notes are displayed in the form of visual cards.  - `useEffect()` function is used to get notes and display them.  - On clicking '+' button user can create notes.  - `useDisclosure()` function is used to open a dialog box/Modal where user can enter title and body of note.  - On clicking 'Create' button, create notes function is dispatched from Note redux.  - Or user can click 'Close' button to cancel operation. #### Note Card  - Contains title and body of note  - Contains two buttons: Update and Delete.  - On clicking 'Update' button a dialog box is opened where user can enter the new title and body of note.  - On clicking 'Update' button update notes function is dispatched.  - On clicking 'Delete' button delete notes function is dispatched from Note redux. #### Navigation Bar  - It uses `useSelector()` function to find out the current state of the application.  - When not logged in, it contains buttons for Log In and Sign Up.  - On Log In, it displays an avatar for the user and a button 'All Notes'.  - On clicking the avatar a dropdown menu is displayed containing the username and Logout button.  - Clicking 'Logout' button will dispatch the logout state from User redux. ### Routing  - The Notes page (and in turn all notes functionality) is routed through a private route that first checks whether user is logged in or not.  - It does so by using `useSelector()` function that takes as a parameter the selector function which takes in the current state of the program from the Redux store. ### Redux  - Two types: User redux and Note redux  - User redux contains various states such as initial state, login loading state, login successful state, login error state and logout state (leading back to initial state).  - On successful login, the user ID and the token generated are returned.  - The login is handled by [Axios](https://www.npmjs.com/package/axios) which makes the proper HTTP request and returns the response.  - Note redux contains 3 types of states - get notes, update notes and delete notes, each of which are then divided into 3 states - loading, success and error.  - All note actions are also handled by Axios and HTTP requests are made according to the format given in backend.  - The `useDispatch()` function is used to transmit the Redux states for operating the frontend.  - Both user and note reducers are combined and stored with `redux-thunk` middleware applied.  - It allows us to handle asynchronous actions and side effects by dispatching actions from within the thunk. ## Deployment  - The backend API is deployed on [Cyclic.sh](https://www.cyclic.sh/)  - The frontend is deployed on [Vercel](https://vercel.com/)"
Fake Product Identification,Blockchain,https://github.com/A4ANK/Fake-Product-Identification,"# Fake Product Identification using Blockchain  ## Packages Required:- - Truffle v5.6.7 (core: 5.6.7) - Ganache v7.5.0 - Solidity v0.5.16 (solc-js) - Node v15.8.0 - Web3.js v1.7.4 - npm 7.5.1  ## Other Requirements:- 1. Any chromium based browser i.e. Chrome  2. Metamask browser extension      ## setup process   1. Clone the project ``` git clone https://github.com/A4ANK/Fake-Product-Identification.git ``` 2. Go to the project folder, open terminal there and run following command to install required node_modules:- ``` npm install ``` 3. Compile contract source files. (Compilation and deployment can be done using truffle migrate):- ``` truffle compile ``` 4. Open Ganache, (to setup local blockchain)     - crerate new workspace     - add truffle-config.js  in truffle project      - change port to 7545 in server settings (same as port in truffle-config.js) 5. In chrome, open metamask     - add new test network using           - NETWORK ID (i.e. 5777 ,from Ganache Server settings)          - RPC SERVER (i.e HTTP://127.0.0.1:8545 ,from Ganache Server settings)         - CHAIN CODE (i.e. 1337)    - import account using private key of any account from local blockchain available in Ganache. 6. In terminal, run following commands:- - Run migrations to deploy contracts. ``` truffle migrate ```  - To start a server and it will open a homepage (index.html) file in the default browser. ``` npm run dev  ```  7. Login to metamask ,and connect the added account to local blockchain (i.e.localhost:3000) 8. Interact with website"
Electronic Voting System,Blockchain,https://github.com/Krish-Depani/Decentralized-Voting-System,"# Decentralized-Voting-System-Using-Ethereum-Blockchain  #### The Decentralized Voting System using Ethereum Blockchain is a secure and transparent solution for conducting elections. Leveraging Ethereum's blockchain technology, this system ensures tamper-proof voting records, enabling users to cast their votes remotely while maintaining anonymity and preventing fraud. Explore this innovative project for trustworthy and decentralized voting processes. #### For a cool demo of this project watch this [YouTube video](https://www.youtube.com/watch?v=a5CJ70D2P-E). #### For more details checkout [Project Report](https://github.com/Krish-Depani/Decentralized-Voting-System-Using-Ethereum-Blockchain/blob/main/Project%20Report%20github.pdf). #### PS: This project is not maintained anymore.  ## Features -  Implements JWT for secure voter authentication and authorization. -  Utilizes Ethereum blockchain for tamper-proof and transparent voting records. -  Removes the need for intermediaries, ensuring a trustless voting process. -  Admin panel to manage candidates, set voting dates, and monitor results. -  Intuitive UI for voters to cast votes and view candidate information.  ## Requirements - Node.js (version ‚Äì 18.14.0) - Metamask - Python (version ‚Äì 3.9) - FastAPI - MySQL Database (port ‚Äì 3306)  ## Screenshots  ![Login Page](https://github.com/Krish-Depani/Decentralized-Voting-System-Using-Ethereum-Blockchain/blob/main/public/login%20ss.png)  ![Admin Page](https://github.com/Krish-Depani/Decentralized-Voting-System-Using-Ethereum-Blockchain/blob/main/public/admin%20ss.png)  ![Voter Page](https://github.com/Krish-Depani/Decentralized-Voting-System-Using-Ethereum-Blockchain/blob/main/public/index%20ss.png)  ## Installation  1. Open a terminal.  2. Clone the repository by using the command                  git clone https://github.com/Krish-Depani/Decentralized-Voting-System-Using-Ethereum-Blockchain.git  3. Download and install [Ganache](https://trufflesuite.com/ganache/).  4. Create a workspace named <b>developement</b>, in the truffle projects section add `truffle-config.js` by clicking `ADD PROJECT` button.  5. Download [Metamask](https://metamask.io/download/) extension for the browser.  6. Now create wallet (if you don't have one), then import accounts from ganache.  7. Add network to the metamask. ( Network name - Localhost 7575, RPC URl - http://localhost:7545, Chain ID - 1337, Currency symbol - ETH)  8. Open MySQL and create database named <b>voter_db</b>. (DON'T USE XAMPP)  9. In the database created, create new table named <b>voters</b> in the given format and add some values.             CREATE TABLE voters (            voter_id VARCHAR(36) PRIMARY KEY NOT NULL,            role ENUM('admin', 'user') NOT NULL,            password VARCHAR(255) NOT NULL            );    <br>          +--------------------------------------+-------+-----------+         | voter_id                             | role  | password  |         +--------------------------------------+-------+-----------+         |                                      |       |           |         +--------------------------------------+-------+-----------+  12. Install truffle globally              npm install -g truffle  14. Go to the root directory of repo and install node modules          npm install  15. Install python dependencies          pip install fastapi mysql-connector-python pydantic python-dotenv uvicorn uvicorn[standard] PyJWT  ## Usage  #### Note: Update the database credentials in the `./Database_API/.env` file.  1. Open terminal at the project directory  2. Open Ganache and it's <b>development</b> workspace.  3. open terminal in project's root directory and run the command          truffle console    then compile the smart contracts with command          compile    exit the truffle console  5. Bundle app.js with browserify              browserify ./src/js/app.js -o ./src/dist/app.bundle.js  2. Start the node server server              node index.js  3. Navigate to `Database_API` folder in another terminal              cd Database_API     then start the database server by following command          uvicorn main:app --reload --host 127.0.0.1  4. In a new terminal migrate the truffle contract to local blockchain              truffle migrate  You're all set! The Voting app should be up and running now at http://localhost:8080/.<br> For more info about usage checkout [YouTube video](https://www.youtube.com/watch?v=a5CJ70D2P-E).  ## Code Structure      ‚îú‚îÄ‚îÄ blockchain-voting-dapp            # Root directory of the project.         ‚îú‚îÄ‚îÄ build                         # Directory containing compiled contract artifacts.         |   ‚îî‚îÄ‚îÄ contracts                          |       ‚îú‚îÄ‚îÄ Migrations.json                |       ‚îî‚îÄ‚îÄ Voting.json                    ‚îú‚îÄ‚îÄ contracts                     # Directory containing smart contract source code.         |   ‚îú‚îÄ‚îÄ 2_deploy_contracts.js              |   ‚îú‚îÄ‚îÄ Migrations.sol                     |   ‚îî‚îÄ‚îÄ Voting.sol                         ‚îú‚îÄ‚îÄ Database_API                  # API code for database communication.         |   ‚îî‚îÄ‚îÄ main.py                            ‚îú‚îÄ‚îÄ migrations                    # Ethereum contract deployment scripts.         |   ‚îî‚îÄ‚îÄ 1_initial_migration.js             ‚îú‚îÄ‚îÄ node_modules                  # Node.js modules and dependencies.         ‚îú‚îÄ‚îÄ public                        # Public assets like favicon.         |   ‚îî‚îÄ‚îÄ favicon.ico                        ‚îú‚îÄ‚îÄ src                                    |   ‚îú‚îÄ‚îÄ assets                    # Project images.         |   |   ‚îî‚îÄ‚îÄ eth5.jpg                       |   ‚îú‚îÄ‚îÄ css                       # CSS stylesheets.         |   |   ‚îú‚îÄ‚îÄ admin.css                      |   |   ‚îú‚îÄ‚îÄ index.css                      |   |   ‚îî‚îÄ‚îÄ login.css                      |   ‚îú‚îÄ‚îÄ dist                      # Compiled JavaScript bundles.         |   |   ‚îú‚îÄ‚îÄ app.bundle.js                  |   |   ‚îî‚îÄ‚îÄ login.bundle.js                |   ‚îú‚îÄ‚îÄ html                      # HTML templates.         |   |   ‚îú‚îÄ‚îÄ admin.html                     |   |   ‚îú‚îÄ‚îÄ index.html                     |   |   ‚îî‚îÄ‚îÄ login.html                     |   ‚îî‚îÄ‚îÄ js                        # JavaScript logic files.         |       ‚îú‚îÄ‚îÄ app.js                         |       ‚îî‚îÄ‚îÄ login.js                       ‚îú‚îÄ‚îÄ index.js                      # Main entry point for Node.js application.         ‚îú‚îÄ‚îÄ package.json                  # Node.js package configuration.         ‚îú‚îÄ‚îÄ package-lock.json             # Lockfile for package dependencies.         ‚îú‚îÄ‚îÄ README.md                     # Project documentation.         ‚îî‚îÄ‚îÄ truffle-config.js                    # Truffle configuration file.  ## License  The code in this repository is licensed under the MIT License. This means that you are free to use, modify, and distribute the code, as long as you include the original copyright and license notice. For more information about LICENSE please click [here](https://github.com/Krish-Depani/Decentralized-Voting-System-Using-Ethereum-Blockchain/blob/main/LICENSE).  ## If you like this project, please give it a üåü. ## Thank you üòä."
Insurance Management System,Blockchain,https://github.com/IBM/build-blockchain-insurance-app,"# WARNING: This repository is no longer maintained :warning:  > This repository will not be updated. I will check periodically for pull requests, but do not expect a quick response.   *Read this in other languages: [‰∏≠ÂõΩË™û](README-cn.md),[Êó•Êú¨Ë™û](README-ja.md).*  # Build Blockchain Insurance Application  This project showcases the use of blockchain in insurance domain for claim processing. In this application, we have four participants, namely insurance, police, repair shop and the shop. Furthermore, each participant will own its own peer node. The insurance peer is the insurance company providing the insurance for the products and it is responsible for processing the claims. Police peer is responsible for verifying the theft claims. Repair shop peer is responsible for repairs of the product while shop peer sells the products to consumer. The value of running this network on the IBM Blockchain Platform is that  you can easily customize the network infrastructure as needed, whether  that is the location of the nodes, the CPU and RAM of the hardware, the endorsement policy needed to reach consensus, or adding new organizations and members to the network.    *Note:* This code pattern can either be run locally, or connected to the IBM Blockchain Platform. <b>If you only care  about running this pattern locally, please find the local instructions [here](./README-local.md).</b>  Audience level : Intermediate Developers  When the reader has completed this code pattern, they will understand how to:  * Create a Kubernetes Cluster using the IBM Kubernetes Service * Create an IBM Blockchain service, and launch the service onto the Kubernetes cluster * Create a network, including all relevant components, such as Certificate Authority, MSP (Membership Service Providers),   peers, orderers, and channels. * Deploy a packaged smart contract onto the IBM Blockchain Platform by installing and instantiating it on the peers. * Use the connection profile from IBM Blockchain Platform to create application admins, and submit transactions from our  client application. * View transaction details on our channel from IBM Blockchain Platform.  ## Application Workflow Diagram ![Workflow](images/app-arch.png)  1. The blockchain operator creates a IBM Kubernetes Service cluster (<b>32CPU, 32RAM, 3 workers recommended</b>) and an IBM Blockchain  Platform 2.0 service. 2. The IBM Blockchain Platform 2.0 creates a Hyperledger Fabric network on an IBM Kubernetes  Service, and the operator installs and instantiates the smart contract on the network. 3. The Node.js application server uses the Fabric SDK to interact with the deployed network on IBM Blockchain Platform 2.0. 4. The React UI uses the Node.js application API to interact and submit transactions to the network. 5. The user interacts with the insurance application web interface to update and query the blockchain ledger and state.  ## Included Components *	[IBM Blockchain Platform V2 Beta](https://console.bluemix.net/docs/services/blockchain/howto/ibp-v2-deploy-iks.html#ibp-v2-deploy-iks) gives you total control of your blockchain network with a user interface that can simplify and accelerate your journey to deploy and manage blockchain components on the IBM Cloud Kubernetes Service. *	[IBM Cloud Kubernetes Service](https://www.ibm.com/cloud/container-service) creates a cluster of compute hosts and deploys highly available containers. A Kubernetes cluster lets you securely manage the resources that you need to quickly deploy, update, and scale applications.   ## Featured technologies * [Hyperledger Fabric v1.4](https://hyperledger-fabric.readthedocs.io) is a platform for distributed ledger solutions, underpinned by a modular architecture that delivers high degrees of confidentiality, resiliency, flexibility, and scalability. * [Node.js](https://nodejs.org) is an open source, cross-platform JavaScript run-time environment that executes server-side JavaScript code. * [React](https://reactjs.org/) A declarative, efficient, and flexible JavaScript library for building user interfaces. * [Docker](https://www.docker.com/) Docker is a computer program that performs operating-system-level virtualization. It was first released in 2013 and is developed by Docker, Inc.  # Watch the Video - Multiple Organization and Multiple Peer App Demo #1 - Intro   [![](images/part1.png)](https://www.youtube.com/watch?v=Lr3EFKayP0o)  # Watch the Video - IBM Blockchain Tutorial: Multiple Organization and Multiple Peer App Demo #2 - Build Nodes   [![](images/part2.png)](https://www.youtube.com/watch?v=wi6i28vRigs)  # Watch the Video - Multiple Organization and Multiple Peer App Demo #1 - Intro   [![](images/part3.png)](https://www.youtube.com/watch?v=3dRckqZvFqw)  ## Prerequisites We find that Blockchain can be finicky when it comes to installing Node. We want to share this [StackOverflow response](https://stackoverflow.com/questions/49744276/error-cannot-find-module-api-hyperledger-composer) - because many times the errors you see with Compose are derived in having installed either the wrong Node version or took an approach that is not supported by Compose:  * [IBM Cloud account](https://cloud.ibm.com/registration) * [Docker](https://www.docker.com/products) - latest * [Docker Compose](https://docs.docker.com/compose/overview/) - latest * [NPM](https://www.npmjs.com/get-npm) - latest * [nvm]() - latest * [Node.js](https://nodejs.org/en/download/) - Node v8.9.x * [Git client](https://git-scm.com/downloads) - latest * **[Python](https://www.python.org/downloads/) - 2.7.x** * [React](https://reactjs.org/) - 15.6.1  # Steps  # Steps (Local Network) > To run a local network, you can find steps [here](./README-local.md)  ### Steps (Cloud Network)  1. [Create IBM Cloud services](#step-1-Create-IBM-Cloud-services) 2. [Build a network - Certificate Authority](#step-2-Build-a-network---Certificate-Authority) 3. [Build a network - Create MSP Definitions](#step-3-Build-a-network---Create-MSP-Definitions) 4. [Build a network - Create Peers](#step-4-build-a-network---Create-Peers) 5. [Build a network - Create Orderer](#step-5-Build-a-network---Create-Orderer) 6. [Build a network - Create and Join Channel](#step-6-Build-a-network---Create-and-Join-Channel) 7. [Deploy Insurance Smart Contract on the network](#step-7-Deploy-Insurance-Smart-Contract-on-the-network) 8. [Connect application to the network](#step-8-Connect-application-to-the-network) 9. [Enroll App Admin Identities](#step-9-Enroll-App-Admin-Identities) 10. [Run the application](#step-10-Run-the-application)  **Important Note:** This pattern is more advanced because it uses four organizations. For this reason, you will likely have to get a paid kubernetes cluster to run this pattern on the cloud, since a free cluster will not have the CPU/storage  necessary to deploy all of the pods that we need to run this pattern. There are other patterns that leverage a free Kubernetes cluster (and only two organizations), so if you want to try that one out first, go [here](https://github.com/IBM/blockchainbean2).   ## Step 1. Create IBM Cloud services  * Create the [IBM Cloud Kubernetes Service](https://cloud.ibm.com/catalog/infrastructure/containers-kubernetes).  You can  find the service in the `Catalog`. Note that <b>for this code pattern, we need to use the 32CPU, 32GB RAM cluster.</b>  * Once you reach the <b> create a new cluster page </b> you will need to do the following:   - Choose <b>standard</b> cluster type   - Fill out cluster name   - choose Geography: <b>North America</b>   - Choose Location and availability: <b>Multizone</b>   - Choose Metro: <b>Dallas</b>   - Choose Worker nodes: <b>Dallas 10 only</b>   - Choose Master service endpoint: <b>Both private & public endpoints</b>   - Choose Default worker pool: <b>1.12.7 (Stable, Default)</b>   - Choose Master service endpoint: <b>Both private & public endpoints</b>   - Choose Flavor <b>32 Cores 32GB RAM, Ubuntu 18</b>   - Choose Encrypt local disk <b>Yes</b>   - Choose Worker nodes <b>3</b>   - Click on <b>create cluster. </b> <b>The cluster takes around 15-20 minutes to provision, so please be patient!</b>  <br> <p align=""center"">   <img src=""images/gifs/createCluster.gif""> </p> <br>  * After your kubernetes cluster is up and running, you can deploy your IBM Blockchain Platform V2 Beta on the cluster.  The service walks through few steps and finds your cluster on the IBM Cloud to deploy the service on.  <br> <p align=""center"">   <img src=""images/gifs/deploy-blockchain-on-cluster.gif""> </p> <br>  * Once the Blockchain Platform is deployed on the Kubernetes cluster, you can launch the console to start operating on your blockchain network.  <br> <p align=""center"">   <img src=""images/gifs/launch-ibm-blockchain.gif""> </p> <br>  ## Step 2. Build a network - Certificate Authority  We will build a network as provided by the IBM Blockchain Platform [documentation](https://console.bluemix.net/docs/services/blockchain/howto/ibp-console-build-network.html#ibp-console-build-network).  This will include creating a channel with a single peer organization with its own MSP and CA (Certificate Authority), and an orderer organization with its own MSP and CA. We will create the respective identities to deploy peers and operate nodes.  * #### Create your insurance organization CA    - Click <b>Add Certificate Authority</b>.   - Click <b>IBM Cloud</b> under <b>Create Certificate Authority</b> and <b>Next</b>.   - Give it a <b>Display name</b> of `Insurance CA`.     - Specify an <b>Admin ID</b> of `admin` and <b>Admin Secret</b> of `adminpw`.  <br> <p align=""center"">   <img src=""images/gifs/insuranceIBP2.gif""> </p> <br>  * #### Create your shop organization CA (process is same as shown in gif above)   - Click <b>Add Certificate Authority</b>.   - Click <b>IBM Cloud</b> under <b>Create Certificate Authority</b> and <b>Next</b>.   - Give it a <b>Display name</b> of `Shop CA`.     - Specify an <b>Admin ID</b> of `admin` and <b>Admin Secret</b> of `adminpw`.  * #### Create your repair shop organization CA (process is same as shown in gif above)   - Click <b>Add Certificate Authority</b>.   - Click <b>IBM Cloud</b> under <b>Create Certificate Authority</b> and <b>Next</b>.   - Give it a <b>Display name</b> of `Repair Shop CA`.     - Specify an <b>Admin ID</b> of `admin` and <b>Admin Secret</b> of `adminpw`.  * #### Create your police organization CA (process is same as shown in gif above)   - Click <b>Add Certificate Authority</b>.   - Click <b>IBM Cloud</b> under <b>Create Certificate Authority</b> and <b>Next</b>.   - Give it a <b>Display name</b> of `Police CA`.     - Specify an <b>Admin ID</b> of `admin` and <b>Admin Secret</b> of `adminpw`.   * #### Use your CA to register insurance identities   - Select the <b>Insurance CA</b> Certificate Authority that we created.   - First, we will register an admin for our Insurance Organization. Click on the <b>Register User</b> button.  Give an <b>Enroll ID</b> of `insuranceAdmin`, and <b>Enroll Secret</b> of `insuranceAdminpw`.  Click <b>Next</b>.  Set the <b>Type</b> for this identity as `client` and select `org1` from the affiliated organizations drop-down list. We will leave the <b>Maximum enrollments</b> and <b>Add Attributes</b> fields blank.   - We will repeat the process to create an identity of the peer. Click on the <b>Register User</b> button.  Give an <b>Enroll ID</b> of `insurancePeer`, and <b>Enroll Secret</b> of `insurancePeerpw`.  Click <b>Next</b>.  Set the <b>Type</b> for this identity as `peer` and select `org1` from the affiliated organizations drop-down list. We will leave the <b>Maximum enrollments</b> and <b>Add Attributes</b> fields blank.  <br> <p align=""center"">   <img src=""images/gifs/registerInsuranceAdminPeer.gif""> </p> <br>  * #### Use your CA to register shop identities (process is same as shown in gif above)   - Select the <b>Shop CA</b> Certificate Authority that we created.   - First, we will register an admin for our Insurance Organization. Click on the <b>Register User</b> button.  Give an <b>Enroll ID</b> of `shopAdmin`, and <b>Enroll Secret</b> of `shopAdminpw`.  Click <b>Next</b>.  Set the <b>Type</b> for this identity as `client` and select `org1` from the affiliated organizations drop-down list. We will leave the <b>Maximum enrollments</b> and <b>Add Attributes</b> fields blank.   - We will repeat the process to create an identity of the peer. Click on the <b>Register User</b> button.  Give an <b>Enroll ID</b> of `shopPeer`, and <b>Enroll Secret</b> of `shopPeerpw`.  Click <b>Next</b>.  Set the <b>Type</b> for this identity as `peer` and select `org1` from the affiliated organizations drop-down list. We will leave the <b>Maximum enrollments</b> and <b>Add Attributes</b> fields blank.  * #### Use your CA to register repair shop identities (process is same as shown in gif above)   - Select the <b>Repair Shop CA</b> Certificate Authority that we created.   - First, we will register an admin for our Insurance Organization. Click on the <b>Register User</b> button.  Give an <b>Enroll ID</b> of `repairShopAdmin`, and <b>Enroll Secret</b> of `repairShopAdminpw`.  Click <b>Next</b>.  Set the <b>Type</b> for this identity as `client` and select `org1` from the affiliated organizations drop-down list. We will leave the <b>Maximum enrollments</b> and <b>Add Attributes</b> fields blank.   - We will repeat the process to create an identity of the peer. Click on the <b>Register User</b> button.  Give an <b>Enroll ID</b> of `repairShopPeer`, and <b>Enroll Secret</b> of `repairShopPeerpw`.  Click <b>Next</b>.  Set the <b>Type</b> for this identity as `peer` and select `org1` from the affiliated organizations drop-down list. We will leave the <b>Maximum enrollments</b> and <b>Add Attributes</b> fields blank.  * #### Use your CA to register police shop identities (process is same as shown in gif above)   - Select the <b>Police CA</b> Certificate Authority that we created.   - First, we will register an admin for our Insurance Organization. Click on the <b>Register User</b> button.  Give an <b>Enroll ID</b> of `policeAdmin`, and <b>Enroll Secret</b> of `policeAdminpw`.  Click <b>Next</b>.  Set the <b>Type</b> for this identity as `client` and select `org1` from the affiliated organizations drop-down list. We will leave the <b>Maximum enrollments</b> and <b>Add Attributes</b> fields blank.   - We will repeat the process to create an identity of the peer. Click on the <b>Register User</b> button.  Give an <b>Enroll ID</b> of `policePeer`, and <b>Enroll Secret</b> of `policePeerpw`.  Click <b>Next</b>.  Set the <b>Type</b> for this identity as `peer` and select `org1` from the affiliated organizations drop-down list. We will leave the <b>Maximum enrollments</b> and <b>Add Attributes</b> fields blank.   ## Step 3. Build a network - Create MSP Definitions  * #### Create the insurance MSP definition   - Navigate to the <b>Organizations</b> tab in the left navigation and click <b>Create MSP definition</b>.   - Enter the <b>MSP Display name</b> as `Insurance MSP` and an <b>MSP ID</b> of `insurancemsp`.   - Under <b>Root Certificate Authority</b> details, specify the peer CA that we created `Insurance CA` as the root CA for the organization.   - Give the <b>Enroll ID</b> and <b>Enroll secret</b> for your organization admin, `insuranceAdmin` and `insuranceAdminpw`. Then, give the Identity name, `Insurance Admin`.   - Click the <b>Generate</b> button to enroll this identity as the admin of your organization and export the identity to the wallet. Click <b>Export</b> to export the admin certificates to your file system. Finally click <b>Create MSP definition</b>.  <br> <p align=""center"">   <img src=""images/gifs/insuranceMSP.gif""> </p> <br>  * #### Create the shop MSP definition (same process as shown in gif above)   - Navigate to the <b>Organizations</b> tab in the left navigation and click <b>Create MSP definition</b>.   - Enter the <b>MSP Display name</b> as `Shop MSP` and an <b>MSP ID</b> of `shopmsp`.   - Under <b>Root Certificate Authority</b> details, specify the peer CA that we created `Shop CA` as the root CA for the organization.   - Give the <b>Enroll ID</b> and <b>Enroll secret</b> for your organization admin, `shopAdmin` and `shopAdminpw`. Then, give the Identity name, `Shop Admin`.   - Click the <b>Generate</b> button to enroll this identity as the admin of your organization and export the identity to the wallet. Click <b>Export</b> to export the admin certificates to your file system. Finally click <b>Create MSP definition</b>.  * #### Create the repair shop MSP definition (same process as shown in gif above)   - Navigate to the <b>Organizations</b> tab in the left navigation and click <b>Create MSP definition</b>.   - Enter the <b>MSP Display name</b> as `Repair Shop MSP` and an <b>MSP ID</b> of `repairshopmsp`.   - Under <b>Root Certificate Authority</b> details, specify the peer CA that we created `Repair Shop CA` as the root CA for the organization.   - Give the <b>Enroll ID</b> and <b>Enroll secret</b> for your organization admin, `repairShopAdmin` and `repairShopAdminpw`. Then, give the Identity name, `Repair Shop Admin`.   - Click the <b>Generate</b> button to enroll this identity as the admin of your organization and export the identity to the wallet. Click <b>Export</b> to export the admin certificates to your file system. Finally click <b>Create MSP definition</b>.  * #### Create the police MSP definition (same process as shown in gif above)   - Navigate to the <b>Organizations</b> tab in the left navigation and click <b>Create MSP definition</b>.   - Enter the <b>MSP Display name</b> as `Police MSP` and an <b>MSP ID</b> of `policemsp`.   - Under <b>Root Certificate Authority</b> details, specify the peer CA that we created `Police CA` as the root CA for the organization.   - Give the <b>Enroll ID</b> and <b>Enroll secret</b> for your organization admin, `policeAdmin` and `policeAdminpw`. Then, give the Identity name, `Police Admin`.   - Click the <b>Generate</b> button to enroll this identity as the admin of your organization and export the identity to the wallet. Click <b>Export</b> to export the admin certificates to your file system. Finally click <b>Create MSP definition</b>.   ## Step 4. Build a network - Create Peers  * Create an insurance peer   - On the <b>Nodes</b> page, click <b>Add peer</b>.   - Click <b>IBM Cloud</b> under Create a new peer and <b>Next</b>.   - Give your peer a <b>Display name</b> of `Insurance Peer`.   - On the next screen, select `Insurance CA` as your <b>Certificate Authority</b>. Then, give the <b>Enroll ID</b> and <b>Enroll secret</b> for the peer identity that you created for your peer, `insurancePeer`, and `insurancePeerpw`. Then, select the <b>Administrator Certificate (from MSP)</b>, `Insurance MSP`, from the drop-down list and click <b>Next</b>.   - Give the <b>TLS Enroll ID</b>, `admin`, and <b>TLS Enroll secret</b>, `adminpw`, the same values are the Enroll ID and Enroll secret that you gave when creating the CA.  Leave the <b>TLS CSR hostname</b> blank.   - The last side panel will ask you to <b>Associate an identity</b> and make it the admin of your peer. Select your peer admin identity `Insurance Admin`.   - Review the summary and click <b>Submit</b>.  <br> <p align=""center"">   <img src=""images/gifs/addInsurancePeer.gif""> </p> <br>   * Create a shop peer (same process as shown in gif above)   - On the <b>Nodes</b> page, click <b>Add peer</b>.   - Click <b>IBM Cloud</b> under Create a new peer and <b>Next</b>.   - Give your peer a <b>Display name</b> of `Shop Peer`.   - On the next screen, select `Shop CA` as your <b>Certificate Authority</b>. Then, give the <b>Enroll ID</b> and <b>Enroll secret</b> for the peer identity that you created for your peer, `shopPeer`, and `shopPeerpw`. Then, select the <b>Administrator Certificate (from MSP)</b>, `Shop MSP`, from the drop-down list and click <b>Next</b>.   - Give the <b>TLS Enroll ID</b>, `admin`, and <b>TLS Enroll secret</b>, `adminpw`, the same values are the Enroll ID and Enroll secret that you gave when creating the CA.  Leave the <b>TLS CSR hostname</b> blank.   - The last side panel will ask you to <b>Associate an identity</b> and make it the admin of your peer. Select your peer admin identity `Shop Admin`.   - Review the summary and click <b>Submit</b>.  * Create a repair shop peer (same process as shown in gif above)   - On the <b>Nodes</b> page, click <b>Add peer</b>.   - Click <b>IBM Cloud</b> under Create a new peer and <b>Next</b>.   - Give your peer a <b>Display name</b> of `Repair Shop Peer`.   - On the next screen, select `Repair Shop CA` as your <b>Certificate Authority</b>. Then, give the <b>Enroll ID</b> and <b>Enroll secret</b> for the peer identity that you created for your peer, `repairShopPeer`, and `repairShopPeerpw`. Then, select the <b>Administrator Certificate (from MSP)</b>, `Repair Shop MSP`, from the drop-down list and click <b>Next</b>.   - Give the <b>TLS Enroll ID</b>, `admin`, and <b>TLS Enroll secret</b>, `adminpw`, the same values are the Enroll ID and Enroll secret that you gave when creating the CA.  Leave the <b>TLS CSR hostname</b> blank.   - The last side panel will ask you to <b>Associate an identity</b> and make it the admin of your peer. Select your peer admin identity `Repair Shop Admin`.   - Review the summary and click <b>Submit</b>.  * Create a police peer (same process as shown in gif above)   - On the <b>Nodes</b> page, click <b>Add peer</b>.   - Click <b>IBM Cloud</b> under Create a new peer and <b>Next</b>.   - Give your peer a <b>Display name</b> of `Police Peer`.   - On the next screen, select `Police CA` as your <b>Certificate Authority</b>. Then, give the <b>Enroll ID</b> and <b>Enroll secret</b> for the peer identity that you created for your peer, `policePeer`, and `policePeerpw`. Then, select the <b>Administrator Certificate (from MSP)</b>, `Police MSP`, from the drop-down list and click <b>Next</b>.   - Give the <b>TLS Enroll ID</b>, `admin`, and <b>TLS Enroll secret</b>, `adminpw`, the same values are the Enroll ID and Enroll secret that you gave when creating the CA.  Leave the <b>TLS CSR hostname</b> blank.   - The last side panel will ask you to <b>Associate an identity</b> and make it the admin of your peer. Select your peer admin identity `Police Admin`.   - Review the summary and click <b>Submit</b>.  ## Step 5. Build a network - Create Orderer  * #### Create your orderer organization CA   - Click <b>Add Certificate Authority</b>.   - Click <b>IBM Cloud</b> under <b>Create Certificate Authority</b> and <b>Next</b>.   - Give it a unique <b>Display name</b> of `Orderer CA`.     - Specify an <b>Admin ID</b> of `admin` and <b>Admin Secret</b> of `adminpw`.  <br> <p align=""center"">   <img src=""images/gifs/createOrderer.gif""> </p> <br>  * #### Use your CA to register orderer and orderer admin identities (shown in gif above)   - In the <b>Nodes</b> tab, select the <b>Orderer CA</b> Certificate Authority that we created.   - First, we will register an admin for our organization. Click on the <b>Register User</b> button.  Give an <b>Enroll ID</b> of `ordereradmin`, and <b>Enroll Secret</b> of `ordereradminpw`.  Click <b>Next</b>.  Set the <b>Type</b> for this identity as `client` and select `org1` from the affiliated organizations drop-down list. We will leave the <b>Maximum enrollments</b> and <b>Add Attributes</b> fields blank.   - We will repeat the process to create an identity of the orderer. Click on the <b>Register User</b> button.  Give an <b>Enroll ID</b> of `orderer1`, and <b>Enroll Secret</b> of `orderer1pw`.  Click <b>Next</b>.  Set the <b>Type</b> for this identity as `peer` and select `org1` from the affiliated organizations drop-down list. We will leave the <b>Maximum enrollments</b> and <b>Add Attributes</b> fields blank.  * #### Create the orderer organization MSP definition   - Navigate to the <b>Organizations</b> tab in the left navigation and click <b>Create MSP definition</b>.   - Enter the <b>MSP Display name</b> as `Orderer MSP` and an <b>MSP ID</b> of `orderermsp`.   - Under <b>Root Certificate Authority</b> details, specify the peer CA that we created `Orderer CA` as the root CA for the organization.   - Give the <b>Enroll ID</b> and <b>Enroll secret</b> for your organization admin, `ordereradmin` and `ordereradminpw`. Then, give the <b>Identity name</b>, `Orderer Admin`.   - Click the <b>Generate</b> button to enroll this identity as the admin of your organization and export the identity to the wallet. Click <b>Export</b> to export the admin certificates to your file system. Finally click <b>Create MSP definition</b>.  <br> <p align=""center"">   <img src=""images/gifs/ordererMSP.gif""> </p> <br>  * #### Create an orderer   - On the <b>Nodes</b> page, click <b>Add orderer</b>.   - Click <b>IBM Cloud</b> and proceed with <b>Next</b>.   - Give your peer a <b>Display name</b> of `Orderer`.   - On the next screen, select `Orderer CA` as your <b>Certificate Authority</b>. Then, give the <b>Enroll ID</b> and <b>Enroll secret</b> for the peer identity that you created for your orderer, `orderer1`, and `orderer1pw`. Then, select the <b>Administrator Certificate (from MSP)</b>, `Orderer MSP`, from the drop-down list and click <b>Next</b>.   - Give the <b>TLS Enroll ID</b>, `admin`, and <b>TLS Enroll secret</b>, `adminpw`, the same values are the Enroll ID and Enroll secret that you gave when creating the CA.  Leave the <b>TLS CSR hostname</b> blank.   - The last side panel will ask to <b>Associate an identity</b> and make it the admin of your peer. Select your peer admin identity `Orderer Admin`.   - Review the summary and click <b>Submit</b>.  <br> <p align=""center"">   <img src=""images/gifs/createOrdererNode.gif""> </p> <br>  * #### Add organizations as Consortium Member on the orderer to transact   - Navigate to the <b>Nodes</b> tab, and click on the <b>Orderer</b> that we created.   - Under <b>Consortium Members</b>, click <b>Add organization</b>.   - From the drop-down list, select `Insurance MSP`.   - Click <b>Submit</b>.   - Repeat the same steps, but add `Shop MSP`, `Repair Shop MSP`, and `Police MSP` as well.  <br> <p align=""center"">   <img src=""images/gifs/addConsortium.gif""> </p> <br>  ## Step 6. Build a network - Create and Join Channel  * #### Create the channel   - Navigate to the <b>Channels</b> tab in the left navigation.   - Click <b>Create channel</b>.   - Give the channel a name, `mychannel`.   - Select the orderer you created, `Orderer` from the orderers drop-down list.   - Select the MSP identifying the organization of the channel creator from the drop-down list. This should be `Insurance MSP (insurancemsp)`.   - Associate available identity as `Insurance Admin`.   - Click <b>Add</b> next to the insurance organization. Make the insurance organization an <b>Operator</b>.   - Click <b>Add</b> next to the shop organization. Make the shop organization an <b>Operator</b>.   - Click <b>Add</b> next to the repair shop organization. Make the repair shop organization an <b>Operator</b>.   - Click <b>Add</b> next to the police organization. Make the insurance organpoliceization an <b>Operator</b>.   - Click <b>Create</b>.  <br> <p align=""center"">   <img src=""images/gifs/createChannel.gif""> </p> <br>   * #### Join your peer to the channel   - Click <b>Join channel</b> to launch the side panels.   - Select your `Orderer` and click <b>Next</b>.   - Enter the name of the channel you just created. `mychannel` and click <b>Next</b>.   - Select which peers you want to join the channel, click `Insurance Peer`, `Shop Peer`, `Repair Shop Peer`, and `Police Peer`.   - Click <b>Submit</b>.  <br> <p align=""center"">   <img src=""images/gifs/joinChannel.gif""> </p> <br>  * #### Add anchor peers to the channel   - In order to communicate between organizations, we need to enroll anchor peers.   - From the channels tab, click on the channel you have created, `mychannel`.   - From the channel overview page, click on `channel details`. Scroll all the way down until you see `Anchor peers`.   - Click `Add anchor peer` and add the Insurance, Police, Repair Shop,     and Shop peers.   - Select which peers you want to join the channel, click `Insurance Peer`, `Shop Peer`, `Repair Shop Peer`, and `Police Peer`.   - Click <b>Add anchor peer</b>.   - If all went well, your channel Anchor peers should look like below:  <br> <p align=""center"">   <img src=""images/gifs/addAnchorPeer.gif""> </p> <br>  ## Step 7. Deploy Insurance Smart Contract on the network  * #### Install a smart contract * Clone the repository:   ```bash   git clone https://github.com/IBM/build-blockchain-insurance-app   ```   - Click the <b>Smart contracts</b> tab to install the smart contract.   - Click <b>Install smart contract</b> to upload the insurance smart contract package file.   - Click on <b>Add file</b> and find your packaged smart contract. It is the file in the `build-blockchain-insurance-app/chaincodePackage` directory.    - Select all peers - we need to install the contract on each peer.   - Once the contract is uploaded, click <b>Install</b>.   <br> <p align=""center"">   <img src=""images/gifs/installSmartContract.gif""> </p> <br>  * #### Instantiate smart contract   - On the smart contracts tab, find the smart contract from the list installed on your peers and click <b>Instantiate</b> from the overflow menu on the right side of the row.   - On the side panel that opens, select the channel, `mychannel` to instantiate the smart contract on. Click <b>Next</b>.   - Select the organization members to be included in the policy, `insurancemsp`, `shopmsp`, `repairshopmsp`, `policemsp`.  Click <b>Next</b>.   - Give <b>Function name</b> of `Init` and leave <b>Arguments</b> blank.   - Click <b>Instantiate</b>.  <br> <p align=""center"">   <img src=""images/gifs/instantiateContract.gif""> </p> <br>  ## Step 8. Connect application to the network  * #### Connect with sdk through connection profile   - Under the Instantiated Smart Contract, click on `Connect with SDK` from the overflow menu on the right side of the row.   - Choose from the dropdown for <b>MSP for connection</b>, `insurancemsp`.   - Choose from <b>Certificate Authority</b> dropdown, `Insurance CA`.   - Download the connection profile by scrolling down and clicking <b>Download Connection Profile</b>.  This will download the connection json which we will use soon to establish connection.   - You can click <b>Close</b> once the download completes.  <br> <p align=""center"">   <img src=""images/gifs/downloadConnection.gif""> </p> <br>  * #### Create insurance application admin   - Go to the <b>Nodes</b> tab on the left bar, and under <b>Certificate Authorities</b>, choose your <b>Insurance CA</b>.   - Click on <b>Register user</b>.   - Give an <b>Enroll ID</b> and <b>Enroll Secret</b> to administer your application users, `insuranceApp-admin` and `insuranceApp-adminpw`.   - Choose `client` as <b>Type</b>.   - You can leave the <b>Use root affiliation</b> box checked.   - You can leave the <b>Maximum enrollments</b> blank.   - Under <b>Attributes</b>, click on <b>Add attribute</b>.  Give attribute as `hf.Registrar.Roles` = `*`.  This will allow this identity to act as registrar and issues identities for our app.  Click <b>Add-attribute</b>.   - Click <b>Register</b>.  <br> <p align=""center"">   <img src=""images/gifs/appAdmin.gif""> </p> <br>  * #### Create shop application admin (same process as shown above in the gif)   - Go to the <b>Nodes</b> tab on the left bar, and under <b>Certificate Authorities</b>, choose your <b>Shop CA</b>.   - Click on <b>Register user</b>.   - Give an <b>Enroll ID</b> and <b>Enroll Secret</b> to administer your application users, `shopApp-admin` and `shopApp-adminpw`.   - Choose `client` as <b>Type</b>.   - You can leave the <b>Use root affiliation</b> box checked.   - You can leave the <b>Maximum enrollments</b> blank.   - Under <b>Attributes</b>, click on <b>Add attribute</b>.  Give attribute as `hf.Registrar.Roles` = `*`.  This will allow this identity to act as registrar and issues identities for our app.  Click <b>Add-attribute</b>.   - Click <b>Register</b>.  * #### Create repair shop application admin (same process as shown above in the gif)   - Go to the <b>Nodes</b> tab on the left bar, and under <b>Certificate Authorities</b>, choose your <b>Repair Shop CA</b>.   - Click on <b>Register user</b>.   - Give an <b>Enroll ID</b> and <b>Enroll Secret</b> to administer your application users, `repairShopApp-admin` and `repairShopApp-adminpw`.   - Choose `client` as <b>Type</b>.   - You can leave the <b>Use root affiliation</b> box checked.   - You can leave the <b>Maximum enrollments</b> blank.   - Under <b>Attributes</b>, click on <b>Add attribute</b>.  Give attribute as `hf.Registrar.Roles` = `*`.  This will allow this identity to act as registrar and issues identities for our app.  Click <b>Add-attribute</b>.   - Click <b>Register</b>.  * #### Create police application admin (same process as shown above in the gif)   - Go to the <b>Nodes</b> tab on the left bar, and under <b>Certificate Authorities</b>, choose your <b>Police CA</b>.   - Click on <b>Register user</b>.   - Give an <b>Enroll ID</b> and <b>Enroll Secret</b> to administer your application users, `policeApp-admin` and `policeApp-adminpw`.   - Choose `client` as <b>Type</b>.   - You can leave the <b>Use root affiliation</b> box checked.   - You can leave the <b>Maximum enrollments</b> blank.   - Under <b>Attributes</b>, click on <b>Add attribute</b>.  Give attribute as `hf.Registrar.Roles` = `*`.  This will allow this identity to act as registrar and issues identities for our app.  Click <b>Add-attribute</b>.   - Click <b>Register</b>.   #### Update application connection   - Copy the connection profile you downloaded into the `web/www/blockchain` directory.   - Copy and paste everything in the connection profile, and overwrite   the **ibpConnection.json**.     <p align=""center"">     <img src=""images/gifs/ibpConnection.gif"">   </p>  ## Step 9. Enroll App Admin Identities  * #### Enroll insurnaceApp-admin   - First, navigate to the `web/www/blockchain` directory.     ```bash     cd web/www/blockchain/     ```   - Open the `config.json` file, and update the caName with the URL      of the <b>insurance</b> certificate authority from your `ibpConnection.json` file. Save the file.        - Run the `enrollAdmin.js` script     ```bash     node enrollAdmin.js     ```    - You should see the following in the terminal:     ```bash     msg: Successfully enrolled admin user insuranceApp-admin and imported it into the wallet     ``` <p align=""center"">   <img src=""images/gifs/enrollAdmin.gif""> </p>  * Enroll shopApp-admin   - First, change the appAdmin, appAdminSecret, and caName properties in your `config.json` file,    so that it looks something like this (your caName should be different than mine):      ```js     {         ""connection_file"": ""ibpConnection.json"",         ""appAdmin"": ""shopApp-admin"",         ""appAdminSecret"": ""shopApp-adminpw"",         ""orgMSPID"": ""shopmsp"",         ""caName"": ""https://fa707c454921423c80ec3c3c38d7545c-caf2e287.horeainsurancetest.us-south.containers.appdomain.cloud:7054"",         ""userName"": ""shopUser"",         ""gatewayDiscovery"": { ""enabled"": true, ""asLocalhost"": false }     }     ```   - To find the other CA urls, you will need to click on the `Nodes` tab in IBM Blockchain Platform, then on      the Shop CA, and on the settings cog icon at the top of the page. That will take you to the certificate      authority settings, as shown in the picture below, and you can copy that endpoint URL into your `config.json` **caName**     field.   <p align=""center"">   <img src=""images/gifs/enrollShopAdmin.gif""> </p>   <br>    - Run the `enrollAdmin.js` script          ```bash     node enrollAdmin.js     ```    - You should see the following in the terminal:      ```bash     msg: Successfully enrolled admin user shopApp-admin and imported it into the wallet     ```  * #### Enroll repairShopApp-admin (same process as shown in gif above)   - First, change the appAdmin, appAdminSecret, and caName properties in your `config.json` file,    so that it looks something like this (your caName should be different than mine):      ```js     {         ""connection_file"": ""ibpConnection.json"",         ""appAdmin"": ""repairShopApp-admin"",         ""appAdminSecret"": ""repairShopApp-adminpw"",         ""orgMSPID"": ""repairshopmsp"",         ""caName"": ""https://fa707c454921423c80ec3c3c38d7545c-caf2e287.horeainsurancetest.us-south.containers.appdomain.cloud:7054"",         ""userName"": ""repairUser"",         ""gatewayDiscovery"": { ""enabled"": true, ""asLocalhost"": false }     }     ```   - Run the `enrollAdmin.js` script       ```bash       node enrollAdmin.js       ```    - You should see the following in the terminal:     ```bash     msg: Successfully enrolled admin user repairShopApp-admin and imported it into the wallet     ```  * #### Enroll policeApp-admin (same process as shown in gif above)   - First, change the appAdmin, appAdminSecret, and caName properties in your `config.json` file,    so that it looks something like this (your caName should be different than mine):      ```js     {         ""connection_file"": ""ibpConnection.json"",         ""appAdmin"": ""policeApp-admin"",         ""appAdminSecret"": ""policeApp-adminpw"",         ""orgMSPID"": ""policemsp"",         ""caName"": ""https://fa707c454921423c80ec3c3c38d7545c-caf2e287.horeainsurancetest.us-south.containers.appdomain.cloud:7054"",         ""userName"": ""policeUser"",         ""gatewayDiscovery"": { ""enabled"": true, ""asLocalhost"": false }     }     ```    - Run the `enrollAdmin.js` script       ```bash       node enrollAdmin.js       ```    - You should see the following in the terminal:       ```bash       msg: Successfully enrolled admin user policeApp-admin and imported it into the wallet       ```    ## Step 10. Run the application  Navigate to the directory blockchain directory which contains the [config.js file](https://github.com/IBM/build-blockchain-insurance-app/blob/ubuntu/local-fix/web/www/blockchain/config.js):   ```bash   cd build-blockchain-insurance-app/web/www/blockchain/   ```  In the editor of choice, change [line 8](https://github.com/IBM/build-blockchain-insurance-app/blob/ubuntu/local-fix/web/www/blockchain/config.js#L8) of the `config.js` file to `isCloud: true` as  shown in the image below:   ![Is Cloud](images/isCloud.png)  If you are using Mac, save the changes. Otherwise, if you are using an Ubuntu system, change [line 9](https://github.com/IBM/build-blockchain-insurance-app/blob/ubuntu/local-fix/web/www/blockchain/config.js#L9) of `config.js` file to `isUbuntu: true` as shown in the image below:  ![Is Ubuntu](images/isUbuntuAndCloud.png)  Next, from the `blockchain` directory navigate to the root project directory: ```bash blockchain$ cd ../../../ build-blockchain-insurance-app$    ```  Login using your [docker hub](https://hub.docker.com/) credentials. ```bash docker login ```  Run the build script to download and create docker images for the orderer, insurance-peer, police-peer, shop-peer, repairshop-peer, web application and certificate authorities for each peer. This will run for a few minutes.  For Mac user: ```bash cd build-blockchain-insurance-app ./build_mac.sh ```  For Ubuntu user **Make sure isUbuntu:true is saved in the [line 9](https://github.com/IBM/build-blockchain-insurance-app/blob/ubuntu/local-fix/web/www/blockchain/config.js#L9) of `config.js`**: ```bash cd build-blockchain-insurance-app ./build_ubuntu.sh ```  You should see the following output on console: ``` Creating repairshop-ca ... Creating insurance-ca ... Creating shop-ca ... Creating police-ca ... Creating orderer0 ... Creating repairshop-ca Creating insurance-ca Creating police-ca Creating shop-ca Creating orderer0 ... done Creating insurance-peer ... Creating insurance-peer ... done Creating shop-peer ... Creating shop-peer ... done Creating repairshop-peer ... Creating repairshop-peer ... done Creating web ... Creating police-peer ... Creating web Creating police-peer ... done ```   <p align=""center"">     <img src=""images/gifs/buildApp.gif"">   </p>     <br>  **Wait for few minutes for application to install and instantiate the chaincode on network**  Check the status of installation using command: ```bash docker logs web ``` On completion, you should see the following output on console: ``` > blockchain-for-insurance@2.1.0 serve /app > cross-env NODE_ENV=production&&node ./bin/server  /app/app/static/js Server running on port: 3000 Default channel not found, attempting creation... Successfully created a new default channel. Joining peers to the default channel. Chaincode is not installed, attempting installation... Base container image present. info: [packager/Golang.js]: packaging GOLANG from bcins info: [packager/Golang.js]: packaging GOLANG from bcins info: [packager/Golang.js]: packaging GOLANG from bcins info: [packager/Golang.js]: packaging GOLANG from bcins Successfully installed chaincode on the default channel. Successfully instantiated chaincode on all peers. ```  Use the link http://localhost:3000 to load the web application in browser.  The home page shows the participants (Peers) in the network. You can see that there is an Insurance, Repair Shop, Police and Shop Peer implemented. They are the participants of the network.  ![Blockchain Insurance](images/home.png)  Imagine being a consumer (hereinafter called ‚ÄúBiker‚Äù) that wants to buy a phone, bike or Ski. By clicking on the ‚ÄúGo to the shop‚Äù section, you will be redirected to the shop (shop peer) that offers you the following products.  ![Customer Shopping](images/Picture1.png)  You can see the three products offered by the shop(s) now. In addition, you have insurance contracts available for them. In our scenario, you are an outdoor sport enthusiast who wants to buy a new Bike. Therefore, you‚Äôll click on the Bike Shop section.  ![Shopping](images/Picture2.png)  In this section, you are viewing the different bikes available in the store. You can select within four different Bikes. By clicking on next you‚Äôll be forwarded to the next page which will ask for the customer‚Äôs personal data.  ![Bike Shop](images/Picture3.png)  You have the choice between different insurance contracts that feature different coverage as well as terms and conditions. You are required to type-in your personal data and select a start and end date of the contract. Since there is a trend of short-term or event-driven contracts in the insurance industry you have the chance to select the duration of the contract on a daily basis. The daily price of the insurance contract is being calculated by a formula that had been defined in the chaincode. By clicking on next you will be forwarded to a screen that summarizes your purchase and shows you the total sum.  ![Bike Insurance](images/Picture4.png)  The application will show you the total sum of your purchase. By clicking on ‚Äúorder‚Äù you agree to the terms and conditions and close the deal (signing of the contract). In addition, you‚Äôll receive a unique username and password. The login credentials will be used once you file a claim.  A block is being written to the Blockchain.  >note You can see the block by clicking on the black arrow on the bottom-right.  At this point, you should be able to go into your IBM Blockchain Platform console, click on the channels, and then  be able to see the contract_create block being added.   <p align=""center"">   <img src=""images/gifs/runAppIBP.gif""> </p>   <br>  For additional steps on how to file more claims, and use the rest of the application, please go [here](./README-local.md).  Congratulations! You've successfully connection your React app to the IBM Blockchain Platform! Now each time you submit transactions with the UI, they will be logged by the blockchain service.   ## Additional resources Following is a list of additional blockchain resources: * [Fundamentals of IBM Blockchain](https://www.ibm.com/blockchain/what-is-blockchain) * [Hyperledger Fabric Documentation](https://hyperledger-fabric.readthedocs.io/) * [Hyperledger Fabric code on GitHub](https://github.com/hyperledger/fabric)  ## Troubleshooting  * Run `clean.sh` to remove the docker images and containers for the insurance network. ```bash ./clean.sh ``` ## License This code pattern is licensed under the Apache Software License, Version 2.  Separate third party code objects invoked within this code pattern are licensed by their respective providers pursuant to their own separate licenses. Contributions are subject to the [Developer Certificate of Origin, Version 1.1 (DCO)](https://developercertificate.org/) and the [Apache Software License, Version 2](https://www.apache.org/licenses/LICENSE-2.0.txt).  [Apache Software License (ASL) FAQ](https://www.apache.org/foundation/license-faq.html#WhatDoesItMEAN)"
Financial Record Management System,Blockchain,https://github.com/prateeshreddy/blockchain-finance,"# blockchain-finance  This project is Capstone project for my final year Bachelor's degree in Computer Science at Gandhi Institute of Technology and Management, India  This project was generated with [Angular CLI](https://github.com/angular/angular-cli) version 8.3.21.  It is inspired by and a practical implementation of the paper [Research of a Possibility of Using Blockchain Technology without Tokens to Protect Banking Transactions](https://ieeexplore.ieee.org/document/8657279)  ## Development  Run `ng generate component component-name` to generate a new component. You can also use `ng generate directive|pipe|service|class|guard|interface|enum|module`.  Run `ng build` to build the project. The build artifacts will be stored in the `dist/` directory. Use the `--prod` flag for a production build.  ## Testing  ### Running unit tests  Run `ng test` to execute the unit tests via [Karma](https://karma-runner.github.io).  ### Running end-to-end tests  Run `ng e2e` to execute the end-to-end tests via [Protractor](http://www.protractortest.org/).   ## Dashboard   User Login [Dashboard](https://blockchain-finance.firebaseapp.com/login) to add money or transfer money to other users.  [Contact me](https://prateeshreddy007.wixsite.com/mysite/contact) or connect with me on [Linkedin](https://www.linkedin.com/in/prateeshreddy) to get Login Credentials for the dashboard if u want to play around with the Project.  Also Database showing all transactions with columns  From Account, To Account,	Amount,	Date of Transaction, Transaction ID, Current Hash and Previous Hash.   This Project is unique as it is uses blockchain to authenticate transactions by checking if prev hash is equal to current hash.  If any hacker tries to interupt these transactions our dashboard gives pop-up showing Blockchain tampered.  <img src=""src/app/dashboard/image.PNG"" width = ""800"">    ## Citation  N. A. Popova and N. G. Butakova, [Research of a Possibility of Using Blockchain Technology without Tokens to Protect Banking Transactions](https://ieeexplore.ieee.org/document/8657279) 2019 IEEE Conference of Russian Young Researchers in Electrical and Electronic Engineering (EIConRus), St.Petersburg and Moscow, Russia, 2019, pp. 1764-1768.    "
Android Document Sharing System using Blockchain,Blockchain,https://github.com/ruchi26/Blockchain-based-Decentralized-File-Sharing-System-using-IPFS,"# Blockchain-based-File-Sharing-System The aim is to share files in a peer to peer manner using a blockchain to ensure decentralization.   <h2>METHODOLOGY</h2>  <h3>4.1 Creating the Blockchain</h3>  <h4>4.1.1 Block Structure</h4> In Data Share, a single block in a blockchain has the following structure:<br><br>    The Block contains : <br> <b>Block number</b> - Simply displays the index number of the block. Block 0 refers to the genesis block.<br> <b>Timestamp</b> - This field indicates as to when the block was created and added to the blockchain.  <br> <b>Proof</b> - Also called a nonce, it stands for ""number only used once,"" which is a number added to a hashed‚Äîor encrypted‚Äîblock in a blockchain that, when rehashed, meets the difficulty level restrictions i.e by varying the proof we can vary the hash generated so that a new block can be created. <br> <b>Previous hash</b> - This field represents the hash of the previous block. (In this case block index 2). The hash of the entire block is generated using the SHA-256 hashing algorithm. This field creates a chain of blocks and is the main element behind blockchain architecture‚Äôs security.<br> <b>Sender</b> - The person who uploads the file enters his identity proof or name when he uploads the file.<br> <b>Receiver</b> - Displays who the intended receiver of the shall be.<br> <b>Hash of the file shared</b> - The uploaded file is first encrypted with the file key given by the uploader using the AES encryption mechanism and subsequently using the SHA-256 hashing algorithm when it is uploaded to ipfs. The hash, then received from the IPFS after the encryption is the hash of the shared file which is added to the block.<br>   <h4>4.1.2 Creating the Peer to Peer Network </h4> In order to create a peer to peer network (p2p) for the blockchain to function, all the connected nodes must be in the same network. Only those users who are connected to the blockchain‚Äôs p2p network should have access to the blockchain‚Äôs data. This p2p network is created using  Socket Programming. We are working on a permissioned blockchains which require access to be a part of the blockchain. This access is granted when a user clicks on ‚ÄòConnect to the blockchain‚Äô displayed on the home screen. Using socket programming, the list of connected nodes gets updated as soon as a new user gets connected or disconnected to the network and the updated list is broadcasted to the whole p2p  network. As soon as all the connected nodes get the updated list of the nodes in the network, the consensus protocol works smoothly whenever a new block is added or the blockchain gets updated. Thus, the peer to peer network works effectively.   <h4>4.1.3 File Key</h4> The unique key/password shared between the sender and the receiver of the shared file to increase the security of the file(s) on the blockchain network.  The upload page is to be filled out by the uploader eager to share the file. The file key entered here will be used to encrypt the file using AES encryption before uploading it to the IPFS network. The uploader will have to share the key only with the intended receiver(s) so he/she can download the file. The type of files that can be uploaded are .pdf , .png , .jpeg and .txt. As of now the size of the file that can be uploaded to the network is limited to 16 Megabytes.   The download page is to be filled by the receiver who has the valid file key shared by the sender and intends to download the shared file from the blockchain to his/her local computer. The file key here is used to decrypt: the AES encrypted file downloaded from the IPFS network so that the file can be interpretable. Make sure you enter the correct file key and hash for a successful download.   <h3>4.2 Integrating with IPFS</h3>   Our blockchain relies on IPFS for keeping it lightweight and scalable. If the files were stored directly on the blockchain, it would render the blockchain very heavy and inefficient. Combining IPFS and blockchain, we get to access the IPFS‚Äôs power of decentralized storage and enhance the blockchain‚Äôs security and accessibility. Instead of storing the file directly on the blockchain, we store the files on the IPFS network while the blockchain stores only the file‚Äô hash. Each file will have a unique hash as IPFS employs the SHA-256 hashing algorithm. Thus, the file is stored in a secure decentralized network and is easily accessible through the blockchain. The file can be retrieved using its generated hash easily. Hence IPFS eliminates the bottleneck of storing entire files on the blockchain.     <h3>4.3 Using Cryptographic Encryption</h3> <h4>4.3.1 SHA-256 Hashing Algorithm</h4>  We use the SHA-256 algorithm to generate a unique hash of the entire block that is used by the corresponding blocks to form the chain (via the previous hashes). IPFS as well uses this algorithm to generate the hash of the shared file. The SHA-256 hashing algorithm is employed because of the following advantages: One-way:- Once the hash is generated, we can‚Äôt revert to the original data from the hash. Deterministic:- For a particular input, the hash generated, always remains the same i.e. same input always gives the same hash. Quick computation of the hash. Avalanche-effect:- Even a slight change in the input will bring about a large change in the final hash, making it untraceable Withstand collisions:- There is a very rare chance that the hash generated for two different inputs will be the same. Think of it as a human fingerprint!<br><br> <h4>4.3.2 AES Encryption</h4> AES Encryption is a form of symmetric, cryptographic encryption that depends on a shared key between the sender and receiver to access any file (here the file key) . If we had not employed the AES encryption, any connected user to the blockchain can access the file hash and thus, the shared file using the hash, directly from the IPFS. Using the AES Encryption, we encrypt the file using the file key of the uploader. Thus, if any user tries to download the file directly from the IPFS, all they get is a non-readable file. Thus, only users with a valid file key can access the readable file contents, thereby enhancing the security of the blockchain and the file contents.   <h2>RESULT</h2>  To test our application, we ran two instances of  Data Share on the computer locally at different ports, which served as two different and independent nodes (let‚Äôs say node A and node B). From both of them, we connected to the blockchain network and shared files using file keys. From node A, we uploaded a file ‚Äòx‚Äô using file key ‚ÄòP‚Äô.  We then downloaded the file ‚Äòx‚Äô  from node B using the same key P‚Äô. Subsequently, we uploaded a file ‚Äòy‚Äô from node B using file key ‚ÄòQ‚Äô and downloaded the file ‚Äòy‚Äô from node ‚ÄòA‚Äô using the file key ‚ÄòQ‚Äô After the consequent sharing of the two files, the blockchain was updated at both the nodes.  "
State Government Fund Allocation & Tracking System over Blockchain,Blockchain,https://github.com/maddydevgits/government-fund-allocation-tracking-system-blockchain,"# Project Name Official Repo for Government Fund Allocation and Tracking System  # Softwares 1. VS Code 2. NodeJs 3. Truffle 4. Ganache 5. Visual C++ Build Tools 6. Python3  # Steps 1. Clone Repo 2. Compile the contract (truffle compile) 3. Migrate Contract (truffle migrate) 4. Replace Contract Addresses of Register and Funds in app.py 5. cd src 6. python app.py to run server  # Questions For questions, do reach me on <a href=""https://linkedin.com/in/MadhuPIoT"">LinkedIn</a>"
Document Storage System,Blockchain,https://github.com/codingBeast25/Blockchain-based-File-Storage,"<h1>Blockchain-based-File-Storage</h1>    <h2>How to run the application</h2>  1. Install required libraries using :    `pip install -r requirements.txt` 2. Open one terminal and start server/peer:    `python peer.py` 3. Open another terminal and start a client:    `python run_app.py` 4. Copy the link from the client terminal and paste it in any browser. 5. To run our experiment of different Proof of Work concepts: `python POW_Comparison.py` <h2> Project Information </h2> We developed a web-based application for decentralized file storing using blockchain. In this application any user can upload as many files(one at a time) as he/she likes. All other peers and the user himself can download and access those file in their system. File can be of any type and any size. Refer to project demo link to see the detailed explanation. <br /> We are using randomly generated nonce in proof of work concept to acheive the required difficulty (diff = 3). Once peer uploads the file, the file is stored in a block including username, filesize and file data. These block gets appended to the current blockchain, which makes it impossible to edit or delete the file/block.<br />  The reason to implement file storing using blockchain is its abilitiy to avoid any modification or deletion. No one can delete or corrupt our files that are stored.  <h2> Project Demo </h2> <h3> Youtube Demo Link : https://youtu.be/Z6JiYkk8Qt8 </h3>  <h2>Importance of Blockchain:</h2>  Blockchain data structure is used to store digital information securely. Blockchain is an open ledger which can be accessed by multiple parties all at once. Blockchain holds various types of digital information such as transaction information, files, messages, etc. The successful implementation of blockchain holds different types of functionalities such as consensus algorithm, mining block, validation of block etc.  As part of this project, we have implemented a blockchain containing file information, which also allows users to upload/download all types of files from publicly available website. It uses SHA256 cryptographic algorithm to ensure that the block is kept secret. As a consensus algorithm, proof of work is implemented, which requires miners to solve any cryptographic puzzle before they get to announce new block on chain. In our application, we require to find a hash value that starts with three 0's as part of a puzzle.  <h2>Proof of Work Algorithm:</h2>  Blockchain-based applications are typically peer-to-peer networks that must be as decentralized as possible. To support and enhance network decentralization, all peers in the network should be able to add new blocks to the network. Proof of work algorithms simulate the idea of each peer being able to add a new block. The goal of proof of work algorithms is to solve various cryptographic puzzles. Peers who solve these puzzles more quickly can add a new block to the blockchain.  Two different proof of work algorithms are included in our blockchain implementation. Both are attempting to solve the same cryptographic puzzle, but in different ways. According to these proof of work algorithms, whoever finds a hash value for their block that contains a certain number of leading zeros will be able to announce it to the chain. For example, our blockchain only allows miners to add a block if the hash value of their block begins with three zeros, indicating that the difficulty of the blockchain is 3. The term miner refers to peers who attempt to add blocks to the blockchain.  Based on this idea, we have implemented two algorithms. Both calculate nonce differently. One calculates nonce randomly in each iteration and another one simply increments nonce by one in each iteration. We have analyzed running time and behavior of both algorithms in the file named POW_Comparison.py. Based on the output of the file, we have concluded some results for both the methods.  <h2>Comparison of proof of work algorithms:</h2>  <h3>Difference:</h3>  1. Nonce = random.randint(0,99999999) (first algorithm: p_o_w) 2. Nonce += 1 (second algorithm: p_o_w_2)  The running time for first algorithm:  |               | Attempt #1 | Attempt #2 | Attempt #3 | Attempt #4 | |---------------|------------|------------|------------|------------| | Difficulty #2 | 0.00018    | 0.00281    | 0.00102    | 0.00039    |             | Difficulty #3 | 0.00069    | 0.03207    | 0.00485    | 0.00356    |             | Difficulty #4 | 0.13479    | 0.22688    | 0.34565    | 0.19841    |             | Difficulty #5 | 4.06034    | 2.08288    | 0.58391    | 0.2094     |              The running time of Second algorithm:  |               | Attempt #1 | Attempt #2 | Attempt #3 | Attempt #4 | |---------------|------------|------------|------------|------------| | Difficulty #2 | 0.00035    | 0.00080    | 0.00062    | 0.00108    |             | Difficulty #3 | 0.02190    | 0.02463    | 0.02104    | 0.01625    |             | Difficulty #4 | 0.00366    | 0.03813    | 0.32095    | 0.02145    |             | Difficulty #5 | 0.04403    | 3.10820    | 1.53688    | 1.50288    |              <h3>Why First Algorithm is better than Second one?</h3>  <b><h4> Probability of Valid output & running time </h4> </b>  Based on the output of the POW_Comparison.py file, we can conclude that for lower difficulty levels, the running time does not vary significantly. However, for higher difficulty levels, the first algorithm, where the nonce is generated at random, can be faster. One reason for this is that transactions are added concurrently while POW is still running. If a new transaction is added after POW has started running, the entire equation to generate hash will change, and the ultimate nonce value will change compared to the previous value POW was searching for. In 2nd algorithm, previously tested nonce will not be repeated even though there is still a probability that previous nonce can be the solution.  For Example,  Assume that POW has started running.  Nonce has reached to 99978. So, probability for 0-99977 being a solution is 0.  At this point, new transaction is added to the block, which will change entire equation of calculating the hash value. Now, probability of 0-99977 being a solution is not 0. However, those values will not be tested again. This behavior may affect the run time and there can be such case where there is no solution or very less probability for any of the later nonce to be the solution. In that case, miner will fail to solve the puzzle or running time is considerably higher.  On the other hand, in first algorithm, where nonce is chosen randomly, each nonce is equally probable of getting picked at any given time. So, algorithm has higher probability of giving output in less time for larger difficulty level.  <b><h4>Security</h4></b>  Secondly, 2nd algorithm is not quite secure because nonce value can be estimated based on the running time of the algorithm. For example, Assume that Nonce value is between (0,10000). If running time is less, nonce will be small number such as between 0-1000. If running time is higher, then nonce can be close to 10000. Here, we might wonder why the security of nonce is important. If someone has information about one block and they can also figure out nonce for that block, then they can easily break the entire blockchain system because all the blocks are connected to each other with their hash values.  <h2>Some Issues with first algorithm</h2>  Calculating random values can be expensive. So, we might need to find random functions that have constant running time or quicker compared to other random functions. For example, python random.random() function is faster than random.randint().  Overall, any proof of work algorithm is computationally expensive and requires too many resources. There is an alternative to proof of work algorithm, that is proof of stack algorithms, which are also effective in terms of supporting decentralized network. In proof of stack algorithm, validators are randomly chosen. The probability of being chosen also depends on the value of stacks they hold for that blockchain. The chosen validator acquires a right to add a new block to the chain. Based on the validity of the block, the value of stack that the validator hold will increase or decrease. This method is not expensive yet effective.  <h2>Comparison for On-chain and Off-Chain Blockchain:</h2>  Blockchain applications can also be divided into two types: On-chain Blockchain & Off-chain Blockchain. On-chain Blockchain refers to storing information inside blocks and Off-chain Blockchain refers to storing actual data outside the block and only keep metadata in block.   <h3>Advantages of On-chain blockchain: </h3>  On-chain blockchain can be more secure because information is capsulated in secure blocks. Infomation can be recovered easily in case of break in the system.  <h3>Disadvantages of On-chain blockchain:</h3>  Runnning time of the insertion and other block-operations can be slow because it holds too much data to process. It is expensive and requires more resources to maintain.  Here, issues with On-chain blockchain can be solved by using off-chain blockchain but On-chain blockchain is more effective where main concern is security and back-up of information. For this project, we have implemented On-chain blockchain which contains entire file data in block including file size and file name.  <h2>References:</h2> 1. https://github.com/JungWinter/file-on-blockchain<br /> 2. https://github.com/MoTechStore/Python-Flask-Blockchain-Based-Content-Sharing<br/> 3. https://medium.com/@amannagpal4/how-to-create-your-own-decentralized-file-sharing-service-using-python-2e00005bdc4a  <h2> Authors </h2>  1. Name: Bhautik Sojitra Student Id: 7900140  2. Name: Kabir Bhakta Student Id: 7900098"
Charity Applications,Blockchain,https://github.com/harshagr18/CharityBlockchain,"# Charity Blockchain Project  ### Problem Statement - There are no platforms that can be used by charities to ensure security while providing accessibility to maximum people. - The biggest problem faced is transparency, where people can rightly exercise their Right To Information by asking for a record of expenditure by the charity - Even if such applications are available, they are inaccessible to smaller organisations with a good user interface for ease of access.  ### Requirements of the Project  - **Solidity Web3** 	* Solidity provides Inheritance properties in contracts including multiple level inheritance properties.  - **Metamask** 	* To have an easy-to-use and secure wallet service, the platform connects to users' MetaMask automatically.  - **Node.js** 	* Node.js is used a backend for our application. It records transactions communicates between the Applications and integrates frontend.  - **Ganache** 	* Ganache provides the GUI-based local Ethereum blockchain development environment to deploy and test contracts.  ### Project Working * A metamask connection is required to run the application for any transaction. * A ganache RPC Server is run with metamask as the wallet, using the node.js interface. * Charity and organisation details are saved in the application, and a hash value is generated * A transaction is carried out, between organisation and charity and transaction hash is generated for each transaction * A block is created, when the user mines all the transactions updates.   ### YouTube Video  <div align=""center""> <a href=""https://youtu.be/4CIUYSnVEIo""><img src=""http://img.youtube.com/vi/4CIUYSnVEIo/0.jpg"" width=""30%""></a> <br> <a href=""https://youtu.be/4CIUYSnVEIo"">Demonstration Video</a></div> <br><br>   ### License  	Copyright (C) 2020 Harsh Sanjay Agrawal  	Licensed under the Apache License, Version 2.0 (the ""License""); 	you may not use this file except in compliance with the License. 	You may obtain a copy of the License at  	   http://www.apache.org/licenses/LICENSE-2.0  	Unless required by applicable law or agreed to in writing, software 	distributed under the License is distributed on an ""AS IS"" BASIS, 	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. 	See the License for the specific language governing permissions and 	limitations under the License."
Simple Voting Dapp,Blockchain,https://github.com/ashishlamsal/voting-dapp,"# Block-Chain Based Voting System  This project is blockchain based voting dapp created in React and Solidity.  ## Project Description  ‚ÄúBlockchain Based Voting System‚Äù is a web based online voting system primarily based on ethereum blockchain technology. Blockchain is a transparent, distributed, immutable and trustless ledger and to overcome the problems of the traditional voting system, blockchain plays a vital role in terms of security, trust and more. Here anyone eligible for voting can vote for their favorite candidate and they can see the result after the end of the election. It is fast, secure, and has low cost as compared to traditional voting systems. Some key features of our system are:  a) Decentralized & Transparent  b) Trustless  c) Immutable  This project is a simple implementation of a voting system done to understand the basics of ethereum blockchain technology and the working mechanism of decentralized application made in Solidity and React. Here, one account who deploys the smart contract is the Admin and he/she can add voters and candidates that are eligible to cast the vote.  Then, the admin starts the election and eligible voters can vote for their favorite candidate. Finally, when admin ends the election, the voters can immediately see the final result of the election.  ## Screenshots  ![coverpage](screenshots/cover.png) &nbsp; ![vote](screenshots/vote.png) &nbsp; ![admin](screenshots/admin.png) &nbsp;  ## Installation  ### Step 1. Clone the project  ```git clone https://github.com/ashishlamsal/voting-dapp```  ### Step 2. Start Ganache  Open the Ganache GUI client to start the local blockchain instance.  ### Step 3. Compile & Deploy Election Smart Contract  ```truffle migrate --reset```  We must migrate the election smart contract each time restart ganache.  ### Step 4. Configure Metamask  - Unlock Metamask - Connect metamask to the local Etherum blockchain provided by Ganache. - Import an account provided by Ganache.  ### Step 5. Run the Front End Application  ```powershell cd .\client yarn install yarn start ```  Visit URL in your browser: <http://localhost:3000>  ## Next Steps  - Better legitimate user checking through biometrics - Better voters and candidate data insertion - Multiple candidate positions for voting  ## Note  This project is not intended to be a fully fledged voting system. It is just a proof of concept to understand the basics of blockchain technology. It is done as a final project of Blockchain Fellowship 2022 by [esatya.io](https://esatya.io/fellowship/2021)."
Digital Land Registry System,Blockchain,https://github.com/vrii14/Land-Registration-with-Blockchain,"# Land Registration System with Blockchain   ## This work was presented at IEEE ICAECC'23 - <a href=""https://ieeexplore.ieee.org/document/10560138"">Checkout</a> <img src=""https://img.shields.io/badge/Ethereum-20232A?style=for-the-badge&logo=ethereum&logoColor=white""> <img src=""https://img.shields.io/badge/React-20232A?style=for-the-badge&logo=react&logoColor=61DAFB"">   ## Project Description:  This is an application of Land Registration System.  Land registry in India as well as in many parts of the world is a very slow and inconvenient process. Current land registration & verification systems include an increasing number of fraud cases and loss of paperwork and court cases due to thousands of land records to maintain.   The intuition behind building this was to make the process of land registration resilient and decreases the cases of fraud in the process. Using the system, validation of the lands is also possible as immutable transactions are being stored in the public ledger.   So the Land Registration system using blockchain is a distributed system that will store all the transactions made during the process of land buying. This will also be helpful for buyers, sellers and government registrars to transfer the land ownership from seller to new buyer as well as it will accelerate the process of registration.     ## Tech Stack Used:  	Frontend: 	* Javascript     * React Framework 	* CSS     * Metamask Chrome Extension  	Backend: 	* Ethereum Blockchain (Truffle Suite)     * Solidity     * Ganache   ## Application features:    * **Registration Page**: Seller & Buyer can register for an account on the application.  * **Land Inspector Dashboard**: Land Inspector works as the admin and is already registered. He can then verify the Sellers, Buyers and approve Land Transfer Process. * **User Profile**: Seller & Buyer can view their profile via their respective Dashboards. * **Edit Profile**: Seller & Buyer can edit their profile. * **Seller Dashboard**: A Brief Description of Added Lands and features to Add a new Land and approve a Land request from a Buyer. * **Add Land**: Seller can add a land after he/she is verified by the Land Inspector. * **Approve Land Request**: Approve a Request by Buyer to Buy a Land. * **Buyer Dashboard**: A Brief Description of all Lands and features to Request a Land to Land Owner of the particular Land.  * **Owned Lands**: Details of Lands owned by the Buyer after Buying some lands. * **Make Payment**: Complete Payment transfer to Seller after Land Request is approved. * **View Lands**: Complete Information of Lands along with its Images and Required Documents. * **Land Ownership Transfer**: Transfer of Land Ownership from Seller to Buyer via Land Inspector.      ## Steps to run the application: 1. Clone the github repository and cd to the folder  2. Open _Ganache_ and keep it running in the Background. 3. Make sure you have Metamask Extension in your browser. 4. In the root directory run _truffle migrate --reset_. 5. cd to the _client_ folder and run _npm install_. 6. Run _npm start_.  ## [Project Demo Link](https://youtu.be/6VLaAa8GNDc)  ## Some features of the application:-   Landing Page                   |                   Buyer Registration :---------------------------------:        |      :------------------------------: <img src=""Screenshots/landing.png"" height=""200"">  | <img src=""Screenshots/registration.png"" height=""200"">  Buyer Dashboard                   |                   Seller Dashboard :---------------------------------:        |      :------------------------------: <img src=""Screenshots/buyer dashboard.png"" height=""200"">     |<img src=""Screenshots/seller dashboard2.png"" height=""200"">  Add Land(by Seller)            |                   View all Lands Details :---------------------------------:        |      :------------------------------: <img src=""Screenshots/add land.png"" height=""200"">     |<img src=""Screenshots/Land Gallery.png"" height=""200"">  Help & FAQ Page                |                   Verify Buyer(by Land Inspector) :---------------------------------:        |      :------------------------------: <img src=""Screenshots/help.png"" height=""200"" >     |<img src=""Screenshots/verify buyer.png"" height=""200""  >  Approve Land Request(by Seller)               |             Payment by Buyer  :---------------------------------:        |      :------------------------------: <img src=""Screenshots/approve request.png"" height=""200"">     |<img src=""Screenshots/payment.png"" height=""200"">  Verify Land Transaction(by Land Inspector)    |                   Owned Lands(Buyer) :---------------------------------:        |      :------------------------------: <img src=""Screenshots/verify transaction.png"" height=""200"">     |<img src=""Screenshots/owned lands.png"" height=""200"">  View Profile(Before Verification)                  |             Edit Profile(After Verification) :---------------------------------:        |      :------------------------------: <img src=""Screenshots/profile.png"" height=""200"" width=""100%"">     |<img src=""Screenshots/edit profile.png"" height=""200"" width=""80%"">  ### Make sure to star the repository if you find it helpful! ![visitors](https://visitor-badge.laobi.icu/badge?page_id=vrii14.Land-Registration-with-Blockchain) <a href=""https://github.com/vrii14/Land-Registration-with-Blockchain/stargazers""><img src=""https://img.shields.io/github/stars/vrii14/Land-Registration-with-Blockchain?color=yellow"" alt=""Stars Badge""/></a> <a href=""https://github.com/vrii14/Land-Registration-with-Blockchain/graphs/contributors""><img alt=""GitHub contributors"" src=""https://img.shields.io/github/contributors/vrii14/Land-Registration-with-Blockchain?color=2b9348""></a> "
Personal Identity System,Blockchain,https://github.com/jibolagithub/Blockchain-Based-personal-identity-security-system,"# Blockchain-Based-Digital-Identity-System In today's life, the demand for privacy and transparency is  everywhere. Blockchain is going to lead the modern world with its revolutionary technology because of its security, transparency altogether. Identity is an important aspect in our daily life as a citizen and digital identity will play an important role to make our life easy and hassle free. The use of blockchain in digital identity is important to keep it secure and undoubtedly prevents the violation of privacy. % However, there is lack of works that includes the most vital bio-metric information of an individual for creating a blockchain-based digital identity. % In this paper, we have proposed a system on blockchain-based digital identity for individuals using bio-information. We have implemented our proposed digital identity system using ethereum smart contract. The system has fulfilled all the criteria of an identity system. %  Our results show that an intruder cannot access the personal data of a citizen; any unauthorized access attempt is denied instantly, thereby ensuring the privacy of private citizen data.  All of the functionalities and the security issues have been tested. % Our proposed blockchain-based digital identity system can be useful for the government of a country to provide its citizen a highly secured digital identity. It will make the life of all citizens hassle free without requiring them to carry any paper document. # Blockchain-Based-personal-identity-security-system"
Supply Chain Management System,Blockchain,https://github.com/Faizack/Supply-Chain-Blockchain,"<h1 align=""center"">   <br>   <a><img src=""https://www.mdpi.com/logistics/logistics-03-00005/article_deploy/html/images/logistics-03-00005-g001.png"" width=""200""></a>   <br>     Supply-Chain-Dapp   <br> </h1>  <p align=""center"">      <a href=""https://docs.godechain.com/welcome/"">     <img src=""https://s3.coinmarketcap.com/static-gravity/thumbnail/medium/12b1f4d9727b4aab83cd5398bf6e080d.jpg"" width=""35"" height='35'>   </a>   <a href=""https://soliditylang.org/"">     <img src=""https://github.com/rishav4101/eth-supplychain-dapp/blob/main/images/Solidity.svg"" width=""80"">          </a>   <a href=""https://reactjs.org/""><img src=""https://github.com/rishav4101/eth-supplychain-dapp/blob/main/images/react.png"" width=""80""></a>      <a href=""https://www.trufflesuite.com/"">     <img src=""https://github.com/rishav4101/eth-supplychain-dapp/blob/main/images/trufflenew.png"" width=""50"">   </a>    &nbsp;&nbsp;&nbsp;   <a href=""https://www.npmjs.com/package/web3"">     <img src=""https://github.com/rishav4101/eth-supplychain-dapp/blob/main/images/web3.jpg"" width=""60"">   </a> </p>  <h4 align=""center"">A simple Supply Chain setup with <a href=""https://docs.soliditylang.org/en/v0.8.4/"" target=""_blank"">Solidity</a>.</h4>  <p align=""center"">   <a >     <img src=""https://img.shields.io/badge/dependencies-up%20to%20date-brightgreen.svg"">           </a>    </p>  ## Demo  https://www.canva.com/design/DAFb-i9v_cM/-fK0pKTuOkFq5dfCPQxh_w/watch?utm_content=DAFb-i9v_cM&utm_campaign=designshare&utm_medium=link&utm_source=publishsharelink  ## Description Supply chain is always hard to manage and requires a lot of admistrative machinery. However, when managed with smart contracts using blockchain, a lot of the paperwork is reduced. Also it leads to an increase in the transparency and helps to build an efficient Root of Trust. Supply-chain-dapp is such an implementation of a supply chain management system which uses blockchain to ensure a transparent and secure transfer of product from the manufacturer to the customer via the online e-commerce websites.  ## Architecture The smart contract is being written with Solidity which is then compiled, migrated and deployed using Truffle.js on the Gode Testnet blockchain network.The frontend uses Web3.js to communicate with the smart contract and Gode Testnet blockchain network and Meta Musk Wallet is connect to Gode Test Network to do Transaction between each component in Supply . **** ![https://raw.githubusercontent.com/faizack619/Supply-Chain-Gode-Blockchain/master/client/public/Blank%20diagram.png](https://raw.githubusercontent.com/faizack619/Supply-Chain-Gode-Blockchain/master/client/public/Blank%20diagram.png)  ## Supply Chain Flow   ![[https://cdn.vectorstock.com/i/1000x1000/35/51/diagram-of-supply-chain-management-vector-41743551.webp](https://cdn.vectorstock.com/i/1000x1000/35/51/diagram-of-supply-chain-management-vector-41743551.webp)](https://cdn-wordpress-info.futurelearn.com/info/wp-content/uploads/8d54ad89-e86f-4d7c-8208-74455976a4a9-2-768x489.png)    ## Smart Contract Working Flow  ![https://raw.githubusercontent.com/faizack619/Supply-Chain-Gode-Blockchain/master/client/public/Supply%20Chain%20Design%20(1).png?token=GHSAT0AAAAAAB52SPAT5YHI3AALNPFXL27AY7OU3IQ](https://raw.githubusercontent.com/faizack619/Supply-Chain-Gode-Blockchain/master/client/public/Supply%20Chain%20Design%20(1).png?token=GHSAT0AAAAAAB52SPAT5YHI3AALNPFXL27AY7OU3IQ)  This is a SupplyChain smart contract written in Solidity. The contract models the various roles and stages involved in the supply chain of a pharmaceutical product.  The contract owner is the person who deploys the contract and is the only one who can authorize various roles like retailer, manufacturer, etc.  There are several roles involved in the supply chain of the pharmaceutical product. These include the raw material supplier, manufacturer, distributor, and retailer.  The smart contract stores information about the medicine, such as its name, description, and current stage in the supply chain. There is also a function to show the current stage of a medicine, which can be used by client applications.  The smart contract also stores information about the various players in the supply chain, such as their name, address, and place of operation.  The addRMS(), addManufacturer(), addDistributor(), and addRetailer() functions can be used by the contract owner to add new players to the supply chain.  Overall, this smart contract provides a way to track the various stages of a pharmaceutical product in the supply chain, ensuring transparency and accountability.   ##  üîß Setting up Local Development  ### Step1. ## Installation and Setup  * **VSCODE** : VSCode can be downloaded from https://code.visualstudio.com/ * **Node.js** : Download the latest version of Node.js from https://nodejs.org/ and after installation check     Version using terimal: node -v . * **Git** : Download the latest version of Git from the official website at https://git-scm.com/downloads and   check Version using terimal: git --version.  * **Ganache** : Download the latest version of Ganache from the official website at https://www.trufflesuite.com/ganache. * **MetaMask** : can be installed as a browser extension from the Chrome Web Store or Firefox Add-ons store.    ### Step2. ## Create,Compile & Deploy Smart Contract.   * Open VScode and open VScode Terimal by Ctrl + ' . * **Clone Project** Type the following command and press Enter : git clone : ` https://github.com/faizack619/Supply-Chain-Blockchain.git   * **Install truffle** : Type the following command and press Enter: `npm install -g truffle` * **Install dependencies** : Type the following command and press Enter: `npm i` * **File structure for  DApp** :         **contracts**: This folder contains the Solidity smart contracts for the DApp. The Migrations.sol contract is automatically created by Truffle and is used for managing migrations.      **migrations**: This folder contains the JavaScript migration files used to deploy the smart contracts to the blockchain network.      **test**: This folder contains the JavaScript test files used to test the smart contracts.      **truffle-config.js**: This file contains the configuration for the Truffle project, including the blockchain network to be used and any necessary settings.      **package.jso**n: This file contains information about the dependencies and scripts used in the project.      **package-lock.json**: This file is generated automatically and contains the exact version of each dependency used in the project.      **Client**s: This Folder contains the client-side code, typically HTML, CSS, and JavaScript, can be organized into a client folder. * **Compile the smart contract** :  In the terminal, use the following command to compile the smart contract: `truffle compile`  * **Deploy the smart contract** :         * After Compile We Need To Deploy Your Smart Contract on Blockchain. In Our Case We are Using Ganache Which is personal blockchain for Ethereum development, used to test and develop Smart Contracts.      * Open Ganache and create new WorkSpace.Copy Rpc Server Address.      * ![https://miro.medium.com/max/1248/1*4rzNT0muOXelP22Ky9178g.png](https://miro.medium.com/max/1248/1*4rzNT0muOXelP22Ky9178g.png)      * The RPC server is used to allow applications to communicate with the Ethereum blockchain and execute smart contract transactions, query the state of the blockchain, and interact with the Ethereum network.      * Now to add Rcp address in our truffle-config.js and the replace host address and port address with Our Ganache Rcp.      * ![https://developers.rsk.co/assets/img/tutorials/truffle-test/image-04.png](https://developers.rsk.co/assets/img/tutorials/truffle-test/image-04.png)        * After Changing RCP address.Open terminal and run this cmd : `truffle migrate`.     * This Command Will deploye Smart Contract to Blockchain.  ### Step 3. ## Run DAPP.  * Open a second terminal and enter the client folder   * `cd client`   * Install all packages in the package.json file   * `npm i`    * Install Web3 in the package.json file   * `npm install -save web3`   * Run this Command :   * `npm`   * Run the app    * `npm start`  * The app gets hosted by default at port 3000.  ### Step 4. ## Connect Meta Musk with Ganache.   ![https://media.licdn.com/dms/image/C4D12AQHMatPDpLjwkA/article-cover_image-shrink_720_1280/0/1547586411238?e=2147483647&v=beta&t=UDYOS05BSkdrYoGOR5LW7v2uHz1Sca5uNzzWLrQG1nk](https://media.licdn.com/dms/image/C4D12AQHMatPDpLjwkA/article-cover_image-shrink_720_1280/0/1547586411238?e=2147483647&v=beta&t=UDYOS05BSkdrYoGOR5LW7v2uHz1Sca5uNzzWLrQG1nk) 1. Start Ganache: Start the Ganache application and make note of the RPC server URL and port number.  1. Connect MetaMask: Open MetaMask in your browser and click on the network dropdown in the top-right corner. ![https://metamask.zendesk.com/hc/article_attachments/10080831633947](https://metamask.zendesk.com/hc/article_attachments/10080831633947)![https://kimsereylam.com/assets/posts/2022-02-25-setup-local-development-blockchain-with-ganache/ganache_network.png](https://kimsereylam.com/assets/posts/2022-02-25-setup-local-development-blockchain-with-ganache/ganache_network.png) Select ""Custom RPC"" and enter the RPC server URL and port number for your Ganache instance. Click ""Save"".  1. Import an account: In Ganache, click on the ""Accounts"" tab and select the first account listed. Click on the ""Copy"" button next to the ""Private Key"" field copy the private key.      ![https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSoSc_d4naUQwI8qo8ClC1NXa4aJA7blvrgn4Xq1looUOiWY3wTGd5x8g5fgCrMzyrOzQ8&usqp=CAUto](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSoSc_d4naUQwI8qo8ClC1NXa4aJA7blvrgn4Xq1looUOiWY3wTGd5x8g5fgCrMzyrOzQ8&usqp=CAU)  2. In MetaMask, click on the three dots in the top-right corner, select ""Import Account"", and paste the private key into the private key field. Click ""Import"".       ![https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQs76Q1oyMK717kRZ8FMC_i2VCstu8H2yZFqlfgccSsalxBXWm2PBwzS-peIFv4DqGos9g&usqp=CAU](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQs76Q1oyMK717kRZ8FMC_i2VCstu8H2yZFqlfgccSsalxBXWm2PBwzS-peIFv4DqGos9g&usqp=CAU)  3. Add All participate(Raw Material,Supplier,Manufacture,Retail). by following above Step  ### License This project uses an [MIT](https://opensource.org/licenses/MIT) license. ## Documentation to help with Solidity https://docs.soliditylang.org/en/v0.8.4/ ## Documentation to help with React https://reactjs.org/docs/getting-started.html ## Documentation to help with Truffle https://www.trufflesuite.com/docs/truffle/reference/configuration ## Documentation to help with Ganache-cli https://www.trufflesuite.com/docs/ganache/overview"
Medical UseCase,Blockchain,https://github.com/IBM/Medical-Blockchain,"# Store private healthcare data off-chain and manage medical data using blockchain  Electronic medical records and data craves the need for innovation. The way patient health records are stored and secured today do not showcase our technological advancement in this area in the past decade, and hospitals continue to use age-old data management systems for patient data. This is partly due to strict regulations around privacy and security of medical data, which has stifled the use of latest technology to make medical data management more transparent and useful for both patients as well as doctors.  This code pattern showcases a medical data/access management platform built using blockchain. The application shows the platform from the point of view of 4 stakeholders - * The solution admin is the admin of a conglomerate of hospitals, and has the highest of access levels in the hierarchy. They have the ability to onboard a new organization (hospital) to the conglomerate and assign/de-assign hospital admins on their dashboard. * The organization (hospital) admin is the admin of a particular hospital which is part of the conglomerate/solution. They have the ability to onboard new users with the role of either patient or doctor, or remove a user. * The doctor is a user in the organization with the appropriate role and has the ability to upload documents for their patients and download/view documents of their patients to which they have been granted access. * The patient is a user in the organization with the appropriate role and has the ability to upload documents on their own, view them, view the document access logs and also manage access to their documents on their dashboard.  This code pattern is for developers who want to integrate with the Blockchain Solution Manager, Blockchain Document Store and the IBM Blockchain Platform. When you have completed it, you will understand how to:  * Connect the Blockchain Solution Manager and Blockchain Document Store with the IBM Blockchain Platform. * Create a VueJS web app that has multiple dashboards on a single page application, which can communicate in realtime with each other. * Create a NodeJS server that is deployed to Kubernetes on IBM Cloud, and connected with a Redis database deployed on the IBM Cloud. * Store and retrieve data from a Redis datastore for persistent storage through a NodeJS server. * Make REST calls to an external service. * Use JWT (JSON web token) tokens for user management.  # Architecture flow  ![Architecture flow](docs/doc-images/arch-flow.png?raw=true)  ### Login flow 1. All the stakeholders of the application (solution admin, hospital admin, doctor and patient) begin the user flow by logging into their respective dashboards. 2. Clicking the login button leads to the login portal of the Blockchain Solution Manager, hosted on the IBM cloud. 3. The login portal uses OpenAPI Connect and allows the user the login through any onboarded identity provider (in our example, we have on-boarded IBMID ad GoogleID). Successful authentication leads to the JWT credentials for the user.  ### Admin dashboard 4. The solution admin flow begins at the admin component, and requires the user to authenticate themselves through the login flow described above. 5. After successful authentication, the user can access the solution admin dashboard. They are able to view the solution, and add/remove hospitals from the solution using the Admin API's. 6. All the admin API's connect with the Blockchain Solution Manager through REST to process the user queries. 7. The Blockchain Solution Manager connects with the IBM Blockchain Platform and updates the ledger appropriately.  ### Organization dashboard 8. The hospital admin flow begins at the organization component, and requires the user to authenticate themselves through the login flow described above. 9. After successful authentication, the user can access the hospital admin dashboard. They are able to add/remove any user in their respective hospital with the on-boarded roles (patient/doctor in our case) using the organization API's. 10. All the organization API's connect with the Blockchain Solution Manager through REST to process the user queries. 11. The Blockchain Solution Manager connects with the IBM Blockchain Platform and updates the ledger appropriately.  ### Doctor dashboard 12. The doctor flow begins at the doctor component, and requires the user to authenticate themselves through the login flow described above. 13. After successful authentication, the user can access the doctor dashboard. They are able to upload a medical record for a patient who is part of their hospital and download any medical record associated with a patient to which they have access to, using the Doctor API's. The ACL's for all the patient documents is application level and is maintained through the Document ACL flow described below. 14. All the doctor API's connect with the Blockchain Document Store through REST to process the user queries. 15. The Blockchain Document Store connects with the IBM Blockchain Platform and updates the ledger appropriately.  ### Patient dashboard 16. The patient flow begins at the patient component, and requires the user to authenticate themselves through the login flow described above. 17. After successful authentication, the user can access the patient dashboard. They are able to upload a medical record for themselves, download any of their medical records, view the access logs of their documents, and view/manage permissions to their documents, using the Patient API's. The ACL's for all the documents is application level and is maintained through the document ACL flow described below. 18. All the patient API's connect with the Blockchain Document Store through REST to process the user queries. 19. The Blockchain Document Store connects with the IBM Blockchain Platform and updates the ledger appropriately.  ### Document access control list (ACL) flow 20. The doctor and patient component are connected with the Redis API's that invoke methods to manage the document level access control across hospitals. 21. The Redis API's talk to a NodeJS server deployed in a Docker container in a Kubernetes cluster on the IBM Cloud. 22. The server talks to two Redis databases which hold the access-per-document and access-per-user permissions.   # Included components  + [IBM Blockchain Platform](https://console.bluemix.net/docs/services/blockchain/howto/ibp-v2-deploy-iks.html#ibp-v2-deploy-iks) gives you total control of your blockchain network with a user interface that can simplify and accelerate your journey to deploy and manage blockchain components on the IBM Cloud Kubernetes Service. + [IBM Blockchain Solution Manager:](https://cloud.ibm.com/docs/services/blockchain-document-store?topic=blockchain-document-store-blockchain-solution-manager-api-acls) The Blockchain Document Store service includes the IBM Blockchain Solution Manager component, which enables organizations to easily manage blockchain networks, solutions, services, and users. + [IBM Blockchain Document Store](https://cloud.ibm.com/docs/services/blockchain-document-store?topic=blockchain-document-store-getting-started#getting-started) is a comprehensive document management service for IBM Blockchain Platform business networks. + [IBM Cloud Kubernetes Service](https://www.ibm.com/cloud/container-service) creates a cluster of compute hosts and deploys highly available containers. A Kubernetes cluster lets you securely manage the resources that you need to quickly deploy, update, and scale applications. + [IBM Cloud Databases for Redis Service:](https://console.bluemix.net/catalog/services/databases-for-redis) Redis is an open source, in-memory data structure store, used as a database, cache and message broker. It supports data structures such as strings, hashes, lists, sets, sorted sets with range queries, bitmaps, hyperloglogs and geospatial indexes with radius queries.  ## Featured technologies  * [Nodejs](https://www.nodejs.org/) is an open-source, cross-platform JavaScript run-time environment that executes JavaScript code server-side. * [Vuejs](https://vuejs.org/) is a progressive framework for building user interfaces. * [Redis](https://redis.io/) is an open source (BSD licensed), in-memory data structure store, used as a database, cache and message broker. * [Bootstrap](https://getbootstrap.com/) is a free and open-source front-end Web framework. It contains HTML and CSS-based design templates for typography, forms, buttons, navigation and other interface components, as well as optional JavaScript extensions. * [Docker](https://www.docker.com/) is a computer program that performs operating-system-level virtualization, also known as Containerization.  ## Prerequisites  We find that Blockchain can be finicky when it comes to installing Node. We want to share this [StackOverflow response](https://stackoverflow.com/questions/49744276/error-cannot-find-module-api-hyperledger-composer) - because many times the errors you see with Compose are derived in having installed either the wrong Node version or took an approach that is not supported by Compose:  * [IBM Cloud account](https://cloud.ibm.com/registration/?target=%2Fdashboard%2Fapps) * [Docker](https://www.docker.com/products) - latest * [Docker Compose](https://docs.docker.com/compose/overview/) - latest * [NPM](https://www.npmjs.com/get-npm) - latest * [nvm]() - latest * [Node.js](https://nodejs.org/en/download/) - Node v8.9.x * [Git client](https://git-scm.com/downloads) - latest   # Running the application  ## Manually deploy to local machine 1. [Set up your machine](#1-set-up-your-machine) 2. [Create IBM cloud services](#2-create-ibm-cloud-services) 3. [Create a solution](#3-create-a-solution) 4. [Clone the repository](#4-clone-the-repository) 5. [Modify the configuration files](#5-modify-the-configuration-files) 6. [Run the application](#6-run-the-application)  ### 1. Set up your machine  Install the following dependencies -  - [Docker](https://www.docker.com/): Go to the Docker website and download the installer. After installation, run Docker. - [git](https://git-scm.com/): Install `git` which is a free and open source distributed version control system.  ### 2. Create IBM cloud services  * Create the [IBM Cloud Kubernetes Service](https://cloud.ibm.com/catalog/infrastructure/containers-kubernetes).  You can find the service in the `Catalog`. For this code pattern, we can use the `Free` cluster, and give it a name.  Note, that the IBM Cloud allows one instance of a free cluster and expires after 30 days.  <br> <p align=""center"">   <img src=""docs/doc-gifs/1.gif""> </p> <br>  * Create two instances of [Databases for Redis Service](https://cloud.ibm.com/catalog/services/databases-for-redis).  You can find the service in the `Catalog`.  <br> <p align=""center"">   <img src=""docs/doc-gifs/2.gif""> </p> <br>    > Note: You can use just one instance of Redis as well. Modify the code in the server repository to allow for this.  * Create the [IBM Blockchain Service](https://cloud.ibm.com/catalog/services/ibm-blockchain-5-prod). You can find the service in the `Catalog`.  <br> <p align=""center"">   <img src=""docs/doc-gifs/3.gif""> </p> <br>  * Create the `Blockchain document store` and `Blockchain solution manager` services. These services are not currently available publicly on the `IBM cloud catalog`. You can reach out to `Rak-Joon Choi (rak-joon.choi@us.ibm.com)` to provision these services for you. Follow the service [documentation](https://cloud.ibm.com/docs/services/blockchain-document-store?topic=blockchain-document-store-getting-started#getting-started) to connect the `Blockchain document store` to the `Blockchain service`.  <br> <p align=""center"">   <img src=""docs/doc-gifs/4.gif""> </p> <br>  ### 3. Create a solution  * After configuring your services in the previous step, we now move on to creating a solution using our custom swagger url for the `blockchain solution manager` service. Go to the `Patch endpoint (/v1/solutions)` under `Solution` and authorize using the api by going to the `/v1/logins` url in a new tab, logging in as `Administrator`, and getting the JWT. Add the token prepended by `bearer` such that it looks like `bearer <JWT>`. After authorization, click on `try it out` to execute the api, and paste the following JSON in the `on-boarding` section. Give the name `medrec_demo` to the solution.  ``` {   ""onboardingdata"": {     ""solution"": {       ""id"": ""medrec_demo"",       ""name"": ""demo for medrec pattern""     },     ""roles"": [       {         ""id"": ""role_patient"",         ""name"": ""Patient"",         ""solutionId"": ""medrec_demo"",         ""isBlockchainRole"": true       },       {         ""id"": ""role_doctor"",         ""name"": ""Doctor"",         ""solutionId"": ""medrec_demo"",         ""isBlockchainRole"": true       }     ]   } } ```  <br> <p align=""center"">   <img src=""docs/doc-gifs/5.gif""> </p> <br>  * After creating the solution successfully, add yourself as the admin of the solution. Go to the `Post endpoint (/v1/solutions/{solutionId}/administrators)` under `Solution` and authorize using the api by going to the `/v1/logins` url in a new tab, logging in as `Administrator`, and getting the JWT. Add the token prepended by `bearer` such that it looks like `bearer <JWT>`. After authorization, click on `try it out` to execute the api, and type your email id under `solutionAdministrators` in the JSON object. Provide `medrec_demo` as the `solutionId`.  <br> <p align=""center"">   <img src=""docs/doc-gifs/6.gif""> </p> <br>  ### 4. Clone the repository  ``` git clone https://github.com/IBM/Medical-Blockchain.git cd Medical-Blockchain ```  ### 5. Modify the configuration files  * Modify the redis config file:   - Go to the previously provisioned redis services on IBM Cloud.   - Click on `Service credentials`.   - Click on `New credential` button.   - Once the new credentials are created, click on `view credentials`.   - From the JSON object, extract the URI from `connection.rediss.composed[0]`.   - From the JSON object, extract the certificate from `connection.rediss.certificate.certificate_base64`.   - Navigate to the `server/config.json` file in the cloned repository.   - Replace the URI and certificate values in the marked places.   - Repeat the steps for the second provisioned service, and enter it in the second spot in the config file.  <br> <p align=""center"">   <img src=""docs/doc-gifs/7.gif""> </p> <br>  * Modify the blockchain config file:   - Go to the `/v1/logins` url for your blockchain document store service.   - Login as administrator.   - Extract the `iss` field from the decoded JWT and remove `/onboarding` string from it.   - Navigate to the `src/secrets/config.json` file in the cloned repository.   - Replace the `iss` field with the extracted value above.   - Replace the `blockchain_channel` field with the name of the channel provided during connecting the blockchain service to the document store.  <br> <p align=""center"">   <img src=""docs/doc-gifs/8.gif""> </p> <br>  ### 6. Run the application  * Running the application locally:   - To run the application on the local system, execute the `run-application.sh` file.   - Go to `localhost:8080` to see the running application.  <br> <p align=""center"">   <img src=""docs/doc-gifs/9.gif""> </p> <br>  * Running the application on kubernetes:   - Navigate to server directory - `cd server`.   - Build the docker image for the server - `docker build -t <DOCKERHUB_USERNAME>/medrec-server .`   - Replace the image name in `manifest.yml`, where indicated.   - Apply the manifest to the previously provisioned kubernetes cluster.   - Navigate to `/src/apis/RedisApi.js` and replace the `baseURL` value with the Kubernetes load balancer IP.   - Build and run the Vue application by executing the below in the repository home.   - Go to `localhost:8080` to see the running application.  ``` docker build -t medrec-vue . docker run -d --restart always --name medrec-vue -p 8080:8080 medrec-vue ```  > Note: You can also deploy the Vue App to Kubernetes, by modifying the manifest.yml to support two pods.  # License  This code pattern is licensed under the Apache Software License, Version 2.  Separate third-party code objects invoked within this code pattern are licensed by their respective providers pursuant to their own separate licenses. Contributions are subject to the [Developer Certificate of Origin, Version 1.1 (DCO)](https://developercertificate.org/) and the [Apache Software License, Version 2](http://www.apache.org/licenses/LICENSE-2.0.txt).  [Apache Software License (ASL) FAQ](http://www.apache.org/foundation/license-faq.html#WhatDoesItMEAN)"
Todo App,Python-Django,https://github.com/shreys7/django-todo,"# django-todo A simple todo app built with django  ![todo App](https://raw.githubusercontent.com/shreys7/django-todo/develop/staticfiles/todoApp.png) ### Setup To get this repository, run the following command inside your git enabled terminal ```bash $ git clone https://github.com/shreys7/django-todo.git ``` You will need django to be installed in you computer to run this app. Head over to https://www.djangoproject.com/download/ for the download guide  Once you have downloaded django, go to the cloned repo directory and run the following command  ```bash $ python manage.py makemigrations ```  This will create all the migrations file (database migrations) required to run this App.  Now, to apply this migrations run the following command ```bash $ python manage.py migrate ```  One last step and then our todo App will be live. We need to create an admin user to run this App. On the terminal, type the following command and provide username, password and email for the admin user ```bash $ python manage.py createsuperuser ```  That was pretty simple, right? Now let's make the App live. We just need to start the server now and then we can start using our simple todo App. Start the server by following command  ```bash $ python manage.py runserver ```  Once the server is hosted, head over to http://127.0.0.1:8000/todos for the App.  Cheers and Happy Coding :)"
Password Generator,Python-Django,https://github.com/vinit-modi/Django-Password-Generator,"# Django-Password-Generator  <!-- PROJECT LOGO --> <br /> <p align=""center"">   <a href=""https://github.com/vinit-modi/Django-Password-Generator"">     <img src=""./static/logo.svg"" alt=""Logo"" width=""300"" height=""300"">   </a>    <h3 align=""center"">Password Generator</h3>    <p align=""center"">     A Password generator tool that creates secure passwords that are impossible to crack.!     <br />   </p> </p>  <!-- TABLE OF CONTENTS --> <br>  ## Table of Contents  * [About the Project](#about-the-project)   * [Built With](#built-with) * [Getting Started](#getting-started)   * [Prerequisites](#prerequisites)   * [Installation](#installation) * [Roadmap](#roadmap) * [Contributing](#contributing) * [Contact](#contact)  ## About The Project  [![Product Name Screen Shot][product-screenshot]](https://github.com/vinit-modi/Django-Password-Generator/blob/master/static/screenshot1.jpg) [![Product Name Screen Shot][product-screenshot-2]](https://github.com/vinit-modi/Django-Password-Generator/blob/master/static/screenshot2.jpg)   [product-screenshot]: /static/screenshot1.jpg [product-screenshot-2]: /static/screenshot2.jpg   ‚Ä¢ To prevent the passwords from being hacked by social engineering, brute force or dictionary attack method, and keep your online accounts safe. <br /> ‚Ä¢ A Password generator is a simple website that generates a random password based on user-selected criteria. <br /> ‚Ä¢ User can select among Uppercase, special character and numbers. <br /> ‚Ä¢ Also, User is able to select the length of generated password. <br /> ‚Ä¢ The app will run in the browser and feature dynamically updated HTML and CSS powered by the Django code. <br />  ### Built With I had fun while making this website with django environment. Also we all know the power of Bootstrap. * [Bootstrap](https://getbootstrap.com) * [Django](https://www.djangoproject.com/)  <!-- GETTING STARTED --> ## Getting Started  To setting up project locally, You need to follow these simple example steps.  ### Prerequisites  Download the python. * [Python](https://www.python.org/downloads/)  You need to have Django installed in your machine. * Django ```sh $ python -m pip install Django ``` Also, make sure git is installed in your machine. * [Git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git)  ### Installation  1. Clone the repo ```sh git clone https://github.com/vinit-modi/Django-Password-Generator ``` 2. Open the folder in code editor  3. Run the following command in Command Prompt ```sh python manage.py runserver ```  <!-- ROADMAP --> ## Roadmap  See the [open issues](https://github.com/vinit-modi/Django-Password-Generator/issues) for a list of proposed features (and known issues).    <!-- CONTRIBUTING --> ## Contributing  Contributions are what make the open source community such an amazing place to be learn, inspire, and create. Any contributions you make are **greatly appreciated**.  1. Fork the Project 2. Create your Feature Branch (`git checkout -b 'Branch Name'`) 3. Commit your Changes (`git commit -m 'Add some AmazingFeature'`) 4. Push to the Branch (`git push origin 'Branch Name'`) 5. Open a Pull Request  <!-- CONTACT --> ## Contact  Vinit Modi - [@LinkedIn](https://www.linkedin.com/in/vinit-modi/) - vinitmodi108@gmail.com  "
Personal Portfolio,Python-Django,https://github.com/BurhanMohammad/Django-Portfolio-website,"# # My Personal Portfilio Website (Django)  Personal portfolio website made with Django framework in the backend, and with CSS, JS, and Bootstrap for the frontend. It is a dynamic site so that you can control the content of the site through the admin area  ## Features  - Light/dark mode toggle - Live previews - Fullscreen mode   ## Demo  (https://user-images.githubusercontent.com/104616403/210330707-c8607c2b-6929-435f-bb58-d3176454b4fa.gif)   ## Screenshots  ![image](https://user-images.githubusercontent.com/104616403/210331817-c266db91-fcfa-40e2-99ad-8c857c55051d.png)  ![image](https://user-images.githubusercontent.com/104616403/210332043-779ddac8-b1a7-4414-b5bd-ebe67d4200df.png) ![image](https://user-images.githubusercontent.com/104616403/210332104-4a4556c7-5583-43bc-b39a-bc62b6b5e367.png)  ![image](https://user-images.githubusercontent.com/104616403/210332171-190afc75-9174-4ffa-b9d2-3541b5fd0ebb.png)  ![image](https://user-images.githubusercontent.com/104616403/210332226-cbfbe6e5-732d-4442-9aae-218b6e6292b0.png)  ![image](https://user-images.githubusercontent.com/104616403/210332284-9d69c838-ab8e-4e71-ac6b-2f0ed514561a.png)    ## üîó Links [![linkedin](https://img.shields.io/badge/linkedin-0A66C2?style=for-the-badge&logo=linkedin&logoColor=white)](www.linkedin.com/in/burhanmohammad)   ## üõ† Skills Javascript, HTML, CSS, python, Django, Bootstrap...   # Hi, I'm Mohammad Burhan! üëã   ## üöÄ About Me I'm a full stack developer...   ## Feedback  If you have any feedback, please reach out to us at burhanmohammad1234@out.com   ## Tech Stack  **Client:** Html, css, Bootstrap  **Server:** python, Django   ## Run Locally  Clone the project  ```bash   git clone https://github.com/BurhanMohammad/Django-portfilio-website.git ```  Go to the project directory  ```bash   cd Django-portfilio-websitet ```  MAKE  Migration  ```bash   python manage.py makemigrations ```  MAKE  Migration  ```bash   python manage.py migrate      ``` Start the server  ```bash   python manage.py runserver      ```   ## Authors  - [@Mohammad Burhan](https://github.com/BurhanMohammad) "
Stone Paper Scissor,Python-Django,https://github.com/ShivamRohilllaa/rock-paper-scissors,# rock-paper-scissors  # Rock Paper and Scissors Game -- Python and Django  # Live Demo:- https://stonepsgame.herokuapp.com/   ###### pip install -r requirements.txt  ## Homepage ![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/y5ntqhzchkgcaonp5byk.png)  ## Start Game ![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/fpu0jjhmbx6kafiduaz5.png)  ## Play Game ![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/wv74ldkvg20aevbccxov.png)  ## Result ![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/xxip1o4fa6a79s8iwdk8.png)
Billing System,Python-Django,https://github.com/mohdishaq786/Django-Billing-system,"# Django Billing system this project "" billing system"" provides us a simple interface for maintenance of student,any type of billing record. It can be used by educational institutes to maintain the records of student easily SOFTWARE REQUIREMENT python 3 or above django vscode  first step create virtual evironment in your system     1.pip install virualenvwrapper-win     2.mkvirtual test                        (test=anyname u take as virtual env name)  To start virtual environment  use this code  workon test  Seond step   intsall django    pip install django     mkdir projects     now start projects   -- django-admin startproject projectname                    # to run server      python manage.py runserver    "
Movie Ticket Booking System,Python-Django,https://github.com/bhaskars9/bookmyticket,"# BookMyTicket  An Application to book movie tickets.    ## Description  The application has two users Theatre staff and users who book tickets. The theatre staff can Create, Update, Delete Movies and add Shows for each movie on any day.  The end users can book tickets after creating an account. The application is developed using Django the database used is PostgreSQL.   #### Flow of app names (dependency chain, for reference) ``` django-admin startproject movieticket cd movieticket django-admin startapp accounts django-admin startapp staff django-admin startapp booking ``` - add app names,  - change default user model, to create custom user model - change database configuration in .env file file.  ## UI Preview  ### User End / Customer / Booking App (gif @1fps) ![movie ticket project](https://user-images.githubusercontent.com/37036491/211182417-084e8044-2733-4272-ab7e-7c50ce8f5a89.gif)  ### Admin End / Theatre Staff (gif @1fps) ![movie ticket project 2](https://user-images.githubusercontent.com/37036491/211182565-7e12ab00-0f85-448a-be37-76fd35195167.gif)   ## Getting Started  ### Dependencies  Python, Pip, Virtualenv, Django, Other (psycopg2-binary, crispy-bootstrap5, django-crispy-forms, psycopg2, python-dotenv) ``` pip install -r requirements.txt ```  ### Running the program  ``` Python manage.py runserver ```  ## Authors [Bhaksara](https://github.com/bhaskars9)  ## License  This project is licensed under the MIT License - see the LICENSE.md file for details  ## Acknowledgments  Inspiration, code snippets, etc. * [StartBootstrap](https://github.com/StartBootstrap/startbootstrap-sb-admin) * [Caroline Rodrigues](https://codepen.io/loracsilva/pen/ZrRYVL) * [Pavlos](https://codepen.io/paulantoniou/pen/RdBogQ?editors=1100) * [Coolors](https://coolors.co/02010a-04052e-140152-fff309-0d00a4)"
Email Sender,Python-Django,https://github.com/sukanya-pai/Django-Email-Sender,"# Django-Email-Sender Send email from Gmail Id in Django using HTML Template  ## Pre-Requisites - Python 3.8 - Pip - Git  ## Steps to run:  - Clone the project using the command **git clone https://github.com/sukanya-pai/Django-Email-Sender.git**  - In PipFile, the dependencies are mentioned. It is recommended to run the app inside a virtual environment to avoid conflict of existing dependencies.    - Run the command `pip install pipenv`    - Run `pipenv shell`. Creates virtual env    - Run `pipenv install`. Installs dependencies from the PipFile and creates PipFile.lock    - Run `python email_project/manage.py runserver` to start the server    - Open [http://127.0.0.1:8000/api/send-mail](http://127.0.0.1:8000/api/send-mail) to start  - Make changes in the **views.py** file of the **email_sender_app** directory inside the `send_mail()` method to add your *mail address* and the *recipients mail address* accordingly.  - Make changes in the **settings.py** of the **email_project** directory file with your **gmail id** and **your app password**.     - Since this is a demo project, the password is written directly in settings.py. For security reasons of your account, it is recommended you store the password in key vault or encrypt it and then host the application or push your changes to GitHub.  - Read [change log](change.log) to see what has changed.  ## Mandatory changes to be done in Host Gmail account - Go to https://myaccount.google.com/security  - Scroll till you find **Signing in to Google**    - In that section, you will see the ""App Passwords"" option as shown in the below image.    ![img.png](images/path_to_app_pwd.png)    - When you click on **App Passwords**, you will be asked to enter your Gmail account password. Enter it and the page would open:    - Follow the below steps as shown in the image to setup your app password.    ![SetupAppPassword](images/setup_app_pwd.jpg)  - Only if you follow the above steps, then you can send mail from your Gmail account using your django code. "
Web Scraper,Python-Django,https://github.com/shubham17sm/web-scraping-django,# web-scraping-django A news aggregator app build using [Django](https://www.djangoproject.com/) web framework and [beautifulsoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) which is use to scrape the news articles from the web and uses [celery](https://docs.celeryproject.org/en/stable/django/first-steps-with-django.html) as a task queue to add srcaped article in database. This app is also provided with API using [Django rest-framework](https://www.django-rest-framework.org/)    # Getting Started  First Create and start your virtual environment: Clone or download this repository ```   virtualenv -p python3 .   pip install -r requirement.txt   python3 manage.py runserver  ```  Command for starting worker process in celery: ``` celery -A myproject worker --loglevel=info  ```  # Post Installation Go to the web browser and visit http://127.0.0.1:8000/  Admin username: admin  Admin password: admin 
Palindrome Checker,Python-Django,https://github.com/ramshree/palindromer-checker,# palindromer-checker   A django/Python app to validate strings if its a palindrome or not.   <h3>System requirements</h3>  1. python 3 or greater 2. mac or windows 3. django 2.0.4 or greater 4. latest version of PyCharm editor 5. github for version control 6. Heroku to host the app   <h3>Setup instructions on Mac (Use terminal)</h3>  1. Open Terminal (Applications > Utilities > Terminal)  2. Install python 3.6 with homebrew from terminal     ~$ brew install python  3. Install pip (Python Package installer)     ~$ sudo easy_install pip  4. Install virtualenv (this is to contain all the changes we make instead of global)     ~$ sudo pip install virtualenv  5. Create a folder pythondev to store all the code     a. ~$ mkdir pythondev     b. ~$ cd pythondev  6. Create a new virtualenv (django-dev is our virtual env)     ~$ virtualenv django-dev -p python3.6  7. Activate virtualenv (the resulting command line will point to django-dev)     ~$ . bin/activate  8. Install Django     ~$ pip install django  9. Create a project     ~$ django-admin startproject palindrome-checker     ~$ cd palindrome-checker  10. I usually start the server to see how my project is building. Leave this open and server running.     To shutdown Ctrl+C     ~$ python manage.py runserver  11. Open the folder from PyCharm (File-> Open) and select the palindrome-checker folder  12. To run the tests. Look for the test result     ~$ python manage.py test      <h3>Hosting on heroku</h3>  1. Create or login to <href>https://heroku.com</href>  2. Link your github account to pull the app   3. Setup automatic deployment of app after commit 
Sorting Visualizer,Python-Django,https://github.com/jntushar/Sorting-Visualizer,"# Sorting-Visualizer  ![Django](https://img.shields.io/badge/Django-red) ![Python](https://img.shields.io/badge/Python-yellowgreen)  Welcome to Sorting Visualizer! I built this application because I was fascinated by sorting algorithms, and I wanted to visualize them in action. I hope that you enjoy playing around with this visualization tool just as much as I enjoyed building it. You can access it here: http://sortingvisualize.herokuapp.com  ![Sorting-Visualizer](VS.gif)"
Reminder System,Python-Django,https://github.com/arianshnsz/Django-Task-Reminder,"# Django Task Reminder  This is a simple application for managing to-do lists.  Users would be allowed to:    - Add, Modify, Finish, and Remove tasks.    - Assign tasks to other users.    - Set Reminders for their tasks.     For notifications, this project uses DWR-910 4G LTE USB Router to send SMS to users.  ## How to run  1. Make sure `python3` and `pip` are installed in your system. 2. clone the project and make the development environment ready:  ```bash git clone https://github.com/arianshnsz/Django-Task-Reminder.git python -m venv .venv # Create a virtual environment called .venv source .venv/bin/activate # activate the virtual environment pip install -r requirements.txt # install the required packages ``` <details>  <summary>  3. Generate the Django Secrete key (click to show the steps):  </summary>     * Access the Python Interactive Shell:        ```bash    django-admin shell    ```        * Import the `get_random_secret_key()` function from `django.core.management.utils`:        ```bash    from django.core.management.utils import get_random_secret_key    ```        * Generate the Secrete key using `get_random_secret_key()` function:        ```bash    get_random_secret_key()    ```        * In the existing directory, create a file name `.env` and paste the following line inside it:        ```    SECRET_KEY = ""... paste your generated secret key ...""    ``` </details>  4. Create database tables: ```bash python manage.py migrate ``` 5. Run the project and visit the following website. ```bash python manage.py runserver ```  6. (Optional) Connect to DWR-910 device and run the sms script for sending sms to users:  ```bash python manage.py runscript task_notification ```    This will run `scripts/task_notification.py`.     ## TODO  - [ ] add SIM800 module - [ ] accept requests before assigning tasks - [ ] international phone number check"
Automatic Birthdays Mailer,Python-Django,https://github.com/ashutoshkrris/PyWisher,"![Open Source? Yes!](https://badgen.net/badge/Open%20Source%20%3F/Yes%21/blue?icon=github) [![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT) ![GitHub stars](https://img.shields.io/github/stars/ashutoshkrris/PyWisher?style=social) ![GitHub forks](https://img.shields.io/github/forks/ashutoshkrris/PyWisher?style=social) [![GitHub pull-requests](https://img.shields.io/github/issues-pr/ashutoshkrris/PyWisher.svg)](https://GitHub.com/ashutoshkrris/PyWisher/pull/)  # PyWisher  We often forget birthdays of our friends and families. But PyWisher will never. Just upload an excelsheet or add your data once and PyWisher will remember it forever and will send birthday wishes on their email every year on your behalf. So stop missing birthdays and start sending wishes.  ## Website Link   You can visit the site [here](https://pywisher.ml/) or https://pywisher.ml/  ## Features   - Upload Excelsheet - Add data manually - Send emails automatically  ## Working Demo  * **Homepage**  <img src=""https://github.com/ashutoshkrris/PyWisher/blob/development/demo/home.png"" alt=""Homepage"" width=1000 height=500/>  * **Upload Excelsheet**  <img src=""https://github.com/ashutoshkrris/PyWisher/blob/development/demo/upload.png"" alt=""Upload Excelsheet"" width=1000 height=500/>  * **Add Data**  <img src=""https://github.com/ashutoshkrris/PyWisher/blob/development/demo/add.png"" alt=""Add Data"" width=1000 height=500/>  * **Email**  <img src=""https://i.imgur.com/gHuhMmN.png"" alt=""Email Template""/>  * **Birthday Card**  <img src=""https://github.com/ashutoshkrris/PyWisher/blob/development/demo/card.png"" alt=""Birthday Card"" width=1000 height=500/>   ## Getting Started  * [Fork this repository](https://github.com/ashutoshkrris/PyWisher/fork) and clone the forked repository  * Change the working directory to the folder where you downloaded the files  * Install all the dependencies using the pip command :    `pipenv install`  * After successful installation of all packages, run the follwing Django commands :      `py manage.py migrate`    `py manage.py runserver`  * Visit `127.0.0.1:8000` in your browser and enjoy the app.  Built using Django 3.1 and Python 3.8.5 by Ashutosh Krishna  Please don't forget to ‚≠ê the repository if you liked it."
Snake Water Gun Game,Python-Django,https://github.com/sagargoswami2001/Snake-Water-Gun,"# Snake Water Gun Game Using Python:- Snake Water Gun is one of the famous two-player game played by many people. It is a hand game in which the player randomly chooses any of the three forms i.e. snake, water, and gun. Here, we are going to implement this game using python.   This python project is to build a game for a single player that plays with the computer  ## Following are the rules of the game: **Snake vs. Water:** Snake drinks the water hence wins.  **Water vs. Gun:** The gun will drown in water, hence a point for water  **Gun vs. Snake:** Gun will kill the snake and win.  In situations where both players choose the same object, the result will be a draw.  ## Random module: The random module is a built-in module to generate the pseudo-random variables. It can be used perform some action randomly such as to get a random number, selecting a random elements from a list, shuffle elements randomly, etc.  We will use random.choice() method and nested if-else statements to select a random item from a list."
Snake Water Gun Game,Python-Django,https://github.com/sagargoswami2001/Snake-Water-Gun,"# Snake Water Gun Game Using Python:- Snake Water Gun is one of the famous two-player game played by many people. It is a hand game in which the player randomly chooses any of the three forms i.e. snake, water, and gun. Here, we are going to implement this game using python.   This python project is to build a game for a single player that plays with the computer  ## Following are the rules of the game: **Snake vs. Water:** Snake drinks the water hence wins.  **Water vs. Gun:** The gun will drown in water, hence a point for water  **Gun vs. Snake:** Gun will kill the snake and win.  In situations where both players choose the same object, the result will be a draw.  ## Random module: The random module is a built-in module to generate the pseudo-random variables. It can be used perform some action randomly such as to get a random number, selecting a random elements from a list, shuffle elements randomly, etc.  We will use random.choice() method and nested if-else statements to select a random item from a list."
Mobile Assistant,Python-Django,https://github.com/vintasoftware/django-ai-assistant/,"[![CI](https://github.com/vintasoftware/django-ai-assistant/actions/workflows/ci.yml/badge.svg)](https://github.com/vintasoftware/django-ai-assistant/actions/workflows/ci.yml) [![Coverage Status](https://coveralls.io/repos/github/vintasoftware/django-ai-assistant/badge.svg?branch=main)](https://coveralls.io/github/vintasoftware/django-ai-assistant?branch=main) [![Discord Server](https://img.shields.io/discord/1260577482122203206)](https://discord.gg/mqdubnPb)  # django-ai-assistant  <img align=""left"" src=""https://raw.githubusercontent.com/vintasoftware/django-ai-assistant/main/docs/images/favicon.svg"" height=""120"" alt=""robot logo"">  Combine the power of LLMs with Django's productivity to build intelligent applications. Let AI Assistants call methods from Django's side and do anything your users need!  Use AI Tool Calling and RAG with Django to easily build state of the art AI Assistants.  Please check the documentation: [https://vintasoftware.github.io/django-ai-assistant/](https://vintasoftware.github.io/django-ai-assistant/)  ## 5 minute demo  https://github.com/vintasoftware/django-ai-assistant/assets/397989/715d7608-e657-4dce-a7be-2909679f814b  ## Community  Feel free to ask questions and share your work on our [Discord Server](https://discord.gg/fQfH9PkJM6)!  ## Contributing  You're welcome to contribute with Django AI Assistant! Please feel free to tackle existing [issues](https://github.com/vintasoftware/django-ai-assistant/issues). If you have a new idea, please create a thread on [Discussions](https://github.com/vintasoftware/django-ai-assistant/discussions).   Check our [contributing guide](CONTRIBUTING.md) to learn more about how to develop and test the project locally, before opening a pull request.  ## Commercial Support  [![alt text](https://avatars2.githubusercontent.com/u/5529080?s=80&v=4 ""Vinta Logo"")](https://www.vintasoftware.com/)  This is an open-source project maintained by [Vinta Software](https://www.vinta.com.br/). We are always looking for exciting work! If you need any commercial support, feel free to get in touch: contact@vinta.com.br"
Calculator,Python-Django,https://github.com/BurhanMohammad/Django-Calculator,"  # Django based  Calculator   #### Django Calculator is a simple web application built with Django that provides basic calculator functionality. >   [![Maintenance](https://img.shields.io/badge/maintained-yes-green.svg)](https://github.com/rajaprerak/MusicPlayer/commits/master) [![License](http://img.shields.io/:license-mit-blue.svg?style=flat-square)](http://badges.mit-license.org)  This project is built with :  ![HTML5](https://www.w3.org/html/logo/downloads/HTML5_Logo_64.png) , ![CSS3](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d5/CSS3_logo_and_wordmark.svg/48px-CSS3_logo_and_wordmark.svg.png) , ![Vanilla JS](https://upload.wikimedia.org/wikipedia/commons/thumb/9/99/Unofficial_JavaScript_logo_2.svg/64px-Unofficial_JavaScript_logo_2.svg.png) , ![Python](https://www.quintagroup.com/++theme++quintagroup-theme/images/logo_python_section.png) , ![Django](https://www.quintagroup.com/++theme++quintagroup-theme/images/logo_django_section.png)    ## Installation üì¶ ### To install Django Music Player, follow these steps: ## 1. Clone this repository: >'https://github.com/BurhanMohammad/Django-Calculator.git' ## 2. Navigate to the project directory:  ```bash   cd Django-Calculator ``` ## 3 . Create a virtual environment: ```bash   python3 -m venv env ``` ## 4. Activate the virtual environment: ```bash   source env/bin/activate ``` ## 5. Install the project dependencies: ```bash   pip install -r requirements.txt ``` ## 6 . Run the server ```bash   python manage.py runserver ``` ## 7 . Go to localhost:8000 ---  ## Features of this project:  ### Django Calculator includes the following features:  #### 1 . Basic calculator functions: Users can add, subtract, multiply, and divide two numbers. #### 2 . Responsive design: The app is designed to be mobile-friendly. ---  ## Usage: ### To install Django Music Player, follow these steps: ## 1. Run the server:  ```bash   python manage.py runserver ```  ## 2. Open your web browser and go to: >'http://localhost:8000/' ## 3 . Sign up for a new account or log in with an existing one.  ## 4. Enter two numbers and select the operation you want to perform.   ## 5. Click the ""Calculate"" button to perform the operation.    ## Contributing üí°  #### If you'd like to contribute to Django Calculator, feel free to fork this repository and submit a pull request. For more information on contributing to the project, please check out my repository.   #### Step 1  - **Option 1**     - üç¥ Fork this repo!  - **Option 2**     - üëØ Clone this repo to your local machine.   #### Step 2  - **Build your code** üî®üî®üî®  #### Step 3  - üîÉ Create a new pull request. ## Creadits :  #### Django Calculator was created by Mohammad Burhan   ## License [![License](http://img.shields.io/:license-mit-blue.svg?style=flat-square)](http://badges.mit-license.org)  - **[MIT license](http://opensource.org/licenses/mit-license.php)**"
CRUD REST API in Flask and MongoDB,Python-Django,https://github.com/rishi772001/Django-MongoDb-CRUD-RESTAPI,"# Django-MongoDB-CRUD-RESTAPI  Basic create, read, update and delete for students marks and total is here with super cool User Interface. MongoDB, A NoSQL database is used to store data. A Restful web service for accessing the students details is created and deployed into a docker container for easy management of infrastructure  ## Run application in docker container 1. Install Docker Desktop [here](https://www.docker.com/get-started) 2. Go to the project root folder and run the following commands       - `docker-compose build`     - `docker-compose up` 3. Go to [127.0.0.1:8000](http://127.0.0.1:8000) to view the output  ## Dependencies - Django `pip install django`   - Djongo `pip install djongo`   - [MongoDB](https://fastdl.mongodb.org/windows/mongodb-windows-x86_64-4.4.2-signed.msi)   - Django Rest Framework `pip install djangorestfamework`    ## Start the MongoDB server First we need to create the db directory where the database files will live in. In your terminal navigate to the root of your system by doing `cd..` until you reach the top directory. You can create the directory by running `mkdir /data/db`. Now open a different tab in your terminal and run `mongod` to start the Mongo server.  ## Run Go to the project directory and run `python manage.py runserver`  ## Access REST API `/students` retrieves all student details   `/students/<id>` retrieves details of that particular student  ## Screenshot ![CRUD](https://github.com/rishi772001/django-mongodb-crud/blob/main/screenshots/Capture.PNG) --- ![REST API](https://github.com/rishi772001/django-mongodb-crud/blob/main/screenshots/Capture1.PNG)  "
Calorie Counter,Python-Django,https://github.com/nitish-gautam/django-calorie-tracker,"# django-calorie-tracker The intention here is to create a web app using Djnago that tracks calories intake and track the calories consumptions.  # SKILL SET REQUIRED:  Django, Python, BootStrap, HTML, Javascript, CSS   # RESULT: ![image](https://user-images.githubusercontent.com/46977634/92306334-36119700-ef86-11ea-83ac-ef08373529ff.png)"
Student Management System,Python-Django,https://github.com/jobic10/student-management-using-django,"# Student Management System Created Using Django This is a Simple Student Management System Developed While Learning Django. Feel free to make changes based on your requirements.  [Front-end Template](http://adminlte.io ""Admin LTE.io"")   [Project Demo on YouTube](https://www.youtube.com/watch?v=kArCR96m7uo ""Django Student Management System Demo"")  And if you like this project, then ADD a STAR ‚≠êÔ∏è  to this project üëÜ ## Deployed to https://smswithdjango.herokuapp.com/   ## Features of this Project  ### A. Admin Users Can 1. See Overall Summary Charts of Students Performances, Staff Performances, Courses, Subjects, Leave, etc. 2. Manage Staff (Add, Update and Delete) 3. Manage Students (Add, Update and Delete) 4. Manage Course (Add, Update and Delete) 5. Manage Subjects (Add, Update and Delete) 6. Manage Sessions (Add, Update and Delete) 7. View Student Attendance 8. Review and Reply Student/Staff Feedback 9. Review (Approve/Reject) Student/Staff Leave  ### B. Staff/Teachers Can 1. See the Overall Summary Charts related to their students, their subjects, leave status, etc. 2. Take/Update Students Attendance 3. Add/Update Result 4. Apply for Leave 5. Send Feedback to HOD  ### C. Students Can 1. See the Overall Summary Charts related to their attendance, their subjects, leave status, etc. 2. View Attendance 3. View Result 4. Apply for Leave 5. Send Feedback to HOD   ## üì∏ ScreenShots  <img src=""ss/1.png""/> <img src=""ss/2.png""/> <img src=""ss/3.png""/> <img src=""ss/4.png""/> <img src=""ss/5.png""/>  | Admin| Staff| Student | |------|-------|---------| |<img src=""ss/admin5.png"" width=""400"">|<img src=""ss/staff1.png"" width=""400"">|<img src=""ss/student1.png"" width=""400"">| |<img src=""ss/admin2.png"" width=""400"">|<img src=""ss/staff2.png"" width=""400"">|<img src=""ss/student2.png"" width=""400"">| |<img src=""ss/admin3.png"" width=""400"">|<img src=""ss/staff3.png"" width=""400"">|<img src=""ss/student3.png"" width=""400"">| |<img src=""ss/admin4.png"" width=""400"">|<img src=""ss/staff4.png"" width=""400"">|<img src=""ss/student4.png"" width=""400"">| |<img src=""ss/admin1.png"" width=""400"">|<img src=""ss/staff5.png"" width=""400"">|<img src=""ss/student5.png"" width=""400"">| |<img src=""ss/admin6.png"" width=""400"">|<img src=""ss/staff6.png"" width=""400"">|<img src=""ss/student6.png"" width=""400"">|    ## Support Developer 1. Add a Star üåü  to this üëÜ Repository 2. Follow on Twitter/Github   ## Passport/Images Images are from [Unsplash](https://unsplash.com)   ## How to Install and Run this project?  ### Pre-Requisites: 1. Install Git Version Control [ https://git-scm.com/ ]  2. Install Python Latest Version [ https://www.python.org/downloads/ ]  3. Install Pip (Package Manager) [ https://pip.pypa.io/en/stable/installing/ ]  *Alternative to Pip is Homebrew*  ### Installation **1. Create a Folder where you want to save the project**  **2. Create a Virtual Environment and Activate**  Install Virtual Environment First ``` $  pip install virtualenv ```  Create Virtual Environment  For Windows ``` $  python -m venv venv ``` For Mac ``` $  python3 -m venv venv ``` For Linux ``` $  virtualenv . ```  Activate Virtual Environment  For Windows ``` $  source venv/scripts/activate ```  For Mac ``` $  source venv/bin/activate ```  For Linux ``` $  source bin/activate ```  **3. Clone this project** ``` $  git clone https://github.com/jobic10/student-management-using-django.git ```  Then, Enter the project ``` $  cd student-management-using-django ```  **4. Install Requirements from 'requirements.txt'** ```python $  pip3 install -r requirements.txt ```  **5. Add the hosts**  - Got to settings.py file  - Then, On allowed hosts, Use **[]** as your host.  ```python ALLOWED_HOSTS = [] ``` *Do not use the fault allowed settings in this repo. It has security risk!*   **6. Now Run Server**  Command for PC: ```python $ python manage.py runserver ```  Command for Mac: ```python $ python3 manage.py runserver ```  Command for Linux: ```python $ python3 manage.py runserver ```  **7. Login Credentials**  Create Super User (HOD) Command for PC: ``` $  python manage.py createsuperuser ```  Command for Mac: ``` $  python3 manage.py createsuperuser ```  Command for Linux: ``` $  python3 manage.py createsuperuser ```    Then Add Email and Password  **or Use Default Credentials**  *For HOD /SuperAdmin* Email: admin@admin.com Password: admin  *For Staff* Email: staff@staff.com Password: staff  *For Student* Email: student@student.com Password: student    ## For Sponsor or Projects Enquiry 1. Email - jobowonubi@gmail.com 2. LinkedIn - [jobic10](https://www.linkedin.com/in/jobic10 ""Owonubi Job Sunday on LinkedIn"") 2. Twitter - [jobic10](https://www.twitter.com/jobic10 ""Owonubi Job Sunday on Twitter"")    ## Project's Journey - [x] Admin/Staff/Student Login - [x] Add and Edit Course - [x] Add and Edit Staff - [x] Add and Edit Student - [x] Add and Edit Subject - [x] Upload Staff's Picture - [x] Upload Student's Picture - [x] Sidebar Active Status - [x] Named URLs - [x] Model Forms for adding  student - [x] Model Forms for all - [x] Views Permission (MiddleWareMixin) - [x] Attendance and Update Attendance - [x] Password Reset Via Email - [x] Apply For Leave - [x] Students Can Check Attendance - [x] Check Email Availability - [x] Reply to Leave Applications - [x] Reply to Feedback - [x] Admin View Attendance - [x] Password Change for Admin, Staff and Students using *set_password()* - [x] Admin Profile Edit - [x] Staff Profile Edit - [x] Student Profile Edit - [x] Student Dashboard Fixed - [x] Passing Page Title From View  - Improved - [x] Staff Dashboard Fixed - [x] Admin Dashboard Fixed - [x] Firebase Web Push Notifications - [x] Staff Add Student's Result - [x] Staff Edit Result Using CBVs (Class Based Views) - [x] Google CAPTCHA - [x] Student View Result - [x] Change all links to be dynamic - [x] Code Restructure - Very Important   ## Questions I asked While Developing This - https://stackoverflow.com/questions/63829896/is-there-a-specific-way-of-adding-apps-in-django/   ## Helpful Links - https://stackoverflow.com/questions/55969952/how-can-i-avoid-a-user-from-registering-an-already-used-email-in-django - https://stackoverflow.com/questions/7562573/how-do-i-get-django-forms-to-show-the-html-required-attribute - https://stackoverflow.com/questions/40910149/django-exists-versus-doesnotexist - https://www.edureka.co/community/80982/how-can-i-have-multiple-models-in-a-single-django-modelform - https://stackoverflow.com/questions/12848605/django-modelform-what-is-savecommit-false-used-for - https://simpleisbetterthancomplex.com/tutorial/2018/01/18/how-to-implement-multiple-user-types-with-django.html - https://stackoverflow.com/questions/32576348/how-can-i-create-django-modelform-for-an-abstract-model - https://www.fomfus.com/articles/how-to-use-email-as-username-for-django-authentication-removing-the-username - https://stackoverflow.com/questions/64145745/create-user-missing-1-required-positional-argument-username?noredirect=1#64145844 - https://stackoverflow.com/questions/36059194/what-is-the-difference-between-json-dump-and-json-dumps-in-python - https://stackoverflow.com/questions/64188313/django-can-i-delete-apps-static-files-after-running-collectstatic/64189244#64189244 - https://stackoverflow.com/questions/29416478/change-form-field-value-before-saving - https://support.google.com/mail/thread/38519529?hl=en - https://stackoverflow.com/questions/46155/how-to-validate-an-email-address-in-javascript - https://stackoverflow.com/questions/3429084/why-do-i-get-an-object-is-not-iterable-error"
Social Media backend,Python-Django,https://github.com/Ronik22/Django_Social_Network_App,"# Django Social Network  A social media web-application with Django.  ## Features :  <li>Sign Up, Login, OAuth 2.0(Google, Github), Logout, Forgot Password</li> <li>Public Profile view</li> <li>Create, Edit, Delete Posts with customized text, pictures and links</li> <li>Like, Comment / Reply, Save and Search posts</li> <li>Follow and Unfollow users to view their posts</li> <li>Friend Request</li> <li>Notifications</li> <li>Chats using websockets</li> <li>Video Calls</li>  ## Demo (Not up-to-date)   https://user-images.githubusercontent.com/64803043/118666705-02735600-b811-11eb-80f9-445bbe6e55c9.mp4   <a href=""./demo/demo1.mp4"">Video Link</a>  ## Adding env variables  - Add env variables to "".test.env"" and rename it to "".env""  - Add GOOGLE_RECAPTCHA_SECRET_KEY to both .env and the file mentioned below https://github.com/Ronik22/Django_Social_Network_App/blob/main/users/templates/users/register.html#L45  - Add agora app_id to .env and to https://github.com/Ronik22/Django_Social_Network_App/blob/main/blog/static/blog/js/streams.js#L2  ## Installation  ```bash     $ python -m venv venv     $ source venv/Scripts/activate     (venv) pip install -r requirements.txt     (venv) cd Django_Social_Network_App     (venv) python manage.py makemigrations     (venv) python manage.py migrate     (venv) python manage.py createsuperuser     (venv) python manage.py runserver ```   ## Add django-allauth config  https://django-allauth.readthedocs.io/en/latest/installation.html#post-installation  ## Others  - To use other DB edit this https://github.com/Ronik22/Django_Social_Network_App/blob/main/myproject/settings.py#L107 - To use other providers edit this https://github.com/Ronik22/Django_Social_Network_App/blob/main/myproject/settings.py#L205 - To use redis instead edit this https://github.com/Ronik22/Django_Social_Network_App/blob/main/myproject/settings.py#L197  ## Running Tests  To run tests, run the following command  ```bash   python manage.py test ```  ## Deploy to Heroku  https://devcenter.heroku.com/articles/getting-started-with-python  https://realpython.com/django-hosting-on-heroku/"
Content Management system,Python-Django,https://github.com/django-cms/django-cms,"########## django CMS ########## .. image:: https://img.shields.io/pypi/v/django-cms.svg     :target: https://pypi.python.org/pypi/django-cms/ .. image:: https://img.shields.io/badge/wheel-yes-green.svg     :target: https://pypi.python.org/pypi/django-cms/ .. image:: https://img.shields.io/pypi/l/django-cms.svg     :target: https://pypi.python.org/pypi/django-cms/ .. image:: https://codeclimate.com/github/divio/django-cms/badges/gpa.svg    :target: https://codeclimate.com/github/divio/django-cms    :alt: Code Climate  Open source enterprise content management system based on the Django framework and backed by the non-profit django CMS Association (`Sponsor us! <https://www.django-cms.org/en/memberships/>`_).  ******************************************* Contribute to this project and win rewards *******************************************  Because django CMS is a community-driven project, we welcome everyone to `get involved in the project <https://www.django-cms.org/en/contribute/>`_. Become part of a fantastic community and help us make django CMS the best open source CMS in the world.   .. ATTENTION::      Please use the ``main`` branch as the target for pull requests for on-going development.      Security fixes will be backported to older branches by the core team as appropriate.   ******** Features ********  * hierarchical pages * extensive built-in support for multilingual websites * multi-site support * draft/publish workflows * version control * a sophisticated publishing architecture, that's also usable in your own applications * frontend content editing * a hierarchical content structure for nested plugins * an extensible navigation system that your own applications can hook into * SEO-friendly URLs * designed to integrate thoroughly into other applications  Developing applications that integrate with and take advantage of django CMS features is easy and well-documented.  More information on `our website <https://www.django-cms.org>`_.  ************ Requirements ************  See the `Python/Django requirements for the current release version <http://docs.django-cms.org/en/latest/#software-version-requirements-and-release-notes>`_ in our documentation.  See the `installation how-to guide for an overview of some other requirements and dependencies of the current release <https://docs.django-cms.org/en/latest/introduction/01-install.html>`_.   *************** Getting started ***************  These `tutorials <http://docs.django-cms.org/en/latest/introduction/index.html>`_ take you step-by-step through some key aspects of django CMS.   ************* Documentation *************  Our documentation working group maintains documentation for several versions of the project. Key versions are:  * `stable <http://docs.django-cms.org>`_ (default), for the **current release** version * `latest <http://docs.django-cms.org/en/latest/>`_, representing the latest build of the **main branch**  For more information about our branch policy, see `Branches <http://docs.django-cms.org/en/latest/contributing/development-policies.html>`_.  Our documentation is hosted courtesy of `Read the Docs <https://readthedocs.org>`_.  The dependencies for the docs are compiled by `pip-tools <https://github.com/jazzband/pip-tools>`_.   *************************** Test django CMS in our demo ***************************  The demo platform is kindly provided by Divio, platinum member of the django CMS Association.  .. image:: https://raw.githubusercontent.com/django-cms/django-cms/develop/docs/images/try-with-divio.png    :target: https://www.django-cms.org/en/django-cms-demo/    :alt: Try demo with Divio Cloud  ************ Getting Help ************  Please head over to our `Discord Server <https://discord-support-channel.django-cms.org>`_ or Stackoverflow for support.  ******************** Professional support ********************  Choose from a list of `trusted tech partner <https://www.django-cms.org/en/tech-partners/>`_ of the django CMS Association to get your website project delivered successfully.  Choose a `trusted web host <https://www.django-cms.org/en/hosting-services/>`_ for your django CMS project and get your website online today.   ************************** The django CMS Association **************************  The django CMS Association is a non-profit organization that was founded in 2020 with the goal to drive the success of django CMS, by increasing customer happiness, market share and open-source contributions. We provide infrastructure and guidance for the django CMS project.  The non-profit django CMS Association is dependent on donations to fulfill its purpose. The best way to donate is to become a member of the association and pay membership fees. The funding will be funneled back into core development and community projects.  `Join the django CMS Association <https://www.django-cms.org/en/contribute/>`_.   ******* Credits *******  * Includes icons and adapted icons from `Bootstrap <https://icons.getbootstrap.com>`_. * Includes icons from `FamFamFam <http://www.famfamfam.com>`_. * Python tree engine powered by   `django-treebeard <https://tabo.pe/projects/django-treebeard/>`_. * JavaScript tree in admin uses `jsTree <https://www.jstree.com>`_. * Many thanks to   `all the contributors <https://github.com/django-cms/django-cms/graphs/contributors>`_   to django CMS!"
Tic-Tac-Toe Game,C++,https://github.com/ramanbansal9876/Tic-Tac-Toe-Game-using-Cpp,"# Tic-Tac-Toe-Game-using-C++ (OOPS) Tic-tac-toe is a game in which two players take turns in drawing either an 'X' or an ' O' in one square of a grid consisting of nine squares. The winner is the first player to get three of the same symbols in a row or a column or a diagonal.  ### OBJECTIVE: This project aims to develop a Tic Tac Toe game using OOPS concept in C++. It mainly consists of developing and implementing a computer program that plays Tic Tac Toe against another player.<br/> In order to understand what Tic Tac Toe game is and how to play the game, below is the description.  ### GAME DESCRIPTION: Tic Tac Toe is a two-player game (one of them being played by computer or human). In this game, there is a board with 3 x 3 squares.<br/>  The two players take turns putting marks on a 3x3 board. The goal of Tic Tac Toe game is to be one of the players to get three same symbols in a row - horizontally, vertically or diagonally on a 3 x 3 grid.  The player who first gets 3 of his/her symbols (marks) in a row - vertically, horizontally, or diagonally wins the game, and the other loses the game.  The game can be played by two players.   ### GAME RULES: A player can choose between two symbols with his opponent, usual game uses ""X"" and ""O"".  1.	Initially, the player gets an option to choose to play with either ""X"" or ""O""  2.	Player 1 and 2 take turns making moves with Player 1 playing mark ‚ÄúX‚Äù and Player 2 playing mark ‚ÄúO‚Äù.  3.	A player marks any of the 3x3 squares with his mark (""X"" or ""O"") and their aim is to create a straight line horizontally, vertically or diagonally with two intensions:<br/>  a.	One of the players gets three of his/her marks in a row (vertically, horizontally, or diagonally) i.e. that player wins the game.<br/>  b.	If no one can create a straight line with their own mark and all the positions on the board are occupied, then the game ends in a draw/tie."
Chess Game,C++,https://github.com/jironghuang/chess,"chess =====  A C++ chess program.  This is a 2-player game, written for Object-Oriented Programming class."
Library Management System,C++,https://github.com/abdulsamie10/Library-Management-System,"# Library Management System in C++ # Project Overview: The Library Management System is a console-based application developed in C++ that allows admins and students to manage library resources effectively. The system enables students to access the library data by registering or logging in to their accounts. Admins have the ability to manage the books and students' accounts. The system supports adding, editing, and viewing books, as well as managing student accounts, issuing books, and handling fines.  # Key Features: User Authentication: The system provides a login option for both admins and students, protected by a password.  # Admin Functions:   1. Add a book to the system: The admin can add new books to the library.   2. Edit the details of the book: The admin can edit the title and author of a book using its ISBN number.   3. View the status of books: The admin can view the list and availability of books in the library.   4. View enrolled students: The admin can view the list of students registered in the system, sorted by their roll numbers.   5. View student balance: The admin can view the account balance of a specific student.  # Student Functions:   1. Create an account: A new student can register by providing their roll number, name, and initial deposit.   2. View balance: Students can view their account balance.   3. Deposit amount: Students can deposit money into their accounts.   4. Issue a book: Students can issue books from the list of available books.  Account management: The system supports up to 20 students, who pay $20 for account opening and $30 as a security deposit. Students can issue any book for $2 for a 10-day period. Fines are imposed for late returns as per the specified rules.  Data storage: The system uses 2D arrays to store the details of students and books. Initially, 15 books are stored in the library. Each student account contains the roll number, balance, and first name.  # Implementation Details: The application is implemented in C++ without using classes, pointers, or structures. It employs 2D arrays, functions, loops, if-else, and switch operators to achieve the desired functionality. The main() function handles user inputs, presents options to the user, and calls appropriate functions based on the selected options. The program runs in a loop until the user decides to exit.  This project is suitable for students who have completed a Programming Fundamentals course or lab and want to demonstrate their skills in C++ programming without using advanced concepts like classes or pointers. It provides a solid foundation for further learning and improvement in C++ and software development."
File Management System,C++,https://github.com/krshrimali/CPP-File-Manager,"# CPP-File-Manager  C++ File Manager allows you to list files in C++, ignore extensions (or just list specified extensions) and write the whole directory structure into a fancy tree-like structure which can be integrated in Markdown files. This is the first release. Please file issues with `[FEATURE REQUEST]` in the title, and we'll be happy to take a look at it.  The C++ File Manager provides the following functions to users:  1. `clear()` - Clears the memory allocated to corePath. 2. `clear(std::string new_path)` - Assigns a new path to the corePath variable. 3. `info()` - Prints the corePath assigned to the FileManager object to the console. 4. `list_files(std::vector<std::string> extensions, bool ignore_extensions)` - Lists the files and directories in corePath. The first argument is *extensions* which is a vector of file extensions to be ignored. These extensions are ignored only when the second argument i.e. `ignore_extensions` is set to `true`. 5. `writeToFile(std::vector<string> ignore_dirs, std::vector<std::string> ignore_extension)` - Writes the tree structure for the files and directories in corePath into a `.txt` file. This functions iterates till the innermost files of all the directories. The first argument `ignore_dirs` contains a vector of directories to be ignored in the tree structures. Similarly, the second argument `ignore_extensions` contains a vector of files extensions to be ignored in the final tree structure.  ## Usage   ### Using Binary  The **fmanager** binary has the following options :  ```   -h  --help                        Print usage.   -p  --path filePath               Input path (to be iterated).   -l  --list_files                  Call the list files function.   -t  --tree                        Call the tree function.   -d --ignore_dirs dir1,dir2        Ignore dirs while creating tree   -e --ignore_extensions ext1,ext2  Ignore extensions while creating tree   -s --separator                    Separator you want for your tree output ```  **Listing files in a directory**  Command: `fmanager -p samples`  By default, it will list files in the given directory (here `samples`). Output will look like this:  ```bash Got path: samples sample.cpp README.md libcpp-file-manager.a CMakeLists.txt FileManager.hpp ```  **Building tree of the given directory and ignoring directories and extensions**  `./fmanager -p ./ -t -d include,.git -e .cpp`\ _OR_ \ `./fmanager --path ./ --tree --ignore_dirs include,.git --ignore_extensions .cpp`  ```  Got path: ./ tree.txt CMakeLists.txt samples src .github README.md .git include ```  The `tree.txt` file stores the following directory structure:  ``` |-- tree.txt |-- CMakeLists.txt |-- samples     |-- libcpp-file-manager.a     |-- CMakeLists.txt     |-- README.md     |-- FileManager.hpp |-- src |-- .github     |-- workflows         |-- build-filemanager.yml |-- README.md |-- .git |-- include ```  In case you want to change the separator from default (`|--`) to something like `-` or `*`, do:  ```bash ./fmanager -s '-' -t ./fmanager --separator '-' --tree ```  The generated `tree.txt` will now contain `-` instead of `|--`:  ``` - tree.txt - CMakeLists.txt ... ```  ### Using the library in your C++ Code  To be able to use `FileManager` library, head over to the latest release and download `.a` (library file) and `.hpp` (header file). Copy these files in your current folder (or wherever you desire, just remember the path). To compile, use:  ```bash g++ filename.cpp -L . -lcpp-file-manager -o out ```  Here are a few steps on using the library in your code:  1. Create an object of `Filemanager` class and initialize it with a path:  ```  std::string path = ""/home/BuffetCodes/Documents/CPP-File-Manager""; // Change this with your path, either relative or absolute FileManager file(path); ```  2. The `file.list_files()` function returns a vector containing names of files/directories with additional information. We can iterate through it as follows:  ```cpp // The type returned is a struct, head over to the header file for more details on it for (auto const& item: file.list_files()) {     // Use item.rname if you want ""just"" the name of the file or folder     // item.name returns absolute path (with respect to the path given)     // item.is_dir returns true if it's a directory, else false     std::cout << item.name << ""\n""; } ```  The output will be as follows:  ``` ./tree.txt ./CMakeLists.txt ./samples ./src ./.github ./README.md ./.git ./include ```  3. The `file.writeToFile()` call, creates a text file `tree.txt` representing the directory structure:   ```cpp std::vector<std::string> ignore_dirs = {"".git"", "".github"", "".vscode"", ""build""}; std::vector<std::string> ignore_extensions = {"".pyc"", "".swp""}; file.writeToFile(/*ignore_folders=*/ ignore_dirs, /*ignore_extensions=*/ ignore_extensions); ```  ## Build  The current release (1.0) only supports GNU/Linux Systems. Please head to the relevant opened issues to see the progress on Windows and MacOS. Use the following steps to build from source:  ```bash git clone https://github.com/BuffetCodes/CPP-File-Manager.git && cd CPP-File-Manager mkdir build && cd build cmake .. && make ```  The library file: `libcpp-file-manager.a` will be generated in `build/` directory. Copy the header file in `include/` and library file from `build/` directory to your folder to use it. Or, just head over to the `samples` folder in this project on how to compile using `CMake` or `g++`."
Restaurant Ordering System,C++,https://github.com/TzeYuan14/Food-Ordering-System,"# Food-Ordering-System  We live in challenging yet interesting times. With COVID-19, social distancing becoming a solid norm  and eating in a crowded restaurant is no longer possible. However, we can still enjoy the delicious food  from the restaurants by having Food Ordering system.   To maintain business, restaurant needs this system to showcase their menu by keeping track the prices,  delivery time, number of orders by customers and sales for each item in the menu. The manager will be able to view the total sales, the most popular dish in the menu and number of customers for the day.  The pandemic will turn you the system developer into warriors of the modern world.  ### Program Specification Food Ordering and Delivering system has 2 users, Restaurant Manager and Customer with the following  basic process: * Restaurant Manager(s)   * Create/update menu   * Update prices   * Accept orders   * Calculate total payments per order   * Calculate estimated delivery time   * Calculate total sales for a day * Customer(s)   * Order online   * Make payments  ### Output Main Menu Interface  ![image](https://user-images.githubusercontent.com/85170160/212027193-863e03b2-ac0c-475f-b164-7d7610200de2.png)  Food Menu Interface  ![image](https://user-images.githubusercontent.com/85170160/212027767-47bb685b-ba16-428b-8c62-4a990c267af1.png)"
Alumni Management System,C++,https://github.com/ali-soban/alumni_management_system_cpp,"# DSA_PROJ  A group project based on Data Structures, where we implemented various data structures such as Linked Lists and Queues to create a CRUD project. It is an alumni management system where records of students from the same organization can be added, updated, deleted and edited. "
Gym Management System,C++,https://github.com/DhanushAnbalagan/Gym-Management-System,"# Gym-Management-System The C++ Gym Management System provides a user-friendly menu interface to create, view, modify, cancel, and search gym memberships. With input validation and dynamic storage using vectors, it efficiently handles membership operations for gym administrators.  ## Features  - Create new memberships by entering the member's name and timing preference. - View existing memberships sorted by timing preference (morning, afternoon, evening). - Modify memberships by updating the name or timing preference. - Cancel memberships to remove them from the system. - Search for memberships by entering a member's name.  ## Usage  1. Compile the program using a C++ compiler. 2. Run the compiled executable. 3. Follow the on-screen instructions to navigate the menu and perform desired operations.  ## Input Validation  The program includes input validation to ensure that users enter valid input for various operations, preventing errors and maintaining data integrity.  ## Data Storage  Gym memberships are stored using a vector, allowing for dynamic management and storage of membership data.  ## Contributing  Contributions to enhance the program's functionality or improve code quality are welcome. Fork the repository, make your changes, and submit a pull request.  ## Author  Dhanush Anbalagan ## License  This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details."
File Compression,C++,https://github.com/aniketpathak028/File-Compressor,"# File-Compressor ---  A File-Compressor based on Huffman Coding (A lossless, bottom-up compression algorithm) built using C++ that can compress and decompress any text files.  ## Compressing ---  To compress a text file using the following command:  <img width=""659"" alt=""compressing"" src=""https://github.com/aniketpathak028/File-Compressor/assets/74519511/de2f513d-6090-42f6-9855-9a1fa9877e02"">  The compressed file is generated:  <img width=""217"" alt=""compressed-file"" src=""https://github.com/aniketpathak028/File-Compressor/assets/74519511/6d38682b-ad2d-4f96-9805-886057199f6b"">  ## Decompressing ---  To decompress a text file using the following command:  <img width=""661"" alt=""decompressing-file"" src=""https://github.com/aniketpathak028/File-Compressor/assets/74519511/265eba46-73fd-467b-8eac-fecd75c7d784"">  The decompressed file is generated:  <img width=""217"" alt=""decompressed-file"" src=""https://github.com/aniketpathak028/File-Compressor/assets/74519511/0b16ab93-87d0-4637-876e-3373cc23d565"">  "
Implementation of Minesweeper Game,C++,https://github.com/burakssen/minesweeper,"# A simple minesweeper game written in C++ using raylib library  This is a simple minesweeper game written in C++ using the raylib library. The game is a simple implementation of the classic minesweeper game. The game is written in C++ and uses the raylib library for rendering and input handling.  ## Screenshots  | <img src=""screenshots/1.png""> | <img src=""screenshots/2.png""> | | ----------------------------- | ----------------------------- | | <img src=""screenshots/3.png""> |  ## Building the game  To build the game first clone the repository recursively to get the raylib library as well. Then use the provided cmake to build the game. The game can be built using the following commands:  ```sh git clone --recursive https://github.com/burakssen/minesweeper.git cd minesweeper mkdir build cd build cmake .. make ```  ## Running the game  The game can be run from the build directory using the following command:  ```sh ./minesweeper ```  ## Playing the game  The game is a simple implementation of the classic minesweeper game. The game is played using the mouse. The left mouse button is used to reveal a tile and the right mouse button is used to flag a tile. The game is won when all the tiles that do not contain a mine are revealed and the game is lost when a mine is revealed. The game can be restarted using the `R` key.  ## Credits  The game uses the following assets:  - [Tiles](https://kia.itch.io/16x16-tileset-for-minesweeper)  The game uses the following libraries:  - [raylib](https://www.raylib.com/)  The game uses the following tools:  - [cmake](https://cmake.org/)  The game uses the sound effects from the following sources:  - [rFXGen](https://raylibtech.itch.io/rfxgen)"
Program to remotely Power On a PC over the internet using the Wake-on-LAN protocol,C++,https://github.com/herzhenr/simple-wake-on-lan,"<img src=""docs/icon.png"" width=""96"" align=""right""  alt=""""/>  # SimpleWoL - Simple Wake on Lan  <!-- <a href=""https://github.com/herzhenr/simple-wake-on-lan/actions/workflows/release-android.yml/badge.svg?branch=main""><img src=""https://github.com/herzhenr/simple-wake-on-lan/actions/workflows/release-android.yml""></a>   <a href=""https://github.com/herzhenr/simple-wake-on-lan/actions/workflows/release-ios.yml/badge.svg?branch=main""><img src=""https://github.com/herzhenr/simple-wake-on-lan/actions/workflows/release-ios.yml""></a> -->  <p float=""center"">   <a href=""https://opensource.org/licenses/MIT""><img src=""https://img.shields.io/badge/License-MIT-green.svg"" alt=""MIT License""></a>   <a href=""https://flutter.dev""><img src=""https://img.shields.io/badge/Flutter-%2302569B.svg?logo=Flutter&logoColor=white"" alt=""Flutter""></a>   <a href=""https://www.dart.dev""><img src=""https://img.shields.io/badge/Dart-%230175C2.svg?logo=dart&logoColor=white"" alt=""Dart""></a>   <a href=""https://play.google.com/store/apps/details?id=com.henrikherzig.simplewol""><img src=""https://img.shields.io/badge/Google_Play-414141?logo=google-play&logoColor=white"" alt=""Google Play""></a>   <a href=""https://apps.apple.com/de/app/simple-wake-on-lan/id6449835474""><img src=""https://img.shields.io/badge/App_Store-0D96F6?logo=app-store&logoColor=white"" alt=""App Store""></a>   <a href=""https://github.com/herzhenr/simple-wake-on-lan/actions/workflows/lint.yml""><img src=""https://github.com/herzhenr/simple-wake-on-lan/actions/workflows/lint.yml/badge.svg?branch=main"" alt=""Workflow Lint""></a>   <a href=""https://github.com/herzhenr/simple-wake-on-lan/releases""><img src=""https://img.shields.io/github/release/herzhenr/simple-wake-on-lan.svg?logo=github&color=blue"" alt=""GitHub Release""></a> </p>  Simple Wake on Lan is a simple cross-platform flutter application for Android and iOS to send Wake On Lan packets to a device.  <p align=""center""> <a href=""https://play.google.com/store/apps/details?id=com.henrikherzig.simplewol""><img src=""docs/googlePlay.png"" style=""height: 60px;"" alt=""Get it on Google Play""></a> &nbsp &nbsp <a href=""https://apps.apple.com/de/app/simple-wake-on-lan/id6449835474""><img src=""docs/appStore.svg"" style=""height: 60px;"" alt=""Download on the App Store""></a> </p>  ## Usage Wake on LAN (WoL) is a network protocol that allows a device to be turned on or awakened remotely over a network while it is sleeping. This project aims to make the process of waking devices easy with a mobile application.   <!--- by including features like automatic device discovery so the user does not have to enter details of a device they want to wake up manually, a simple interface to send the Wake On Lan packets and the possibility to export and import the user data as a `json` file. -->  ## Screenshots   |                                          |                                     | |:----------------------------------------:|:-----------------------------------:| | ![play_integrity](docs/screenshot-1.png) | ![dark_mode](docs/screenshot-2.png) |    |                                    |                                 | |:----------------------------------:|:-------------------------------:| | ![settings](docs/screenshot-3.png) | ![about](docs/screenshot-4.png) |  ## Features  - Automatic device discovery - Simple interface to send Wake On Lan packets - Export and import user data as a `json` file (see below)   The app stores the added devices in a `json` file which can be exported and imported within the app UI. An example of the file structure is shown below: ```json [   {     ""id"": ""6b353440-d183-11ed-964b-69a9facd6cfd"",     ""hostName"": ""Raspberry Pi"",     ""ipAddress"": ""192.168.1.9"",     ""macAddress"": ""12:12:12:12:12:12"",     ""wolPort"": 9,     ""deviceType"": ""computer"",     ""modified"": ""2023-04-14T14:17:45.974511""   },   {     ""id"": ""87c87ab0-d184-13ed-9d56-a5f550305985"",     ""hostName"": ""Printer"",     ""ipAddress"": ""192.168.1.10"",     ""macAddress"": ""f0:f0:f0:f0:f0:f0"",     ""wolPort"": 9,     ""deviceType"": ""printer"",     ""modified"": ""2023-04-14T14:18:14.997081""   } ] ```  ## Download  - You can download the latest version of the app from [GitHub Releases]() - Download from the [PlayStore](https://play.google.com/store/apps/details?) - Download from the [App Store](https://apps.apple.com/de/app/)  ## Architecture The app is built using the [Flutter](https://flutter.dev/) framework. It uses the [Material 3](https://m3.material.io) design system from Google.  ## Build To build the app yourself, you need to have the Flutter SDK installed. You can find the installation instructions [here](https://flutter.dev/docs/get-started/install).  ## License This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details  ```License Copyright (c) 2023 Henrik Herzig  Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.  THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. ```"
Finding cabs nearby using Great Circle Distance formula,C++,https://github.com/Nirbhay-Gaur/ride-o-meter,"# Ride-O-Meter  > Given GPS co-ordinates(in degrees) of a person who needs a cab and co-ordinates of all the cabs in the city stored in a text file in JSON format, this program will find the user-id and name of all the cab drivers available in 50 km proximity.  ## Instruction to run the program  - Compile the code using cmd: `g++ main.cpp`.  - After successful compilation, run it along with passing the file name   customers.json using cmd: `./a.out ./customers.json` - A new file named result.json will be created on the same directory where the   code and customers.json file is existing.  ## Approach Used - Obtain latitude and longitude of each cab in string format along with their user-id and name from the JSON encoded input file. - Convert latitude and longitude of the cab present in string format to double. -  Convert latitude and longitude of both, the user and the cab present in degrees to radians. - Calculate distance between the user‚Äôs location and the cab using Great Circle Distance formula. - If distance is found to be less than or equal to 50 kms then output the user- id and name of the cab driver to a new file else take no action."
Teacher's Attendance Management System,C++,https://github.com/kashishahuja2002/Teachers-Attendance-Management-System,  https://user-images.githubusercontent.com/55057608/146685913-bbc54432-21d4-4805-abeb-7f576207a917.mp4 
Snake Game,C++,https://github.com/KaranJagtiani/Snake-Game-in-CPP,"# Snake Game in C++ Snake game created using C++ and the graphics.h library.  ![](images/menu.jpg)  ![](images/in_game.png)  ## Snake's body created using Doubly Linked List ![](images/snakes_body.jpg)  Logic for displaying the trailing body of the snake.  i)	Traverse the list till the end using next pointer.  ii)	From the last node, traverse back to the head by using previous pointer, while:      head->x = head->prev->x     head->y = head->prev->y      iii)	Now, we are back to the head node, display the squares by traversing back to the last node through next pointer.  iv)	Repeat the process till game is over, or is complete, or ESC is pressed.  ## Forest Fire algorithm for polygon filling ![](images/forest_fire.jpg)  Queue Data Structure implemented using Linked List was used for implementing this algorithm for polygon filling.  ``` while(head){ 	r = deQueue(&head); 	colorBool = getpixel(r->x, r->y) == colorToRemove;  	if(r){ // i.e Not Null 		putpixel(r->x, r->y, colorToAdd); 	} 	if(colorBool){ 		if(!inQueue(head, r->x+1, r->y)) 			enQueue(&head, r->x+1, r->y); 		if(!inQueue(head, r->x-1, r->y)) 			enQueue(&head, r->x-1, r->y); 		if(!inQueue(head, r->x, r->y+1)) 			enQueue(&head, r->x, r->y+1); 		if(!inQueue(head, r->x, r->y-1)) 			enQueue(&head, r->x, r->y-1); 	} 	delete r; } ```"
Facial recognition using Python OpenCV library,Python,https://github.com/informramiz/opencv-face-recognition-python," # Face Recognition with OpenCV and Python  ## Introduction  What is face recognition? Or what is recognition? When you look at an apple fruit, your mind immediately tells you that this is an apple fruit. This process, your mind telling you that this is an apple fruit is recognition in simple words. So what is face recognition then? I am sure you have guessed it right. When you look at your friend walking down the street or a picture of him, you recognize that he is your friend Paulo. Interestingly when you look at your friend or a picture of him you look at his face first before looking at anything else. Ever wondered why you do that? This is so that you can recognize him by looking at his face. Well, this is you doing face recognition.   But the real question is how does face recognition works? It is quite simple and intuitive. Take a real life example, when you meet someone first time in your life you don't recognize him, right? While he talks or shakes hands with you, you look at his face, eyes, nose, mouth, color and overall look. This is your mind learning or training for the face recognition of that person by gathering face data. Then he tells you that his name is Paulo. At this point your mind knows that the face data it just learned belongs to Paulo. Now your mind is trained and ready to do face recognition on Paulo's face. Next time when you will see Paulo or his face in a picture you will immediately recognize him. This is how face recognition work. The more you will meet Paulo, the more data your mind will collect about Paulo and especially his face and the better you will become at recognizing him.   Now the next question is how to code face recognition with OpenCV, after all this is the only reason why you are reading this article, right? OK then. You might say that our mind can do these things easily but to actually code them into a computer is difficult? Don't worry, it is not. Thanks to OpenCV, coding face recognition is as easier as it feels. The coding steps for face recognition are same as we discussed it in real life example above.  - **Training Data Gathering:** Gather face data (face images in this case) of the persons you want to recognize - **Training of Recognizer:** Feed that face data (and respective names of each face) to the face recognizer so that it can learn. - **Recognition:** Feed new faces of the persons and see if the face recognizer you just trained recognizes them.  OpenCV comes equipped with built in face recognizer, all you have to do is feed it the face data. It's that simple and this how it will look once we are done coding it.  ![visualization](output/tom-shahrukh.png)  ## OpenCV Face Recognizers  OpenCV has three built in face recognizers and thanks to OpenCV's clean coding, you can use any of them by just changing a single line of code. Below are the names of those face recognizers and their OpenCV calls.   1. EigenFaces Face Recognizer Recognizer - `cv2.face.createEigenFaceRecognizer()` 2. FisherFaces Face Recognizer Recognizer - `cv2.face.createFisherFaceRecognizer()` 3. Local Binary Patterns Histograms (LBPH) Face Recognizer - `cv2.face.createLBPHFaceRecognizer()`  We have got three face recognizers but do you know which one to use and when? Or which one is better? I guess not. So why not go through a brief summary of each, what you say? I am assuming you said yes :) So let's dive into the theory of each.   ### EigenFaces Face Recognizer  This algorithm considers the fact that not all parts of a face are equally important and equally useful. When you look at some one you recognize him/her by his distinct features like eyes, nose, cheeks, forehead and how they vary with respect to each other. So you are actually focusing on the areas of maximum change (mathematically speaking, this change is variance) of the face. For example, from eyes to nose there is a significant change and same is the case from nose to mouth. When you look at multiple faces you compare them by looking at these parts of the faces because these parts are the most useful and important components of a face. Important because they catch the maximum change among faces, change the helps you differentiate one face from the other. This is exactly how EigenFaces face recognizer works.    EigenFaces face recognizer looks at all the training images of all the persons as a whole and try to extract the components which are important and useful (the components that catch the maximum variance/change) and discards the rest of the components. This way it not only extracts the important components from the training data but also saves memory by discarding the less important components. These important components it extracts are called **principal components**. Below is an image showing the principal components extracted from a list of faces.  **Principal Components** ![eigenfaces_opencv](visualization/eigenfaces_opencv.png) **[source](http://docs.opencv.org/2.4/modules/contrib/doc/facerec/facerec_tutorial.html)**  You can see that principal components actually represent faces and these faces are called **eigen faces** and hence the name of the algorithm.   So this is how EigenFaces face recognizer trains itself (by extracting principal components). Remember, it also keeps a record of which principal component belongs to which person. One thing to note in above image is that **Eigenfaces algorithm also considers illumination as an important component**.   Later during recognition, when you feed a new image to the algorithm, it repeats the same process on that image as well. It extracts the principal component from that new image and compares that component with the list of components it stored during training and finds the component with the best match and returns the person label associated with that best match component.   Easy peasy, right? Next one is easier than this one.   ### FisherFaces Face Recognizer   This algorithm is an improved version of EigenFaces face recognizer. Eigenfaces face recognizer looks at all the training faces of all the persons at once and finds principal components from all of them combined. By capturing principal components from all the of them combined you are not focusing on the features that discriminate one person from the other but the features that represent all the persons in the training data as a whole.  This approach has drawbacks, for example, **images with sharp changes (like light changes which is not a useful feature at all) may dominate the rest of the images** and you may end up with features that are from external source like light and are not useful for discrimination at all. In the end, your principal components will represent light changes and not the actual face features.   Fisherfaces algorithm, instead of extracting useful features that represent all the faces of all the persons, it extracts useful features that discriminate one person from the others. This way features of one person do not dominate over the others and you have the features that discriminate one person from the others.   Below is an image of features extracted using Fisherfaces algorithm.  **Fisher Faces** ![eigenfaces_opencv](visualization/fisherfaces_opencv.png) **[source](http://docs.opencv.org/2.4/modules/contrib/doc/facerec/facerec_tutorial.html)**  You can see that features extracted actually represent faces and these faces are called **fisher faces** and hence the name of the algorithm.   One thing to note here is that **even in Fisherfaces algorithm if multiple persons have images with sharp changes due to external sources like light they will dominate over other features and affect recognition accuracy**.   Getting bored with this theory? Don't worry, only one face recognizer is left and then we will dive deep into the coding part.   ### Local Binary Patterns Histograms (LBPH) Face Recognizer   I wrote a detailed explaination on Local Binary Patterns Histograms in my previous article on [face detection](https://www.superdatascience.com/opencv-face-detection/) using local binary patterns histograms. So here I will just give a brief overview of how it works.  We know that Eigenfaces and Fisherfaces are both affected by light and in real life we can't guarantee perfect light conditions. LBPH face recognizer is an improvement to overcome this drawback.  Idea is to not look at the image as a whole instead find the local features of an image. LBPH alogrithm try to find the local structure of an image and it does that by comparing each pixel with its neighboring pixels.   Take a 3x3 window and move it one image, at each move (each local part of an image), compare the pixel at the center with its neighbor pixels. The neighbors with intensity value less than or equal to center pixel are denoted by 1 and others by 0. Then you read these 0/1 values under 3x3 window in a clockwise order and you will have a binary pattern like 11100011 and this pattern is local to some area of the image. You do this on whole image and you will have a list of local binary patterns.   **LBP Labeling** ![LBP labeling](visualization/lbp-labeling.png)  Now you get why this algorithm has Local Binary Patterns in its name? Because you get a list of local binary patterns. Now you may be wondering, what about the histogram part of the LBPH? Well after you get a list of local binary patterns, you convert each binary pattern into a decimal number (as shown in above image) and then you make a [histogram](https://www.mathsisfun.com/data/histograms.html) of all of those values. A sample histogram looks like this.   **Sample Histogram** ![LBP labeling](visualization/histogram.png)   I guess this answers the question about histogram part. So in the end you will have **one histogram for each face** image in the training data set. That means if there were 100 images in training data set then LBPH will extract 100 histograms after training and store them for later recognition. Remember, **algorithm also keeps track of which histogram belongs to which person**.  Later during recognition, when you will feed a new image to the recognizer for recognition it will generate a histogram for that new image, compare that histogram with the histograms it already has, find the best match histogram and return the person label associated with that best match histogram.  <br><br> Below is a list of faces and their respective local binary patterns images. You can see that the LBP images are not affected by changes in light conditions.  **LBP Faces** ![LBP faces](visualization/lbph-faces.jpg) **[source](http://docs.opencv.org/2.4/modules/contrib/doc/facerec/facerec_tutorial.html)**   The theory part is over and now comes the coding part! Ready to dive into coding? Let's get into it then.   # Coding Face Recognition with OpenCV  The Face Recognition process in this tutorial is divided into three steps.  1. **Prepare training data:** In this step we will read training images for each person/subject along with their labels, detect faces from each image and assign each detected face an integer label of the person it belongs to. 2. **Train Face Recognizer:** In this step we will train OpenCV's LBPH face recognizer by feeding it the data we prepared in step 1. 3. **Testing:** In this step we will pass some test images to face recognizer and see if it predicts them correctly.  **[There should be a visualization diagram for above steps here]**  To detect faces, I will use the code from my previous article on [face detection](https://www.superdatascience.com/opencv-face-detection/). So if you have not read it, I encourage you to do so to understand how face detection works and its Python coding.   ### Import Required Modules  Before starting the actual coding we need to import the required modules for coding. So let's import them first.   - **cv2:** is _OpenCV_ module for Python which we will use for face detection and face recognition. - **os:** We will use this Python module to read our training directories and file names. - **numpy:** We will use this module to convert Python lists to numpy arrays as OpenCV face recognizers accept numpy arrays.   ```python #import OpenCV module import cv2 #import os module for reading training data directories and paths import os #import numpy to convert python lists to numpy arrays as  #it is needed by OpenCV face recognizers import numpy as np  #matplotlib for display our images import matplotlib.pyplot as plt %matplotlib inline  ```  ### Training Data  The more images used in training the better. Normally a lot of images are used for training a face recognizer so that it can learn different looks of the same person, for example with glasses, without glasses, laughing, sad, happy, crying, with beard, without beard etc. To keep our tutorial simple we are going to use only 12 images for each person.   So our training data consists of total 2 persons with 12 images of each person. All training data is inside _`training-data`_ folder. _`training-data`_ folder contains one folder for each person and **each folder is named with format `sLabel (e.g. s1, s2)` where label is actually the integer label assigned to that person**. For example folder named s1 means that this folder contains images for person 1. The directory structure tree for training data is as follows:  ``` training-data |-------------- s1 |               |-- 1.jpg |               |-- ... |               |-- 12.jpg |-------------- s2 |               |-- 1.jpg |               |-- ... |               |-- 12.jpg ```  The _`test-data`_ folder contains images that we will use to test our face recognizer after it has been successfully trained.  As OpenCV face recognizer accepts labels as integers so we need to define a mapping between integer labels and persons actual names so below I am defining a mapping of persons integer labels and their respective names.   **Note:** As we have not assigned `label 0` to any person so **the mapping for label 0 is empty**.    ```python #there is no label 0 in our training data so subject name for index/label 0 is empty subjects = ["""", ""Tom Cruise"", ""Shahrukh Khan""] ```  ### Prepare training data  You may be wondering why data preparation, right? Well, OpenCV face recognizer accepts data in a specific format. It accepts two vectors, one vector is of faces of all the persons and the second vector is of integer labels for each face so that when processing a face the face recognizer knows which person that particular face belongs too.   For example, if we had 2 persons and 2 images for each person.   ``` PERSON-1    PERSON-2     img1        img1          img2        img2 ```  Then the prepare data step will produce following face and label vectors.  ``` FACES                        LABELS  person1_img1_face              1 person1_img2_face              1 person2_img1_face              2 person2_img2_face              2 ```   Preparing data step can be further divided into following sub-steps.  1. Read all the folder names of subjects/persons provided in training data folder. So for example, in this tutorial we have folder names: `s1, s2`.  2. For each subject, extract label number. **Do you remember that our folders have a special naming convention?** Folder names follow the format `sLabel` where `Label` is an integer representing the label we have assigned to that subject. So for example, folder name `s1` means that the subject has label 1, s2 means subject label is 2 and so on. The label extracted in this step is assigned to each face detected in the next step.  3. Read all the images of the subject, detect face from each image. 4. Add each face to faces vector with corresponding subject label (extracted in above step) added to labels vector.   **[There should be a visualization for above steps here]**  Did you read my last article on [face detection](https://www.superdatascience.com/opencv-face-detection/)? No? Then you better do so right now because to detect faces, I am going to use the code from my previous article on [face detection](https://www.superdatascience.com/opencv-face-detection/). So if you have not read it, I encourage you to do so to understand how face detection works and its coding. Below is the same code.   ```python #function to detect face using OpenCV def detect_face(img):     #convert the test image to gray image as opencv face detector expects gray images     gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)          #load OpenCV face detector, I am using LBP which is fast     #there is also a more accurate but slow Haar classifier     face_cascade = cv2.CascadeClassifier('opencv-files/lbpcascade_frontalface.xml')      #let's detect multiscale (some images may be closer to camera than others) images     #result is a list of faces     faces = face_cascade.detectMultiScale(gray, scaleFactor=1.2, minNeighbors=5);          #if no faces are detected then return original img     if (len(faces) == 0):         return None, None          #under the assumption that there will be only one face,     #extract the face area     (x, y, w, h) = faces[0]          #return only the face part of the image     return gray[y:y+w, x:x+h], faces[0] ```  I am using OpenCV's **LBP face detector**. On _line 4_, I convert the image to grayscale because most operations in OpenCV are performed in gray scale, then on _line 8_ I load LBP face detector using `cv2.CascadeClassifier` class. After that on _line 12_ I use `cv2.CascadeClassifier` class' `detectMultiScale` method to detect all the faces in the image. on _line 20_, from detected faces I only pick the first face because in one image there will be only one face (under the assumption that there will be only one prominent face). As faces returned by `detectMultiScale` method are actually rectangles (x, y, width, height) and not actual faces images so we have to extract face image area from the main image. So on _line 23_ I extract face area from gray image and return both the face image area and face rectangle.  Now you have got a face detector and you know the 4 steps to prepare the data, so are you ready to code the prepare data step? Yes? So let's do it.    ```python #this function will read all persons' training images, detect face from each image #and will return two lists of exactly same size, one list  # of faces and another list of labels for each face def prepare_training_data(data_folder_path):          #------STEP-1--------     #get the directories (one directory for each subject) in data folder     dirs = os.listdir(data_folder_path)          #list to hold all subject faces     faces = []     #list to hold labels for all subjects     labels = []          #let's go through each directory and read images within it     for dir_name in dirs:                  #our subject directories start with letter 's' so         #ignore any non-relevant directories if any         if not dir_name.startswith(""s""):             continue;                      #------STEP-2--------         #extract label number of subject from dir_name         #format of dir name = slabel         #, so removing letter 's' from dir_name will give us label         label = int(dir_name.replace(""s"", """"))                  #build path of directory containin images for current subject subject         #sample subject_dir_path = ""training-data/s1""         subject_dir_path = data_folder_path + ""/"" + dir_name                  #get the images names that are inside the given subject directory         subject_images_names = os.listdir(subject_dir_path)                  #------STEP-3--------         #go through each image name, read image,          #detect face and add face to list of faces         for image_name in subject_images_names:                          #ignore system files like .DS_Store             if image_name.startswith("".""):                 continue;                          #build image path             #sample image path = training-data/s1/1.pgm             image_path = subject_dir_path + ""/"" + image_name              #read image             image = cv2.imread(image_path)                          #display an image window to show the image              cv2.imshow(""Training on image..."", image)             cv2.waitKey(100)                          #detect face             face, rect = detect_face(image)                          #------STEP-4--------             #for the purpose of this tutorial             #we will ignore faces that are not detected             if face is not None:                 #add face to list of faces                 faces.append(face)                 #add label for this face                 labels.append(label)                  cv2.destroyAllWindows()     cv2.waitKey(1)     cv2.destroyAllWindows()          return faces, labels ```  I have defined a function that takes the path, where training subjects' folders are stored, as parameter. This function follows the same 4 prepare data substeps mentioned above.   **(step-1)** On _line 8_ I am using `os.listdir` method to read names of all folders stored on path passed to function as parameter. On _line 10-13_ I am defining labels and faces vectors.   **(step-2)** After that I traverse through all subjects' folder names and from each subject's folder name on _line 27_ I am extracting the label information. As folder names follow the `sLabel` naming convention so removing the  letter `s` from folder name will give us the label assigned to that subject.   **(step-3)** On _line 34_, I read all the images names of of the current subject being traversed and on _line 39-66_ I traverse those images one by one. On _line 53-54_ I am using OpenCV's `imshow(window_title, image)` along with OpenCV's `waitKey(interval)` method to display the current image being traveresed. The `waitKey(interval)` method pauses the code flow for the given interval (milliseconds), I am using it with 100ms interval so that we can view the image window for 100ms. On _line 57_, I detect face from the current image being traversed.   **(step-4)** On _line 62-66_, I add the detected face and label to their respective vectors.  But a function can't do anything unless we call it on some data that it has to prepare, right? Don't worry, I have got data of two beautiful and famous celebrities. I am sure you will recognize them!  ![training-data](visualization/tom-shahrukh.png)  Let's call this function on images of these beautiful celebrities to prepare data for training of our Face Recognizer. Below is a simple code to do that.   ```python #let's first prepare our training data #data will be in two lists of same size #one list will contain all the faces #and other list will contain respective labels for each face print(""Preparing data..."") faces, labels = prepare_training_data(""training-data"") print(""Data prepared"")  #print total faces and labels print(""Total faces: "", len(faces)) print(""Total labels: "", len(labels)) ```      Preparing data...     Data prepared     Total faces:  23     Total labels:  23   This was probably the boring part, right? Don't worry, the fun stuff is coming up next. It's time to train our own face recognizer so that once trained it can recognize new faces of the persons it was trained on. Read? Ok then let's train our face recognizer.   ### Train Face Recognizer  As we know, OpenCV comes equipped with three face recognizers.  1. EigenFace Recognizer: This can be created with `cv2.face.createEigenFaceRecognizer()` 2. FisherFace Recognizer: This can be created with `cv2.face.createFisherFaceRecognizer()` 3. Local Binary Patterns Histogram (LBPH): This can be created with `cv2.face.LBPHFisherFaceRecognizer()`  I am going to use LBPH face recognizer but you can use any face recognizer of your choice. No matter which of the OpenCV's face recognizer you use the code will remain the same. You just have to change one line, the face recognizer initialization line given below.    ```python #create our LBPH face recognizer  face_recognizer = cv2.face.createLBPHFaceRecognizer()  #or use EigenFaceRecognizer by replacing above line with  #face_recognizer = cv2.face.createEigenFaceRecognizer()  #or use FisherFaceRecognizer by replacing above line with  #face_recognizer = cv2.face.createFisherFaceRecognizer() ```  Now that we have initialized our face recognizer and we also have prepared our training data, it's time to train the face recognizer. We will do that by calling the `train(faces-vector, labels-vector)` method of face recognizer.    ```python #train our face recognizer of our training faces face_recognizer.train(faces, np.array(labels)) ```  **Did you notice** that instead of passing `labels` vector directly to face recognizer I am first converting it to **numpy** array? This is because OpenCV expects labels vector to be a `numpy` array.   Still not satisfied? Want to see some action? Next step is the real action, I promise!   ### Prediction  Now comes my favorite part, the prediction part. This is where we actually get to see if our algorithm is actually recognizing our trained subjects's faces or not. We will take two test images of our celeberities, detect faces from each of them and then pass those faces to our trained face recognizer to see if it recognizes them.   Below are some utility functions that we will use for drawing bounding box (rectangle) around face and putting celeberity name near the face bounding box.    ```python #function to draw rectangle on image  #according to given (x, y) coordinates and  #given width and heigh def draw_rectangle(img, rect):     (x, y, w, h) = rect     cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)      #function to draw text on give image starting from #passed (x, y) coordinates.  def draw_text(img, text, x, y):     cv2.putText(img, text, (x, y), cv2.FONT_HERSHEY_PLAIN, 1.5, (0, 255, 0), 2) ```  First function `draw_rectangle` draws a rectangle on image based on passed rectangle coordinates. It uses OpenCV's built in function `cv2.rectangle(img, topLeftPoint, bottomRightPoint, rgbColor, lineWidth)` to draw rectangle. We will use it to draw a rectangle around the face detected in test image.  Second function `draw_text` uses OpenCV's built in function `cv2.putText(img, text, startPoint, font, fontSize, rgbColor, lineWidth)` to draw text on image.   Now that we have the drawing functions, we just need to call the face recognizer's `predict(face)` method to test our face recognizer on test images. Following function does the prediction for us.   ```python #this function recognizes the person in image passed #and draws a rectangle around detected face with name of the  #subject def predict(test_img):     #make a copy of the image as we don't want to chang original image     img = test_img.copy()     #detect face from the image     face, rect = detect_face(img)      #predict the image using our face recognizer      label= face_recognizer.predict(face)     #get name of respective label returned by face recognizer     label_text = subjects[label]          #draw a rectangle around face detected     draw_rectangle(img, rect)     #draw name of predicted person     draw_text(img, label_text, rect[0], rect[1]-5)          return img ```  * **line-6** read the test image * **line-7** detect face from test image * **line-11** recognize the face by calling face recognizer's `predict(face)` method. This method will return a lable * **line-12** get the name associated with the label * **line-16** draw rectangle around the detected face * **line-18** draw name of predicted subject above face rectangle  Now that we have the prediction function well defined, next step is to actually call this function on our test images and display those test images to see if our face recognizer correctly recognized them. So let's do it. This is what we have been waiting for.    ```python print(""Predicting images..."")  #load test images test_img1 = cv2.imread(""test-data/test1.jpg"") test_img2 = cv2.imread(""test-data/test2.jpg"")  #perform a prediction predicted_img1 = predict(test_img1) predicted_img2 = predict(test_img2) print(""Prediction complete"")  #create a figure of 2 plots (one for each test image) f, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))  #display test image1 result ax1.imshow(cv2.cvtColor(predicted_img1, cv2.COLOR_BGR2RGB))  #display test image2 result ax2.imshow(cv2.cvtColor(predicted_img2, cv2.COLOR_BGR2RGB))  #display both images cv2.imshow(""Tom cruise test"", predicted_img1) cv2.imshow(""Shahrukh Khan test"", predicted_img2) cv2.waitKey(0) cv2.destroyAllWindows() cv2.waitKey(1) cv2.destroyAllWindows() ```      Predicting images...     Prediction complete    ![png](output_43_1.png)   wohooo! Is'nt it beautiful? Indeed, it is!   ## End Notes  Face Recognition is a fascinating idea to work on and OpenCV has made it extremely simple and easy for us to code it. It just takes a few lines of code to have a fully working face recognition application and we can switch between all three face recognizers with a single line of code change. It's that simple.   Although EigenFaces, FisherFaces and LBPH face recognizers are good but there are even better ways to perform face recognition like using Histogram of Oriented Gradients (HOGs) and Neural Networks. So the more advanced face recognition algorithms are now a days implemented using a combination of OpenCV and Machine learning. I have plans to write some articles on those more advanced methods as well, so stay tuned!    ```python  ```"
Magic 8 Ball,Python,https://github.com/Vaibhavabhaysharma/Magic-8-Ball-Game-by-Python/tree/master,"# Magic-8-Ball-Game-by-Python Magic 8  Ball Game Magic 8 Ball is the fantastic game from 90's where you could ask your question to the ball and it would come up with an answer. Now you can play it in your device ,start playing! This game is made with the help of python language,using random function. This program also writes questions asked during the gameplay in a csv file line by line. I hope this will be helpful!!"
Sudoku Solver,Python,https://github.com/dhhruv/Sudoku-Solver,"<p align=""center"">   <img src=""https://github.com/dhhruv/Sudoku-Solver/blob/master/assets/thumbnail.png"" width=""256"" height=""256"">   <h2 align=""center"" style=""margin-top: -4px !important;"">Sudoku Solver: A Fun and Challenging Way to Exercise Your Brain!</h2>   <p align=""center"">     <a href=""https://github.com/dhhruv/Sudoku-Solver/blob/master/LICENSE"">       <img src=""https://img.shields.io/badge/license-MIT-informational"">     </a>     <a href=""https://www.python.org/"">     	<img src=""https://img.shields.io/badge/python-v3.8-informational"">     </a>   </p> </p> <p align=""center""> 	<img src=""http://ForTheBadge.com/images/badges/made-with-python.svg""> </p> <p align=""center"">    	<a href=""https://dev.to/dhhruv/sudoku-solver-a-visualizer-made-using-backtracking-algorithm-5f0d"">     	<img src=""https://img.shields.io/badge/dev.to-0A0A0A?style=for-the-badge&logo=dev.to&logoColor=white"">     </a> </p>   - Are you ready to put your logical thinking and reasoning skills to the test? Look no further than Sudoku Solver, the ultimate tool for solving Sudoku puzzles quickly and easily.  - Sudoku is a popular number-placement puzzle game that requires players to fill a nine-by-nine grid with digits so that each row, column, and 3x3 section contains numbers between 1 and 9, with each number used once and only once in each section. It's a challenging game that helps improve concentration, memory, and cognitive abilities.  - With Sudoku Solver, you don't have to spend hours solving puzzles by hand. Our powerful algorithm solves almost any Sudoku puzzle in seconds, allowing you to move on to more challenging puzzles quickly. Plus, our visualizer feature demonstrates how the backtracking algorithm works, so you can understand the logic behind each solution.  ## Features: - Generates a random, solvable Sudoku board every time the script is executed - Easy-to-use GUI for inputting and solving puzzles - Visualizer feature demonstrates how the backtracking algorithm works - Option to receive hints for tricky puzzles - Improves cognitive abilities, including concentration and logical thinking  ## How to Use: To use Sudoku Solver, simply download the repository and run the SudokuGUI.py file. You'll be presented with a random Sudoku board and a GUI for inputting your solutions. Use the left-click to select the box you want to fill, then enter your value and confirm it by pressing the Enter key.  <p align=""center""> 	<img src=""https://github.com/dhhruv/Sudoku-Solver/blob/master/assets/Entering%20Values.gif""> </p>  If you get stuck on a tricky puzzle, press the Spacebar key to solve the board using the backtracking algorithm. You can also receive hints by pressing the ""h"" key, which will display a random correct value on the board.  <p align=""center""> 	<img src=""https://github.com/dhhruv/Sudoku-Solver/blob/master/assets/Visualizer.gif""> </p>  ## Input:  | Keys              | Actions                                                         | |-------------------|-----------------------------------------------------------------| | `Left Click`      | Selects the Box to enter a value into that cell.                | | `Enter`           | Confirms the Value written on the board.     | | `Backspace/Delete`| Deletes the value in that cell.                                 | | `Space`           | Solves the Board using the Algorithm.                           | | `h`               | Gives a Hint. Displays a random correct value on the board.     |  ## Requirements: To run Sudoku Solver, you'll need to have Python and PyGame installed on your system. You can install the necessary requirements using pip: ``` pip install -r requirements.txt ```  ## Execution: - Clone this repository using ``` git clone https://github.com/dhhruv/Sudoku-Solver.git ``` **OR** - Zip Download the Repository and Extract it's contents. - Now run the [SudokuGUI](https://github.com/dhhruv/Sudoku-Solver/blob/master/SudokuGUI.py) file directly in your Terminal using ``` python SudokuGUI.py ``` **OR** ``` python3 SudokuGUI.py ```  ## Conclusion: If you're a fan of Sudoku puzzles, Sudoku Solver is the perfect tool to help you solve them quickly and easily. With its powerful algorithm and easy-to-use GUI, you'll be solving puzzles in no time. Plus, Sudoku is a great way to exercise your brain and improve your cognitive abilities, making it a fun and challenging game for people of all ages. So what are you waiting for? Download Sudoku Solver today and start playing!  <p align='center'><b>Made with ‚ù§ by Dhruv Panchal</b></p>"
Ping Pong Game,Python,https://github.com/prathameshparit/Ping-Pong-Game,"# Ping-Pong-Game  This project is a classic Ping Pong Game implementation using Python and the Tkinter library. The game allows two players to play against each other, hitting the ball back and forth with paddles, and scoring points when the opponent misses.  ## Screenshots  <img align=""top"" alt=""python"" src=""https://github.com/prathameshparit/Ping-Pong-Game/blob/ba2285a9d107d4282c724fab6655354ecf1c90f3/Ping%20Pong%20Screenshot.png"">  </p>  ## Getting Started ### Prerequisites To run this project, you will need to have Python 3.x installed on your system. You can download it from the official Python website here.  ### Installation Clone the repository to your local machine: ``` git clone https://github.com/prathameshparit/Ping-Pong-Game.git ``` Navigate to the project directory: ``` cd Ping-Pong-Game ``` Install the required dependencies: ``` pip install -r requirements.txt ``` Usage To start the game, run the pythonballgame.py script:  ``` python pythonballgame.py ``` The game window will appear, and you can use the 'w' and 's' keys to move the left paddle up and down, and the 'up' and 'down' arrow keys to move the right paddle up and down. You can customize the game settings, such as the ball speed and paddle size, by modifying the config.ini file.  ## License This project is licensed under the MIT License - see the LICENSE file for details.  ## Acknowledgments This project was inspired by the classic Ping Pong Game and built using the knowledge gained from learning Python and Tkinter.     "
3D AR with Python (OpenCV and NumPy),Python,https://github.com/jayantjain100/Augmented-Reality,"# Augmented Reality    ""ar.py"" is a python program that uses OpenCV to render .obj files on Aruco markers. For an in-depth explanation, check out my [blog tutorial](https://medium.com/swlh/augmented-reality-diy-3fc138274561).    <!-- ![Demo](https://user-images.githubusercontent.com/31953115/121981314-0b712c00-cdab-11eb-98d4-decf737824ea.gif) -->    <p align = ""center"">    <img src=""https://user-images.githubusercontent.com/31953115/121981314-0b712c00-cdab-11eb-98d4-decf737824ea.gif"" alt=""animated"" />  </p>      ## Dependencies    It uses OpenCV(3.4.2) and Numpy(1.17.2). The python version is 3.7.5    Before installing the dependencies, it is recommended to create a virual environment to prevent confilcts with the existing environment. Using conda,     ```bash  conda create -n augmented_reality python=3.7.5  conda activate augmented_reality  ```     To install the dependencies -   ```bash  pip install -r requirements.txt  ```    ## Usage  For the main program -     ```bash  python ar.py  ```  It is recommended to print the aruco marker(data/m1.pdf) on a piece of paper. Alternatively, the program should work (but inferiorly) with the marker open on your phone. The white margin around the marker boundary is required for boundary detection. Keeping the marker flat (for example, by sticking it to a piece of cardboard) further helps in detecting the marker.     For a faster version which uses the Lucas-Kanade method for tracking -   ```bash  python ar_with_tracking.py  ```    (Optional) For camera calibration alter the path mentioned in the file and run -     ```bash  python camera_calib.py   ```    Refer to the blog for all the details about camera calibration.    "
Attend Zoom Meetings Automatically,Python,https://github.com/YarinCodes/auto-zoom-meetings-join,"<p align=""center"">   <img src=""https://camo.githubusercontent.com/3f9d1f717974430cff1b747471a93fc833a87fbdf5d5b3e4c7f4da34325d280e/68747470733a2f2f63646e2e646973636f72646170702e636f6d2f6174746163686d656e74732f3738363730383939323239383331393931322f3738393839343734313435373130393033322f62616e6e65722e6a7067"" width=""750px""> </p> <br>  ## Automatic Meetings Joins a simple python script which joins your zoom meetings and works on image recognition.<br> the script will join without audio and without audio so you can keep sleeping.  ## Requirements * python 3.6 or higher (https://www.python.org/) * pip * Clone this repository * Download requirements using `pip install -r requirements.txt` * run `python auto.py` * have zoom installed on your computer * be signed in on zoom  ## Setup * go to `meetings.xlsx` file and add your meetings<br> ***you have two options:***<br> (1) join only with meeting link (leave the `Meetting id` and `Meeting Passcode` empty)<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**or**<br> (2) join only with meeting id and meeting passcode (leave the `Meetting Link` empty, in case meeting has no passcode leave it empty)<br> * make sure to fill in `auto.py` the `Xl_FILE_PATH` and `ZOOM_PATH`<br> * make sure to fill the starting time in the xl file in 24:00 format(1PM = 13:00)"
Message Encrypter,Python,https://github.com/Rupa-Veerala/Secret-Message-Encryption-Decryption-using-Python,"# Secret Message Encryption and Decryption Using Python  **About**  Encryption is the process of translating plain text data (plaintext) into something that appears to be random and meaningless (ciphertext).  Encryption provide us high security and privacy on our chat/ messages ,or in sharing any confidential data on any social media .  if I  want to send your bank no.  password or any other legal pin or code, we can't trust on any messenger. But what happened if we encrypt this messages. Then no one can read this , even can't understand.  **Decryption**  Decryption is a process of converting encrypted text back to normal text.  So the people have this software and password, can ready it easily.  Feel safe and secure with any messages ,keep privacy.  **Encryption**  ![image](https://github.com/Rupa-Veerala/Secret-Message-Encryption-Decryption-using-Python/assets/102415108/10890888-c5c8-46ce-a992-a788b13d5690)            ![image](https://github.com/Rupa-Veerala/Secret-Message-Encryption-Decryption-using-Python/assets/102415108/8efea2e5-8e3f-4878-81b0-fdc34d6d22e4)  WITHOUT PASSWORD  ![image](https://github.com/Rupa-Veerala/Secret-Message-Encryption-Decryption-using-Python/assets/102415108/c09ad6ec-b81f-447b-bde1-9304cffda730)  **Decryption**  ![image](https://github.com/Rupa-Veerala/Secret-Message-Encryption-Decryption-using-Python/assets/102415108/9ba8ccf0-c2ae-48c2-8b3f-83af6838a2ee)  ![image](https://github.com/Rupa-Veerala/Secret-Message-Encryption-Decryption-using-Python/assets/102415108/9a529191-9f12-4360-8dab-394142fc62ea)   "
Virtual Assistant,Python,https://github.com/ashutoshkrris/Virtual-Personal-Assistant-using-Python,* freeCodeCamp article: https://www.freecodecamp.org/news/python-project-how-to-build-your-own-jarvis-using-python/  * Hashnode articles:   * Part 1: https://ashutoshkrris.hashnode.dev/how-to-build-your-personal-ai-assistant-using-python   * Part 2: https://ashutoshkrris.hashnode.dev/how-to-implement-functionality-to-your-personal-ai-assistant-using-python  Demo Video: https://vimeo.com/650156113  Contents of .env file:  ``` USER=None BOTNAME=JARVIS EMAIL=None PASSWORD=None NEWS_API_KEY=None OPENWEATHER_APP_ID=None TMDB_API_KEY=None ```  Replace `None` with your values
Automatic Quote Generator (using Quotes API),Python,https://github.com/Mahitej28/Random-Quote-Generator,"# Random Quote Generator   ## Description - `requests` library is used, to make HTTP requests in Python - `quote.py` script is used to generate random quotes by fetching them from public API endpoint  ## Setup instructions  - Install python from [here](https://www.python.org/.) - Install requests library using command : `pip install requests`  ## Detailed explanation of script, if needed  The function `generate_quote()` make GET request to the public API endpoint, extracts the data in json format and displays in randomly to the user. It handles error if any in the `else` condition.  ## Output  In text Format the output would be as follows : <br> Arnold Schwarzenegger - If you want to turn a vision into reality, you have to give 100% and never stop believing in your dream.   ## Author  [Mahima Churi](https://github.com/Mahitej28)  ## Disclaimers, if any  Any puplic API can be used for generating quotes, also datasets from Kaggle can also be imported and used."
Automatic Quote Generator (using Quotes API),Python,https://github.com/Mahitej28/Random-Quote-Generator,"# Random Quote Generator   ## Description - `requests` library is used, to make HTTP requests in Python - `quote.py` script is used to generate random quotes by fetching them from public API endpoint  ## Setup instructions  - Install python from [here](https://www.python.org/.) - Install requests library using command : `pip install requests`  ## Detailed explanation of script, if needed  The function `generate_quote()` make GET request to the public API endpoint, extracts the data in json format and displays in randomly to the user. It handles error if any in the `else` condition.  ## Output  In text Format the output would be as follows : <br> Arnold Schwarzenegger - If you want to turn a vision into reality, you have to give 100% and never stop believing in your dream.   ## Author  [Mahima Churi](https://github.com/Mahitej28)  ## Disclaimers, if any  Any puplic API can be used for generating quotes, also datasets from Kaggle can also be imported and used."
Guessing Game,Python,https://github.com/tombh/Python-Stuff/tree/master/Numer%20Guessing%20Game,Some Random Python Stuff I've Made ============
Hangman Game,Python,https://github.com/meet244/Hangman-game,"# Hangman Game ü™ìüë§üë§üë§üë§üë§üë§  üéÆ Welcome to the Hangman game! This is a classic word-guessing game implemented using the tkinter library in Python. Test your vocabulary skills and try to guess the hidden word before the hangman is complete!   ![Hangman Screenshot 1](1.png) ![Hangman Screenshot 2](2.png)  ## How to Play  üî§ Run the Python script using a Python interpreter. A window will appear with a canvas displaying a scaffold for the hangman. The word to guess will be represented by blank spaces, and the number of guesses remaining will be displayed.  üî† Enter a single letter in the entry field and press Enter to make a guess. If the guessed letter is in the word, it will be revealed in the word display. If the letter is not in the word, the hangman will be drawn step by step, and the number of guesses remaining will decrease.  üîÑ Keep guessing letters until you either guess the word correctly or run out of guesses. If you win, a congratulatory message will be displayed, and you can choose to retry the game or exit. If you lose, a message will be displayed with the correct word, and you can choose to retry the game or exit.  ## Dependencies  The following dependencies are required to run the Hangman game:  - Python 3.x - tkinter library - playsound library  You can install the required libraries using the following commands:  ``` pip install tkinter pip install playsound ```  Note: The playsound library requires the availability of a media player on your system to play the sound effects.  ## Project Structure  The Hangman game consists of the following components:  - üìù `words`: A list of words from which the game selects a random word to be guessed. - üî¢ `guesses`: The number of remaining guesses. - üà≤ `guessed_letters`: A list of letters that have been guessed. - üñºÔ∏è `root`: The main window of the game. - üñåÔ∏è `canvas`: The canvas for drawing the hangman. - üè∑Ô∏è `word_label`: A label for displaying the word with blank spaces and correctly guessed letters. - üéØ `guesses_label`: A label for displaying the number of remaining guesses. - üìö `guessed_label`: A label for displaying the letters that have been guessed. - üñäÔ∏è `guess_entry`: An entry field for the player to input their guesses. - ‚úÖ `check_guess()`: A function to check the player's guess and update the game state accordingly. - üîÅ `retry_game()`: A function to reset the game and start a new round. - üõë `exit_game()`: A function to exit the game. - üîÅ `retry_button`: A button to retry the game. - üõë `exit_button`: A button to exit the game.  ## How to Contribute  üéÅ If you'd like to contribute to this project, you can follow these steps:  1. üç¥ Fork the repository and clone it to your local machine. 2. üõ†Ô∏è Make your changes or add new features. 3. ‚úîÔ∏è Test the changes to ensure everything is working correctly. 4. üìù Commit your changes and push them to your forked repository. 5. üîÑ Submit a pull request explaining the changes you've made.  ## Credits  üôå This project is based on the Hangman game implemented in Python using the tkinter library. The game logic and graphical elements were created by me.  Feel free to improve and enhance this project. Enjoy playing Hangman! üéâ"
Tic-Tac-Toe Game,Python,https://github.com/aqeelanwar/Tic-Tac-Toe,"# Tic-Tac-Toe  This repository contains python based interactive Tic-Tac-Toe game.  ## Running Tic-Tac-Toe:  ``` git clone https://github.com/aqeelanwar/Tic-Tac-Toe.git cd Tic-Tac-Toe python main.py ```  <p align=""center""> <img src=""/images/preview.gif""> </p>  ## Screenshots <p align=""center""> <img width=1000 src=""/images/screenshot.png"">  </p>  ## Controls 1. Player X starts the game 2. Click on each grid to place symbol 3. The result of the game is displayed at the end of the game 4. A track of player scores is maintained 5. Click anywhere on the result screen to play again    ## Author [Aqeel Anwar](https://www.prism.gatech.edu/~manwar8) "
Stock Scanner,Python,https://github.com/pranjal-joshi/Screeni-py,"| | | :-: | | ![Screeni-py](https://user-images.githubusercontent.com/6128978/217816268-74c40180-fc47-434d-938b-3639898ee3e0.png) | | [![GitHub release (latest by date)](https://img.shields.io/github/v/release/pranjal-joshi/Screeni-py?style=for-the-badge)](https://github.com/pranjal-joshi/Screeni-py/releases/latest) [![GitHub all releases](https://img.shields.io/github/downloads/pranjal-joshi/Screeni-py/total?color=Green&label=Downloads&style=for-the-badge)](#) ![Docker Pulls](https://img.shields.io/docker/pulls/joshipranjal/screeni-py?style=for-the-badge&logo=docker) [![GitHub](https://img.shields.io/github/license/pranjal-joshi/Screeni-py?style=for-the-badge)](https://github.com/pranjal-joshi/Screeni-py/blob/main/LICENSE) [![CodeFactor](https://www.codefactor.io/repository/github/pranjal-joshi/screeni-py/badge?style=for-the-badge)](https://www.codefactor.io/repository/github/pranjal-joshi/screeni-py) [![MADE-IN-INDIA](https://img.shields.io/badge/MADE%20WITH%20%E2%9D%A4%20IN-INDIA-orange?style=for-the-badge)](https://en.wikipedia.org/wiki/India) [![BADGE](https://img.shields.io/badge/PULL%20REQUEST-GUIDELINES-red?style=for-the-badge)](https://github.com/pranjal-joshi/Screeni-py/blob/new-features/CONTRIBUTING.md) | | [![Screenipy Test - New Features](https://github.com/pranjal-joshi/Screeni-py/actions/workflows/workflow-test.yml/badge.svg?branch=new-features)](https://github.com/pranjal-joshi/Screeni-py/actions/workflows/workflow-test.yml) [![Screenipy Build - New Release](https://github.com/pranjal-joshi/Screeni-py/actions/workflows/workflow-build-matrix.yml/badge.svg)](https://github.com/pranjal-joshi/Screeni-py/actions/workflows/workflow-build-matrix.yml) | | ![Windows](https://img.shields.io/badge/Windows-0078D6?style=for-the-badge&logo=windows&logoColor=white) ![Linux](https://img.shields.io/badge/Linux-FCC624?style=for-the-badge&logo=linux&logoColor=black) ![Mac OS](https://img.shields.io/badge/mac%20os-D3D3D3?style=for-the-badge&logo=apple&logoColor=000000) ![Docker](https://img.shields.io/badge/docker-%230db7ed.svg?style=for-the-badge&logo=docker&logoColor=white) | | <img width=""240"" src=""https://user-images.githubusercontent.com/6128978/217814499-7934edf6-fcc3-46d7-887e-7757c94e1632.png""><h2>Scan QR Code to join [Official Telegram Group](https://t.me/+0Tzy08mR0do0MzNl) for Additional Discussions</h2> |  | **YouTube** | **Use** | **Discussion** | **Bugs/Issues** | **Documentation** | | :---: | :---: | :---: | :---: | :---: | | [![youtube](https://github.com/pranjal-joshi/Screeni-py/assets/6128978/9673bbcf-4798-48f4-918d-692b90e28d37)](https://www.youtube.com/playlist?list=PLsGnKKT_974J3UVS8M6bxqePfWLeuMsBi) | [![docker](https://github.com/pranjal-joshi/Screeni-py/assets/6128978/f44054b8-9fcb-465c-a38b-63f6ecc4a0c9)](https://hub.docker.com/r/joshipranjal/screeni-py/tags) | [![meeting](https://user-images.githubusercontent.com/6128978/149935812-31266023-cc5b-4c98-a416-1d4cf8800c0c.png)](https://github.com/pranjal-joshi/Screeni-py/discussions) | [![warning](https://user-images.githubusercontent.com/6128978/149936142-04d7cf1c-5bc5-45c1-a8e4-015454a2de48.png)](https://github.com/pranjal-joshi/Screeni-py/issues?q=is%3Aissue) | [![help](https://user-images.githubusercontent.com/6128978/149937331-5ee5c00a-748d-4fbf-a9f9-e2273480d8a2.png)](https://github.com/pranjal-joshi/Screeni-py/blob/main/README.md#what-is-screeni-py) | | Watch our [YouTube](https://www.youtube.com/playlist?list=PLsGnKKT_974J3UVS8M6bxqePfWLeuMsBi) playlist | Get started quickly using Docker | Join/Read the Community Discussion | Raise an Issue about a Problem | Get Help about Usage |  <!-- ## [**Click to Download the Latest Version**](https://github.com/pranjal-joshi/Screeni-py/releases/latest) -->  ---  ## What is Screeni-py?  ### A Python-based stock screener for NSE, India  **Screenipy** is an advanced stock screener to find potential breakout stocks from NSE and tell its possible breakout values. It also helps to find the stocks that are consolidating and may breakout, or the particular chart patterns that you're looking for specifically to make your decisions. Screenipy is totally customizable and it can screen stocks with the settings that you have provided.  ## How to use? (New Version - GUI Based)  [![Screeni-py - Detailed Installation Guide](https://markdown-videos-api.jorgenkh.no/url?url=https%3A%2F%2Fyoutu.be%2F2HMN0ac4H20)](https://youtu.be/2HMN0ac4H20) [![Screeni-py - Configuration and Usage | Screenipy - Python NSE Stock Screener](https://markdown-videos-api.jorgenkh.no/url?url=https%3A%2F%2Fyoutu.be%2FJCn6z1A7INI)](https://youtu.be/JCn6z1A7INI) [![Screeni-py - How to install Software Updates? | Screenipy - Python NSE Stock Screener](https://markdown-videos-api.jorgenkh.no/url?url=https%3A%2F%2Fyoutu.be%2FT41m13iMyJc)](https://youtu.be/T41m13iMyJc)  * Install Docker Desktop and pull the `latest` docker image from the [release](https://github.com/pranjal-joshi/Screeni-py/releases/latest) page. * Checkout this [YouTube Video](https://youtu.be/2HMN0ac4H20) for detailed installation guide.    <img width=""1438"" alt=""image"" src=""https://github.com/pranjal-joshi/Screeni-py/assets/6128978/2016be00-5892-4735-8ab3-5f5b70add103""> <img width=""1438"" alt=""image"" src=""https://github.com/pranjal-joshi/Screeni-py/assets/6128978/28947290-7f42-4f6f-9fc0-0bae1ee6d6f4""> <img width=""1438"" alt=""image"" src=""https://github.com/pranjal-joshi/Screeni-py/assets/6128978/857f8acc-a4e8-4b86-a748-c26057b0e8b1""> <img width=""1438"" alt=""image"" src=""https://github.com/pranjal-joshi/Screeni-py/assets/6128978/360b5faa-f4f4-4df6-bec1-90985889bee6""> <img width=""1438"" alt=""image"" src=""https://github.com/pranjal-joshi/Screeni-py/assets/6128978/99903d67-d450-4c04-93ae-1f5bb4b905a5"">  ## How to use? (Older Version - CLI Based - DEPRECATED)  * Download the suitable file according to your OS or install Docker Desktop and pull the `latest` docker image. * Linux & Mac users should make sure that the `screenipy.bin or screenipy.run` has `execute` permission. * **Run** the file. The following window will appear after a brief delay.      ![home](screenshots/screenipy_demo.gif)  * **Configure** the parameters as per your requirement using `Option > 8`.      ![config](screenshots/config.png)  * Following are the screenshots of screening and output results.      ![screening](screenshots/screening.png)     ![results](screenshots/results.png)     ![done](screenshots/done.png)  * Once done, you can also save the results in an Excel file.  ## Understanding the Result Table  The Result table contains a lot of different parameters which can be pretty overwhelming to the new users, so here's the description and significance of each parameter.  | Sr | Parameter | Description | Example | |:---:|:---:|:---|:---| |1|**Stock**|This is a NSE scrip symbol. If your OS/Terminal supports unicode, You can directly open **[TradingView](https://in.tradingview.com/)** charts by pressing `Ctrl+Click` on the stock name.|[TATAMOTORS](https://in.tradingview.com/chart?symbol=NSE%3ATATAMOTORS)| |2|**Consolidating**|It gives the price range in which stock is trading since last `N` days. `N` is configurable and can be modified by executing `Edit User Configuration` option.|If stock is trading between price 100-120 in last 30 days, Output will be `Range = 20.0 %`| |3|**Breakout (N Days)**|This is pure magic! The `BO` is Breakout level in last N days while `R` is the next resistance level if available. An investor should consider both BO & R level to decide entry/exits in their trades.|`B:302, R:313`(Breakout level is 100 & Next resistance is 102)| |4|**LTP**|LTP is the Last Traded Price of an asset traded on NSE.|`298.7` (Stock is trading at this price)| |5|**Volume**|Volume shows the relative volume of the recent candle with respect to 20 period MA of Volume. It could be `Unknown` for newly listed stocks.|if 20MA(Volume) is 1M and todays Volume is 2.8M, then `Volume = 2.8x`| |6|**MA-Signal**|It describes the price trend of an asset by analyzing various 50-200 MA/EMA crossover strategies.|`200MA-Support`,`BullCross-50MA` etc| |7|**RSI**|For the momentum traders, it describes 14-period RSI for quick decision-making about their trading plans|`0 to 100`| |8|**Trend**|By using advanced algorithms, the average trendlines are computed for `N` days and their strength is displayed depending on the steepness of the trendlines. (This does NOT show any trendline on a chart, it is calculated internally)|`Strong Up`, `Weak Down` etc.| |9|**Pattern**|If the chart or the candle itself forming any important pattern in the recent timeframe or as per the selected screening option, various important patterns will be indicated here.|`Momentum Gainer`, `Inside Bar (N)`,`Bullish Engulfing` etc.|  ## Hack it your way  Feel free to Edit the parameters in the `screenipy.ini` file which will be generated by the application.  ```ini [config] period = 300d daystolookback = 30 duration = 1d minprice = 30 maxprice = 10000 volumeratio = 2 consolidationpercentage = 10 shuffle = y cachestockdata = y onlystagetwostocks = y useema = n ```  Try to tweak these parameters as per your trading styles. For example, If you're comfortable with weekly charts, make `duration=5d` and so on.  ## Installation Guide  ### YouTube Video of Detailed Installation Guide  [![YouTube Video Views](https://img.shields.io/youtube/views/2HMN0ac4H20?style=for-the-badge&logo=youtube)](https://youtu.be/2HMN0ac4H20)  [![Screeni-py - Detailed Installation Guide](https://markdown-videos-api.jorgenkh.no/url?url=https%3A%2F%2Fyoutu.be%2F2HMN0ac4H20)](https://youtu.be/2HMN0ac4H20)  ![Windows](https://img.shields.io/badge/Windows-0078D6?style=for-the-badge&logo=windows&logoColor=white) ![Linux](https://img.shields.io/badge/Linux-FCC624?style=for-the-badge&logo=linux&logoColor=black) ![Mac OS](https://img.shields.io/badge/mac%20os-D3D3D3?style=for-the-badge&logo=apple&logoColor=000000) ![Docker](https://img.shields.io/badge/docker-%230db7ed.svg?style=for-the-badge&logo=docker&logoColor=white)  ### Why we shifted to Docker from the Good old EXEs?  | Executable/Binary File | Docker | | :-- | :-- | | [![GitHub Downloads](https://img.shields.io/github/downloads/pranjal-joshi/Screeni-py/total?color=Green&label=Downloads&style=for-the-badge)](#) | ![Docker Pulls](https://img.shields.io/docker/pulls/joshipranjal/screeni-py?style=for-the-badge&logo=docker) | | Download Directly from the [Release](https://github.com/pranjal-joshi/Screeni-py/releases/latest) page (DEPRECATED) | Need to Install [Docker Desktop](https://www.docker.com/products/docker-desktop/) ‚ö†Ô∏è| | May take a long time to open the app | Loads quickly | | Slower screening | Performance boosted as per your CPU capabilities | | You may face errors/warnings due to different CPU arch of your system ‚ö†Ô∏è | Compatible with all x86_64/amd64/arm64 CPUs irrespective of OS (including Mac M1/M2) | | Works only with Windows 10/11 ‚ö†Ô∏è | Works with older versions of Windows as well | | Different file for each OS | Same container is compatible with everyone | | Antivirus may block this as untrusted file ‚ö†Ô∏è | No issues with Antivirus | | Need to download new file for every update | Updates quickly with minimal downloading | | No need of commands/technical knowledge | Very basic command execution skills may be required | | Incompatible with Vector Database ‚ö†Ô∏è | Compatible with all Python libraries |  ### How to set up and use Screeni-py with Docker?  1. Download and Install [Docker Desktop](https://www.docker.com/products/docker-desktop/) with default settings 2. If you are using Windows, update WSL (Windows subsystem for linux) by running `wsl --update` command in the command prompt 3. Restart your computer after installation 4. Open Docker Desktop and keep it as it is 5. Open Command Prompt (Windows) or Terminal (Mac/Linux) and run command `docker pull joshipranjal/screeni-py:latest` 6. Once installed, always start screenipy by running this command:      ```bash     docker run -p 8501:8501 -p 8000:8000 joshipranjal/screeni-py:latest      OR      docker run -it --entrypoint /bin/bash joshipranjal/screeni-py:latest -c ""run_screenipy.sh --cli""     ```  Check out this [YouTube Video](https://youtu.be/2HMN0ac4H20) for a detailed installation guide.  ## Contributing  * Please feel free to Suggest improvements/report bugs by creating an issue. * Please follow the [Guidelines for Contributing](https://github.com/pranjal-joshi/Screeni-py/blob/new-features/CONTRIBUTING.md) while making a Pull Request.  ## Disclaimer  * DO NOT use the result provided by the software 'solely' to make your trading decisions. * Always backtest and analyze the stocks manually before you trade. * The Author and the software will not be held liable for your losses."
Random Password Generator,Python,https://github.com/Ramesh-Bhutka/-Python-Random-Password-Generator-,# -Python-Random-Password-Generator-   simple_password_generator.py   -  Is the simple 16 digit password generator python function   -  with the help of simple Libraries such as string and random    Gui_password_generator.py    - Is the simplest 15 digit password generator python function    -  with the help of simple Libraries such as string and random   -  tkinter Library
Stack-Visualizer,Python,https://github.com/SamarpanCoder2002/Stack-Visualizer,"![](https://img.shields.io/badge/Programming_Language-Python-blue.svg) ![](https://img.shields.io/badge/Main_Tool_Used-Tkinter-orange.svg) ![](https://img.shields.io/badge/Support_Tool_Used-Pillow-orange.svg) ![](https://img.shields.io/badge/Python_Version-3.7-blue.svg) ![](https://img.shields.io/badge/Application-Visualization-brown.svg) ![](https://img.shields.io/badge/Status-Complete-green.svg)  --- ### <p align=""center"" style=""color: blue"">***Hello Programmers, Here I made a Visualizer <p align=""center"" style=""color: red"">Stack Visualizer</p> <p align=""center"" style=""color: blue"">Using</p> <p align=""center"" style=""color: red"">Python Tkinter***</p></p>  <p align=""center""> <img alt=""gif"" height=""350px"" width=""350px"" src=""stack_gif.gif""/><br></p>  --- ### <p align=""left"" style=""color: brown"">‚öíÔ∏è _Important Module Used Here:_</p> - ***_Tkinter_***  ### <p align=""left"" style=""color: brown"">‚û°Ô∏è _Steps for Download and Install Pillow Module as:_</p>  ``` 1. Open Command Prompt(cmd) or Terminal 2. Write as-->         pip install pillow ```  ---  ## <p style=""color: Blue""> ***_üí° Stack Related Technique Used Here:_***</p> ### ***<p style=""color: green""> ‚û°Ô∏è 1. Push***</p> ### ***<p style=""color: green""> ‚û°Ô∏è 2. Pop***</p>   ### <p align=""left"" style=""color: #FF00FF"">Click On the Star If You Love this Project and Follow me on Github To get New Project Updates</p>    - ###  [Application Related Video Link](https://youtu.be/EANN-DFwOqE ""LCO"")  - ###  [Follow Me on LinkedIn To Get Regular Project Updates](https://www.linkedin.com/in/samarpan-dasgupta-4aa1061b0/ ""LCO"") "
Track Phone Number Location,Python,https://github.com/problemsolvewithridoy/Phone-number-location-tracker-using-python," # Phone Number Location Tracker using Python   Are you curious about the location of a mobile number? Maybe you want to track a lost phone or keep tabs on your child's whereabouts. Whatever your reason, you can use Python to find the location of a mobile number.  In this step-by-step guide, we'll show you how to track a mobile number's location using Python. You'll need some Python coding skills and access to a few geolocation libraries. Let's get started!  To make this project only you need to follow this step:-         ## Installation  Install package with pip  ```bash   pip install phonenumbers   pip install folium   pip install geocoder   pip install opencage ```  Now need to collect Geocoder API Key from https://opencagedata.com/  Step1: Need to log in or sign up  ![github1](https://user-images.githubusercontent.com/123636419/215339770-3cc5ba46-d502-42b9-9f15-856718cf22d1.PNG)  Step2: Need to click Geocoding API  ![github2](https://user-images.githubusercontent.com/123636419/215339775-89aef127-2390-4f8d-8ad6-1129789eabab.PNG)  Step3: From API Keys collect API key  ![github3](https://user-images.githubusercontent.com/123636419/215339773-0171d38c-b9ad-490a-95d8-47366321048a.PNG)     ## Deployment  To deploy this project run  ```bash import phonenumbers from phonenumbers import geocoder from phonenumbers import carrier import opencage from opencage.geocoder import OpenCageGeocode import folium   key = ""your key"" #Geocoder API Key need to paste here ""your key""  number = input(""please giver your number: "") new_number = phonenumbers.parse(number) location = geocoder.description_for_number(new_number, ""en"") print(location)  service_name = carrier.name_for_number(new_number,""en"") print(service_name)  geocoder = OpenCageGeocode(key) query = str(location) result = geocoder.geocode(query) #print(result)  lat = result[0]['geometry']['lat'] lng = result[0]['geometry']['lng']  print(lat,lng)  my_map = folium.Map(location=[lat,lng], zoom_start=9) folium.Marker([lat, lng], popup= location).add_to(my_map)  my_map.save(""location.html"")  print(""location tracking completed"") print(""Thank you"") ```   You can follow me  Facebook:- https://www.facebook.com/problemsolvewithridoy/  Linkedin:- https://www.linkedin.com/in/ridoyhossain/  YouTube:- https://www.youtube.com/@problemsolvewithridoy  If you have any confusion, please feel free to contact me.  Thank you  ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è Attention Please ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è  Tracking the location of a phone number typically involves using specialized services or apps that can access databases and provide information on the general geographic location of a phone. However, it's important to note that tracking someone's location without their explicit consent can raise ethical and legal concerns.  Here are some common methods used for tracking phone number locations:  1. Mobile Tracking Apps: There are several apps available that claim to track phone numbers. Some require installation on the target device, while others might use different methods like phone number databases or social media profiles to provide location information.  2. Reverse Phone Lookup Services: Websites or services like Truecaller, Whitepages, or Spokeo offer reverse phone lookup services. They might provide information on the registered location of a phone number based on public records or user-contributed data.  3. GPS Tracking Services: Certain apps or services allow you to track the location of family members or devices with their consent. For example, Find My iPhone for Apple devices or Google's Find My Device for Android devices can help locate a device linked to a specific account.  4. Telecom Service Providers: In some cases, telecom service providers might offer location tracking services for authorized purposes like finding a lost device or for legal investigations. This is usually done with proper authorization and legal documentation.  Remember, accessing someone's location without their consent can infringe upon their privacy and might be illegal in some jurisdictions. Always ensure you have permission or legal authorization before attempting to track someone's location.  If you're trying to locate a lost device or have concerns about someone's safety, it's often best to involve the authorities or use legitimate methods designed for such situations.  Note:- This project was developed solely for entertainment purposes. Moreover, this particular method is ineffective. Thank you "
Library Management System,Python,https://github.com/kunzbhatia/Library-Management-System,"# Library Management System with Python and MySQL Database Connectivity  ## Description: The Library Management System is a software application that facilitates the efficient management of a library's day-to-day operations. This project is implemented using Python programming language and utilizes the MySQL database to store and retrieve library-related data. The system provides functionalities to manage books, borrowers, and transactions within the library.  ## Key Features:  1. Book Management:    - Add new books to the library database with details like title, author, ISBN, genre, and quantity.    - Update existing book information (e.g., title, author, quantity) if needed.    - Remove books from the library database when they are no longer available.  2. Borrower Management:    - Add new borrowers to the library system, including information like name, contact details, and membership ID.    - Update borrower information when required (e.g., contact details).    - Remove borrowers from the system if necessary.  3. Book Borrowing and Returning:    - Allow borrowers to borrow books by linking the borrower's membership ID to the book details.    - Record the due date for each borrowed book and handle overdue books.    - Implement a mechanism for borrowers to return books, updating the database accordingly.  4. Book Search and Availability:    - Provide a search feature that enables users to find books by title, author, or genre.    - Show the availability status (number of copies available) for each book in the search results.  5. Fine Calculation:    - Calculate and manage fines for late book returns based on pre-defined rules.    - Update fines automatically when books are returned after the due date.  6. Database Connectivity:    - Establish a connection between the Python application and the MySQL database.    - Create appropriate database tables to store books, borrowers, transactions, and fines data.    - Implement CRUD (Create, Read, Update, Delete) operations for seamless data management.  7. User-friendly Interface:    - Design a simple and intuitive command-line or graphical user interface (GUI) for users to interact with the system efficiently.  8. Security and Validation:    - Implement user authentication and authorization mechanisms to ensure that only authorized personnel can access certain functionalities.    - Validate user inputs to prevent potential data corruption or security breaches.  9. Reporting:    - Generate reports and statistics related to book availability, borrower activity, and fines collected.  ## Technologies Used:  - Python: The core programming language used for the development of the Library Management System. - MySQL Database: The database management system used to store and manage library-related data. - MySQL Connector: A Python library to facilitate connectivity between the Python application and the MySQL database.  Note: This description provides an overview of the Library Management System project and its features. Depending on the project's scope and requirements, additional functionalities and enhancements can be included to create a comprehensive and efficient library management solution."
Binary-Search-Tree-Visualizer,Python,https://github.com/SamarpanCoder2002/Binary-Search-Tree-Visualizer,"<p align=""center"">     ![](https://img.shields.io/badge/Programming_Language-Python-blue.svg)  ![](https://img.shields.io/badge/Tool_Used-Tkinter-orange.svg)  ![](https://img.shields.io/badge/Python_Version-3.7-blue.svg)  ![](https://img.shields.io/badge/Data_Structure-BST-brown.svg)  ![](https://img.shields.io/badge/Functionality-7-red.svg)  ![](https://img.shields.io/badge/Status-Complete-green.svg)     </p>      ### <p align=""center"">Hello Developers</p> <p align=""center"">Here I made Binary Search Tree Visualizer using Python Tkinter    <p align=""center""> <img alt=""GIF"" height=""500px""  width=""800px"" src=""bst_gif.gif""/><br></p>    ---    ### ***_<p align=""left""> üí° Functionality I Provide Here---->_***  #### ‚û°Ô∏è 1. Insertion in the Binary Search Tree  #### ‚û°Ô∏è 2. Deletion in the Binary Search Tree  #### ‚û°Ô∏è 3. Pre-Order-Traversal in the Binary Search Tree  #### ‚û°Ô∏è 4. Post-Order-Traversal in the Binary Search Tree  #### ‚û°Ô∏è 5. In-Order-Traversal in the Binary Search Tree  #### ‚û°Ô∏è 6. Level-Order-Traversal in the Binary Search Tree  #### ‚û°Ô∏è 7. Level-Order-Traversal-Logic-Visualization Using the Circular Queue</p>  ---  <h3> üí° Click on the Star If You Love This Project and Follow me on GitHub To Get New Poject Updates<h3>    - #####  [Click Here To See the Project Video](https://youtu.be/9MZDMAiR24I ""LCO"")  - #####  [Follow Me on LinkedIn To Get Regular Project Updates](https://www.linkedin.com/in/samarpan-dasgupta-4aa1061b0/ ""LCO"")    "
Ludo Game,Python,https://github.com/mansiag/Ludo,"# LUDO **Ludo** is strategy board game, which can be played by 2 to 4 players with each player having a set of 4 pieces (usually identified by color) placed in the starting zone, which is at the left corner for each player. By rolling the dice, players race their four pieces on the board from start to finish, while trying to avoid being killed or captured by their opponents.  The main objective for each player is to move all his/her tokens or pieces to the finish before his/her opponents. Of course, en-route there are some more obstacles that the player has to overcome. Go through these step-by-step instructions to get a better perspective on how to master this easy game.  ## How to Run ```bash git clone https://gitlab.com/mansiag/Ludo cd Ludo pip3 install -r requirement.txt python3 main.py ```  ## Instruction  - Each player gets a chance to roll the die. Game starts with one who has Green Coin. The other players start rolling the die in an **anticlockwise** direction. - To enter a token/piece into play,or from its staging area to its starting square, a player must roll a 6. If the player does not roll a 6, the turn passes to the next player. Once a player has one or more tokens in play, he can choose any one of the tokens in play and move them forward. The number on the rolled die will determine the number of squares the chosen piece moves forward. - If a player lands on a square he/she already occupies, the pair of tokens form a block. Opponents can pass or land on the block but would not able to kill. If the advance of a token ends on a square occupied by an opponent's token, the opponent's token is returned to its owner's starting position. The returned token may only be reentered into play when the owner again rolls a 6. - When the player rolls a six, he/she can get a single pawn out off a pocket. Then you roll again and move that pawn the number of spaces corresponding to the second roll.     -   If the player rolls a six on the second roll, then the player can choose to place another pawn out of a pocket or move the first pawn. If you moved a second pawn out of your pocket, roll a third time and proceed to move a pawn.     -   If a player rolls a six on their third roll, they cannot place any pawns out of their pocket. The third six ends a player‚Äôs turn - ***If there is two or more than two coins on a single block of path then player has to click on the visible part of that coin for moving the coin.*** - Now, once a token travels around the entire board and reaches his home stretch or home column, it has to be moved towards the home. For this, a player has to roll the exact number the token has to move in order to reach home. For example, if you roll a four, and your token requires to move only three squares, you have to move another token or pass. The first player to move all his tokens to the finishing square wins the game.  ### Screenshots  ![alt text](./assets/main.png?raw=true)  ![alt text](./assets/name.png?raw=true)  ![alt text](./assets/midphase.png?raw=true)  ![alt text](./assets/kill.png?raw=true)  ## Contributors  - [Mansi Agrawal (@mansiag)](https://github.com/mansiag) - [Shivam Gupta (@shivg7706)](https://github.com/shivg7706)"
Searching Algo Visualizer,Python,https://github.com/SamarpanCoder2002/Searching-Algo-Visualizer,"![](https://img.shields.io/badge/Programming_Language-Python-blue.svg)  ![](https://img.shields.io/badge/Main_Tool_Used-Tkinter-orange.svg)  ![](https://img.shields.io/badge/Support_Tool_Used-Pillow-orange.svg)  ![](https://img.shields.io/badge/Python_Version-3.7-blue.svg)  ![](https://img.shields.io/badge/Application-Visualization-brown.svg)  ![](https://img.shields.io/badge/Status-Complete-green.svg)    ---  ### <p align=""center"" style=""color: blue"">***Hello Programmers, Here I made a Visualizer <p align=""center"" style=""color: red"">Searching Algo Visualizer</p> <p align=""center"" style=""color: blue"">Using</p> <p align=""center"" style=""color: red"">Python Tkinter***</p></p>    <p align=""center""> <img alt=""gif"" height=""400px"" width=""550px"" src=""searching_algo_gif.gif""/><br></p>    ---  ### <p align=""left"" style=""color: brown"">‚öíÔ∏è _Important Module Used Here:_</p>  - ***_Tkinter_***  - ***_Pillow_***    ### <p align=""left"" style=""color: brown"">‚û°Ô∏è _Steps for Download and Install Pillow Module as:_</p>    ```  1. Open Command Prompt(cmd) or Terminal  2. Write as-->          pip install pillow  ```    ---      # <p style=""color: Blue""> ***_üí° Algo Used in This Project:_***</p>  ### ***<p style=""color: green""> ‚û°Ô∏è 1. Linear Search***</p>  ### ***<p style=""color: green""> ‚û°Ô∏è 2. Binary Search***</p>      ### <p align=""left"" style=""color: #FF00FF"">Click On the Star If You Love this Project and Follow me on Github To get New Project Updates</p>        - ###  [Application Related Video Link](https://youtu.be/qsuTFxZ_Fzg ""LCO"")    - ###  [Follow Me on LinkedIn To Get Regular Project Updates](https://www.linkedin.com/in/samarpan-dasgupta-4aa1061b0/ ""LCO"")    "
"Graph Traversing Visualizer (BFS, DFS)",Python,https://github.com/SamarpanCoder2002/Graph-Traversing-Visualizer,"![](https://img.shields.io/badge/Programming_Language-Python-blue.svg)  ![](https://img.shields.io/badge/Tool_Used-Tkinter-orange.svg)  ![](https://img.shields.io/badge/Python_Version-3.7-blue.svg)  ![](https://img.shields.io/badge/Application-Visualization-brown.svg)  ![](https://img.shields.io/badge/Status-Complete-green.svg)    ---  ### <p align=""center"" style=""color: blue"">***Hello Programmers, Here I made a Drawing Application <p align=""center"" style=""color: red"">Graph Traversal Visualizer</p> <p align=""center"" style=""color: blue"">Using</p> <p align=""center"" style=""color: red"">Python Tkinter***</p></p>    <p align=""center""> <img alt=""gif"" height=""400px""  width=""300px"" src=""graph_visualizer_algo.gif""/><br></p>    ---  ### <p align=""left"" style=""color: brown"">‚öíÔ∏è _Important Module Used Here:_</p>  ***_Tkinter_***    # <p style=""color: Blue""> ***_üí° Functionality I Provide Here:_***</p>  ### ***<p style=""color: green""> ‚û°Ô∏è 1. BFS***</p>  ### ***<p style=""color: green""> ‚û°Ô∏è 2. DFS***</p>      ---  ### <p align=""left"" style=""color: #FF00FF"">Click On the Star If You Love this Project and Follow me on Github To get New Project Updates</p>        - ###  [Application Related Video Link](https://youtu.be/FydVN5qnzF8 ""LCO"")    - ###  [Follow Me on LinkedIn To Get Regular Project Updates](https://www.linkedin.com/in/samarpan-dasgupta-4aa1061b0/ ""LCO"")    "
Circular Queue Visualizer,Python,https://github.com/SamarpanCoder2002/Circular-Queue-Visualizer,"![](https://img.shields.io/badge/Programming_Language-Python-blue.svg) ![](https://img.shields.io/badge/Main_Tool_Used-Tkinter-orange.svg) ![](https://img.shields.io/badge/Support_Tool_Used-Pillow-orange.svg) ![](https://img.shields.io/badge/Python_Version-3.7-blue.svg) ![](https://img.shields.io/badge/Application-Visualization-brown.svg) ![](https://img.shields.io/badge/Status-Complete-green.svg)  --- ### <p align=""center"" style=""color: blue"">***Hello Programmers, Here I made a Visualizer <p align=""center"" style=""color: red"">Circular Queue Visualizer</p> <p align=""center"" style=""color: blue"">Using</p> <p align=""center"" style=""color: red"">Python Tkinter***</p></p>  <p align=""center""> <img alt=""gif"" height=""300px"" width=""400px"" src=""CQ_gif.gif""/><br></p>  --- ### <p align=""left"" style=""color: brown"">‚öíÔ∏è _Important Module Used Here:_</p> - ***_Tkinter_*** - ***_Pillow_***  ### <p align=""left"" style=""color: brown"">‚û°Ô∏è _Steps for Download and Install Pillow Module as:_</p>  ``` 1. Open Command Prompt(cmd) or Terminal 2. Write as-->         pip install pillow ```  ---   # <p style=""color: Blue""> ***_üí° Concepts Used Here:_***</p> ### ***<p style=""color: green""> ‚û°Ô∏è 1. Insertion in Circular Queue***</p> ### ***<p style=""color: green""> ‚û°Ô∏è 2. Deletion from Circular Queue***</p>  ---  ### <p align=""left"" style=""color: #FF00FF"">üí° Click On the Star If You Love this Project and Follow me on Github To get New Project Updates</p>    - ###  [Application Related Video Link](https://youtu.be/QhxRqCFWZXA ""LCO"")  - ###  [Follow Me on LinkedIn To Get Regular Project Updates](https://www.linkedin.com/in/samarpan-dasgupta-4aa1061b0/ ""LCO"")  "
Link-List Visualizer,Python,https://github.com/SamarpanCoder2002/Singly-Link-List-Visualizer,"![](https://img.shields.io/badge/Programming_Language-Python-blue.svg) ![](https://img.shields.io/badge/Tool_Used-Tkinter-chocolate.svg) ![](https://img.shields.io/badge/Visualizer-Singly_Link_List-orange.svg) ![](https://img.shields.io/badge/Concept-Data_Structure-gold.svg) ![](https://img.shields.io/badge/Python_Version-3.7-brown.svg) ![](https://img.shields.io/badge/Status-Complete-green.svg)  ## <p align=""center"">   <img src=""Images_for_README/heart.png"" width=30> ***_Introducing My New Project Singly Link List Visualizer_*** <img src=""Images_for_README/heart.png"" width=30></p>  <p align=""center""><img src=""Images_for_README/link_list.png""></p>  ## <p align=""center""> <img src=""Images_for_README/heart.png"" width=30> ***_Made with Python Tkinter_*** <img src=""Images_for_README/heart.png"" width=30> </p>  ## üí° ***_Functionality Provided Here:-_*** - ### ***_Insert node at first_*** - ### ***_Insert node at last_*** - ### ***_Delete node from first_*** - ### ***_Delete node from last_*** - ### ***_Insert a node after a particular node_*** - ### ***_Delete a particular node_***  --- ## üéØ ***_Useful Links:-_***  - ### ***_[Click Here To See the Project Video](https://youtu.be/Tv2Pba1dwNY)_*** - ### ***_[Connect With Me on LinkedIn to Get Regular Project Updates](https://www.linkedin.com/in/samarpan-dasgupta-4aa1061b0/ ""LCO"")_***"
Music Player,Python,https://github.com/das88768/music-player,"# Music Player It is a tkinter based GUI Python Music Player app.  ## Technologies This project is created with: * Python 3.7 * Tkinter * Pygame * pygame Mixer * Audio Metadata * Pillow/PIL * VS-Code  ## Setup To run this project on local environment: * Open Git Bash. * Change the current working directory to the location where you want the cloned directory. * Type git clone, and then paste the below URL.  ``` $ git clone https://github.com/das88768/music-player.git ```  * Press Enter to create your local clone.  ``` $ git clone https://github.com/das88768/music-player.git > Cloning into `file-name`... > remote: Counting objects: 10, done. > remote: Compressing objects: 100% (8/8), done. > remove: Total 10 (delta 1), reused 10 (delta 1) > Unpacking objects: 100% (10/10), done. ``` * And you are done with the setup.  ## How to run the app * Open the VS-Code * Go to the project cloned directory * And you will see something like this on your left side Explorer of vs code.    ![image](https://user-images.githubusercontent.com/89207002/177941606-9409e9ce-6508-4d97-97fa-fad02794413a.png)  * Open the **requirements.txt** file * Install all the modules listed using:  ```  pip install module-name  ``` * Then open the **music_app.py** * And enter the following command on terminal: ``` python music_app.py ```  * A tkinter GUI will appear:    ![image](https://user-images.githubusercontent.com/89207002/177944288-5ac0e395-af5e-4988-a44e-af2d2e03dc2f.png)  * Click on **[Load Directory]** Button. * Navigate to the folder where songs are stored on your pc and click on **[Select Folder]**.      ![image](https://user-images.githubusercontent.com/89207002/177945231-07aa0591-699a-4d4a-b2ef-c8946907c1ed.png)    * Now the GUI will look like this (Playlist box will filled with song names.)    ![image](https://user-images.githubusercontent.com/89207002/177945694-6aa3e9f4-4396-4cbe-8c3a-025fb327df97.png)  * Select any song from the playlist box and click on **[Play]** Button. * The selected song will be start to play the song.    ![image](https://user-images.githubusercontent.com/89207002/177946383-89295775-4713-451f-981e-afb1ca00b727.png)"
Currency Converter,Python,https://github.com/gerardo5797/PythonCurrencyConverter,"# Python Currency Converter  Develop a Python program that will perform currency conversion using data fetched from an open-source API: https://www.frankfurter.app/  Display the current conversion rate between 2 currency codes. It will also calculate the inverse conversion rate between the given 2 currencies. To do so, you will need to call 2 different API endpoints from the Frankfurter app:  Extracting the list of available currency codes https://www.frankfurter.app/docs/#currencies Extracting the current conversion rate for the specified currency codes https://www.frankfurter.app/docs/#latest   Command for running the script: python main.py GBP AUD  The script should return one of the following outputs: Today's (Date) conversion rate from GBP to AUD is #####. The inverse rate is #### [ERROR] You haven't provided 2 currency codes AAA is not a valid option There is an error with API call  The Currency converter consist of the following files: main.py : main program used for entering the input parameters (currency codes) and display the results api.py : python script that will contain the code for calling API endpoints currency.py : python script that will contain the code for checking if currency code is valid, store results and format final output test_api.py : python script for testing code from api.py test_currency.py : python script for testing code from currency.py  ![image](https://github.com/gerardo5797/PythonCurrencyConverter/assets/88528474/44e9219f-5ee1-44af-871b-f7e659daaffa)   "
QR Code Generator,Python,https://github.com/lincolnloop/python-qrcode,"============================= Pure python QR Code generator =============================  Generate QR codes.  A standard install uses pypng_ to generate PNG files and can also render QR codes directly to the console. A standard install is just::      pip install qrcode  For more image functionality, install qrcode with the ``pil`` dependency so that pillow_ is installed and can be used for generating images::      pip install ""qrcode[pil]""  .. _pypng: https://pypi.python.org/pypi/pypng .. _pillow: https://pypi.python.org/pypi/Pillow   What is a QR Code? ==================  A Quick Response code is a two-dimensional pictographic code used for its fast readability and comparatively large storage capacity. The code consists of black modules arranged in a square pattern on a white background. The information encoded can be made up of any kind of data (e.g., binary, alphanumeric, or Kanji symbols)  Usage =====  From the command line, use the installed ``qr`` script::      qr ""Some text"" > test.png  Or in Python, use the ``make`` shortcut function:  .. code:: python      import qrcode     img = qrcode.make('Some data here')     type(img)  # qrcode.image.pil.PilImage     img.save(""some_file.png"")  Advanced Usage --------------  For more control, use the ``QRCode`` class. For example:  .. code:: python      import qrcode     qr = qrcode.QRCode(         version=1,         error_correction=qrcode.constants.ERROR_CORRECT_L,         box_size=10,         border=4,     )     qr.add_data('Some data')     qr.make(fit=True)      img = qr.make_image(fill_color=""black"", back_color=""white"")  The ``version`` parameter is an integer from 1 to 40 that controls the size of the QR Code (the smallest, version 1, is a 21x21 matrix). Set to ``None`` and use the ``fit`` parameter when making the code to determine this automatically.  ``fill_color`` and ``back_color`` can change the background and the painting color of the QR, when using the default image factory. Both parameters accept RGB color tuples.  .. code:: python       img = qr.make_image(back_color=(255, 195, 235), fill_color=(55, 95, 35))  The ``error_correction`` parameter controls the error correction used for the QR Code. The following four constants are made available on the ``qrcode`` package:  ``ERROR_CORRECT_L``     About 7% or less errors can be corrected. ``ERROR_CORRECT_M`` (default)     About 15% or less errors can be corrected. ``ERROR_CORRECT_Q``     About 25% or less errors can be corrected. ``ERROR_CORRECT_H``.     About 30% or less errors can be corrected.  The ``box_size`` parameter controls how many pixels each ""box"" of the QR code is.  The ``border`` parameter controls how many boxes thick the border should be (the default is 4, which is the minimum according to the specs).  Other image factories =====================  You can encode as SVG, or use a new pure Python image processor to encode to PNG images.  The Python examples below use the ``make`` shortcut. The same ``image_factory`` keyword argument is a valid option for the ``QRCode`` class for more advanced usage.  SVG ---  You can create the entire SVG or an SVG fragment. When building an entire SVG image, you can use the factory that combines as a path (recommended, and default for the script) or a factory that creates a simple set of rectangles.  From your command line::      qr --factory=svg-path ""Some text"" > test.svg     qr --factory=svg ""Some text"" > test.svg     qr --factory=svg-fragment ""Some text"" > test.svg  Or in Python:  .. code:: python      import qrcode     import qrcode.image.svg      if method == 'basic':         # Simple factory, just a set of rects.         factory = qrcode.image.svg.SvgImage     elif method == 'fragment':         # Fragment factory (also just a set of rects)         factory = qrcode.image.svg.SvgFragmentImage     else:         # Combined path factory, fixes white space that may occur when zooming         factory = qrcode.image.svg.SvgPathImage      img = qrcode.make('Some data here', image_factory=factory)  Two other related factories are available that work the same, but also fill the background of the SVG with white::      qrcode.image.svg.SvgFillImage     qrcode.image.svg.SvgPathFillImage  The ``QRCode.make_image()`` method forwards additional keyword arguments to the underlying ElementTree XML library. This helps to fine tune the root element of the resulting SVG:  .. code:: python      import qrcode     qr = qrcode.QRCode(image_factory=qrcode.image.svg.SvgPathImage)     qr.add_data('Some data')     qr.make(fit=True)      img = qr.make_image(attrib={'class': 'some-css-class'})  You can convert the SVG image into strings using the ``to_string()`` method. Additional keyword arguments are forwarded to ElementTrees ``tostring()``:  .. code:: python      img.to_string(encoding='unicode')   Pure Python PNG ---------------  If Pillow is not installed, the default image factory will be a pure Python PNG encoder that uses `pypng`.  You can use the factory explicitly from your command line::      qr --factory=png ""Some text"" > test.png  Or in Python:  .. code:: python      import qrcode     from qrcode.image.pure import PyPNGImage     img = qrcode.make('Some data here', image_factory=PyPNGImage)   Styled Image ------------  Works only with versions_ >=7.2 (SVG styled images require 7.4).  .. _versions: https://github.com/lincolnloop/python-qrcode/blob/master/CHANGES.rst#72-19-july-2021  To apply styles to the QRCode, use the ``StyledPilImage`` or one of the standard SVG_ image factories. These accept an optional ``module_drawer`` parameter to control the shape of the QR Code.  These QR Codes are not guaranteed to work with all readers, so do some experimentation and set the error correction to high (especially if embedding an image).  Other PIL module drawers:      .. image:: doc/module_drawers.png  For SVGs, use ``SvgSquareDrawer``, ``SvgCircleDrawer``, ``SvgPathSquareDrawer``, or ``SvgPathCircleDrawer``.  These all accept a ``size_ratio`` argument which allows for ""gapped"" squares or circles by reducing this less than the default of ``Decimal(1)``.   The ``StyledPilImage`` additionally accepts an optional ``color_mask`` parameter to change the colors of the QR Code, and an optional ``embeded_image_path`` to embed an image in the center of the code.  Other color masks:      .. image:: doc/color_masks.png  Here is a code example to draw a QR code with rounded corners, radial gradient and an embedded image:  .. code:: python      import qrcode     from qrcode.image.styledpil import StyledPilImage     from qrcode.image.styles.moduledrawers.pil import RoundedModuleDrawer     from qrcode.image.styles.colormasks import RadialGradiantColorMask      qr = qrcode.QRCode(error_correction=qrcode.constants.ERROR_CORRECT_H)     qr.add_data('Some data')      img_1 = qr.make_image(image_factory=StyledPilImage, module_drawer=RoundedModuleDrawer())     img_2 = qr.make_image(image_factory=StyledPilImage, color_mask=RadialGradiantColorMask())     img_3 = qr.make_image(image_factory=StyledPilImage, embeded_image_path=""/path/to/image.png"")  Examples ========  Get the text content from `print_ascii`:  .. code:: python      import io     import qrcode     qr = qrcode.QRCode()     qr.add_data(""Some text"")     f = io.StringIO()     qr.print_ascii(out=f)     f.seek(0)     print(f.read())  The `add_data` method will append data to the current QR object. To add new data by replacing previous content in the same object, first use clear method:  .. code:: python      import qrcode     qr = qrcode.QRCode()     qr.add_data('Some data')     img = qr.make_image()     qr.clear()     qr.add_data('New data')     other_img = qr.make_image()  Pipe ascii output to text file in command line::      qr --ascii ""Some data"" > ""test.txt""     cat test.txt  Alternative to piping output to file to avoid PowerShell issues::      # qr ""Some data"" > test.png     qr --output=test.png ""Some data"""
Cone Detection,Python,https://github.com/alexandsquirrel/cone-detection,"# Cone Detection for Autonomous Driving  Authors: Alex Fang, Gibbs Geng  ## Overview  This is the final project of CSE 455: Computer Vision. The goal of our project is to develop a perception pipeline for a formula racing car. Technically, out goal is to find the exact locations of every cone given the camera input. Such locations can be therefore converted to the 3D locations in the world and then be fed into the SLAM module in the formula racing car.  Our project involves two stages. First, we obtain the bounding boxes around all cones with machine learning. Then, for each cone, we compute its precise location within the bounding box using a traditional CV approach. While stage 2 should be performed for every bounding box drawn in stage 1, however, given our limitation of training data, we did not establish such a connection. Instead, we demonstrated the feasibility of stage 1 by training a neural network on a generic cone-detection dataset. For stage 2, on the other hand, we ran our algorithm on a few hand-crafted bounding boxes on the official racing cones.   ## Stage One: Bounding Box (mostly adapted from existing code with some modifications)  The approach is based on an [off-the-shelf implementation](https://github.com/zzh8829/yolov3-tf2) of YOLOv3. Then, we did transfer learning from [a set of pretrained weights](https://pjreddie.com/darknet/yolo/). We modified several hyperparameters as well as the prediction classes so that it can learn and predit well on our small [dataset](https://www.dropbox.com/s/fag8b45ijv14noy/cone_dataset.tar.gz?dl=0). The inferences results on both the training set and the validation set is available in `/detections` (yes, if we had more data, we would have a dedicated test set).  ## Stage Two: Finding Precise Locations (implemented by our own in `./utils.py`) At this stage, we aim to find the exact positions of the cone within each (loose) bounding box. We accomplished this with feature engineering. First, what is the characteristic feature of a racing cone? Our response to this question is the two sides and the band in the middle. Given a bounding box, if we know the two lines (not line segments) on which the two sidelines reside, plus the location of the middle trapezoid that is the band, since we have prior information about the size of the band, we will know about the exact location of this cone.  ### Noise Reduction & Edge Detection The racing cones are either yellow, blue or orange, which all have a rather stark contrast with the color of asphalt. Therefore, if we run an edge detector on the image, at least the two sidelines could be clearly marked. However, it turned out that a lot of noise are also marked, for example, the gravels on the ground. To reduce the noise, we pass the image through a bilaterial filter (a Gaussian filter will make later edge-detection impossible). Then, we run canny edge detection algorithm (provided in `cv2` library) on our image. It turns out that with the best set of parameters, the two sidelines are marked with some occasional discontinuity (due to the darkness of the environment). In addition, some curves on the base are also marked.  ### Corner Detection Given the detected edges as an image, we then aim to find which two straight lines are the real sidelines. We observed that given the nature of our cone, there is a very high chance that the sideline lies on the straight line formed by two corners, because on a sideline there are three obvious corners: the top and the two transitions in/out of the band. Given this observation, we run the Shi-Tomasi Corner Detector (`cv2.goodFeaturesToTrack`) on the image containing the edges to get a bunch of corners.  ### Sideline Extraction Given a bunch of corners, we want to get extract the two lines that form the sideline of the cone. Intuitively, since the two sidelines are the only two long line segments in the edge image, a ""good"" line would overlap greatly with those two line segments. For each pair of corners `(p1, p2)`, we ""score"" the line with high abs slope formed by the two points. Scoring a line involves a bunch of heuristics; the higher the score is, the more likely it is to contain the sideline.  - We first check the slope of the lines. Given the slidelines of the cone should always be large, we filter only the lines with high slope (the absolute value of the slope should be larger than 2). -  A desired line should overlap with the edges of the cone side. Therefore, we first score the line with the number of edges detected with in a threshold of the line -  A desired line should also cross several corner on the side. Therefore, we add the number of corner of the slide * corner_wight to the score    When calculating the score, we also grab the nearby corners of each line in order to form two horizontal lines of the trapezoid. Since the corners form the two horizontal lines should be in the middle of the two sideline, we only grab those corner from the middle 3/5 part of the line segment. We then use the highest score line with positive slope and the highest score line with the negative to form the two sidelines, as well as returning the set of interest corner to check the band latter.  ### Band Detection We follow a similar procedure to locate the upper and the lower edges of the band. Since the two endpoints of either edge is likely to be a corner (because it involves a shift in color) which is very close to the sideline, we can enumerate all pairs of corners `(c1, c2)` where `c1` is a corner on the left sideline (except for the top and the bottom one) and `c2` likewise. The socring heuristic is as follows: - The slope should be small relative to the orientation (or rotation) of the cone, because the band is horizontal. - The line should roughly overlap with the band edges. Since the band edge is a convex curve, we allow for a larger margin. - The ""amount"" of overlapping edges should be normalized by the length of this line segment, because a longer line segment will inevitably overlap more with the band edge than a shorter line segment which is just as good.  After scoring all candidate lines, we return the top two choices.   ## Future Work  - Connect the object detection stage with the later stage once the YOLO model can be trained on the real racing cone dataset. - Heavier feature engineering in finding the exact location. Or, we may explore machine learning method instead of traditional CV in this stage. - Converting the exact locations of the cones into 3D coordinates with respect to the vehicle position. This requires the specification (focal length) of the camera being used.  ## Usage  -  To train the network (make sure that `train.tfrecord` and `val.tfrecord` are in `/data`): ``` python train.py  ```  - To generate bounding boxes for an image (some of the flags may need to be modified): ``` python detect.py ```  - To draw the sidelines and the two band edges on an image with a single cone: ``` python script.py ``` (This part of code has not been cleaned up; `script.py` is only used to use the feasibility of the algorithms implemented in `utils.py`, which will be integrated more cleanly into the final pipeline.)"
Random quote generator,Python,https://github.com/Mahitej28/Random-Quote-Generator,"# Random Quote Generator   ## Description - `requests` library is used, to make HTTP requests in Python - `quote.py` script is used to generate random quotes by fetching them from public API endpoint  ## Setup instructions  - Install python from [here](https://www.python.org/.) - Install requests library using command : `pip install requests`  ## Detailed explanation of script, if needed  The function `generate_quote()` make GET request to the public API endpoint, extracts the data in json format and displays in randomly to the user. It handles error if any in the `else` condition.  ## Output  In text Format the output would be as follows : <br> Arnold Schwarzenegger - If you want to turn a vision into reality, you have to give 100% and never stop believing in your dream.   ## Author  [Mahima Churi](https://github.com/Mahitej28)  ## Disclaimers, if any  Any puplic API can be used for generating quotes, also datasets from Kaggle can also be imported and used."
Random quote generator,Python,https://github.com/Mahitej28/Random-Quote-Generator,"# Random Quote Generator   ## Description - `requests` library is used, to make HTTP requests in Python - `quote.py` script is used to generate random quotes by fetching them from public API endpoint  ## Setup instructions  - Install python from [here](https://www.python.org/.) - Install requests library using command : `pip install requests`  ## Detailed explanation of script, if needed  The function `generate_quote()` make GET request to the public API endpoint, extracts the data in json format and displays in randomly to the user. It handles error if any in the `else` condition.  ## Output  In text Format the output would be as follows : <br> Arnold Schwarzenegger - If you want to turn a vision into reality, you have to give 100% and never stop believing in your dream.   ## Author  [Mahima Churi](https://github.com/Mahitej28)  ## Disclaimers, if any  Any puplic API can be used for generating quotes, also datasets from Kaggle can also be imported and used."
Snake Water Gun game,Python,https://github.com/sagargoswami2001/Snake-Water-Gun,"# Snake Water Gun Game Using Python:- Snake Water Gun is one of the famous two-player game played by many people. It is a hand game in which the player randomly chooses any of the three forms i.e. snake, water, and gun. Here, we are going to implement this game using python.   This python project is to build a game for a single player that plays with the computer  ## Following are the rules of the game: **Snake vs. Water:** Snake drinks the water hence wins.  **Water vs. Gun:** The gun will drown in water, hence a point for water  **Gun vs. Snake:** Gun will kill the snake and win.  In situations where both players choose the same object, the result will be a draw.  ## Random module: The random module is a built-in module to generate the pseudo-random variables. It can be used perform some action randomly such as to get a random number, selecting a random elements from a list, shuffle elements randomly, etc.  We will use random.choice() method and nested if-else statements to select a random item from a list."
Snake Water Gun game,Python,https://github.com/sagargoswami2001/Snake-Water-Gun,"# Snake Water Gun Game Using Python:- Snake Water Gun is one of the famous two-player game played by many people. It is a hand game in which the player randomly chooses any of the three forms i.e. snake, water, and gun. Here, we are going to implement this game using python.   This python project is to build a game for a single player that plays with the computer  ## Following are the rules of the game: **Snake vs. Water:** Snake drinks the water hence wins.  **Water vs. Gun:** The gun will drown in water, hence a point for water  **Gun vs. Snake:** Gun will kill the snake and win.  In situations where both players choose the same object, the result will be a draw.  ## Random module: The random module is a built-in module to generate the pseudo-random variables. It can be used perform some action randomly such as to get a random number, selecting a random elements from a list, shuffle elements randomly, etc.  We will use random.choice() method and nested if-else statements to select a random item from a list."
URL shortener,Python,https://github.com/amitt001/pygmy,"<p align=""center""><img src=""pygmyui/static/logo/logov2.png"" alt=""pygmy"" height=""200px""></p>  <div align=""center"">   <h1>Pygmy</h1>  <!-- [![Build Status](https://travis-ci.org/amitt001/pygmy.svg?branch=master)](https://travis-ci.org/amitt001/pygmy) -->  [![Coverage Status](https://img.shields.io/coveralls/github/amitt001/pygmy.svg?color=yellowgreen)](https://coveralls.io/github/amitt001/pygmy?branch=master) ![PyPI - Python Version](https://img.shields.io/pypi/pyversions/Django.svg) [![PyPI license](https://img.shields.io/pypi/l/ansicolortags.svg)](https://github.com/amitt001/pygmy/blob/master/LICENSE) ![Docker Pulls](https://img.shields.io/docker/pulls/amit19/pygmy.svg)  Demo Version: [https://demo.pygy.co](https://demo.pygy.co)  Link stats(add **+** to the URL) example: [demo.pygy.co/pygmy+](https://demo.pygy.co/pygmy+)  Hackernews Thread: https://news.ycombinator.com/item?id=17690559 </div>  # Table of Contents - [Table of Contents](#table-of-contents) - [Features](#features) - [Technical Info](#technical-info) - [Setup](#setup)   - [Docker](#docker)   - [Manual(from source)](#manualfrom-source)   - [DB Setup:](#db-setup)     - [Use MySQL](#use-mysql)     - [Use Postgresql](#use-postgresql)     - [Use SQLite](#use-sqlite)   - [Docker](#docker-1)   - [Using Pygmy API](#using-pygmy-api)     - [Create User](#create-user)   - [Shell Usage](#shell-usage) - [Development](#development)   - [Run tests:](#run-tests)       - [Run tests with coverage report](#run-tests-with-coverage-report) - [Sponsorship](#sponsorship) - [License](#license)  Pygmy or `pygy.co` is an open-source, extensible & easy-to-use but powerful URL shortener. It's created keeping in mind that it should be easy to host and run your custom URL shortener without much effort. [Open-source Python URL shortener]  The architecture is very loosely coupled which allows custom integrations easily.  **The project has 3 major parts**  - The core URL shortening code - A REST API on top. Uses Flask framework - The UI layer for rendering the UI. It uses the Django framework  # Features  - URL shortener - Customized short URL's(ex: `pygy.co/pygmy`) - Support to create auto expiry URL after some time. - Secret key protected URL's - User Login/Sign up to track shortened URL's and link stats - User dashboard - Link Analytics(add + to the tiny URL to get link stats)  # Technical Info  - Python 3, Javascript, JQuery, HTML, CSS - REST API: Flask - Pygmyui: Django(It serves the web user-interface) - Supported DBs: PostgreSQL/MySQL/SQLite - Others: SQLAlchmey, JWT - Docker - Docker-compose  # Setup  ## Docker  1. In terminal run this command: `docker pull amit19/pygmy` 2. Then run the container: `docker run -it -p 8000:8000 amit19/pygmy` 3. Open http://localhost:8000 in your browser  ## Manual(from source)  1. Clone `git clone https://github.com/amitt001/pygmy.git & cd pygmy` 2. (Optional) Install virtualenv (optional but recommended)     - `virtualenv -p python3 env`     - `source env/bin/activate` 3. Install dependencies: `pip3 install -r requirements.txt` (if you are using MySQL or PostgreSQL check [DB setup](#db-setup) section) 4. `python run.py` (It runs Flask and Django servers using gunicorn) 5. Visit `127.0.0.1:8000` to use the app 6. Logs can be viewed at `pygmy/data/pygmy.log`  Note:   - **This module only supports Python 3. Make sure pip and virtualenv are both python 3 based versions.**(To install Python 3 on Mac: http://docs.python-guide.org/en/latest/starting/install3/osx/)  - The project has two config files:     - pygmy.cfg: `pygmy/config/pygmy.cfg` rest API and pygmy core settings file     - settings.py: `pygmyui/pygmyui/settings.py` Django settings file  - SQLite is default DB, if you are using PostgreSQL or MySQL with this project, make sure they are installed into the system.  - You can run pygmy shell also. Present in the root directory. To run the program on the terminal: `python shell`  - By default, DEBUG is set to True in `pygmyui/pygmyui/settings.py` file, set it to False in production.  ## DB Setup:  By default, Pygmy uses SQLite but any of the DB, SQLite, MySQL or PostgreSQL, can be used. Configs is present at `pygmy/config/pygmy.cfg`.  Use DB specific instruction below. Make sure to check and modify values in pygmy.cfg file according to your DB setup.  ### Use MySQL  1. Install pymysql: `pip install pymysql`  2. Check correct port: `mysqladmin variables | grep port`  3. Change below line in `pygmy/core/pygmy.cfg` file:  ``` [database] engine: mysql url: {engine}://{user}:{password}@{host}:{port}/{db_name} user: root password: root host: 127.0.0.1 port: 3306 db_name: pygmy ```  4. Enter MySQL URL `CREATE DATABASE pygmy;`  Note: It's better to use Mysql with version > `5.6.5` to use the default value of `CURRENT_TIMESTAMP` for `DATETIME`.  ### Use Postgresql  1. Change below line in `pygmy/core/pygmy.cfg` file:  ``` [database] engine: postgresql url: {engine}://{user}:{password}@{host}:{port}/{db_name} user: root password: root host: 127.0.0.1 port: 5432 db_name: pygmy ```  ### Use SQLite  > SQLite is natively supported in Python  1. Update `sqlite:////var/lib/pygmy/pygmy.db` file  ``` [database] engine: sqlite3 sqlite_data_dir: data sqlite_db_file_name: pygmy.db ```  ## Docker  Docker image name: `amit19/pygmy`. Docker image can be built by running `docker build -t amit19/pygmy .` command. Both the Dockerfile and docker-compose file are present at the root of the project. To use docker-compose you need to pass DB credentials in the docker-compose file.  ## Using Pygmy API  ### Create User      curl -XPOST http://127.0.0.1:9119/api/user/1 -H 'Content-Type: application/json' -d '{     ""email"": ""amit@gmail.com"",     ""f_name"": ""Amit"",     ""l_name"": ""Tripathi"",     ""password"": ""a_safe_one""     }'  ## Shell Usage  Open shell using ./shell. Available context in shell are: pygmy, Config, DB, etc. See all context by using pygmy_context.  Shorten a link:  ``` In [1]: shorten('http://iamit.xyz') Out[1]: {'created_at': '15 Nov, 2017 17:33:42',  'description': None,  'expire_after': None,  'hits_counter': 0,  'id': 'http://0.0.0.0:9119/api/link/5',  'is_custom': False,  'is_disabled': False,  'is_protected': False,  'long_url': 'http://iamit.xyz',  'owner': None,  'secret_key': '',  'short_code': 'f',  'short_url': 'http://pygy.co/f',  'updated_at': '2017-11-15T17:33:42.772520+00:00'}  In [2]: shorten('http://iamit.xyz', request=1) Out[2]: <pygmy.model.link.Link at 0x105ca1b70>  In [3]: unshorten('f') Out[3]: {'created_at': '15 Nov, 2017 17:33:42',  'description': None,  'expire_after': None,  'hits_counter': 0,  'id': 'http://0.0.0.0:9119/api/link/5',  'is_custom': False,  'is_disabled': False,  'is_protected': False,  'long_url': 'http://iamit.xyz',  'owner': None,  'secret_key': '',  'short_code': 'f',  'short_url': 'http://pygy.co/f',  'updated_at': '2017-11-15T17:33:42.772520+00:00'}  In [4]: link_stats('f') Out[4]: {'country_stats': 0,  'created_at': datetime.datetime(2017, 11, 15, 17, 33, 42, 772520),  'long_url': 'http://iamit.xyz',  'referrer': 0,  'short_code': 'f',  'time_series_base': None,  'time_stats': 0,  'total_hits': 0}  In [5]: # check the available context of the shell In [6]: pygmy_context  In [7]: # Create custom short URL  In [8]: shorten('http://iamit.xyz', short_code='amit') Out[8]: {'long_url': 'http://iamit.xyz',  'short_code': 'amit',  'short_url': 'http://pygy.co/amit'}  In [9]: shorten? Signature: shorten(long_url, short_code=None, expire_after=None, description=None, secret_key=None, owner=None, request=None) Docstring:     Helper class that has been delegated the task of inserting the     passed url in DB, base 62 encoding from DB id and return the short     URL value. ```  Q. How Link Stats Are Generated? > For getting geo location stats from IP maxminds' [GeoLite2-Country.mmd](http://demo.pygy.co/cm) database is used. It's in `pygmy/app` directory.  Q. How Pygmy Auth Token Works? > It uses JWT. When user logs in using username and password two tokens are generated, refresh token and auth token. Auth token is used for authentication with the Pygmy API. The refresh token can only be used to generate a new auth token. Auth token has a very short TTL but refresh token has a longer TTL. After 30 minutes. When a request comes with the old auth token and a new token is generated from the refresh token API. User passwords are encrypted by [bcrypt](https://en.wikipedia.org/wiki/Bcrypt) hash algorithm.  # Development  If you find any bug, have a question or a general feature request. Open an issue on the 'Issue' page.  To contribute to the project:  1. Clone the repo and make changes 2. Build the code: `docker build pygmy` 3. Test the changer by running: `docker run -it -p 8000:8000 pygmy` 4. The website will be available at http://127.0.0.1:8000/  ## Run tests:  1. Install pytest (if not already installed): `pip install pytest` 2. In root directory run command: `py.test`  #### Run tests with coverage report  1. Install coverage `pip install coverage` 2. Run command: `coverage run --omit=""*/templates*,*/venv*,*/tests*"" -m py.test` 3. See coverage report(Coverage numbers are low as the coverage for integration tests is not generated): `coverage report`  # Sponsorship The demo version of this website is made possible due to the generous sponsorship of DigitalOcean  <img src=""https://github.com/amitt001/pygmy/assets/7390944/e8df3143-d9a5-4582-8e73-6f1c1822d046"" width=""200"" />  # License  MIT License  Copyright (c) 2022 Amit Tripathi(https://twitter.com/amitt019)  Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.  THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.  [Read License Terms](https://github.com/amitt001/pygmy/blob/master/LICENSE)"
Youtube Video Downloader,Python,https://github.com/pytube/pytube,"<div align=""center"">   <p>     <a href=""#""><img src=""https://assets.nickficano.com/gh-pytube.min.svg"" width=""456"" height=""143"" alt=""pytube logo"" /></a>   </p>   <p align=""center""> 	<a href=""https://pypi.org/project/pytube/""><img src=""https://img.shields.io/pypi/dm/pytube?style=flat-square"" alt=""pypi""/></a> 	<a href=""https://pytube.io/en/latest/""><img src=""https://readthedocs.org/projects/python-pytube/badge/?version=latest&style=flat-square"" /></a> 	<a href=""https://pypi.org/project/pytube/""><img src=""https://img.shields.io/pypi/v/pytube?style=flat-square"" /></a>   </p> </div>  ### Actively soliciting contributors!  Have ideas for how pytube can be improved? Feel free to open an issue or a pull request!  # pytube  *pytube* is a genuine, lightweight, dependency-free Python library (and command-line utility) for downloading YouTube videos.  ## Documentation  Detailed documentation about the usage of the library can be found at [pytube.io](https://pytube.io). This is recommended for most cases. If you want to hastily download a single video, the [quick start](#Quickstart) guide below might be what you're looking for.  ## Description  YouTube is the most popular video-sharing platform in the world and as a hacker, you may encounter a situation where you want to script something to download videos. For this, I present to you: *pytube*.  *pytube* is a lightweight library written in Python. It has no third-party dependencies and aims to be highly reliable.  *pytube* also makes pipelining easy, allowing you to specify callback functions for different download events, such as  ``on progress`` or ``on complete``.  Furthermore, *pytube* includes a command-line utility, allowing you to download videos right from the terminal.  ## Features  - Support for both progressive & DASH streams - Support for downloading the complete playlist - Easily register ``on_download_progress`` & ``on_download_complete`` callbacks - Command-line interfaced included - Caption track support - Outputs caption tracks to .srt format (SubRip Subtitle) - Ability to capture thumbnail URL - Extensively documented source code - No third-party dependencies  ## Quickstart  This guide covers the most basic usage of the library. For more detailed information, please refer to [pytube.io](https://pytube.io).  ### Installation  Pytube requires an installation of Python 3.6 or greater, as well as pip. (Pip is typically bundled with Python [installations](https://python.org/downloads).)  To install from PyPI with pip:  ```bash $ python -m pip install pytube ```  Sometimes, the PyPI release becomes slightly outdated. To install from the source with pip:  ```bash $ python -m pip install git+https://github.com/pytube/pytube ```  ### Using pytube in a Python script  To download a video using the library in a script, you'll need to import the YouTube class from the library and pass an argument of the video URL. From there, you can access the streams and download them.  ```python  >>> from pytube import YouTube  >>> YouTube('https://youtu.be/2lAe1cqCOXo').streams.first().download()  >>> yt = YouTube('http://youtube.com/watch?v=2lAe1cqCOXo')  >>> yt.streams   ... .filter(progressive=True, file_extension='mp4')   ... .order_by('resolution')   ... .desc()   ... .first()   ... .download() ```  ### Using the command-line interface  Using the CLI is remarkably straightforward as well. To download a video at the highest progressive quality, you can use the following command: ```bash $ pytube https://youtube.com/watch?v=2lAe1cqCOXo ```  You can also do the same for a playlist: ```bash $ pytube https://www.youtube.com/playlist?list=PLS1QulWo1RIaJECMeUT4LFwJ-ghgoSH6n ```"
Log Analyser,Python,https://github.com/yurilaaziz/log-analyzer,"[![Latest version on](https://badge.fury.io/py/log-analyzer.svg)](https://badge.fury.io/py/log-analyzer) [![Supported Python versions](https://img.shields.io/pypi/pyversions/log-analyzer.svg)](https://pypi.org/project/log-analyzer/) [![Travis Pipelines build status](https://img.shields.io/travis/com/yurilaaziz/log-analyzer.svg)](https://travis-ci.com/yurilaaziz/log-analyzer/) [![codecov](https://codecov.io/gh/yurilaaziz/log-analyzer/branch/master/graph/badge.svg)](https://codecov.io/gh/yurilaaziz/log-analyzer)  # Description Log-analyzer is a human and straightforward log analyzer for rapid troubleshooting.  *Note*:  This project starts to be an interview project. Now I am using it to monitor my website traffic.  # Get started   ## Install from github  ``` pip install git+http://github.com/yurilaaziz/log-analyzer.git  ```  ## Install from PyPi  ``` pip install log-analyzer  ```  ## Run log analyzer ````commandline log-analyzer  ```` By default, the log-analyzer reads the log from /var/log/access.log  ![screen shot log-analyzer](artwork/screenshot.png)   ## Run log analyzer with external configuration ````commandline LOGANALYZER_CONFIG_FILE=sample.config.yml log-analyzer  ````  ## Run log analyzer with another log file ````commandline LOGANALYZER_PARSER_INPUT=/tmp/test.log log-analyzer  ````  ## Design  ![Design log-analyzer](artwork/design.png)   Log Analyzer containers on three components :  * Parser Process :     * Reads the log file     * Process lines following a pattern class    * Push data to the Persistence driver * Consumer Process (Console Display):    * Reads data from the persistence driver    * Display an array ordered by hits  * Alert Manager:    * Compute rules defined in the pattern alert's class    * Push notification to persistence driver to be displayed on the console        * Persistence Driver:    * Define interfaces between producer and consumer.    * Allow persisting data to different data-store   ## Configuration   Log Analyzer uses Config42 to manage its configuration. that means every variable present in the configuration file could be overloaded from  Environment variables, Configuration file, external data store (ETCD)  Here an example of default configuration in YAML format: [configuration file](sample.config.yml)   ## TO DO  - [x] Rework the Alert Manager process to Delete Alert Class and read Alerting rules from the configuration  - [ ] Rework the __main__ for a better CLI with 'docopt'  - [ ] Write a persistence driver to support ElasticSearch/InfluxDB data-store - [ ] Support multiple log files/sources - [ ] Support multiple Persistence drivers       "
PDF To DOCX Converter,Python,https://github.com/ArtifexSoftware/pdf2docx,"English | [‰∏≠Êñá](README_CN.md)  # pdf2docx   ![python-version](https://img.shields.io/badge/python->=3.6-green.svg) [![codecov](https://codecov.io/gh/dothinking/pdf2docx/branch/master/graph/badge.svg)](https://codecov.io/gh/dothinking/pdf2docx) [![pypi-version](https://img.shields.io/pypi/v/pdf2docx.svg)](https://pypi.python.org/pypi/pdf2docx/) ![license](https://img.shields.io/pypi/l/pdf2docx.svg) ![pypi-downloads](https://img.shields.io/pypi/dm/pdf2docx)  - Extract data from PDF with `PyMuPDF`, e.g. text, images and drawings  - Parse layout with rule, e.g. sections, paragraphs, images and tables - Generate docx with `python-docx`  ## Features  - Parse and re-create page layout     - page margin     - section and column (1 or 2 columns only)     - page header and footer [TODO]  - Parse and re-create paragraph     - OCR text [TODO]     - text in horizontal/vertical direction: from left to right, from bottom to top     - font style, e.g. font name, size, weight, italic and color     - text format, e.g. highlight, underline, strike-through     - list style [TODO]     - external hyper link     - paragraph horizontal alignment (left/right/center/justify) and vertical spacing      - Parse and re-create image 	- in-line image     - image in Gray/RGB/CMYK mode     - transparent image     - floating image, i.e. picture behind text  - Parse and re-create table     - border style, e.g. width, color     - shading style, i.e. background color     - merged cells     - vertical direction cell     - table with partly hidden borders     - nested tables  - Parsing pages with multi-processing  *It can also be used as a tool to extract table contents since both table content and format/style is parsed.*  ## Limitations  - Text-based PDF file - Left to right language - Normal reading direction, no word transformation / rotation - Rule-based method can't 100% convert the PDF layout   ## Documentation  - [Installation](https://pdf2docx.readthedocs.io/en/latest/installation.html) - [Quickstart](https://pdf2docx.readthedocs.io/en/latest/quickstart.html)     - [Convert PDF](https://pdf2docx.readthedocs.io/en/latest/quickstart.convert.html)     - [Extract table](https://pdf2docx.readthedocs.io/en/latest/quickstart.table.html)     - [Command Line Interface](https://pdf2docx.readthedocs.io/en/latest/quickstart.cli.html)     - [Graphic User Interface](https://pdf2docx.readthedocs.io/en/latest/quickstart.gui.html) - [Technical Documentation (In Chinese)](https://pdf2docx.readthedocs.io/en/latest/techdoc.html) - [API Documentation](https://pdf2docx.readthedocs.io/en/latest/modules.html)  ## Sample  ![sample_compare.png](https://s1.ax1x.com/2020/08/04/aDryx1.png)"
AI Resume Analyzer,Python,https://github.com/deepakpadhi986/AI-Resume-Analyzer,"<p><small>Best View in <a href=""https://github.com/settings/appearance"">Light Mode</a> and Desktop Site (Recommended)</small></p><br/>  ![AI-Resume-Analyzer](https://socialify.git.ci/deepakpadhi986/AI-Resume-Analyzer/image?description=1&descriptionEditable=5th%20Sem%20Final%20Year%20Project%20at%20Kirti%20M%20Doongursee%20College%20(2022%20-%2023)&font=Raleway&language=1&pattern=Plus&theme=Light)  <div align=""center"">   <h1>üå¥ AI RESUME ANALYZER üå¥</h1>   <p>A Tool for Resume Analysis, Predictions and Recommendations</p>   <!-- Badges -->   <p>     <img src=""https://img.shields.io/github/last-commit/deepakpadhi986/AI-Resume-Analyzer"" alt=""last update"" />     <img src=""https://badges.frapsoft.com/os/v2/open-source.svg?v=103"" alt=""open source"" />     <img src=""https://img.shields.io/github/languages/top/deepakpadhi986/AI-Resume-Analyzer?color=red"" alt=""language"" />     <img src=""https://img.shields.io/github/languages/code-size/deepakpadhi986/AI-Resume-Analyzer?color=informational"" alt=""code size"" />     <a href=""https://github.com/deepakpadhi986/AI-Resume-Analyzer/blob/main/LICENSE"">       <img src=""https://img.shields.io/github/license/deepakpadhi986/AI-Resume-Analyzer.svg?color=yellow"" alt=""license"" />     </a>   </p>      <!--links-->   <h4>     <a href=""#preview-"">View Demo</a>     <span> ¬∑ </span>     <a href=""#setup--installation-"">Installation</a>     <span> ¬∑ </span>     <a href=""mailto:dnoobnerd@gmail.com?subject=I%20Want%20The%20Project%20Report%20of%20AI-RESUME-ANALYZER%20(2022%20 %2023)&body=Here%20Are%20My%20Details%20%F0%9F%98%89%0D%0A%0D%0AOrganization%2FCollege%20Name%3A%20%0D%0A%0D%0AFull%20Name%3A%20%0D%0A%0D%0AGitHub%20Profile%20%3A%20%0D%0A%0D%0AFrom%20where%20did%20you%20get%20to%20know%20about%20this%20project%3A%0D%0A%0D%0APurpose%20of%20asking%20project%20report%20(describe)%3A%0D%0A%0D%0A%0D%0AIf%20the%20above%20information%20satisfy%20your%20identity%20you%20will%20get%20the%20report%20to%20your%20email."">Project Report</a>   </h4>   <p>     <small align=""justify"">       Built with ü§ç by        <a href=""https://dnoobnerd.netlify.app/"">Deepak Padhi</a> through        <a href=""https://www.linkedin.com/in/mrbriit/"">Dr Bright --(Data Scientist)</a>      </small>   </p>   <small align=""justify"">üöÄ A Project Submitted for the partial fulfilment of the degree B.sc CS at      <a href=""https://kirticollege.edu.in/"">Kirti College</a> during academic year 2022-23   </small> </div><br/><br/>  ## About the Project ü•± <div align=""center"">     <br/><img src=""https://raw.githubusercontent.com/deepakpadhi986/AI-Resume-Analyzer/main/screenshots/RESUME.png"" alt=""screenshot"" /><br/><br/>     <p align=""justify"">        A tool which parses information from a resume using natural language processing and finds the keywords, cluster them onto sectors based on their keywords.        And lastly show recommendations, predictions, analytics to the applicant / recruiter based on keyword matching.     </p> </div>  ## Scope üò≤ i. It can be used for getting all the resume data into a structured tabular format and csv as well, so that the organization can use those data for analytics purposes  ii. By providing recommendations, predictions and overall score user can improve their resume and can keep on testing it on our tool  iii. And it can increase more traffic to our tool because of user section  iv. It can be used by colleges to get insight of students and their resume before placements  v. Also, to get analytics for roles which users are mostly looking for  vi. To improve this tool by getting feedbacks  <!-- TechStack --> ## Tech Stack üçª <details>   <summary>Frontend</summary>   <ul>     <li><a href=""https://streamlit.io/"">Streamlit</a></li>     <li><a href=""https://developer.mozilla.org/en-US/docs/Learn/HTML"">HTML</a></li>     <li><a href=""https://developer.mozilla.org/en-US/docs/Web/CSS"">CSS</a></li>     <li><a href=""https://developer.mozilla.org/en-US/docs/Learn/JavaScript"">JavaScript</a></li>   </ul> </details>  <details>   <summary>Backend</summary>   <ul>     <li><a href=""https://streamlit.io/"">Streamlit</a></li>     <li><a href=""https://www.python.org/"">Python</a></li>   </ul> </details>  <details> <summary>Database</summary>   <ul>     <li><a href=""https://www.mysql.com/"">MySQL</a></li>   </ul> </details>  <details> <summary>Modules</summary>   <ul>     <li><a href=""https://pandas.pydata.org/"">pandas</a></li>     <li><a href=""https://github.com/OmkarPathak/pyresparser"">pyresparser</a></li>     <li><a href=""https://pypi.org/project/pdfminer3/"">pdfminer3</a></li>     <li><a href=""https://plotly.com/"">Plotly</a></li>     <li><a href=""https://www.nltk.org/"">NLTK</a></li>   </ul> </details>  <!-- Features --> ## Features ü§¶‚Äç‚ôÇÔ∏è ### Client: - - Fetching Location and Miscellaneous Data    Using Parsing Techniques to fetch - Basic Info - Skills - Keywords  Using logical programs, it will recommend - Skills that can be added - Predicted job role - Course and certificates - Resume tips and ideas - Overall Score - Interview & Resume tip videos  ### Admin: - - Get all applicant‚Äôs data into tabular format - Download user‚Äôs data into csv file - View all saved uploaded pdf in Uploaded Resume folder - Get user feedback and ratings      Pie Charts for: - - Ratings - Predicted field / roles - Experience level - Resume score - User count - City - State - Country  ### Feedback: - - Form filling - Rating from 1 ‚Äì 5 - Show overall ratings pie chart - Past user comments history   ## Requirements üòÖ ### Have these things installed to make your process smooth  1) Python (3.9.12) https://www.python.org/downloads/release/python-3912/ 2) MySQL https://www.mysql.com/downloads/ 3) Visual Studio Code **(Prefered Code Editor)** https://code.visualstudio.com/Download 4) Visual Studio build tools for C++ https://aka.ms/vs/17/release/vs_BuildTools.exe  ## Setup & Installation üëÄ  To run this project, perform the following tasks üò®  Download the code file manually or via git ```bash git clone https://github.com/deepakpadhi986/AI-Resume-Analyzer.git ```  Create a virtual environment and activate it **(recommended)**  Open your command prompt and change your project directory to ```AI-Resume-Analyzer``` and run the following command  ```bash python -m venv venvapp  cd venvapp/Scripts  activate  ```  Downloading packages from ```requirements.txt``` inside ``App`` folder ```bash cd../..  cd App  pip install -r requirements.txt  python -m spacy download en_core_web_sm  ```  After installation is finished create a Database ```cv```  And change user credentials inside ```App.py``` https://github.com/deepakpadhi986/AI-Resume-Analyzer/blob/17e1cdb207fef62557dc394f4158bda515e541fd/App/App.py#L95  Go to ```venvapp\Lib\site-packages\pyresparser``` folder  And replace the ```resume_parser.py``` with ```resume_parser.py```   which was provided by me inside ```pyresparser``` folder  ``Congratulations ü•≥üò± your set-up üëÜ and installation is finished üòµü§Ø``  I hope that your ``venvapp`` is activated and working directory is inside ``App``  Run the ```App.py``` file using ```bash streamlit run App.py  ```  ## Known Error ü§™ If ``GeocoderUnavailable`` error comes up then just check your internet connection and network speed  ## Issue While Installation and Set-up ü§ß Check-out installation [Video](https://youtu.be/WFruijLC1Nc)  Feel Free to <a href=""mailto:dnoobnerd@gmail.com?subject=I%20have%20an%20issue%20while%20setup%2Finstalling%20of%20AI%20RESUME%20ANALYZER&body=Name%3A%20-%0D%0A%0D%0ADesignation%3A%20-%0D%0A%0D%0APlease%20describe%20your%20problem%20in%20brief%20with%20attached%20photos%20of%20error"">Send mail</a>  ## Usage - After the setup it will do stuff's automatically - You just need to upload a resume and see it's magic - Try first with my resume uploaded in ``Uploaded_Resumes`` folder - Admin userid is ``admin`` and password is ``admin@resume-analyzer``  <!-- Roadmap --> ## Roadmap üõµ * [x] Predict user experience level. * [x] Add resume scoring criteria for skills and projects. * [x] Added fields and recommendations for web, android, ios, data science. * [ ] Add more fields for other roles, and its recommendations respectively.  * [x] Fetch more details from users resume. * [ ] View individual user details.  ## Contributing ü§ò Pull requests are welcome.   For major changes, please open an issue first to discuss what you would like to change.  I've attached the <a href=""https://github.com/deepakpadhi986/AI-Resume-Analyzer/blob/main/RESUME%20ANALYSER%20SYNOPSIS.pdf"">synopsis</a> of the project  If you want the full report of project <a href=""mailto:dnoobnerd@gmail.com?subject=I%20Want%20The%20Project%20Report%20of%20AI-RESUME-ANALYZER%20(2022%20-%2023)&body=Here%20Are%20My%20Details%20%F0%9F%98%89%0D%0A%0D%0AOrganization%2FCollege%20Name%3A%20%0D%0A%0D%0AFull%20Name%3A%20%0D%0A%0D%0AGitHub%20Profile%20%3A%20%0D%0A%0D%0AFrom%20where%20did%20you%20get%20to%20know%20about%20this%20project%3A%0D%0A%0D%0APurpose%20of%20asking%20project%20report%20(describe)%3A%0D%0A%0D%0A%0D%0AIf%20the%20above%20information%20satisfy%20your%20identity%20you%20will%20get%20the%20report%20to%20your%20email."">Email Me</a> ``it's FREE``  ## Acknowledgement ü§ó - <a href=""https://www.linkedin.com/in/mrbriit/"">Dr Bright</a> - <a href=""https://www.udemy.com/course/the-full-stack-data-scientist-bootcamp/"">(The Full Stack Data Scientist BootCamp)</a> - <a href=""https://www.academia.edu/32543544/Resume_Parser_with_Natural_Language_Processing"">Resume Parser with Natural Language Processing</a> - <a href=""https://github.com/OmkarPathak/pyresparser"">pyresparser</a>  ## Preview üëΩ  ### Client Side  **Main Screen**  ![Screenshot](https://github.com/deepakpadhi986/AI-Resume-Analyzer/blob/main/screenshots/user/1-main-screen.png?raw=true)  **Resume Analysis**  ![Screenshot](https://github.com/deepakpadhi986/AI-Resume-Analyzer/blob/main/screenshots/user/2-analysis.jpg?raw=true)  **Skill Recommendation**  ![Screenshot](https://github.com/deepakpadhi986/AI-Resume-Analyzer/blob/main/screenshots/user/3-recom.png?raw=true)  **Course Recommendation**  ![Screenshot](https://github.com/deepakpadhi986/AI-Resume-Analyzer/blob/main/screenshots/user/4-recom.png?raw=true)  **Tips and Overall Score**  ![Screenshot](https://github.com/deepakpadhi986/AI-Resume-Analyzer/blob/main/screenshots/user/5-tipsscore.png?raw=true)  **Video Recommendation**  ![Screenshot](https://github.com/deepakpadhi986/AI-Resume-Analyzer/blob/main/screenshots/user/6-recom.png?raw=true)  ### Feedback  **Feedback Form**  ![Screenshot](https://github.com/deepakpadhi986/AI-Resume-Analyzer/blob/main/screenshots/feedback/1-form.png?raw=true)  **Overall Rating Analysis and Comment History**  ![Screenshot](https://github.com/deepakpadhi986/AI-Resume-Analyzer/blob/main/screenshots/feedback/2-analytics.png?raw=true)  ### Admin  **Login**  ![Screenshot](https://github.com/deepakpadhi986/AI-Resume-Analyzer/blob/main/screenshots/admin/1-main-screen.png?raw=true)  **User Count and it's data**  ![Screenshot](https://github.com/deepakpadhi986/AI-Resume-Analyzer/blob/main/screenshots/admin/2-user-data.png?raw=true)  **Exported csv file**  ![Screenshot](https://github.com/deepakpadhi986/AI-Resume-Analyzer/blob/main/screenshots/admin/3-user-datacsv.png?raw=true)  **Feedback Data**  ![Screenshot](https://github.com/deepakpadhi986/AI-Resume-Analyzer/blob/main/screenshots/admin/4-feed-data.png?raw=true)  **Pie Chart Analytical Representation of clusters**  ![Screenshot](https://github.com/deepakpadhi986/AI-Resume-Analyzer/blob/main/screenshots/admin/5-pieexp.png?raw=true)  ![Screenshot](https://github.com/deepakpadhi986/AI-Resume-Analyzer/blob/main/screenshots/admin/6-piescre.jpg?raw=true)  ![Screenshot](https://github.com/deepakpadhi986/AI-Resume-Analyzer/blob/main/screenshots/admin/7-pielocation.png?raw=true)  ### Built with ü§ç AI RESUME ANALYZER by <a href=""https://dnoobnerd.netlify.app/"">Deepak Padhi</a>"
Hotel Management System,Python,https://github.com/tushar-2223/Hotel-Management-System,"# Hotel-Management-System  Simple hotel booking website with content management system. Users can book rooms for specific dates. Admin can create, update, and delete room details. Admin can manage everything in the app.  ## Video <!--<a href=""https://www.youtube.com/watch?v=rKwBxxVXWkM"">click here..</a>--> https://github.com/tushar-2223/Hotel-Management-System/assets/87109400/08742fd7-5e7b-4459-90ef-c4e6b3e0cabd  ## Tech Stack   ```sh HTML CSS JAVASCRIPT PHP BOOTSTRAP  ```  ## Requirements Windows:  ```sh 1 Download & Install: XAMPP in C:\xampp (default) 2 Clone this repository in C:\xampp\htdocs 3 Run XAMPP and start ""Apache"" and ""MySQL"" 4 Open the link ""localhost/phpmyadmin/"" 5 Click on new at sidebar and create a database name ""bluebirdhotel"" After clicking database click import and select the file ""bluebirdhotel.sql"" Open the link ""http://localhost/Hotel-Management-System/"" Now register and login ```  ## Requirements Linux[Rocky Linux 9]:  ```sh 1 Install dnf package manager 2 Clone this repository in your home directory 3 Enable execute permissions on setup.sh `chmod 755 setup.sh` 4 Login as root or use `sudo su - root` 5 Run setup.sh `./setup.sh` Open the link ""http://localhost/Hotel-Management-System/"" Now register and login ```   ### login page  ```sh == Staff Login ==  Email : Admin@gmail.com Password : 1234 ```"
Facial Recognition Attendance System,Python,https://github.com/Vatshayan/Face-recognition-Attendance-System-Project,"# Face-recognition-Attendance-System-Project Final Year Face recognition Attendance System Project   ### Youtube Implementation Video : https://youtu.be/tLhFaAurhGw   ![FACE DETECTION](https://user-images.githubusercontent.com/28294942/166667109-d2024d8c-9aec-44ed-93f8-8f1d9b66098a.png)   ### Abstract   The management of the attendance can be a great burden on the teachers if it is done by hand. To resolve this problem, smart and auto attendance management system is being utilized. By utilizing this framework, the problem of proxies and students being marked present even though they are not physically present can easily be solved. This system marks the attendance using live video stream. The frames are extracted from video using OpenCV. The main implementation steps used in this type of system are face detection and recognizing the detected face, for which dlib is used. After these, the connection of recognized faces ought to be conceivable by comparing with the database containing student's faces. This model will be a successful technique to manage the attendance of students.  Live Webcam based Face Attendance System Project through python programming  ### Details :  Smart Attendance Management System is an application developed for daily student attendance in colleges or. schools. This project attempts to record attendance through face detection.  This System uses facial recognition technology to record the attendance through a high resolution digital camera/webcam that detects and recognizes faces and compare the recognize faces with students/known faces images stored in faces database(CSV).  ### Reference Base Research Paper : https://ieeexplore.ieee.org/document/9215441  ### Need Code, Documents & Explanation ?   ## How to Reach me :  ### Mail : vatshayan007@gmail.com   ### WhatsApp: **+91 9310631437** (Helping 24*7) **[LINK](https://wa.me/message/CHWN2AHCPMAZK1)**   ### Website : https://www.finalproject.in/  ### 1000 Computer Science Projects : https://www.computer-science-project.in/  ### Youtube Implementation Video : https://youtu.be/tLhFaAurhGw  IMP : If you are getting error/problems/queires then Reach me will help you"
Find Missing Person using AI,Python,https://github.com/gaganmanku96/Finding-missing-person-using-AI,"# Find Missing Person using AI ![Issues](https://img.shields.io/github/issues/gaganmanku96/Finding-missing-person-using-AI) ![Stars](https://img.shields.io/github/stars/gaganmanku96/Finding-missing-person-using-AI?style=social)  # [Web App Version of this Project](https://github.com/gaganmanku96/Finding-missing-person-using-AI/blob/master/more_projects.md)  ## [Youtube Link](https://www.youtube.com/channel/UC7ln87o0Gt8OkkHIqEmeDQw/videos)  Hundreds of people (especially children go missing every day) in India. There are various <b>NGO's and Govt Initiatives</b> to help with it. This project tries to implement an  existing/new way to help.  ## List of contents - ### [News Articles](#news-articles) - ### [Objective](#what-is-the-objective-of-this-project-and-how-will-it-help) - ### [Solution](#solution-projects-implementation) - ### [Installation](#how-to-run) - ### [What is left/not working?](#what-is-left)  ## News Articles #### [Article 1](https://www.thehindu.com/society/indias-missing-children-what-the-whatsapp-rumours-dont-tell-you/article24641527.ece) ![News Article 1](resources/news_1.PNG) #### [Article 2](https://www.deccanchronicle.com/nation/current-affairs/250518/indias-children-174-go-missing-every-day-half-untraced.html) ![News Article 2](resources/news_2.PNG)   ## What is the objective of this Project and how will it help? The objective of this project is to help Police and higher authorities to track down missing people quickly. The usual process to track a person is using investigation which requires time and experience (to ask right questions). Most of the time, investigation method works pretty well but it is time consuming and can be unsuccessful if the person (missing) has been shifted/moved to different location (city/country).<br> In such cases, the ideal approach is to go through CCTV footages and evidences. Again, this can be very time consuming and given the number of people that go missing everyday, it can be a challanage to keep up with it.<br>  ## Solution (Project's Implementation) ### 1. Registering New Cases The first step is to register a new case. The GUI application is built using <b>PyQT5</b> that allows you to collect all relevant information and store it in database <b>Postgres</b>. > Please ignore the SRK's image. It is just for the sake of project :)  ![New Case Window](resources/new_case.PNG)  ### 2. Waiting for Users to submit images So far we have only talked about 'how new cases will be registered', the next thing we have to do is to match these registered cases but who do we match it with? This is where ours Users come in. These users are common people like you and me who wants to make a change in the society.<br> The common people will use an application on their mobile to submit photos of people who they think have lost or found begging while keeping them their identity anonymous. The anonymous part is very important because they fear of local <i>Gundas</i> that might create trouble for them.<br> > Mobile Application ![Mobile Application](resources/mobile_application.PNG)  > An android Application can also be build and used but I have very little experience in it. ### 3. Matching Cases The next step is to match the case images and user submitted images. To match <b>KNN Algorithm </b> is used. ![Main Application](resources/app_window.PNG)  ## How to run #### 1. With Docker (Easy) Prerequisites ``` Docker (docker-compose as well) ``` ``` $ git clone https://github.com/gaganmanku96/Finding-missing-person-using-AI $ cd Finding-missing-person-using-AI $ docker-compose up --build $ cd app $ pip install -r requirements.txt --no-cache-dir $ python login_window.py ``` At this point you'll see a window like this ![Login Window](resources/login_screen.PNG)  > Default username: admin > Default password: admin >  > Login window makes sure that only authenticated can view the registered cases. Each user can only view the cases submitted by him. > NOTE: There is no concpet of superuser.  After logging in you'll see the main screen through which you'll be able to submit cases.  ##### To run the mobile application: ``` $ cd mobile_app ``` or (if you are inside app dir)  ``` $ ../mobile_app ``` ``` $ python ui.py ``` After that you'll see a window like this<br> ![mobile application](resources/mobile_application.PNG)  You can this to submit user images or you can create your own mobile app.  Once done you'll have to <b>Click on Refresh</b> button on train KNN Model and then on <b>Match</b> to start Matching Images.  #### 2. Without Docker (Intermediate) Here are the step you have to do 1. Install Postgres Database and replace the username and password in config/.env.file 2. The next step is to run database and make sure it is working. ``` $ cd database $ pip install -r requirements.txt $ uvicorn main:app --port 8002 ``` 3. Next, the face encoding api ``` $ cd face_encoding $ pip install -r requirements.txt $ uvicorn main:app --port 8000 ``` > If you are using non-conda environment like venv then it might give error while installing dlib library. 4. Running  the application ``` $ cd app $ pip install -r requirements.txt $ python login.py ``` At this point you'll see a window like this ![Login Window](resources/login_screen.PNG)  > Default username: admin > Default password: admin >  > Login window makes sure that only authenticated can view the registered cases. Each user can only view the cases submitted by him. > NOTE: There is no concpet of superuser.  After logging in you'll see the main screen through which you'll be able to submit cases.  ##### To run the mobile application: ``` $ cd mobile_app ``` or (if you are inside app dir)  ``` $ ../mobile_app ``` ``` $ python ui.py ``` After that you'll see a window like this<br> ![mobile application](resources/mobile_application.PNG)  You can this to submit user images or you can create your own mobile app.  Once done you'll have to <b>Click on Refresh</b> button on train KNN Model and then on <b>Match</b> to start Matching Images.   ## What is left?  - [x] Login (Authentication)  - [x] Submit new case  - [x] Mobile Application (to submit user photos)  - [ ] View submitted cases  - [ ] View confirmed cases  - [ ] Unit tests    ## Developer: ## <a href=""https://www.linkedin.com/in/gaganmanku96/"">Gagandeep Singh</a> ## Endorse me at LinkedIn if this project was helpful. [![Linkedin](https://i.stack.imgur.com/gVE0j.png) LinkedIn](https://www.linkedin.com/in/gaganmanku96/)   ## Vote of Thanks - Thanks to [Davis King](https://github.com/davisking) for creating dlib and for providing the trained facial feature   detection and face encoding models used in this project."
Recipe Generation from Food Image,Python,https://github.com/navassherif98/Recipe-Generation-from-Food-Image,"![ViewCount](https://views.whatilearened.today/views/github/navassherif98/Recipe-Generation-from-Food-Image.svg?cache=remove)  # Recipe-Generation-from-Food-Image  ### Demo: ![Recipe_Generation gif](https://user-images.githubusercontent.com/55757415/124395585-8d0d0780-dd22-11eb-86fe-3a23d921b608.gif)  Are you ever in the situation where you have a mouthwatering picture of a delicious dish but no idea how to recreate it? Look no further! Our ""Recipe Generation from Food Images"" project harnesses the power of deep learning to provide you with a solution.  ### Overview:  This innovative application uses state-of-the-art deep learning techniques to analyze food images and generate comprehensive cooking recipes. From a simple snapshot of your favorite dish, our system can extract essential information including:  **Recipe Title**: A catchy and descriptive title for the dish.  **Ingredients**: A detailed list of all the ingredients needed.  **Instructions**: Step-by-step cooking instructions to help you recreate the dish flawlessly.  ### Key Features:  **Deep Learning Magic**: Our model employs cutting-edge computer vision algorithms to identify ingredients and cooking processes within food images.  **Natural Language Generation**: It utilizes advanced natural language processing to create coherent and easy-to-follow cooking instructions.  **User-Friendly Interface**: A user-friendly web or mobile interface for hassle-free recipe generation.  **Endless Culinary Exploration**: Discover new recipes and cuisines by simply snapping a picture of any dish.  ---  ### Prerequisite : Download these files and replace it with the files in this folder ""Foodimg2Ing/data/""  1. Model (Modelbest.ckpt) : [Download Modelbest.ckpt](https://dl.fbaipublicfiles.com/inversecooking/modelbest.ckpt)  2. Ingredients (ingr_vocab.pkl) : [Download ingr_vocab.pkl](https://dl.fbaipublicfiles.com/inversecooking/ingr_vocab.pkl)  3. Instruction (instr_vocab.pkl) : [Download instr_vocab.pkl](https://dl.fbaipublicfiles.com/inversecooking/instr_vocab.pkl)  ### To run the Code :  1. Open Terminal (cmd) 2. Install all the required libraries using ""pip install -r requirements.txt"" 3. run ""python run.py""  You will be getting a localhost link and open that link in your browser   "
Gesture Controlled Virtual Mouse,Python,https://github.com/Viral-Doshi/Gesture-Controlled-Virtual-Mouse,"  # Gesture Controlled Virtual Mouse &nbsp;[![](https://img.shields.io/badge/python-3.8.5-blue.svg)](https://www.python.org/downloads/) [![platform](https://img.shields.io/badge/platform-windows-green.svg)](https://github.com/xenon-19/Gesture_Controller)   Gesture Controlled Virtual Mouse makes human computer interaction simple by making use of Hand Gestures and Voice Commands. The computer requires almost no direct contact. All i/o operations can be virtually controlled by using static and dynamic hand gestures along with a voice assistant. This project makes use of the state-of-art Machine Learning and Computer Vision algorithms to recognize hand gestures and voice commands, which works smoothly without any additional hardware requirements. It leverages models such as CNN implemented by [MediaPipe](https://github.com/google/mediapipe) running on top of pybind11. It consists of two modules: One which works direct on hands by making use of MediaPipe Hand detection, and other which makes use of Gloves of any uniform color. Currently it works on Windows platform.   _Video Demonstration: [link](https://www.youtube.com/watch?v=ufm6tfgo-OA&ab_channel=Proton)_<br> Note: Use Python version: 3.8.5  # Features  _click on dropdown to know more_ <br>  ### Gesture Recognition: <details> <summary>Neutral Gesture</summary>  <figure>   <img src=""https://github.com/xenon-19/Gesture_Controller/blob/9be82cfc75aa4c04fff0e12dd4de853f9d83a101/demo_media/palm.gif"" alt=""Palm"" width=""711"" height=""400""><br>   <figcaption>Neutral Gesture. Used to halt/stop execution of current gesture.</figcaption> </figure> </details>    <details> <summary>Move Cursor</summary>   <img src=""https://github.com/xenon-19/Gesture_Controller/blob/e20edfb1f368ffa600d96bd91031942ec97cb2ab/demo_media/move%20mouse.gif"" alt=""Move Cursor"" width=""711"" height=""400""><br>   <figcaption>Cursor is assigned to the midpoint of index and middle fingertips. This gesture moves the cursor to the desired location. Speed of the cursor movement is proportional to the speed of hand.</figcaption> </details>  <details> <summary>Left Click</summary> <img src=""https://github.com/xenon-19/Gesture_Controller/blob/9be82cfc75aa4c04fff0e12dd4de853f9d83a101/demo_media/left%20click.gif"" alt=""Left Click"" width=""711"" height=""400""><br>  <figcaption>Gesture for single left click</figcaption> </details>  <details> <summary>Right Click</summary> <img src=""https://github.com/xenon-19/Gesture_Controller/blob/9be82cfc75aa4c04fff0e12dd4de853f9d83a101/demo_media/right%20click.gif"" alt=""Right Click"" width=""711"" height=""400""><br>  <figcaption>Gesture for single right click</figcaption> </details>  <details> <summary>Double Click</summary> <img src=""https://github.com/xenon-19/Gesture_Controller/blob/9be82cfc75aa4c04fff0e12dd4de853f9d83a101/demo_media/double%20click.gif"" alt=""Double Click"" width=""711"" height=""400""><br>  <figcaption>Gesture for double click</figcaption> </details>  <details> <summary>Scrolling</summary> <img src=""https://github.com/xenon-19/Gesture_Controller/blob/9be82cfc75aa4c04fff0e12dd4de853f9d83a101/demo_media/Scrolling.gif"" alt=""Scrolling"" width=""711"" height=""400""><br>  <figcaption>Dynamic Gestures for horizontal and vertical scroll. The speed of scroll is proportional to the distance moved by pinch gesture from start point. Vertical and Horizontal scrolls are controlled by vertical and horizontal pinch movements respectively.</figcaption> </details>  <details> <summary>Drag and Drop</summary> <img src=""https://github.com/xenon-19/Gesture_Controller/blob/9be82cfc75aa4c04fff0e12dd4de853f9d83a101/demo_media/drag%20and%20drop.gif"" alt=""Drag and Drop"" width=""711"" height=""400""><br>  <figcaption>Gesture for drag and drop functionality. Can be used to move/tranfer files from one directory to other.</figcaption> </details>  <details> <summary>Multiple Item Selection</summary> <img src=""https://github.com/xenon-19/Gesture_Controller/blob/9be82cfc75aa4c04fff0e12dd4de853f9d83a101/demo_media/multiple%20item%20selection.gif"" alt=""Multiple Item Selection"" width=""711"" height=""400""><br>  <figcaption>Gesture to select multiple items</figcaption> </details>  <details> <summary>Volume Control</summary> <img src=""https://github.com/xenon-19/Gesture_Controller/blob/9be82cfc75aa4c04fff0e12dd4de853f9d83a101/demo_media/Volume%20control.gif"" alt=""Volume Control"" width=""711"" height=""400""><br>  <figcaption>Dynamic Gestures for Volume control. The rate of increase/decrease of volume is proportional to the distance moved by pinch gesture from start point. </figcaption> </details>  <details> <summary>Brightness Control</summary> <img src=""https://github.com/xenon-19/Gesture_Controller/blob/9be82cfc75aa4c04fff0e12dd4de853f9d83a101/demo_media/Brigntness%20Control.gif"" alt=""Brightness Control"" width=""711"" height=""400""><br>  <figcaption>Dynamic Gestures for Brightness control. The rate of increase/decrease of brightness is proportional to the distance moved by pinch gesture from start point. </figcaption> </details>  ### Voice Assistant ( ***Proton*** ): <details> <summary>Launch / Stop  Gesture Recognition</summary> <img src=""https://github.com/xenon-19/Gesture_Controller/blob/4041eedc2f75fa2923902000b606a05a677629e8/demo_media/voice%20commands/proton%20launch%20stop%20gest.png"" alt=""launch stop gesture recognition"" width=""250"" height=""auto""> <ul>   <li>     <code> Proton Launch Gesture Recognition </code><br>     Turns on webcam for hand gesture recognition.   </li>   <li>     <code> Proton Stop Gesture Recognition </code><br>     Turns off webcam and stops gesture recognition.     (Termination of Gesture controller can also be done via pressing <code>Enter</code> key in webcam window)    </li> </ul> </details>  <details> <summary>Google Search</summary> <img src=""https://github.com/xenon-19/Gesture_Controller/blob/4041eedc2f75fa2923902000b606a05a677629e8/demo_media/voice%20commands/proton%20search.png"" alt=""proton search github"" width=""800"" height=""auto""> <ul>   <li>     <code>Proton search {text_you_wish_to_search}</code><br>     Opens a new tab on Chrome Browser if it is running, else opens a new window. Searches the given text on Google.   </li> </ul> </details>  <details> <summary>Find a Location on Google Maps</summary>  <img src=""https://github.com/xenon-19/Gesture_Controller/blob/4041eedc2f75fa2923902000b606a05a677629e8/demo_media/voice%20commands/proton%20find%20location.png"" alt=""proton find location"" width=""800"" height=""auto"">   <ol>     <li>        <code>Proton Find a Location</code><br>       Will ask the user for the location to be searched.     </li>     <li>        <code>{Location_you_wish_to_find}</code><br>       Will find the required location on Google Maps in a new Chrome tab.     </li>   </ol> </details>  <details> <summary>File Navigation</summary> <img src=""https://github.com/xenon-19/Gesture_Controller/blob/4041eedc2f75fa2923902000b606a05a677629e8/demo_media/voice%20commands/proton%20list%20files.png"" alt=""proton list files"" width=""250"" height=""auto"">&emsp;  <img src=""https://github.com/xenon-19/Gesture_Controller/blob/4041eedc2f75fa2923902000b606a05a677629e8/demo_media/voice%20commands/proton%20open.png"" alt=""proton open"" width=""250"" height=""auto"">&emsp;  <img src=""https://github.com/xenon-19/Gesture_Controller/blob/4041eedc2f75fa2923902000b606a05a677629e8/demo_media/voice%20commands/proton%20go%20back.png"" alt=""proton go back"" width=""250"" height=""auto"">   <ul>     <li>       <code>Proton list files</code> / <code> Proton list </code><br>       Will list the files and respective file_numbers in your Current Directory (by default C:)     </li>     <li>         <code> Proton open {file_number} </code><br>       Opens the file / directory corresponding to specified file_number.     </li>     <li>       <code>Proton go back </code> / <code> Proton back </code><br>       Changes the Current Directory to Parent Directory and lists the files.     </li>   </ul> </details>  <details> <summary>Current Date and Time</summary> <img src=""https://github.com/xenon-19/Gesture_Controller/blob/d49c868acc41ac6c89489bfd80e5e5015a8cb571/demo_media/voice%20commands/proton%20date%20time.png"" alt=""proton date / time"" width=""250"" height=""auto"">   <ul>     <li>       <code> Proton what is today's date </code> / <code> Proton date </code><br>       <code> Proton what is the time </code> / <code> Proton time </code><br>       Returns the current date and time.     </li>   </ul> </details>  <details> <summary>Copy and Paste</summary>  <img src=""https://github.com/xenon-19/Gesture_Controller/blob/4041eedc2f75fa2923902000b606a05a677629e8/demo_media/voice%20commands/proton%20copy.png"" alt=""proton copy"" width=""500"" height=""auto"">  <img src=""https://github.com/xenon-19/Gesture_Controller/blob/4041eedc2f75fa2923902000b606a05a677629e8/demo_media/voice%20commands/proton%20paste.png"" alt=""proton paste"" width=""500"" height=""auto"">   <ul>     <li>       <code> Proton Copy </code><br>       Copies the selected text to clipboard.<br>     </li>     <li>       <code> Proton Paste </code><br>       Pastes the copied text.     </li>   </ul> </details>  <details> <summary>Sleep / Wake up Proton</summary>   <img src=""https://github.com/xenon-19/Gesture_Controller/blob/4041eedc2f75fa2923902000b606a05a677629e8/demo_media/voice%20commands/proton%20bye%20wake%20up.png"" alt=""proton sleep / wake up"" width=""250"" height=""auto"">   <ul>     <li>       Sleep<br>       <code> Proton bye </code><br>       Pauses voice command execution till the assistant is woken up.     </li>     <li>       Wake up<br>       <code> Proton wake up </code><br>       Resumes voice command execution.     </li>   </ul> </details>  <details> <summary>Exit</summary>    <img src=""https://github.com/xenon-19/Gesture_Controller/blob/4041eedc2f75fa2923902000b606a05a677629e8/demo_media/voice%20commands/proton%20exit.png"" alt=""proton exit"" width=""250"" height=""auto"">   <ul>     <li>       <code> Proton Exit </code> <br>       Terminates the voice assisstant thread. GUI window needs to be closed manually.     </li>   </ul> </details>  # Getting Started    ### Pre-requisites      Python: (3.6 - 3.8.5)<br>   Anaconda Distribution: To download click [here](https://www.anaconda.com/products/individual).      ### Procedure   ```bash   git clone https://github.com/xenon-19/Gesture-Controlled-Virtual-Mouse.git   ```   For detailed information about cloning visit [here](https://docs.github.com/en/github/creating-cloning-and-archiving-repositories/cloning-a-repository-from-github/cloning-a-repository).      Step 1:    ```bash   conda create --name gest python=3.8.5   ```      Step 2:   ```bash   conda activate gest   ```      Step 3:   ```bash   pip install -r requirements.txt   ```      Step 4:   ```bash    conda install PyAudio   ```   ```bash    conda install pywin32   ```      Step 5:   ```    cd to the GitHub Repo till src folder   ```   Command may look like: `cd C:\Users\.....\Gesture-Controlled-Virtual-Mouse\src`      Step 6:      For running Voice Assistant:   ```bash    python Proton.py   ```   ( You can enable Gesture Recognition by using the command ""Proton Launch Gesture Recognition"" )      Or to run only Gesture Recognition without the voice assisstant:      Uncomment last 2 lines of Code in the file `Gesture_Controller.py`   ```bash    python Gesture_Controller.py   ```        # Collaborators   | |  |  |  |  |   | ------------- | ------------- | ------------- | ------------- | ------------- |   | Viral Doshi | [GitHub](https://github.com/Viral-Doshi) | Email | [LinkedIn](https://www.linkedin.com/in/viral-doshi-5a7737190/) | Instagram |   | Nishiket Bidawat | [Github](https://github.com/xenon-19) | [Email](mailto:bidawatnishiket@gmail.com) | [LinkedIn](https://www.linkedin.com/in/nishiket-bidawat-74b419193/) | [Instagram](https://myanimelist.net/profile/Xenon1901) |   | Ankit Sharma | [GitHub](https://github.com/ankit-4129) | [Email](mailto:ankitsharma.rbt@gmail.com) | LinkedIn | Instagram |   | Parth Sakariya | [Github](https://github.com/parth-12) | [Email](mailto:parthsakariya12@icloud.com) | [LinkedIn](https://www.linkedin.com/in/parth-sakariya-1886b2193/) | [Instagram](https://www.instagram.com/parth_sak12/) |   "
Attendance System,Java,https://github.com/pally2409/Attendance-Management-System,A basic attendance management system coded in Java helping students manage their attendance and provides the following features: 1. Tracking total lectures and days missed. 2. Calculating the attendance in percentage. 3. Display warning message if attendance doesn't fulfill attendance critera. 4. Calculates and displays number of classes to be attended to fulfill attendance criteria. 
Calculator using Swing,Java,https://github.com/AHBRIJESH/Calculator-Using-java,"# Calculator using Java Swing   A simple calculator application implemented in Java Swing with basic arithmetic operations. This calculator supports addition, subtraction, multiplication, and division.  ## Features  - User-friendly interface - Responsive design - Clear button to reset the input - Real-time arithmetic calculations  ## How to Use  1. Enter the numerical values using the number buttons (0-9). 2. Perform arithmetic operations using the respective buttons. 3. Use the decimal point button for decimal numbers. 4. Press the ""CLEAR"" button to reset the input. 5. Press the ""="" button to display the result of the operation.  ## Implementation  The calculator is implemented in Java using Java Swing for the graphical user interface. The `Calculator` class extends `JFrame` and implements the `ActionListener` interface to handle button clicks.  ## Project Structure  - **Calculator.java**: Contains the main implementation of the calculator. - **README.md**: Project documentation. - **images**: Directory for storing images related to the project.  ## Output  ![Calculator Output](Images/Output1.png) ![Calculator Output](Images/Output2.png)  ## How to Run  Compile and run the `Calculator.java` file. Ensure you have a compatible Java Development Kit (JDK) installed on your system.  ```bash javac Calculator.java java Calculator ```  Feel free to contribute, report issues, or suggest improvements!  Happy calculating! üßÆ‚ú®"
Library Management System,Java,https://github.com/harismuneer/Library-Management-System-JAVA,"# üìö Library Management System -Java   <a href=""https://github.com/harismuneer""><img alt=""views"" title=""Github views"" src=""https://komarev.com/ghpvc/?username=harismuneer&style=flat-square"" width=""125""/></a> [![Open Source Love svg1](https://badges.frapsoft.com/os/v1/open-source.svg?v=103)](#) [![GitHub Forks](https://img.shields.io/github/forks/harismuneer/Library-Management-System-JAVA.svg?style=social&label=Fork&maxAge=2592000)](https://www.github.com/harismuneer/Library-Management-System-JAVA/fork) [![GitHub Issues](https://img.shields.io/github/issues/harismuneer/Library-Management-System-JAVA.svg?style=flat&label=Issues&maxAge=2592000)](https://www.github.com/harismuneer/Library-Management-System-JAVA/issues) [![contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat&label=Contributions&colorA=red&colorB=black	)](#)   A Library Management System made using the concepts of Object Oriented Analysis and Design. Minimal Code is written in the GUI and the entities are decoupled as well. The interface is console based. This project was designed during the course ""Object Oriented Analysis and Design CS309"".  The **Class Diagram** of the project is also provided along with the **Database Schema** file. The class diagram file can be opened using [Star UML](http://staruml.io/).  ## Class Diagram ![class diagram](../master/images/diagram.PNG)  **Note**: After Refactoring, new Class ""HoldRequestOperations"" is added to the above structure which lies in between the HoldRequest class and Book class. This class removes the bidirectional dependency between HoldRequest and Book. More details mentioned [here](https://github.com/OSSpk/Library-Management-System-JAVA/issues/9)   ## Interface <p align=""middle"">    <img src=""../master/images/interface.PNG"" width=""400""/>    <img src=""../master/images/interface2.PNG"" width=""400""/> </p>     ## Actors: The actors include the following:  * Librarian * Checkout Clerk * Borrower * Administrator  ## Use Cases: After determining the actors, the second step in use case analysis is to determine the tasks that each actor will need to do with the system. Each task is called a use case because it represents one particular way the system will be used.  **In other words, only those use cases are listed that actors will need to do when they are using the system to solve the customer‚Äôs problem.**   ### Borrower: * ‚ùè Search for items by title. * ‚ùè ... by author. * ‚ùè ... by subject. * ‚ùè Place a book on hold if it is on loan to somebody else. * ‚ùè Check  the  borrower‚Äôs  personal  information  and  list  of  books  currently borrowed.  ### Checkout Clerk: * ‚ùè All the Borrower use cases, plus * ‚ùè Check out an item for a borrower. * ‚ùè Check in an item that has been returned. * ‚ùè Renew an item. * ‚ùè Record that a fine has been paid. * ‚ùè Add a new borrower. * ‚ùè Update a borrower‚Äôs personal information (address, telephone number etc.).  ### Librarian: * ‚ùè All of the Borrower and Checkout Clerk use cases, plus * ‚ùè Add a new item to the collection. * ‚ùè Delete an item from the collection. * ‚ùè Change the information the system has recorded about an item.  ### Administrator: * ‚ùè Add Clerk. * ‚ùè Add Librarian. * ‚ùè View Issued Books History. * ‚ùè View All Books in Library.   ## How to Run 1- Install these:  * [Java SE Development Kit 8 (JDK 8)](http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html)  * After installing JDK 8, install [NetBeans IDE](https://netbeans.org/downloads/)  2- Open NetBeans IDE. Click on File -> Open Project and browse to the downloaded folder named ""Project"" and select it. It will load the NetBeans project.  3- Now everything is setup except the Java DB (Derby) Database of NetBeans. So, follow these steps to setup the database:  **Step 1:** In the Netbeans Window, there is a tab named ""Services"" on the left. Select it. Then right click on JavaDB > Properties and    change database location to ""Database"" folder downloaded with this repository (its placed besides the ""Project"" folder).  ![step1](../master/images/step1.PNG)     **Step 2:** After that a database named LMS will show up under JavaDB tab. Now Right Click Databases > New Connection and select Java DB Network and click Next.   ![step2](../master/images/step2.PNG)     **Step 3:** Provide the following database crendentials in the next popup and click Next.   ```   Host: localhost   Port: 1527   Database: LMS   User Name: haris   Password: 123   ```  ![step3](../master/images/step3.PNG)  **Step 4:** Now just click Next for the rest of the windows. After all this the database connection is made. Make sure that you connect with the database before running the project by right clicking on the connection and selecting connect. Now you are ready to run the project!  ![final](../master/images/final.png)  ## Note The password for Administrative Functions is *lib*. The admin adds new clerks and librarian, then they both do the rest of the functions.  <hr>  ## Authors üëã  You can get in touch with us on our LinkedIn Profiles:  #### Haris Muneer  [![LinkedIn Link](https://img.shields.io/badge/Connect-harismuneer-blue.svg?logo=linkedin&longCache=true&style=social&label=Follow)](https://www.linkedin.com/in/harismuneer)  You can also follow my GitHub Profile to stay updated about my latest projects: [![GitHub Follow](https://img.shields.io/badge/Connect-harismuneer-blue.svg?logo=Github&longCache=true&style=social&label=Follow)](https://github.com/harismuneer)  #### Maham Amjad  [![LinkedIn Link](https://img.shields.io/badge/Connect-maham--amjad-blue.svg?logo=linkedin&longCache=true&style=social&label=Connect)](https://www.linkedin.com/in/maham-amjad-40796b177/)  You can also follow my GitHub Profile to stay updated about my latest projects: [![GitHub Follow](https://img.shields.io/badge/Connect-maham--amjad-blue.svg?logo=Github&longCache=true&style=social&label=Follow)](https://github.com/MahamAmjad)  If you liked the repo then kindly support it by giving it a star ‚≠ê and share in your circles so more people can benefit from the effort.  ## Contributions Welcome [![GitHub Issues](https://img.shields.io/github/issues/harismuneer/Library-Management-System-JAVA.svg?style=flat&label=Issues&maxAge=2592000)](https://www.github.com/harismuneer/Library-Management-System-JAVA/issues)  If you find any bugs, have suggestions, or face issues:  - Open an Issue in the Issues Tab to discuss them. - Submit a Pull Request to propose fixes or improvements. - Review Pull Requests from other contributors to help maintain the project's quality and progress.  This project thrives on community collaboration! Members are encouraged to take the initiative, support one another, and actively engage in all aspects of the project. Whether it‚Äôs debugging, fixing issues, or brainstorming new ideas, your contributions are what keep this project moving forward.  With modern AI tools like ChatGPT, solving challenges and contributing effectively is easier than ever. Let‚Äôs work together to make this project the best it can be! üöÄ  ## License [![MIT](https://img.shields.io/cocoapods/l/AFNetworking.svg?style=style&label=License&maxAge=2592000)](../master/LICENSE)  Copyright (c) 2018-present, harismuneer, MahamAmjad  <!-- PROFILE_INTRO_START -->  <hr>  <h1> <a href=""#""><img src=""https://media.giphy.com/media/hvRJCLFzcasrR4ia7z/giphy.gif"" alt=""Waving hand"" width=""28""></a> Hey there, I'm <a href=""https://www.linkedin.com/in/harismuneer/"">Haris Muneer</a> üë®üèª‚Äçüíª </h1>   <a href=""https://github.com/harismuneer""><img src=""https://img.shields.io/github/stars/harismuneer"" alt=""Total Github Stars""></a> <a href=""https://github.com/harismuneer?tab=followers""><img src=""https://img.shields.io/github/followers/harismuneer"" alt=""Total Github Followers""></a>  <hr>  - <b>üõ†Ô∏è Product Builder:</b> Agile Product Manager with 5+ years of hands-on experience delivering SaaS solutions across sales, recruiting, AI, social media, and public sector domains. Background in Computer Science, with a proven track record of scaling products from inception to $XXM+ ARR, launching 3 top-ranking tools on Product Hunt, and developing solutions adopted by 250+ B2B clients in 40+ countries.     - <b>üåü Open Source Advocate:</b> Passionate about making technology accessible, I‚Äôve developed and open-sourced several software projects for web, mobile, desktop, and AI on my <a href=""https://github.com/harismuneer"">GitHub profile</a>. These projects have been used by thousands of learners worldwide to enhance their skills and knowledge.  - <b>üì´ How to Reach Me:</b> To learn more about my skills and work, visit my <a href=""https://www.linkedin.com/in/harismuneer"">LinkedIn profile</a>. For collaboration or inquiries, feel free to reach out via <a href=""mailto:haris.muneer5@gmail.com"">email</a>.  <hr>  <h2 align=""left"">ü§ù Follow my journey</h2> <p align=""left"">   <a href=""https://www.linkedin.com/in/harismuneer""><img title=""Follow Haris Muneer on LinkedIn"" src=""https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white""/></a>   <a href=""https://github.com/harismuneer""><img title=""Follow Haris Muneer on GitHub"" src=""https://img.shields.io/badge/GitHub-100000?style=for-the-badge&logo=github&logoColor=white""/></a>   <a href=""https://www.youtube.com/@haris_muneer?sub_confirmation=1""><img title=""Subscribe on YouTube"" src=""https://img.shields.io/badge/YouTube-FF0000?style=for-the-badge&logo=youtube&logoColor=white""/></a>    <a href=""mailto:haris.muneer5@gmail.com""><img title=""Email"" src=""https://img.shields.io/badge/Gmail-D14836?style=for-the-badge&logo=gmail&logoColor=white""/></a> </p>    <!-- PROFILE_INTRO_END -->     "
Payroll Management System,Java,https://github.com/sahiljagtap08/Payroll-System-JAVA,"# Payroll-System-JAVA  The Payroll System is a desktop-based application that provides a solution for storing and managing employee data, improving efficiency, and automatically calculating salaries. This project is developed using Spring AWT Java JDBC technology.   ## Features  - Secure login system that only allows the administrator to access employee data. - Add, edit, and delete employee details. - Calculate salary based on employee information and generate a payslip. - View employee information, salary details, and payslips.  ## Technologies Used  - Java - Spring - AWT (Abstract Window Toolkit) - JDBC (Java Database Connectivity) - MySQL  ## Getting Started  To get started with the Payroll System, follow the steps below:  1. Clone this repository using the command below:  git clone https://github.com/sahiljagtap08/Payroll-System-JAVA.git   2. Import the project into your preferred IDE (e.g. Eclipse, IntelliJ IDEA).  3. Set up the database by running the SQL script provided in the `database` directory.  4. Configure the database connection in the `application.properties` file.  5. Run the `PayrollSystemApplication.java` file to start the application.  ## Contributors  - [Sahil Jagtap](https://github.com/sahiljagtap08)  ## Acknowledgements  - [Spring documentation](https://spring.io/docs) - [Java AWT documentation](https://docs.oracle.com/javase/7/docs/api/java/awt/package-summary.html) - [JDBC documentation](https://docs.oracle.com/javase/7/docs/api/java/sql/package-summary.html)"
Email Administration using Core Java,Java,https://github.com/rgovil17/Email-Administration-Application,"# Email Administration Application A basic Java project focused on applying object-oriented design in a real world application.  ### Project Aim You are an IT Support Administrator Specialist and are charged with the task of creating email accounts for new hires.  Your application should do the following: - Generate an email with the following syntax: `firstname.lastname@department.company.com` - Determine the department (sales, development, accounting), if none leave blank - Generate a random String for a password - Have set methods to change the password, set the mailbox capacity, and define an alternate email address - Have get methods to display the name, email, and mailbox capacity"
Symmetric Encryption Cyptography,Java,https://github.com/asadkhan-786-gb/Symmetric-Encryption-Cryptography-in-Java,# Symmetric-Encryption-Cryptography-in-Java Java Project based on Java and Encryption using Cryptography algorithms  ## Project Aim Develop Java program to create encryption program for particular file or text  # Steps of development: 1. Project Setup 2. Understanding different tools used 3. Java Programming Fundamentals 4. Cryptography Algos 5. Java Servlet and encryption 6. Final deployments  # Application Requirements  - Java security module - Java Programming Fundamental Knowledge - Java servlet - Java local hosting servers  # Tools Required: 1. JDK (1.8) / Maven Dependencies 2. IDE - IntelliJ Idea (JetBrains IDE) / NetBeans
Snake Game,Java,https://github.com/janbodnar/Java-Snake-Game,# Java-Snake-Game Java Snake game source code  https://zetcode.com/javagames/snake/    ![Snake game screenshot](snake.png)
Snake Game,Java,https://github.com/janbodnar/Java-Snake-Game,# Java-Snake-Game Java Snake game source code  https://zetcode.com/javagames/snake/    ![Snake game screenshot](snake.png)
Tic-Tac-Toe Game using Swing,Java,https://github.com/ImKennyYip/tictactoe-java,"# [Tic Tac Tie (Java)](https://youtu.be/Nc77ymnm8Ss)  How to code a Tic Tac Toe game in Javafor beginners! Learn how to create a game of Tic Tac Toe in Java using the awt and swing graphics library. Throughout the tutorial, you will learn how to create a graphical user interface (GUI) to display the Tic Tac Toe game, and check win or tie conditions using a 2D array.  [How to setup Java with Visual Studio Code](https://youtu.be/BB0gZFpukJU)  ![tictactoe-java-demo](https://github.com/ImKennyYip/tictactoe-java/assets/78777681/d54b6509-543e-4f91-aabd-bd88f6baef34)  ## Homework: You can continue working on this project if you like. One feature you can add is a JButton for restarting the game every time there is a win or tie. Another feature you can add is score keeping for playerX and playerO and updating the score after every game."
Sentiment Analysis,R,https://github.com/meetttttt/Sentiment-Analysis-in-R-programming,"# Sentiment-Analysis-in-R-programming  About R Programming Language: - R is a language and environment for statistical computing and graphics. It provides a wide variety of statistical and graphical techniques and is highly extensible. - R is available as free software. It‚Äôs easy to learn. - R is helpful at every step of the data analysis process from gathering and cleaning data to analyzing it and reporting the conclusions. - R is a statistical language created by statisticians. Thus, it excels in statistical computation. R is the most used programming language for developing statistical tools.  What is Sentiment Analysis ? - Sentiment analysis is contextual mining of text which identifies and extracts subjective information in source material, and helping a business to understand the social sentiment of their brand, product or service while monitoring online conversations. - Sentiment Analysis is the process of determining whether a piece of writing is positive, negative or neutral. Sentiment analysis helps data analysts within large enterprises gauge public opinion, conduct nuanced market research, monitor brand and product reputation, and understand customer experiences. - R has a rich set of packages for Natural Language Processing (NLP) and generating plots. The foundational steps involve loading the text file into an R Corpus, then cleaning and stemming the data before performing analysis. I will demonstrate these steps and analysis like Word Frequency, Word Cloud, Word Association, Sentiment Scores and Emotion Classification using various plots and charts.  Loading library:(The following packages are used in the project)  -Tm for text mining operations like removing numbers, special characters, punctuations and stop words (Stop words in any language are the most commonly occurring words that have very little value for NLP and should be filtered out. Examples of stop words in English are ‚Äúthe‚Äù, ‚Äúis‚Äù, ‚Äúare‚Äù.) - Snowballc for stemming, which is the process of reducing words to their base or root form. For example, a stemming algorithm would reduce the words ‚Äúfishing‚Äù, ‚Äúfished‚Äù and ‚Äúfisher‚Äù to the stem ‚Äúfish‚Äù. - wordcloud for generating the word cloud plot. - RColorBrewer for color palettes used in various plots - syuzhet for sentiment scores and emotion classification - ggplot2 for plotting graphs   About Data: - X: It represent the ID - rating: It is the user provided for the Kindle book(it ranges from 1-5). - review.Text: Its the whole review of the Kindle book. - summary: Its a short summary of whole review in short words  Cleaning up Text Data - Cleaning the text data starts with making transformations like removing special characters from the text. - This is done using the tm_map() function to replace special characters like /, @ and | with a space. - The next step is to remove the unnecessary whitespace and convert the text to lower case. - Then remove the stopwords.They are the most commonly occurring words in a language and have very little value in terms of gaining useful information. - The last step is text stemming. It is the process of reducing the word to its root form. The stemming process simplifies the word to its common origin.   Generating the Word Cloud - A word cloud is one of the most popular ways to visualize and analyze qualitative data. - It‚Äôs an image composed of keywords found within a body of text, where the size of each word indicates its frequency in that body of text. - Use the word frequency data frame (table) created previously to generate the word cloud   Below is a brief description of the arguments used in the word cloud function; - words ‚Äì words to be plotted - freq ‚Äì frequencies of words - min.freq ‚Äì words whose frequency is at or above this threshold value is plotted (in this case, I have set it to 5) - max.words ‚Äì the maximum number of words to display on the plot (in the code above, I have set it 100) - random.order ‚Äì I have set it to FALSE, so the words are plotted in order of decreasing frequency - rot.per ‚Äì the percentage of words that are displayed as vertical text (with 90-degree rotation). I have set it 0.40 (40 %), please feel free to adjust this setting to suit your preferences - colors ‚Äì changes word colors going from lowest to highest frequencies   Word Association - Correlation is a statistical technique that can demonstrate whether, and how strongly, pairs of variables are related. - This technique can be used effectively to analyze which words occur most often in association with the most frequently occurring words in the survey responses, which helps to see the context around these words   Sentiment Scores - Sentiments can be classified as positive, neutral or negative - They can also be represented on a numeric scale, to better express the degree of positive or negative strength of the sentiment contained in a body of text. - Here we are using the Syuzhet package for generating sentiment scores, which has four sentiment dictionaries and offers a method for accessing the sentiment extraction tool developed in the NLP group at Stanford. - The get_sentiment function accepts two arguments: a character vector (of sentences or words) and a method. - The selected method determines which of the four available sentiment extraction methods will be used. The four methods are syuzhet (this is the default), bing, afinn and nrc. ach method uses a different scale and hence returns slightly different results.  - Please note the outcome of nrc method is more than just a numeric score.   Syuzhet vector - An inspection of the Syuzhet vector shows the first element has the value of 1.85. - It means the sum of the sentiment scores of all meaningful words in the first response(line) in the text file, adds up to 1.85. The scale for sentiment scores using the syuzhet method is decimal and ranges from -1(indicating most negative) to +1(indicating most positive). - Note that the summary statistics of the suyzhet vector show a median value of 1.6, which is above zero and can be interpreted as the overall average sentiment across all the responses is positive.  Emotion Classification - Emotion classification is built on the NRC Word-Emotion Association Lexicon (aka EmoLex). - The NRC Emotion Lexicon is a list of English words and their associations with eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive). - The get_nrc_sentiments function, which returns a data frame with each row representing a sentence from the original file. - The data frame has ten columns (one column for each of the eight emotions, one column for positive sentiment valence and one for negative sentiment valence).¬∂ - The data in the columns (anger, anticipation, disgust, fear, joy, sadness, surprise, trust, negative, positive) can be accessed individually or in sets.  Conclusion: - This project was demonstration of how to create a word frequency table and plot a word cloud, to identify prominent themes occurring in the review. - Word association analysis using correlation, helped gain context around the prominent themes. - It explored four methods to generate sentiment scores, which proved useful in assigning a numeric value to strength (of positivity or negativity) of sentiments in the text and allowed interpreting that the average sentiment through the text is trending positive. - Lastly, it demonstrated how to implement an emotion classification with NRC sentiment and created two plots to analyze and interpret emotions found in the text.  "
Uber Data Analysis,R,https://github.com/AmirMotefaker/Uber-Data-Analysis,"# Uber Data Analysis using R  - With over 118 million users, 5 million drivers, and 6.3 billion trips with 17.4 million trips completed per day - Uber is the company behind the data for moving people and making deliveries hassle-free. - How are drivers assigned to riders cost-efficiently, and how is dynamic pricing leveraged to balance supply and demand?  - Thanks to the large volumes of data Uber collects and the fantastic team that handles Uber Data Analysis using Machine Learning tools and frameworks.  - If you‚Äôre curious to learn more about how data analysis is done at Uber to ensure positive experiences for riders while making the ride profitable for the company - Get your hands dirty working with the Uber dataset to gain in-depth insights.   - Data storytelling is an important component of Machine Learning through which companies are able to understand the background of various operations.  - With the help of visualization, companies can avail the benefit of understanding complex data and gaining insights that would help them to craft decisions.  - This is more of a data visualization project that will guide you towards using the ggplot2 library for understanding the data and for developing an intuition for understanding the customers who avail the trips.  - In this data analysis, we analyze Uber data from 1th April 2014 to 30th September 2014.  - The goal of this project is to learn visualizations in R.  - Dataset: [Kaggle](https://www.kaggle.com/datasets/amirmotefaker/uber-dataset-from-april-to-september-2014)  ## We will import the essential packages that we will use in this uber data analysis project:  >### ggplot2 >This is the backbone of this project. ggplot2 is the most popular data visualization library that is most widely used for creating aesthetic visualization plots.  >### ggthemes >This is more of an add-on to our main ggplot2 library. With this, we can create better create extra themes and scales with the mainstream ggplot2 package.  >### lubridate >Our dataset involves various time frames. In order to understand our data in separate time categories, we will make use of the lubridate package.  >### dplyr >This package is the lingua franca of data manipulation in R.  >### tidyr >This package will help you to tidy your data. The basic principle of tidyr is to tidy the columns where each variable is present in a column, each observation is represented by a row and each value depicts a cell.  >### DT >With the help of this package, we will be able to interface with the JavaScript Library called ‚Äì Datatables.  >### scales >With the help of graphical scales, we can automatically map the data to the correct scales with well-placed axes and legends."
Movie Recommendation System,R,https://github.com/Rpita623/Movie-Recommendation-System-using-R_Project,"# Movie Recommendation System: Project using R and Machine learning  ## Aim of Project The main goal of this machine learning project is to build a recommendation engine that recommends movies to users. This R project is designed to understand the functioning of a recommendation system. I developed an *Item Based Collaborative Filter*. This helped me gain experience of implementing my R, Data Science, and Machine learning skills in a real-life project.  ### Dataset used I have used the MovieLens Dataset. That data I have used consists of 105339 ratings in the ```ratings.csv``` file, applied over 10329 movies in the ```movies.csv```.  ### Essential Libraries ```recommenderlab```, ```ggplot2```, ```data.table``` and ```reshape2```.  ### Data Pre-processing After retrieving data from the ```movies.csv``` and```ratings.csv``` datasets, I observed that the userId column, as well as the movieId column, consisted of integers. Furthermore, I needed to convert the genres present in the movie_data dataframe into a more usable format by the users. In order to do so, I first created a one-hot encoding to create a matrix that comprises of corresponding genres for each of the films. I then created a ```search matrix``` that will allow us to perform an easy search of the films by specifying the genre present in our list.  There are movies that have several genres. For the movie recommendation system to make sense of the ratings through ```recommenderlab```, I convert the matrix into a sparse matrix. This new matrix is of the class ```realRatingMatrix```. I then overviewed some important parameters that provided various options for building recommendation systems for movies.  ### Collaborative Filtering - Exploring similar data Collaborative Filtering involves suggesting movies to the users that are based on collecting preferences from many other users. For example, if a user A likes to watch action films and so does user B, then the movies that the user B will watch in the future will be recommended to A and vice-versa. Therefore, recommending movies is dependent on creating a relationship of similarity between the two users. With the help of ```recommenderlab```, I computed similarities using various operators like cosine, pearson as well as jaccard.  ### Visualisation: Similarity in data I visualised the similarity between the users as explained in the above section as well as the similarity shared between the films.  ### Visualisation: Most viewed movies In this section of the machine learning project, I explored the most viewed movies in the dataset. Before this, I counted the number of views in a film and organized them in a table that would group them in descending order. I visualized the total number of views of the top films as a bar plot.   From the visualisation, it could be observed that 'Pulp Fiction' is the most watched film followed by 'Forrest Gump'.  ### Visualisation: Heatmap of Movie Ratings Now, in this data science project of Recommendation system, I visualized a heatmap of the movie ratings. This heatmap will contain first 25 rows and 25 columns.  ### Data Preparation This is conducted in three steps: 1. Selecting useful data 2. Normalizing data 3. Binarizing the data  **Data Selection:** Through this I visualised the top users and movies through a heatmap. Then I visualized the distribution of the average ratings per user.  **Data Normalization:** In the case of some users, there can be high ratings or low ratings provided to all of the watched films. This will act as a bias while implementing the model. In order to remove this, I normalized the data. Normalization is a data preparation procedure to standardize the numerical values in a column to a common scale value. This is done in such a way that there is no distortion in the range of values. Normalization transforms the average value of our ratings column to 0. I then plotted a heatmap that portrays our normalized ratings.  **Data Binarization:** In the final step of the data preparation, in this data science project, I binarized the data. Binarizing the data means that we have two discrete values 1 and 0, which will allow the recommendation system to work more efficiently. I defined a matrix that will consist of 1 if the rating is above 3 and otherwise it will be 0.  ### Collaborative Filtering System In this section of the data science project, I developed the *Item Based Collaborative Filtering System*. This type of collaborative filtering finds similarity in the items based on the people‚Äôs ratings of them. The algorithm first builds a similar-items table of the customers who have purchased them into a combination of similar items. This is then fed into the recommendation system.  The similarity between single products and related products can be determined with the following algorithm:  - For each Item i<sub>1</sub> present in the product catalog, purchased by customer C. - And, for each item i<sub>2</sub> also purchased by the customer C. - Create record that the customer purchased items i<sub>1</sub> and i<sub>2</sub>. - Calculate the similarity between i<sub>1</sub> and i<sub>2</sub>.  I built this filtering system by splitting the dataset into 80% training set and 20% test set.  ### Building the recommendation system Now, I explored the various parameters of the *Item Based Collaborative Filter*. These parameters are default in nature. In the first step, k denotes the number of items for computing their similarities. Here, k is equal to 30. Therefore, the algorithm will now identify the k most similar items and store their number.   ### Exploring data science recommendation system model Using the ```getModel()``` function, I retrieved the ```recommen_model```. I then found the class and dimensions of the similarity matrix, that is, contained within ```model_info```. Finally, I generated a heatmap, that will contain the top 20 items and visualize the similarity shared between them.  In the next step of the ML project, I carried out the sum of rows and columns with the similarity of the objects above 0. I visualized the sum of columns through a distribution.  ### Building Recommender System on dataset using R I created a ```top_recommendations``` variable which I initialized to 10, specifying the number of films to each user. I then used the ```predict()``` function that identified similar items and ranked them appropriately. Here, each rating is used as a weight. Each weight is multiplied with related similarities. Finally, I added everything in the end.    Cr:DataFlair     "
Customer Segmentation,R,https://github.com/aimee0317/customer-segmentation-R,"# Customer Segmentation Cluster Analysis in R - Author: Amelia Tang   ## About  This project aims to identify customer segments using a subset of the customer information dataset from [kaggle.com](https://www.kaggle.com/datasets/vetrirah/customer). The dataset contains 6718 rows of non-null data. To conduct the customer segmentation analysis, I utilized the k-means clustering algorithm and implemented the algorithm in R.   K-means is a popular unsupervised machine learning algorithm used for clustering data points into K distinct clusters based on their similarity. It's one of the most popular algorithms for customer segmentation analysis. The below diagram illustrates how the algorithm works step by step:  ![k-means](doc/kmeans_steps.png)  After clustering the data, the clusters are visualized using `clusplot`. Clusplot shows first two principal components to explain the clusters. <br> <img src=""results/cluster_for_readme.png"" alt=""cluster plot"" width=""500"">  ## Reports  [EDA Report using ggplot2](doc/Customer_Segmentation_EDA_Report.pdf) <br> [Customer Segmentation using K-means Clustering Project Report](doc/Customer_Segmentation_Project_Report.pdf)  ## Usage To replicate the analysis, clone this GitHub repository, install the dependencies listed below, and run the following command at the command line/terminal from the root directory of this project:      make all  To reset the repo to a clean state, with no intermediate or results files, run the following command at the command line/terminal from the root directory of this project:      make clean  ## Dependencies R version 4.2.3 (2023-03-15) and R packages: - readr - tidyr - dplyr - ggplot2  - magrittr - fastDummies  - factoextra  - cluster - knitr - ggthemes  - kableExtra"
Credit Card Fraud Detection,R,https://github.com/shalakasaraogi/credit-card-fraud-detection/tree/main,"# Project : CREDIT CARD FRAUD DETECTION   ![](https://github.com/shalakasaraogi/credit-card-fraud-detection/blob/main/images/xenonstack-credit-card-fraud-detection.png)  ## What is Fraud?  Fraud is an act of deception used to illegaly deprive another person or entity of money, property or legal rights.  ## Rule-based approach for fraud detection  In Rule-based approach,  * Algorithms are written by fraud analyst. * Based on strict rules. * Changes for detecting a new fraud are done manually. * Increase in customers and data, increases human efforts. * Time Consuming and costly. * Cannot recognize the hidden patterns. * Cannot predict the fraud by going beyond the rules. * Connot responds to new situations, not trained on or explicitly programmed.  ## Data Science approach for fraud detection  To leverage the vast amounts of data collected from online transactions and model it in a way that allows us to flag or predict fraud in future transactions.   So we use different Machine Learning and Deep Learning techniques to detect fraud.  Here we have used Machine Learning Techniques.   **Machine Learning (ML)** - ML encompasses a large collection of algorithms and techniques used in classifications, regression, clustering and anomaly detection.    ## Some of the Challenges of fraud detection model  * Unbalanced Data * Operational Efficiency * Incorrect flagging   ### Dealing with Unbalanced Data  * Classifier tends to favour majority class (=Legitimate) * Large classification error over the fraud cases * Classifiers learn better from a balanced distribution  **Sampling Methods to solve this Unbalanced Data problem**  **Random Over-Sampling (ROS)** -  In this technique we over sample the minority class which is our fraud cases, so we copy the cases that are already present in are fradulent cases, so me copy the same cases multiple times till we reach the threshold value that we want in our dataset. The problem with this technique is that it is done by creating the duplicating lots fraud cases that are already present in our dataset that means we will be training our model with lots of duplicate values which won't explain the varriance in the dataset.  **Random Under-Sampling (RUS)** -  In this technique we under sample the majority class which is our legitimate cases, so we remove some of the cases from our dataset which are from legitimate transactions so we remove some of the cases till we have almost the same distribution as of fraud cases. The problem with this technique is that we will endup with throwing lot of useful data and informations which is not prefered in general.   **Both** -  You can perform both ROS and RUS by increasing the fraud cases and decreasing the legitimate cases.  **Synthetic Minority Over-Sampling (SMOTE)** -  In this technique we over-sample the minority class (i.e fraud) by creating synthetic fraud cases.  Read more about SMOTE [link-1](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/), [link-2](https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis)  ## Implementation  Dataset:  [creditcard.csv](https://github.com/shalakasaraogi/credit-card-fraud-detection/blob/main/csv%20files/creditcard.csv)  Code:  [credit-card-fraud-detection.R](https://github.com/shalakasaraogi/credit-card-fraud-detection/blob/main/Credit-card-fraud-detection.R)  Lab Environment: RStudio  ---              "
Book Library App,Mobile Applications,https://github.com/rohanrao619/Library_Management_Android_App,"# Library Management Android App  This project is an Android App which aims to provide a solution to manage a Book Library. Designed for both Users/Students and Library Admins, various operations such as adding/removing/updating books, searching books, issuing/re-issuing/returning books, seeing issued books with due dates, collecting fines, etc. can be performed. The App uses Cloud Firestore as the back-end database for storing details of the Books and Users. Firebase Cloud Messaging is used to send realtime notifications to Users if a return deadline is approaching, his/her fine is increased, or when a new book is added to the Library. Cloud Functions are used to monitor the database in order to update fines and trigger notifications. The App has a user-friendly and interactive interface.  A detailed description of this project along with the screenshots can be found [here](#project-description-and-screenshots).  ## Usage This App is not live at the moment and won't work off the shelf. * Create a new Firebase Project (with Firestore Database) on Firebase Console and configure it with the App. * Setup a Node.js and Firebase CLI environment to deploy the Cloud Functions. * Setup a cron job to trigger the ""updateFine"" Function once every day through a HTTPS request.  I would recommend following YouTube Tutorials, Online Guides, and Official Documentation to get help in setting up the Project.<br> This is where I started: [Cloud Firestore Android Tutorial](https://youtube.com/playlist?list=PLrnPJCHvNZuDrSqu-dKdDi3Q6nM-VUyxD)  ## Tools Used * [Android Studio](https://developer.android.com/studio) : Primary IDE for Android App Development using Java. * [Cloud Firestore](https://firebase.google.com/products/firestore) : The Database used for storing data in the form of Collections and Documents. * [Firebase Cloud Messaging](https://firebase.google.com/products/cloud-messaging) : Used for sending Push Notifications to specified users using FCM Tokens. * [Cloud Functions](https://firebase.google.com/products/functions) : Used for real-time monitoring on the database and listening to certain triggers for taking required actions. * [Firebase Authentication](https://firebase.google.com/products/auth) : Used to maintain Accounts and perform Login and Signup Actions. * [cron-job.org](https://cron-job.org/en/): Used for scheduling Cloud Functions through HTTPS Requests.  ## Contributing You are welcome to contribute :  1. Fork it (https://github.com/rohanrao619/Library_Management_Android_App/fork) 2. Create new branch : `git checkout -b new_feature` 3. Commit your changes : `git commit -am 'Added new_feature'` 4. Push to the branch : `git push origin new_feature` 5. Submit a pull request !  ## Future Work * UI Improvement and Optimization * New Features/Functionalities * Generalization for Flexible Usage. This App was implemented specifically for my College's Library System:     * Books and Users are identified through unique Book IDs and Card No. respectively.     * While issuing/re-issuing/returning a Book, 2 digits need to be added at end of its Book ID (to specify the unit number). For example, copies of a Book with ID 14532 having 25 units would have the IDs 1453201, 1453202, 1453203 ... 1453225. Book IDs are displayed in the same way in User Account's end.     * Email ID for registration needs to be of the domain @iiitnr.edu.in.     * Books are issued for a duration of 14 days, after which a fine of Rs.1 per day is incurred (if the book is not re-issued or returned).     * A book can be re-issued only 1 time from the User's end.  If you have any new ideas or suggestions to improve this project, feel free to contact me. Your valuable feedback is highly appreciated!  ## License This Project is licensed under the MIT License, see the [LICENSE](LICENSE) file for details.  ## Project Description and Screenshots ### Features * Simple and minimal Layout Designs. * Interlinked Activities for different functions. * Text Views and Toasts for displaying info. * Interaction with the User with the help of Edit Text Views, Buttons, Checkboxes, Alert Dialogs, Card Views, etc. * Updates using Push Notifications. * Real-time Synchronization with Online Database. * Auto Login on App launch if the user/admin is logged in. * Security Rules to protect the database from malicious activities.  ### Functionalities * Admin Account :    * Add new Book to the Library.   * Update details of an existing Book.   * Remove a Book from the Library.   * Issue a Book to a User.   * Return a Book from User.   * Re-Issue a Book to a User.   * Collect Fine from a User.   * Search for a particular Book.    * User Account :    * See Issued Books with Due Dates.   * Re-issue a Book one time.   * Search for a particular Book.    * Push Notifications to Users when :    * New Book added to the Library.   * Fine of the User increases.   * Deadline for a particular Book is nearby (3 days).    * Cloud Functions to :    * Increase Fine of the user if the deadline is crossed, once in a day.   * Trigger Notifications based on events.  ### Screenshots  |![](Screenshots/Log_In_Page.png)|![](Screenshots/User_Registration_Page.png)|![](Screenshots/Admin_Registration_Page.png)| |:---:|:---:|:---:| |**Log In Page**|**User Registration Page**|**Admin Registration Page**|  |![](Screenshots/User_Home_Page.png)|![](Screenshots/Admin_Home_Page.png)|![](Screenshots/Add_Book_Page.png)| |:---:|:---:|:---:| |**User Home Page**|**Admin Home Page**|**Add Book Page**|  |![](Screenshots/Remove_Book_Page.png)|![](Screenshots/Update_Book_Page.png)|![](Screenshots/Issue_Book_Page.png)| |:---:|:---:|:---:| |**Remove Book Page**|**Update Book Page**|**Issue Book Page**|  |![](Screenshots/Return_Book_Page.png)|![](Screenshots/Reissue_Book_Page.png)|![](Screenshots/Collect_Fine_Page.png)| |:---:|:---:|:---:| |**Return Book Page**|**Reissue Book Page**|**Collect Fine Page**|  |![](Screenshots/Collect_Fine_Confirmation_Page.png)|![](Screenshots/Search_Book_Page.png)|![](Screenshots/Search_Book_Results.png)| |:---:|:---:|:---:| |**Collect Fine Confirmation**|**Search Book Page**|**Search Book Results**|  |![](Screenshots/See_Issued_Books_Page.png)|![](Screenshots/User_Reissue_Book_Page.png)|![](Screenshots/New_Book_Added_Notification.png)| |:---:|:---:|:---:| |**See Issued Books Page**|**User Reissue Book Page**|**New Book Added Notification**|  |<img src=Screenshots/Deadline_Approaching_Notification.png width=""267"">|<img src=Screenshots/Fine_Increased_Notification.png width=""267"">| |:---:|:---:| |**Deadline Approaching Notification**|**Fine Increased Notification**|  ## Final Notes **Thanks for going through this Repository! Have a nice day.**</br> </br>**Got any Queries? Feel free to contact me.**</br> </br>**Saini Rohan Rao** <p align=""left""> <a href=""mailto:rohanrao619@gmail.com""><img src=""https://github.com/rohanrao619/Icons/blob/master/SVGs/Gmail.svg"" height =""45"" title=""Gmail"" alt=""mailto:rohanrao619@gmail.com""></a> <a href=""https://github.com/rohanrao619""><img src=""https://github.com/rohanrao619/Icons/blob/master/SVGs/GitHub.svg"" height =""45"" title=""GitHub"" alt=""https://github.com/rohanrao619""></a> <a href=""https://www.linkedin.com/in/rohanrao619""><img src=""https://github.com/rohanrao619/Icons/blob/master/SVGs/LinkedIn.svg"" height =""45"" title=""LinkedIn"" alt=""https://www.linkedin.com/in/rohanrao619""></a> <a href=""https://rohanrao619.github.io/""><img src=""https://github.com/rohanrao619/Icons/blob/master/SVGs/Portfolio.svg"" height =""45"" title=""Portfolio Website"" alt=""https://rohanrao619.github.io/""></a> </p>"
Novel Library App,Mobile Applications,https://github.com/gmathi/NovelLibrary,# Novel Library One stop for reading all novels  | Build | Support Server | |-------|---------| | 0.18.4.beta | [![Discord](https://img.shields.io/discord/339610451711361024.svg?label=discord&labelColor=7289da&color=00aa39&style=flat)](https://discord.gg/qFZX4vdEdF) | <br>  ## Supporting Files:  Download the files from below link to runt he app. Link: https://drive.google.com/drive/folders/0B2NcxiuA0KTIQnY0akJjb0NxWVE?usp=sharing 1. <b>google-services.json</b> - Copy this file to the project-root/<b>app</b> directory. 2. <b>keystore.properties</b> - Copy this file to the root directory of the project. <br>  ## For Extensions Repository:  https://github.com/gmathi/NovelLibrary-Extensions
Women Safety App ,Mobile Applications,https://github.com/HassamTalha/Amaan-Women-Safety-App,"<h1 align=""center"" style=""font-size: 52px;"" ><img height=30 src=""https://user-images.githubusercontent.com/43790152/136710076-c634d2bf-fdfa-439c-9696-c87a7d6b0f2d.jpg""> Amaan - You Deserve to be Safe!</h1>  This is Hackfair 2.0 Submission where team from Google DSC COMSATS University Islamabad developed a women safety app and played their part in women rights and empowerment.  ###  üîΩ[Download][download] Android APK (TESTING)  <br>  <img src=""https://user-images.githubusercontent.com/43790152/136709012-5e27b2f9-06d6-40fc-8ed0-26d67818e204.png"">  ## ‚ö†Ô∏è Problem Statement Unfortunately, over time, violence against the fair sex has been intensified which resulted in an obstacle for them to prosper and grow equitably. Women comprise more than 48% of the population of Pakistan which is nearly half of this state. But due to some conventions and annoying happenings, they are not able to take part in activities as much as men.   ## üòÉ Inspiration & Motivation The thing that inspired us all to work for women‚Äôs empowerment was their capability to perform and deliver, they are as important as men. We as a student, boys, and part of families wanted to contribute to creating a beneficial cause of empowering women and to give them a tap on the shoulder to keep going and not give up!  For this, we developed a mobile application named **‚ÄúAmaan‚Äù - You deserve to be Safe!**  It is a mobile application developed specifically for women. That lets them take a leap that they can count on someone and we made sure that ‚ÄúSOMEONE‚Äù will be none other but their own contacts which they can send alerts, locations, and much more.  ## üì± Features Following are the features available in Amaan: - **Get Home Safe**: Tracking user's location after specific time (Set by the user) - **Safe Shake**: Shake mobile device to send SOS alerts even if the app is closed! - **Helplines**: Contact nearest police stations, hospitals, fire brigade, pharmacies etc.  And related features to women safety :)  ## ü§î Features in Future - **Safe Audio**: Enable user's device audio for recording, that could be used as an evidence by respective department (e.g Police) - **Overcome SMS Charges**: We want to overcome the charges we required to send the SOS SMS in future in collaboration with our Government hopefully.  ## ‚öíÔ∏è Tools & Technology  - Adobe XD (Ui/Ux Designing) - Flutter (Mobile App Development)  ## üëÄ More Look & Feel  Here's what we have developed!  <img src=""https://user-images.githubusercontent.com/43790152/136709005-0d2444b1-0214-43f1-9a4f-7fc4400164fa.png"">  <img src=""https://user-images.githubusercontent.com/43790152/136709006-f2c5bd98-5c16-4f03-aae2-9449a7a3ddd1.png"">  <img src=""https://user-images.githubusercontent.com/43790152/136709008-974e5fde-ca20-492c-ae13-131b4c318c1f.png"">   [download]: https://drive.google.com/file/d/1tEdz0d6l70O9uDjRZzpHKsav-PiPmlan/view?usp=sharing"
Notes App,Mobile Applications,https://github.com/CheezyCode/YetAnotherNotesApp," # Yet Another Notes App  This is a simple note taking app in Android with Integrated API. In this project we have covered everything that is required to create a fully functional app.    ## Features  - MVVM Setup - HILT - Coroutines - API Integration using Retrofit - Interceptors for Authenticated Flows. - Handling validations and loading state. - Complete Signup/Login Flow    ## Demo  ![Demo](https://yt3.ggpht.com/SGWJWplTHs6omu72a_scFQDndpAfgwdj4Pct6Fs8BGO7gU9yyTMV2H4H0epk7OgnTxwFmsIFEP9D3Q=s640-c-fcrop64=1,00000000ffffffff-nd-v1-rwa)   ## üöÄ About this series This series is divided into 2 parts -     - Android App   - Rest API (Node JS)  Tutorials for both of them are available on our channel - CheezyCode.  <a href=""https://www.youtube.com/watch?v=XB247JIDmUI&list=PLRKyZvuMYSIMjYhIwc6vP2eVb9JI6Phsv"" target=""blank"">Rest API Tutorial</a>    ## üîó Links [![portfolio](https://img.shields.io/badge/youtube-ff0000?style=for-the-badge&logo=youtube&logoColor=white)](https://www.youtube.com/c/CheezyCode/) [![instagram](https://img.shields.io/badge/instagram-0A66C2?style=for-the-badge&logo=instagram&logoColor=white)](https://www.instagram.com/cheezycode) [![twitter](https://img.shields.io/badge/twitter-1DA1F2?style=for-the-badge&logo=twitter&logoColor=white)](https://twitter.com/cheezycode) "
Snake Game,Mobile Applications,https://github.com/codepath/android_snake_game,android_snake_game ==================  Simple 2D Snake Game for Android  The game is like the game we use to play in our android phones in our 90s.A very good game to play and have time pass.  You need to also get https://github.com/codepath/android_simple_game_engine and copy the .java files into this directory structure.
Shopping App ,Mobile Applications,https://github.com/alireza4585/Flutter-Shopping-App,"<h1>shopping app</h1>  <h3> this project i creat an awesome  Shopping App UI and backend with flutter and firebase </h3> <p> tutorial : https://www.youtube.com/playlist?list=PLNtn1FgzEKBA2O8caXFup7qQ_gbkMxbjo </p> <hr> <div style = """">  <img src=""https://github.com/alireza4585/ecommerce1/assets/102475069/b3a289a6-9d19-4e5c-9bcb-ccd80ba8b3de"" alt=""Screenshot_1666104775"" width=""32%""/> <img src=""https://github.com/alireza4585/ecommerce1/assets/102475069/fddd1339-714a-4a26-96b9-0bf93dd4cc6e"" alt=""Screenshot_1659640778"" width=""32%""/> <img src=""https://github.com/alireza4585/ecommerce1/assets/102475069/aa54dc35-8e7f-425e-be89-3492615b39d9"" alt=""Screenshot_1666104775"" width=""32%""/>  </div>"
Flappy Bird,Mobile Applications,https://github.com/VadimBoev/FlappyBird,"<img src=""flappy.gif"" alt=""game"" width=""250px"">  **[Readme –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ](README_RU.md)**      [Dev blog in Telegram (ENG/RU)](https://t.me/boevdev)  # üê¶ Flappy Bird in C: APK Size < 100 Kilobytes! üöÄ  ## üìú History:  It all started in 2021 when I stumbled upon [rawdrawandroid](https://github.com/cnlohr/rawdrawandroid). My goal was simple: to create a game with the minimal APK size, yet still be understandable and interesting. üéØ  The idea of making a Flappy Bird clone seemed logical, given that this game had already been ported to many languages. üê§  In 2021, I also studied [Raylib](https://github.com/raysan5/raylib), but my first attempt to make a game in C++ using [ImGui](https://github.com/ocornut/imgui/) failed. üíî  The problems were everywhere: the APK size was around 1 MB, the game crashed, and the APK only contained the armeabi-v7a library (Google's 2022 requirements state that the arm64-v8a library must be included!). ü§Ø  ## üí° Motivation:  In September 2024, seeing Flappy Bird in C# in the Raylib Discord channel, I decided to try implementing this game in C for Android with an APK size of less than 100 KB. üöÄ    The idea seemed crazy, but the competitive interest took over. üí™  ## üõ†Ô∏è Implementation:  I started by compiling a ""Hello World"" in C and packaging the library into an APK. üì¶  Sounds were compressed to MP3 format, and OpenSLES was used for playback. üéµ  For PNG file decoding, I chose [upng](https://github.com/elanthis/upng). üñºÔ∏è  All of this was combined using OpenGL ES 2, shaders, and Android Native Activity. üéÆ  ## üîß Build:  1. Download [Visual Studio 2022](https://visualstudio.microsoft.com/). 2. Install components: C++ Desktop Development and C++ Mobile Development. 3. Download Android Studio (for apktool, sdk, ndk). 4. Install NDK 27.1.12297006 and Android SDK Platform 30. 5. Configure the project for ""Debug ARM"" and make changes to build.bat. 6. Compile via CTRL + B. üõ†Ô∏è  ## üìÑ Copyright:  I do not claim any copyright. The rights to the game and resources belong to **DotGEARS**. üìú  ## üåü Inspiration:  - [rawdrawandroid](https://github.com/cnlohr/rawdrawandroid) - [Flapper](https://github.com/its-Lyn/Flapper) - [Raylib](https://github.com/raysan5/raylib) - [ImGui](https://github.com/ocornut/imgui/)  ## üå† Star History  [![Star History Chart](https://api.star-history.com/svg?repos=VadimBoev/FlappyBird&type=Timeline)](https://star-history.com/#VadimBoev/FlappyBird&Timeline)  ---  üéâ Enjoy the game and don't forget to star it! üåü"
Alarm App,Mobile Applications,https://github.com/Adw41t/Simple_Alarm_Clock,"# Simple Alarm Clock Simple Alarm Clock is a small application showing MVVM architecture and Room persistence library implementation. This app allows user to set one-time or repeating alarms with a simple UI.  <img src=""screenshots/Screenshot_20200929-194719_Simple Alarm Clock.jpg"" height=""400"" alt=""Screenshot""/> <img src=""screenshots/Screenshot_20200929-194746_Simple Alarm Clock.jpg"" height=""400"" alt=""Screenshot""/> <img src=""screenshots/Screenshot_20200929-195011_Simple Alarm Clock.jpg"" height=""400"" alt=""Screenshot""/>"
Stopwatch App,Mobile Applications,https://github.com/stevdza-san/Stopwatch-Android-App,"# Stopwatch - Android App with Jetpack Compose  A simple stopwatch application with jetpack compose using foreground and bound services   <p align=""center"">   <img src=""https://github.com/stevdza-san/Stopwatch-Android-App/blob/master/Stopwatch.png"" href=""https://youtu.be/JflJjPxhFQo""> </p> <p align=""left"">   <a href=""https://youtu.be/JflJjPxhFQo"" align=""center"">Check YouTube for Complete Video Tutorial</a> </p>  This application basically consists of 3 different textunits representing Hours, Minutes and Seconds. Also we got two buttons on the UI. The first button will be changed on the basis of service state and the second button will allow us to cancel the foreground service which will result in resetting of our stopwatch.  You can control the stopwatch either by the UI buttons or with the notification panel. As we are using services, so if we even close the application then also we can track the stopwatch state by notification panel.   ## This project uses:   * Dagger-Hilt * Bound Service * Foreground Service   ### Dagger-Hilt   Hilt is a dependency injection library for Android that reduces the boilerplate of doing manual dependency injection in your project. Doing manual dependency injection requires you to construct every class and its dependencies by hand, and to use containers to reuse and manage dependencies.  Hilt provides a standard way to use DI in your application by providing containers for every Android class in your project and managing their lifecycles automatically. Hilt is built on top of the popular DI library Dagger to benefit from the compile-time correctness, runtime performance, scalability, and Android Studio support that Dagger provides.   ### Bound Service   A bound service is an implementation of the Service class that allows other applications to bind to it and interact with it. To provide binding for a service, you must implement the onBind() callback method. This method returns an IBinder object that defines the programming interface that clients can use to interact with the service.  ### Foreground Service   Foreground services perform operations that are noticeable to the user.  Foreground services show a status bar notification, so that users are actively aware that your app is performing a task in the foreground and is consuming system resources.  Devices that run Android 12 (API level 31) or higher provide a streamlined experience for short-running foreground services. On these devices, the system waits 10 seconds before showing the notification associated with a foreground service. There are a few exceptions; several types of services always display a notification immediately."
Calendar App,Mobile Applications,https://github.com/SimpleMobileTools/Simple-Calendar,"# Simple Calendar <img alt=""Logo"" src=""graphics/icon.png"" width=""120"" />  Simple Calendar 2023 is a highly customizable, offline monthly calendar app for Android. Have an agenda planner in your pocket, designed to do exactly what a personal tiny schedule planner should do in 2023. No complicated features, unnecessary permissions, or ads!   ***Supports syncing events via Google Calendar  Download now!  Daily Digital Schedule App: Take Control of Your Time Whether you are looking for a work calendar for business, a day planner, an appointment scheduler, or organization and scheduling of single and recurring events like birthdays, anniversary, appointment reminder, or anything else, Simple Calendar 2023 makes it easy to stay organized. The calendar widget has an incredible variety of customization options: customize event reminders, notification appearance, tiny calendar reminders widget, and overall appearance.  Schedule Planner: Plan Your Day Appointment scheduler, monthly planner, and family organizer in one! Check your upcoming agenda, schedule business meetings, and events & book appointments easily. Reminders will keep you on time and informed on your daily schedule app. This 2023 calendar widget is remarkably easy to use. You can even view everything as a simple list of events rather than in a monthly view, so you know exactly what‚Äôs coming up in your life and how to organize and plan your agenda.   Simple Calendar 2023 Features  ‚úîÔ∏è The Best User Experience   ‚ûï No ads or annoying popups, truly great user experience!   ‚ûï No internet access is needed, giving you more privacy, security, and stability  ‚úîÔ∏è Flexibility for Your Productivity   ‚ûï Calendar Widget supports exporting & importing events via .ics files   ‚ûï Export settings to .txt files to import to another device   ‚ûï Flexible event creation ‚Äì times, duration, reminders, powerful repetition rules   ‚ûï CalDAV support for syncing events via Google Calendar, Microsoft Outlook, Nextcloud, Exchange, etc  ‚úîÔ∏è Personalized Just for You   ‚ûï Schedule planner - customize and change sound, looping, audio stream, vibrations   ‚ûï Calendar widget - Colorful Calendars and customizable themes   ‚ûï Open source tiny calendar, translated into 30+ languages   ‚ûï Plan your day with others - ability to share events fast on social media, emails, etc   ‚ûï Family Organizer - with hasslefree event duplication, organization, and time management  ‚úîÔ∏è Organization and Time Management   ‚ûï Day planner - the agenda planner will help you to organize your day   ‚ûï Weekly planner - staying ahead of your busy weekly schedule has never been easier   ‚ûï Itinerary manager - business calendar shared between teams at work   ‚ûï Appointment scheduler - organize and maintain your agenda with ease   ‚ûï Planning app - easy to use personal event, appointment reminder, and schedule planner   ‚ûï Plan your day - manage your day with this android schedule planner, event & family organizer  ‚úîÔ∏è #1 Calendar App   ‚ûï Import holidays, contact birthdays, and anniversaries easily   ‚ûï Filter personal events quickly by event type   ‚ûï Daily schedule and event location, shown on a map   ‚ûï Quick business calendar, or personal digital agenda   ‚ûï Quickly switch between daily, weekly, monthly, yearly & event views  DOWNLOAD SIMPLE CALENDAR PLANNER ‚Äì OFFLINE SCHEDULE AND AGENDA PLANNER WITH NO ADS! PLAN YOUR 2023 TIMETABLE!  <a href=""https://f-droid.org/packages/com.simplemobiletools.calendar.pro"">Get it on F-Droid</a>  Support us:   IBAN: SK4083300000002000965231   Bitcoin: 19Hc8A7sWGud8sP19VXDC5a5j28UyJfpyJ   Ethereum: 0xB7a2DD6f2408Bce77334655CF5E7639aE31feb30   Litecoin: LYACbHTKaM9ZubKQGxJ4NRyVy1gHUuztRP   Bitcoin Cash: qz6dvmhq5vzkcsypxpp2mnur30muxdah4gvulx3y85   Tether: 0x250f9cC32863E59b87037a14955Ed64F879653F0   <a href=""https://paypal.me/SimpleMobileTools?country.x=SK&locale.x=en_US"">PayPal</a>   <a href=""https://www.patreon.com/tiborkaputa"">Patreon</a>  <div style=""display:flex;""> <img alt=""App image"" src=""fastlane/metadata/android/en-US/images/phoneScreenshots/1_en-US.jpeg"" width=""30%""> <img alt=""App image"" src=""fastlane/metadata/android/en-US/images/phoneScreenshots/2_en-US.jpeg"" width=""30%""> <img alt=""App image"" src=""fastlane/metadata/android/en-US/images/phoneScreenshots/4_en-US.jpeg"" width=""30%""> </div>"
WhatsApp Clone (+Backend),Mobile Applications,https://github.com/SekhGulamMainuddin/WhatsAppCloneFlutter,"# Whatsapp Clone using Flutter ### Learning Flutter Thanks to [Rivaan Ranawat](https://github.com/RivaanRanawat). I followed his tutorial to build this project - May 24, 2023.  This app is the clone of WhatsApp. It consists of features of WhatsApp Mobile like ___Chatting___, ___Single & Group Chat___, ___Video & Audio Call___, content sharing like ___GIF___, ___Images___, ___Videos___, etc. The backend is build on Firebase and Video & Audio Calling feature is implemented using Agora.    <p><br></p>      <h3 align=""left"">Languages and Tools Used:</h3>  <p>    <a href=""https://flutter.dev"" target=""_blank"" rel=""noreferrer""> <img src=""https://www.vectorlogo.zone/logos/flutterio/flutterio-icon.svg"" alt=""flutter"" width=""60"" height=""60""/> </a> &nbsp;    <a href=""https://dart.dev"" target=""_blank"" rel=""noreferrer""> <img src=""https://dart.dev/assets/img/shared/dart/logo+text/horizontal/white.svg"" alt=""dart"" width=""100"" height=""60""/> </a> &nbsp;   <a href=""https://riverpod.dev/"" target=""_blank"" rel=""noreferrer""> <img src=""https://riverpod.dev/img/logo.png"" alt=""dart"" width=""60"" height=""60""/> </a> &nbsp;    <a href=""https://firebase.google.com/"" target=""_blank"" rel=""noreferrer""> <img src=""https://www.vectorlogo.zone/logos/firebase/firebase-icon.svg"" alt=""firebase"" width=""75"" height=""75""/> </a> &nbsp;   <a href=""https://www.agora.io/en/"" target=""_blank"" rel=""noreferrer""> <img src=""https://www.agora.io/en/wp-content/themes/agora-main/images/agora-logo.svg"" alt=""agora"" width=""100"" height=""60""/> </a> &nbsp;  </p>    <p><br></p>  # Libraries Used -  1. [Firebase Core](https://pub.dev/packages/firebase_core) 2. [Firebase Auth](https://pub.dev/packages/firebase_auth) 3. [Cloud Firestore](https://pub.dev/packages/cloud_firestore) 4. [Firebase Storage](https://pub.dev/packages/firebase_storage) 5. [Country Picker](https://pub.dev/packages/country_picker) 6. [Flutter Riverpod](https://pub.dev/packages/flutter_riverpod) 7. [Image Picker](https://pub.dev/packages/image_picker) 8. [Flutter Contacts](https://pub.dev/packages/flutter_contacts) 9. [UUID](https://pub.dev/packages/uuid) 10. [Intl](https://pub.dev/packages/intl) 11. [Cached Network Image](https://pub.dev/packages/cached_network_image) 12. [Cached Video Player](https://pub.dev/packages/cached_video_player) 13. [Emoji Picker](https://pub.dev/packages/emoji_picker_flutter) 14. [Enough Giphy](https://pub.dev/packages/enough_giphy_flutter) 15. [Flutter Sound](https://pub.dev/packages/flutter_sound) 16. [Path Provider](https://pub.dev/packages/path_provider) 17. [Permission Handler](https://pub.dev/packages/permission_handler) 18. [Audip Players](https://pub.dev/packages/audioplayers) 19. [Swipe To](https://pub.dev/packages/swipe_to) 20. [Story View](https://pub.dev/packages/story_view) 21. [Agora](https://pub.dev/packages/agora_uikit)              "
Twitter Clone (+Backend),Mobile Applications,https://github.com/Chevinjeon/TwitterClone,"# X Clone ![thumbnail](https://github.com/Chevinjeon/TwitterClone/assets/109643560/bdf7c576-360d-4500-bb69-e75b22627c29)  With Twitter's recent rebranding to X, this platform now has the largest user base ever for limitless interactivity. In homage to the iconic bird, I ventured to construct my own social network. Armed with Android development and the elegance of Kotlin, I harnessed the Firebase APIs as the backbone of my network's infrastructure. This enabled seamless user management, login and signout functionalities, user profiles, tweets and retweets, home activities, and fragment screens.    **The UI of the app has been updated in v2 according to the UI of Twitter's Android app v7.24.1** **The dependencies have been updated and the project has been migrated to AndroidX in November 2020.** | Version 1  | Version 2 (Twitter v7.24.1) | | -------------  | -------------| | ![twitter-ui-v1](https://github.com/Chevinjeon/TwitterClone/assets/109643560/1c51b600-ab46-42f1-a658-d7bb1693b3af) |  ![twitter-ui-v2](https://github.com/Chevinjeon/TwitterClone/assets/109643560/881157d4-0499-4272-838a-b72f663697c1) |   # Prerequisites  Having **JDK 8 or higher** installed alongside **Android Studio 3.0 or higher** is a major prerequisite as lambda expressions used in this project are not supported in the older versions of Kotlin/Java and Android Studio.   # Android Versioning Information  | Specification | Setting | | -------------  | -------------| | Target SDK Version | 30 (Android Oreo) | | Minimum SDK Version | 19 (Android KitKat) | | Build Tools Version | 26.0.2 | | Gradle Version | 6.7.1 | | Gradle Plugin Version | 4.1.1 |  # Libraries Used - [Android-Iconics](https://github.com/mikepenz/Android-Iconics) for the UI icons - [MaterialDrawer](https://github.com/mikepenz/MaterialDrawer) for the side navigation drawer - [RoundedImageView](https://github.com/vinc3m1/RoundedImageView) for the rounded images  # Automatic Restart Spring will automatically update your changes and restart the application. To use this, you need to run gradle.  Open two terminals.  terminal A ``` $ gradle bootRun ``` Edit your code after running gradle. you no longer need to bootRun again. your changes will be automatically updated  Terminal B ``` $ gradle classes ```     # Architecture  ![architecture](https://github.com/Chevinjeon/TwitterClone/assets/109643560/6285c64e-8032-4f08-be49-8d9c644830a0)   ### Let's delve into the technological marvels that underpin this endeavour: | Main Features  | Description  |  Preview  |  | -------------  | -------------| ------------- | | Authentication | Create a new user by using the Firebase Authentication service  | ![Frame 75](https://github.com/Chevinjeon/TwitterClone/assets/109643560/2b9923d8-083d-412a-bb6d-2b62c05552bd) | | User Profiles   | A tab to display user details and display all posts made by the user  | ![Frame 74](https://github.com/Chevinjeon/TwitterClone/assets/109643560/4d10897d-952a-4a9f-b55a-41a8966f15f4) | | DM | Users can follow other users and send direct messages | ![Twitter Messages (1)](https://github.com/Chevinjeon/TwitterClone/assets/109643560/3bc5457a-8205-4506-95b6-81a548ebb789) | | Create New Post | Functionality to upload new image/video as post |![Twitter Tweeting](https://github.com/Chevinjeon/TwitterClone/assets/109643560/8421ac05-1f8d-4bdd-a40c-80d7f7148e28) | "
Instagram Clone (+Backend),Mobile Applications,https://github.com/mitchtabian/Android-Instagram-Clone,"<p>***</p> <p>***</p> <p>***</p> <p>***</p> <p>***</p> <b>WARNING: I do not maintain this code and there are MANY bad practice methods. I do not recommend copying any of this code.</b> <p>***</p> <p>***</p> <p>***</p> <p>***</p> <p>***</p> <br><br> <a href='https://www.youtube.com/watch?v=qpJRgr6HzAw&list=PLgCYzUzKIBE9XqkckEJJA0I1wVKbUAOdv' target='_blank'> <img class='header-img' src='https://s3.amazonaws.com/codingwithmitch-static-and-media/media/instagram-clone/images/Instagram.png' /> </a>  <h1>Android Instagram Clone Course</h1> <h4>A step-by-step guide to build your own Instagram Clone</h4> <h4>Watch it here: <a href='https://www.youtube.com/watch?v=qpJRgr6HzAw&list=PLgCYzUzKIBE9XqkckEJJA0I1wVKbUAOdv' target='_blank'>Instagram Clone Course</a></h4> <h4>WARNING: I do not maintain this code and I used many poor coding practices.  <br><br> For courses that use Android best practices see the courses on my website: <a href=""https://codingwithmitch.com/courses/"" target=""_blank"">CodingWithMitch.com</a></h4> <hr>  <p> In the course we'll be using: <ul>   <li>Firebase Email Authentication</li>   <li>Firebase Database</li>   <li>Firebase Cloud-Storage</li> </ul> <p/>  <p>To keep things simple and condensed I'll be using Firebase for everything.</p>  <h2>Lecture Source Code:</h2> <ol> <li><a href='https://github.com/mitchtabian/Android-Instagram-Clone/tree/2cac213283ceafe3b1c627096065bd11f80d4161'> Getting Started</a></li>  <li><a href='https://github.com/mitchtabian/Android-Instagram-Clone/tree/0f0377337204105604e23f08d3939b5be1556684'> Toolbars and NavigationView</a></li>  <li><a href='https://github.com/mitchtabian/Android-Instagram-Clone/tree/69743899065c2b921f99dde9a2aabb5c8f8adc70'> Customizing the BottomNavigationView</a></li>  <li><a href='https://github.com/mitchtabian/Android-Instagram-Clone/tree/b42ec4471f1a63c8d6463783b23ca558c12381c4'> BottomNavigationView Activities</a></li>  <li><a href='https://github.com/mitchtabian/Android-Instagram-Clone/tree/1cbeb887a4e8cab0e319b50d3506cf2977813723'> Organizing Things and Tab-Prep</a></li>  <li><a href='https://github.com/mitchtabian/Android-Instagram-Clone/tree/37a7d8091e96bc0316a7f456b4451fb279d797b5'> SectionsPagerAdapter (Home Screen Tabs)</a></li>  <li><a href='https://github.com/mitchtabian/Android-Instagram-Clone/tree/82996f0a25b98340d4b249eafc904495ea3989ee'> Profile Toolbar and Menu</a></li>  <li><a href='https://github.com/mitchtabian/Android-Instagram-Clone/tree/766ec53ac97cef3d4edccec18819bea2a5825be0'> Building the Profile Part 1</a></li>  <li><a href='https://github.com/mitchtabian/Android-Instagram-Clone/tree/75567884e79c207bd7ddaf9695cfe6b5cfa0f85f'> Building the Profile Part 2</a></li>  <li><a href='https://github.com/mitchtabian/Android-Instagram-Clone/tree/194fbc7e36d15f4ac7656b90d7a3c982ef703a01'> Account Settings Layout</a></li>  <li><a href='https://github.com/mitchtabian/Android-Instagram-Clone/tree/07da29439db27d2bb1a725567ce7805d1601564a'> Account Settings Navigation</a></li>  <li><a href='https://github.com/mitchtabian/Android-Instagram-Clone/tree/b5b71da913bdd831c341dc825ab5cf9844559b1b'> Account Settings Fragments</a></li>  <li><a href='https://github.com/mitchtabian/Android-Instagram-Clone/tree/b29cf20ac12e0d9cda259e3bf0e360ad82544f44'> EditProfile Fragment Layout</a></li>  <li><a href='https://github.com/mitchtabian/Android-Instagram-Clone/tree/06fbce53308bebfdc215a0d997499cf405443b1b'> Universal Image Loader Config</a></li>  <li><a href='https://github.com/mitchtabian/Android-Instagram-Clone/tree/9391edbd31e47ff72773fa2bfe710b086a0046e4'> Testing Images in the User Profile</a></li>  <li><a href='https://github.com/mitchtabian/Android-Instagram-Clone/tree/df348142edcd5e8f76171ad00e5f317f98d19bd5'> Square ImageView Widgets</a></li>  <li><a href='https://goo.gl/Kf8UN8'> Login Layout</a></li>  <li><a href='https://goo.gl/rCfjWv'> Register Layout</a></li>  <li>Get Started with Firebase</li>  <li><a href='https://goo.gl/zTsY17'> Setup Firebase Authentication</a></li>  <li><a href='https://goo.gl/9NmTGC'> Testing Firebase Authentication</a></li>  <li><a href='https://goo.gl/dgWBAF'> Setup Register Activity Widgets</a></li>  <li><a href='https://goo.gl/1Ewh7z'> Register New User with Firebase</a></li>  <li>Firebase Database Structure (no source code) </li>  <li><a href='https://goo.gl/AiqA4A'> Check if Username Already Exists</a></li>  <li><a href='https://goo.gl/8D6cSx'> Insert New Data</a></li>  <li><a href='https://goo.gl/y9SJqE'> Email Verificaiton</a></li>  <li><a href='https://goo.gl/ieYAVE'> Enable User Signout</a></li>  <li><a href='https://goo.gl/ReuZZU'> Profile Fragment</a></li>  <li><a href='https://goo.gl/LQViwp'> Profile Fragment Setup</a></li>  <li><a href='https://goo.gl/dcdw5J'> Retrieving User Data from Firebase</a></li>  <li><a href='https://goo.gl/hzg86h'> Setting Profile Fragment Widgets</a></li>  <li><a href='https://goo.gl/ibo3Hh'> Navigating to EditProfile Fragment</a></li>  <li><a href='https://goo.gl/Yupdcy'> Setting EditProfile Fragment Widgets</a></li>  <li><a href='https://goo.gl/gLgHYX'> Saving User Profile Changes</a></li>  <li><a href='https://goo.gl/8LgdWM'> Query Firebase Database</a></li>  <li><a href='https://goo.gl/mDeYzA'> Changing Firebase Authenticated Email (part 1)</a></li>  <li><a href='https://goo.gl/uccU2R'> Changing Firebase Authenticated Email (part 2)</a></li>  <li><a href='https://goo.gl/TWtdtr'> Changing Firebase Authenticated Email (part 3)</a></li>  <li><a href='https://goo.gl/j7vXME'> Update User Account Settings</a></li>  <li><a href='https://goo.gl/AqhtLL'> Verifying Permissions for Sharing</a></li>  <li><a href='https://goo.gl/ifPz9N'> ShareActivity Layout and Tabs</a></li>  <li><a href='https://goo.gl/C3ft9K'> Setup Gallery and Photo Fragments</a></li>  <li><a href='https://goo.gl/29ufSk'> Camera Intent</a></li>  <li><a href='https://goo.gl/FKqjXX'> GalleryFragment Layout</a></li>  <li><a href='https://goo.gl/9MZRWd'> Phone Directories</a></li>  <li><a href='https://goo.gl/CJaUGM'> ShareActivity GridView</a></li>  <li><a href='https://goo.gl/RgpgN2'> Selected Image to Share</a></li>  <li><a href='https://goo.gl/oRKmRj'> NextActivity Setup</a></li>  <li>How to upload images to Firebase Storage (no source code)</li>  <li><a href='https://goo.gl/mGZgB5'> Getting the Image Count</a></li>  <li><a href='https://goo.gl/6t157B'> Firebase Storage Reference</a></li>  <li><a href='https://goo.gl/YTU5ND'> Convert Bitmap to Byte Array</a></li>  <li><a href='https://goo.gl/epw8Xa'> Upload Photo to Firebase Storage</a></li>  <li><a href='https://goo.gl/f2RxFS'> Insert Photo into Firebase Database</a></li>  <li><a href='https://goo.gl/BEZLor'> Changing Profile Photo</a></li>  <li><a href='https://goo.gl/iq2ZTH'> Upload New Profile Photo to Firebase Storage</a></li>  <li><a href='https://goo.gl/1tNrCV'> Fixing the Navigation</a></li>  <li><a href='https://goo.gl/PrZP7r'> New Profile Photo Using Camera</a></li>  <li><a href='https://goo.gl/yTZtnM'> Share Photo using Camera</a></li>  <li><a href='https://goo.gl/bZc1qN'> Populating User Profile Gridview</a></li>  <li><a href='https://goo.gl/4swnKB'> Activity Animations</a></li>  <li><a href='https://goo.gl/3byc52'> Post Viewing Layout (part1)</a></li>  <li><a href='https://goo.gl/a2HPfb'> Post Viewing Layout (part2)</a></li>  <li><a href='https://goo.gl/3bJ78v'> GridImage Selection Interface</a></li>  <li><a href='https://goo.gl/MTJN4g'> Retrieving Post Image From Bundle</a></li>  <li><a href='https://goo.gl/6rdyox'> Image Post Date</a></li>  <li><a href='https://goo.gl/xF1RKF'> Query Photo Details</a></li>  <li><a href='https://goo.gl/cCbDuq'> Likes Toggle Part1</a></li>  <li><a href='https://goo.gl/GRgVyQ'> Likes Toggle Part2</a></li>  <li><a href='https://goo.gl/Fn39Cr'> Likes Toggle Part3</a></li>  <li><a href='https://goo.gl/5kaxZ2'> Likes Toggle Part4</a></li>  <li><a href='https://goo.gl/T8UUE6'> Likes Toggle Part5</a></li>  <li><a href='https://goo.gl/bzTqMP'> Testing Instagram Likes</a></li>  <li><a href='https://goo.gl/z1Ve71'> Comments Layout</a></li>  <li><a href='https://goo.gl/9QkStK'> Comments ListAdapter part1</a></li>  <li><a href='https://goo.gl/A1fRbt'> Comments ListAdapter part2</a></li>  <li><a href='https://goo.gl/PDWjup'> Displaying the First Comment</a></li>  <li><a href='https://goo.gl/NrYk1L'> Inserting Comments</a></li>  <li><a href='https://goo.gl/NXoATT'> Reading Comments</a></li>  <li><a href='https://goo.gl/3WL56A'> Finishing Comments (IMPORTANT)</a></li>  <li><a href='https://goo.gl/nHN42P'> Search Activity Layout</a></li>  <li><a href='https://goo.gl/CC1WvX'> Searching for Users</a></li>  <li><a href='https://goo.gl/fFPB8M'> Viewing User Profiles</a></li>  <li><a href='https://goo.gl/Wu6P9m'> View Profile Fragment</a></li>  <li><a href='https://goo.gl/JfMbJW'> Following Users (part1)</a></li>  <li><a href='https://goo.gl/4LF6aC'> Following Users (part2)</a></li>  <li><a href='https://goo.gl/zhTqSF'> Fixing a Few Bugs</a></li>  <li><a href='https://goo.gl/j8TFtk'> Mainfeed ListAdapter (part 1/2)</a></li>  <li><a href='https://goo.gl/ejjmZu'> Mainfeed ListAdapter (part 2/2)</a></li>  <li><a href='https://goo.gl/f2cBVJ'> Displaying Posts in the Main Feed (part 1/3)</a></li>  <li><a href='https://goo.gl/9SYKS2'> Displaying Posts in the Main Feed (part 2/3)</a></li>  <li><a href='https://goo.gl/82m8Yc'> Displaying Posts in the Main Feed (part 3/3)</a></li>  <li><a href='https://goo.gl/E6hqLT'> ListView Pagination and Bug Fixes </a></li>  <li>End?!</li> </ol> "
Weather App,Mobile Applications,https://github.com/dev-aniketj/WeatherApp-Android,"# Weather App üåßÔ∏èüåßÔ∏èüíôüíô  ![Platform](https://img.shields.io/badge/platform-Android-brightgreen.svg?color=00ADB5&style=for-the-badge) ![Repo Size](https://img.shields.io/github/repo-size/dev-aniketj/Weather-App?color=00ADB5&style=for-the-badge)  ## Preview  <img src=""https://github.com/dev-aniketj/Weather-App/blob/master/SS/gif1.gif"" width=""200""/>  ## Screenshots  <p float=""left""> 	<img src=""https://github.com/dev-aniketj/Weather-App/blob/master/SS/image1.jpg"" width=""200""/> 	<img src=""https://github.com/dev-aniketj/Weather-App/blob/master/SS/image2.jpg"" width=""200""/> </p>   #### Simple and Beautiful Weather App using Java.  I am using **https://openweathermap.org/** to get all the data using JSON file.  ### Steps :  > First, you have to create a account on it.  > Then, generate a a unique API key to get all the data from the JSON file.  > Paste you **API KEY** in **_LocationCord.java_** file as  ``` public final static String API_KEY = ""81a26c8f0de407b94623e9f43e825679""; ```  <br/>  #### API key calling from this website : **https://openweathermap.org/api/one-call-3**  #### The One Call API provides the following weather data for any geographical coordinates:  - Current weather - Minute forecast for 1 hour - Hourly forecast for 48 hours - Daily forecast for 8 days - National weather alerts - Historical weather data for 40+ years back (since January 1, 1979)  ##### Note :  > Single API key on have  ## Contributing  Please fork this repository and contribute back. Any contributions, large or small, major or minor features, bug fixes, are welcomed and appreciated but will be thoroughly reviewed.  ## Support  [![""Buy Me A Coffee""](https://www.buymeacoffee.com/assets/img/custom_images/orange_img.png)](https://www.buymeacoffee.com/aniketjain)"
Quiz App,Mobile Applications,https://github.com/hellosagar/Quiz-App,"## Quiz App A Quiz Android application üì± built using Java ‚ô®Ô∏è and showing best practices of üõ†Ô∏è Room -------------------  ## ‚ù§Ô∏è Try App ## Scan QR Code  <img src=""https://raw.githubusercontent.com/pikachu404/Quiz-App/master/screenshots/qrCodeForApk.png"" width=""200"" height=""200"">  --------------- ### Get Apk [Download here](https://raw.githubusercontent.com/pikachu404/Quiz-App/master/app/release/app-release.apk) ------------ ## ‚öôÔ∏è Features * App consists of Quiz of primarily three subject - Maths, Geography, Literature * Maintaining history of previous attempts of quiz using Room. * You can switch users using the Login/Register * Feature to change to your current password * Added loader while data is being fetched from API * Showing the result at the end the Quiz. * Implemented login, register,edit password, previous attempts all using SQL Lite DB using Room   ## üöÄ Technology Used  * Quiz App is build using Java * Asynctask for asynchronous * Room Persistence Library  ## üì∏ Screenshots  |||| |:----------------------------------------:|:-----------------------------------------:|:-----------------------------------------: | | ![Imgur](screenshots/0.png) | ![Imgur](screenshots/1.png) | ![Imgur](screenshots/2.png) | | ![Imgur](screenshots/3.png) | ![Imgur](screenshots/4.png) | ![Imgur](screenshots/5.png) | | ![Imgur](screenshots/6.png) | ![Imgur](screenshots/7.png) | ![Imgur](screenshots/8.png) | | ![Imgur](screenshots/9.png) | ![Imgur](screenshots/9.png) | ![Imgur](screenshots/10.png) |  ## ‚ö° Dependencies Used ```sh * Room Persistence Library 2.2.5 * Gson 2.8.6 ```  ## License ``` MIT License  Copyright (c) 2021 Sagar Khurana  Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.  THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. ```"
Stock Market Analyzer,Mobile Applications,https://github.com/rehmanis/stocks-android,"# Stocks Android App An android app for virtual stock trading.   ## Demo [![App Demo gif](./other/demo.gif)]()  ## Screen shots ### Home page  <p float=""left"">   <img src=""./other/home-page1.png"" width=""20%"" />   <img src=""./other/home-page2.png"" width=""20%"" />   <img src=""./other/home-page3.png"" width=""20%"" />   <img src=""./other/home-page4.png"" width=""20%"" /> </p>   ### Stock Details page <p float=""left"">   <img src=""./other/details-page1.png"" width=""20%"" />   <img src=""./other/details-page2.png"" width=""20%"" />   <img src=""./other/details-page3.png"" width=""20%"" />   <img src=""./other/details-page4.png"" width=""20%"" /> </p>   ## Summary This Android app provides a platform for stocks trading, including, features such as searching company stock details, buying/selling stocks, keeping track of stock porfolio/favorites, viewing stock SMA charts and news, allowing sharing news on twitter for a given stock. Custom Nodejs backend deployed using GCP is used for all API calls. The backend uses [Tingo API](https://api.tiingo.com/) for all stock related data, and [News API](https://newsapi.org/) for displaying stock related news. [Highcharts](https://www.highcharts.com/) is used for displaying the SMA chart data for a given ticker.   ## Java File descriptions * ```ApiCall.java```: the class that allows making api GET requests using Volley * ```AutoSuggestApdapter.java```: Adpater used for autocomplete search feature.  * ```Company.java```: used to create a Company object that includes the company name, ticker, number of company shares owned, last company price, price change,   * ``` News.java```: used to create a news object that will be used to display the list of news related to given stock * ```CompanyHeaderViewHolder.java```: the header view holder for the sectioned RecyclerView in home page. Currently we only have two sections portfolio and favorites. * ```CompanyItemViewHolder.java```: item view holder for a given section of RecyclerView. This is used to display the company information inside either portfolio or favorites * ```CustomDividerItemDecoration.java```: extends the DividerItemDecoration class for adding vertical lines between items in RecyclerView * ```CustomGridAdapter.java```: extends base adpater for storing company price summary details (such as last price, previous close, volume, bid size, etc) to be displayed using gridView * ```CustomNewsAdapter.java```: extends RecyclerView.Adapter<CustomNewsAdapter.ViewHolder> for storing a list of all news articles related for a given stock. Also contains the ViewHolder class extention of  RecyclerView.ViewHolder to be used to display these news item inside a RecyclerView * ```MainActivity.java```: extends AppCompatActivity. The main activity of the app that acts as the home page of the app. Home page contains toolbar for searching stocks, the current date, Networth of all the stocks plus excess cash (the app starts with 20,000 cash and no stocks), and portfolio and favorites section. * ```DetailsActivity.java```: extends AppCompatActivity. This is the stock details activity that is reached once the user clicks search icon in home page(the main activity) for a selected stock, or clicks on the company item in either protfolio or favorite section of home page. Contains stock summary, price details, options for trading this stock, and viewing stock related news article. * ```FavoriteSection.java```: extends io.github.luizgrp.sectionedrecyclerviewadapter.Section 3rd party Android library to create favorites section inside the home page Recyclerview * ```PortfolioSection.java```: extends io.github.luizgrp.sectionedrecyclerviewadapter.Section 3rd party Android library to create portfolio section inside the home page Recyclerview * ```ItemMoveCallback.java```: extends ItemTouchHelper.Callback to allow rearranging the positions of company items inside portfolio or favorites section * ``` SwipeToDeleteCallback.java```; extends ItemTouchHelper.Callback to allow removing stocks/company items from favorites sections only. * ```LocalStorage.java```: class used to manage the company data stored in the SharedPreferences. This is used to keep track of all the company items inside either portfolio section or favorites section so if user restarts the app, the stock inside portoflio or favorites section are not lost. Reinstalling the app will however, clear the local storage (SharedPreferences) * ```SplashActivity.java```: the very first activity that runs when the app is load. It display the app logo until the home page is loaded  ## Usage ### Running the app: * Clone the repo using `git clone https://github.com/rehmanis/stocks-android.git` * Open the app inside [Android Studio](https://developer.android.com/studio) * Make sure to use JDK 1.8.0 (The project uses Java 8) * In Android Studio under `File` click `Sync project with Grade Files`. * Add the `Pixel 2 XL 28` (App has not been tested with other emulator device) emulator device by clicking `AVD Manager` as shown below.     [![add emulator](./other/add-emulator.png)]().     * On the pop up window, click the `Create Virtual Device` button and add `Pixel 2 XL 28` [![add virtual device](./other/add-virtual-device.png)]() * Once you have successfully added the virtual device, you should see it as shown below. Now you can click the run button and it should load the emulator with the app. [![add emulator](./other/run-app.png)]() * The emulator will launch and will automatically open the stock app as shown below. [![lauched app](./other/lauch-app.png)]() * Note that **no API calls will work** i.e. the auto complete or search button will not work since the backend has been removed from `Google Cloud platform (GCP)`. You might have to either deploy the backend or try to figure out how to redirect the API call to a locally run backend. The code for backend can be found [here](https://github.com/rehmanis/stocks-angular/blob/master/routes/api.js) for the one used in this app. There is also a [serverless version](https://github.com/rehmanis/CSCI571-Stocks-Serverless/blob/master/index.js) for the backend. **Note** for any API that you use such as [`News API`](https://newsapi.org/) or [`Tingo`](https://api.tiingo.com/) for stocks, you will have to generate a token/API key and replace it in the backend code. As of now the backend is using a hardcoded token for these api in the NodeJs calls (very bad practice) as can be seen [here](https://github.com/rehmanis/stocks-angular/blob/master/routes/api.js#L27) where `token=` is hardcoded. * Places where `GCP` deployed backend API that was being used and is now removed from GCP are as follows:    * [DetailsActivity.java:L56-L59](https://github.com/rehmanis/stocks-android/blob/master/app/src/main/java/com/example/csci571andriodstocks/DetailsActivity.java#L56-L59)   * [MainActivity.java:L61-L62](https://github.com/rehmanis/stocks-android/blob/master/app/src/main/java/com/example/csci571andriodstocks/MainActivity.java#L61-L62)      ## License MIT License  Copyright (c) 2020 Shamsuddin Rehmani  Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.  THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.     "
Document Scanner,Mobile Applications,https://github.com/SDA-SE/document-scanner-android,"# Android Document Scanner  This is an Android library that lets you scan documents. You can use it to create apps that let users scan notes, homework, business cards, receipts, or anything with a rectangular shape.  ![Dollar Android](https://user-images.githubusercontent.com/26162804/160306955-af9c5dd6-5cdf-4e2c-8770-c734a594985d.gif)  ## Install  Open `build.gradle` and add this to `dependencies`  ```bash implementation 'com.websitebeaver:documentscanner:1.0.0' ```  ## Examples  * [Basic Example](#basic-example) * [Limit Number of Scans](#limit-number-of-scans) * [Remove Cropper](#remove-cropper) * [Java Example](#java-example)  ### Basic Example  ```kotlin package com.your.project  import android.graphics.BitmapFactory import android.os.Bundle import android.util.Log import android.widget.ImageView import androidx.appcompat.app.AppCompatActivity import org.sdase.submission.documentscanner.DocumentScanner  class MainActivity : AppCompatActivity() {     private lateinit var croppedImageView: ImageView      private val documentScanner = DocumentScanner(         this,         { croppedImageResults ->             // display the first cropped image             croppedImageView.setImageBitmap(                 BitmapFactory.decodeFile(croppedImageResults.first())             )         },         {             // an error happened             errorMessage -> Log.v(""documentscannerlogs"", errorMessage)         },         {             // user canceled document scan             Log.v(""documentscannerlogs"", ""User canceled document scan"")         }     )      override fun onCreate(savedInstanceState: Bundle?) {         super.onCreate(savedInstanceState)         setContentView(R.layout.activity_main)          // cropped image         croppedImageView = findViewById(R.id.cropped_image_view)          // start document scan         documentScanner.startScan()     } } ```  Here's what this example looks like with several items  <video src=""https://user-images.githubusercontent.com/26162804/160264222-bef1ba3d-d6c1-43c8-ba2e-77ff5baef836.mp4"" data-canonical-src=""https://user-images.githubusercontent.com/26162804/160264222-bef1ba3d-d6c1-43c8-ba2e-77ff5baef836.mp4"" controls=""controls"" muted=""muted"" class=""d-block rounded-bottom-2 border-top width-fit"" style=""max-height:640px;""></video>  <video src=""https://user-images.githubusercontent.com/26162804/160695710-d30ec1ad-0179-4200-be9f-94dcd5d3b920.mp4"" data-canonical-src=""https://user-images.githubusercontent.com/26162804/160695710-d30ec1ad-0179-4200-be9f-94dcd5d3b920.mp4"" controls=""controls"" muted=""muted"" class=""d-block rounded-bottom-2 border-top width-fit"" style=""max-height:640px;""></video>  <video src=""https://user-images.githubusercontent.com/26162804/160695805-992a0a66-a371-4783-afab-35366541afa4.mp4"" data-canonical-src=""https://user-images.githubusercontent.com/26162804/160695805-992a0a66-a371-4783-afab-35366541afa4.mp4"" controls=""controls"" muted=""muted"" class=""d-block rounded-bottom-2 border-top width-fit"" style=""max-height:640px;""></video>  <video src=""https://user-images.githubusercontent.com/26162804/160695839-4494f71b-db5b-4db5-9de6-00e5ede210fa.mp4"" data-canonical-src=""https://user-images.githubusercontent.com/26162804/160695839-4494f71b-db5b-4db5-9de6-00e5ede210fa.mp4"" controls=""controls"" muted=""muted"" class=""d-block rounded-bottom-2 border-top width-fit"" style=""max-height:640px;""></video>  <video src=""https://user-images.githubusercontent.com/26162804/160695861-0d807162-9649-481d-990d-8d031d1a3170.mp4"" data-canonical-src=""https://user-images.githubusercontent.com/26162804/160695861-0d807162-9649-481d-990d-8d031d1a3170.mp4"" controls=""controls"" muted=""muted"" class=""d-block rounded-bottom-2 border-top width-fit"" style=""max-height:640px;""></video>  ### Limit Number of Scans  You can limit the number of scans. For example if your app lets a user scan a business card you might want them to only capture the front and back. In this case you can set maxNumDocuments to 2.  ```kotlin package com.your.project  import android.graphics.BitmapFactory import android.os.Bundle import android.util.Log import android.widget.ImageView import androidx.appcompat.app.AppCompatActivity import org.sdase.submission.documentscanner.DocumentScanner import org.sdase.submission.documentscanner.constants.ResponseType  class MainActivity : AppCompatActivity() {     private lateinit var croppedImageView: ImageView      private val documentScanner = DocumentScanner(         this,         { croppedImageResults ->             // display the first cropped image             croppedImageView.setImageBitmap(                 BitmapFactory.decodeFile(croppedImageResults.first())             )         },         {             // an error happened             errorMessage -> Log.v(""documentscannerlogs"", errorMessage)         },         {             // user canceled document scan             Log.v(""documentscannerlogs"", ""User canceled document scan"")         },         ResponseType.IMAGE_FILE_PATH,         true,         2     )      override fun onCreate(savedInstanceState: Bundle?) {         super.onCreate(savedInstanceState)         setContentView(R.layout.activity_main)          // cropped image         croppedImageView = findViewById(R.id.cropped_image_view)          // start document scan         documentScanner.startScan()     } }  ```  <video src=""https://user-images.githubusercontent.com/26162804/160695619-b1931101-9196-49de-b30f-f63c4af8ce9d.mp4"" data-canonical-src=""https://user-images.githubusercontent.com/26162804/160695619-b1931101-9196-49de-b30f-f63c4af8ce9d.mp4"" controls=""controls"" muted=""muted"" class=""d-block rounded-bottom-2 border-top width-fit"" style=""max-height:640px;""></video>  ### Remove Cropper  You can automatically accept the detected document corners, and prevent the user from making adjustments. Set letUserAdjustCrop to false to skip the crop screen. This limits the max number of scans to 1.  ```kotlin package com.your.project  import android.graphics.BitmapFactory import android.os.Bundle import android.util.Log import android.widget.ImageView import androidx.appcompat.app.AppCompatActivity import org.sdase.submission.documentscanner.DocumentScanner import org.sdase.submission.documentscanner.constants.ResponseType  class MainActivity : AppCompatActivity() {     private lateinit var croppedImageView: ImageView      private val documentScanner = DocumentScanner(         this,         { croppedImageResults ->             // display the first cropped image             croppedImageView.setImageBitmap(                 BitmapFactory.decodeFile(croppedImageResults.first())             )         },         {             // an error happened             errorMessage -> Log.v(""documentscannerlogs"", errorMessage)         },         {             // user canceled document scan             Log.v(""documentscannerlogs"", ""User canceled document scan"")         },         ResponseType.IMAGE_FILE_PATH,         false     )      override fun onCreate(savedInstanceState: Bundle?) {         super.onCreate(savedInstanceState)         setContentView(R.layout.activity_main)          // cropped image         croppedImageView = findViewById(R.id.cropped_image_view)          // start document scan         documentScanner.startScan()     } } ```  <video src=""https://user-images.githubusercontent.com/26162804/160695550-7005f0cc-597d-4b96-8a6c-867d34fb6969.mp4"" data-canonical-src=""https://user-images.githubusercontent.com/26162804/160695550-7005f0cc-597d-4b96-8a6c-867d34fb6969.mp4"" controls=""controls"" muted=""muted"" class=""d-block rounded-bottom-2 border-top width-fit"" style=""max-height:640px;""></video>  ### Java Example  Even though all of the examples so far have been in Kotlin, you can use this library with Java.  ```java package com.your.project;  import android.graphics.BitmapFactory; import android.os.Bundle; import android.util.Log; import android.widget.ImageView;  import androidx.appcompat.app.AppCompatActivity;  import org.sdase.submission.documentscanner.DocumentScanner;  public class MainActivity extends AppCompatActivity {      private ImageView croppedImageView;      DocumentScanner documentScanner = new DocumentScanner(             this,             (croppedImageResults) -> {                 // display the first cropped image                 croppedImageView.setImageBitmap(                         BitmapFactory.decodeFile(croppedImageResults.get(0))                 );                 return null;             },             (errorMessage) -> {                 // an error happened                 Log.v(""documentscannerlogs"", errorMessage);                 return null;             },             () -> {                 // user canceled document scan                 Log.v(""documentscannerlogs"", ""User canceled document scan"");                 return null;             },             null,             null,             null     );      @Override     protected void onCreate(Bundle savedInstanceState) {         super.onCreate(savedInstanceState);         setContentView(R.layout.activity_main);          // cropped image         croppedImageView = findViewById(R.id.cropped_image_view);          // start document scan         documentScanner.startScan();     } } ```  ## License  Copyright 2022 David Marcus  Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.  THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
Medication Reminder,Mobile Applications,https://github.com/waseefakhtar/dose-android,"![Dose App](docs/images/play-store.png ""Dose App"")   <h1 align=""center"">Dose App üíä‚è∞</h1> <p align=""center"">   <a href=""https://devlibrary.withgoogle.com/products/android/repos/waseefakhtar-dose-android""><img alt=""Google"" src=""https://img.shields.io/badge/Google%20DevLibrary-Waseef-3ddc84?logo=android&logoColor=3ddc84""/></a>   <a href=""https://www.youtube.com/shorts/RTNcK_2yhYI""><img alt=""YouTube"" src=""https://img.shields.io/badge/YouTube-Google%20Developers-ff0000?logo=youtube""/></a>   <a href=""https://twitter.com/AndroidDev/status/1560008567587061761""><img alt=""Twitter"" src=""https://img.shields.io/badge/Twitter-Android%20Developers-white?logo=X""/></a>   <a href=""https://www.waseefakhtar.com/android/form-using-jetpack-compose-and-material-design/""><img alt=""Blogpost"" src=""https://img.shields.io/badge/Blog%20Post-Waseef%20Akhtar-white""/></a>   <a href=""https://www.youtube.com/watch?v=taWNluAoyaE""><img alt=""YouTube Tutorial"" src=""https://img.shields.io/badge/YouTube%20Tutorial-Waseef%20Akhtar-ff0000?logo=youtube""/></a><br>   <a href=""https://play.google.com/store/apps/details?id=com.waseefakhtar.doseapp""><img src=""https://img.shields.io/badge/View%20on%20Google%20Play-grey?logo=android"" alt=""Google Play""/></a>   <a href=""https://android-arsenal.com/api?level=21""><img alt=""API"" src=""https://img.shields.io/badge/API-21%2B-brightgreen.svg?style=flat""/></a>   <a href=""https://github.com/waseefakhtar/dose-android/actions""><img alt=""Build Status"" src=""https://github.com/waseefakhtar/dose-android/workflows/Android%20CI/badge.svg?branch=main""/></a>    <a href=""https://github.com/waseefakhtar/dose-android/contributors/""><img src=""https://img.shields.io/github/contributors/waseefakhtar/dose-android.svg"" alt=""Contributors"" /></a> </p>   <p align=""center""> Dose is a medication reminder app for Android, designed to help you stay on top of your health by reminding you to take your medications on time ‚Äî Made with Jetpack Compose, Material Design 3, Room, Navigation Components, Kotlin Coroutines, Hilt, Firebase using the recommended¬†<a href=""https://developer.android.com/topic/architecture"">Android Architecture Guidelines</a>. </p> <p align=""center""> I‚Äôm building it in public. So the idea is for everyone to contribute, leave feedback, suggest ideas, and collaborate! </p>  <p align=""center""> Got any crazy new ideas? Head over to the <a href=""https://github.com/waseefakhtar/dose-android/discussions"">Discussions</a> tab and start a new discussion. </p>  ## MAD Score <a href=""https://madscorecard.withgoogle.com/scorecard/share/1233122117/""> <img src=""https://user-images.githubusercontent.com/4093820/186459147-1b2e7102-498f-4874-841b-2be88336c2a8.png""/>  <img src=""https://user-images.githubusercontent.com/4093820/186459184-eda3a2c5-fe3c-4038-94e7-fbcb45e90946.png""/>  </a>  ## IDE Version Android Studio Koala | 2024.1.1  ## Contributions  If you've found an error in the project, please file an issue.  Patches are encouraged and may be submitted by forking this project and submitting a pull request. Since this project is still in its very early stages, if your change is substantial, please raise an issue first to discuss it.  ## License  Dose App is distributed under the terms of the MIT License. See the [license](LICENSE) for more information."
Calculator App,Mobile Applications,https://github.com/forzzzzz/Calculator-You,"<div align=""center"">  <img src=""fastlane/metadata/android/en-US/images/featureGraphic.png"" alt=""Feature graphic"" style=""display: block; margin: 0 auto 30px;"">  # Calculator You  ### Calculator You is a beautiful calculator for solving simple tasks.   <a href=""https://github.com/forzzzzz/Calculator-You/releases/latest"">       <img alt=""Latest release"" src=""https://img.shields.io/github/v/release/forzzzzz/Calculator-You?color=8E4A4B&style=for-the-badge""> </a> <a href=""https://github.com/forzzzzz/Calculator-You/releases/latest"">       <img alt=""Releases"" src=""https://img.shields.io/github/downloads/forzzzzz/Calculator-You/total?color=8E4A4B&label=GitHub downloads&style=for-the-badge""> </a> <a href=""https://github.com/forzzzzz/Calculator-You/releases/latest"">       <img alt=""Releases"" src=""https://img.shields.io/badge/Android-8.1+-blue?color=8E4A4B&style=for-the-badge""> </a>  ## ‚¨áÔ∏è Download ‚¨áÔ∏è  <a href=""https://github.com/forzzzzz/Calculator-You/releases/latest"">       <img alt=""Get it on GitHub"" src=""https://raw.githubusercontent.com/deckerst/common/main/assets/get-it-on-github.png"" height=""80""> </a> <a href=""https://apt.izzysoft.de/fdroid/index/apk/com.marktka.calculatorYou"">       <img alt=""Get it on IzzyOnDroid"" src=""https://gitlab.com/IzzyOnDroid/repo/-/raw/master/assets/IzzyOnDroid.png"" height=""80""> </a> <a href=""https://play.google.com/store/apps/details?id=com.marktka.calculatorYou"">       <img alt=""Get it on Google Play"" src=""https://play.google.com/intl/en_us/badges/static/images/badges/en_badge_web_generic.png"" height=""80""> </a> <a href=""https://f-droid.org/packages/com.marktka.calculatorYou/"">       <img alt=""Get it on F-Droid"" src=""https://fdroid.gitlab.io/artwork/badge/get-it-on.png"" height=""80""> </a>  </div>  <div align=""center"">  ## What does Calculator You include?  </div>  üé® Beautiful Design & 60+ Color Themes: Personalize your calculator with a choice of 60+ vibrant color themes, making every calculation visually enjoyable.  üî¨ Scientific Mode: Access advanced mathematical functions with our comprehensive scientific calculator, perfect for students, engineers, and math enthusiasts.  üïí History: Easily track and review previous calculations with our handy history feature, ensuring you never lose important work.  üìè Unit Converter: Convert units effortlessly with our built-in converter, supporting a wide range of measurements, including length, weight, and more.  üëå User-Friendly Interface: Enjoy a clean, intuitive interface designed for effortless navigation and efficient calculations.  <div align=""center"">  <img alt=""Screenshot"" src=""fastlane/metadata/android/en-US/images/phoneScreenshots/1.png"" width=""24%""/> <img alt=""Screenshot"" src=""fastlane/metadata/android/en-US/images/phoneScreenshots/2.png"" width=""24%""/> <img alt=""Screenshot"" src=""fastlane/metadata/android/en-US/images/phoneScreenshots/3.png"" width=""24%""/> <img alt=""Screenshot"" src=""fastlane/metadata/android/en-US/images/phoneScreenshots/4.png"" width=""24%""/>  </div>  ## :heart: Support Me  ### ***Thank you very much for your help*** :heart:  Write a review about the app on [Google Play](https://play.google.com/store/apps/details?id=com.marktka.calculatorYou)    Help translate with [Hosted Weblate](https://hosted.weblate.org/engage/calculator-you/)  <a href=""https://hosted.weblate.org/engage/calculator-you/""> <img src=""https://hosted.weblate.org/widget/calculator-you/88x31-white.png"" alt=""Translating status"" height=""40"" /> </a>  Support this app with a [PayPal](https://www.paypal.com/donate/?hosted_button_id=GKC26SLPUTQMU) donation   [<img src=""https://img.shields.io/badge/PayPal-00457C?style=for-the-badge&logo=paypal&logoColor=white"" alt=""Donate with PayPal"" height=""40"">](https://www.paypal.com/donate/?hosted_button_id=GKC26SLPUTQMU)  ## üì¢ Join the telegram channel  Join the [telegram channel](https://t.me/devBlogCalculatorYou) to follow the project  [<img src=""https://img.shields.io/badge/Telegram%20channel-white?style=for-the-badge&logo=telegram"" alt=""Join the telegram channel"" height=""40"">](https://t.me/devBlogCalculatorYou)  ## üî¢ Math Expressions Parser  ### [Mihai Preda / arithmetic](https://github.com/preda/arithmetic)    ## üåê Translation status [![translation status](https://hosted.weblate.org/widget/calculator-you/multi-auto.svg)](https://hosted.weblate.org/engage/calculator-you/)   #### *Made not with love :heart:, but by love :heart:*"
Hangman Game,Mobile Applications,https://github.com/laurensV/Hangman,"Hangman =======  **author:** Laurens Verspeek - 10184465<br>             laurens_verspeek@hotmail.com  **course:** Native App Studios - Univeristy of Amsterdam  _Hangman is an application for Android based on the classic Hangman game for the course Native App Studios_  ###How to run the Hangman android application### - Clone the repository: git clone git@github.com:laurensV/Hangman.git - Get Eclipse and the ADT plugin, see instructions here: http://developer.android.com/sdk/installing/installing-adt.html - Open Eclipse (eclipse.exe) - New > Project> Android > Android Project from Existing Code - Select the hangman app located in the cloned folder in the map 'Hangman' - Run the app on your mobile or set up a virtual android device and run it there.  ###Features### - placeholders for yet-unguessed letters that make clear the word‚Äôs length. - displays how many guesses left until you lose - displays all letters you already guessed - User can give input guesses via an on-screen keyboard. - app only accepts single alphabetical characters as valid input (case-insensitively). - Congratulation popup after you win - Changable settings, like the length of words to be guessed and the maximum number of incorrect guesses allowed - Option to start a new game in settings menu - Maintains a history of high scores that‚Äôs displayed anytime a game is won or lost.   ###Technologies### - Android SDK (java) - Canvas with SurfaceView (http://developer.android.com/guide/topics/graphics/2d-graphics.html)  ###Mockups### ####Main Activity#### ![Hangman](doc/hangman.png ""Main Activity"")  ####Settings#### ![Settings](doc/settings.png ""Settings"")  ####Highscores#### ![Highscores](doc/highscores.png ""Highscores"")"
Audiobook,Mobile Applications,https://github.com/homielab/audiobookapp,"## Audiobook App Open Source  Audiobook mobile application accomplished with React Native and React ecosystem, just a single code base for both android and ios.  ### Installation      git clone git@github.com:homielab/audiobookapp.git     cd audiobookapp     npm install     npx react-native run ios  ### Dependencies  - [react](https://github.com/facebook/react): JS library for building user interfaces - [react-native](https://github.com/facebook/react-native): framework for building native apps with React - [react-native-sound](https://github.com/zmxv/react-native-sound): React Native module for playing audio - [react-native-vector-icons](https://github.com/oblador/react-native-vector-icons): React Native module supports using custom icon sets - [react-navigation](https://github.com/react-navigation/react-navigation/): React Native module support navigation - [react-recontext](https://github.com/homielab/react-recontext): React state management (made by me)  ### Questions?  Feel free to create new issue: https://github.com/homielab/audiobookapp/issues  ### Production Version  The UX/UI is rewritten completely, but the core logic is the same.  - Web: [https://audioaz.com/](https://audioaz.com/?utm_source=github&utm_medium=github&utm_campaign=github) - Android App: [https://play.google.com/store/apps/details?id=app.sachnoi](https://play.google.com/store/apps/details?id=app.sachnoi) - iOS App: [https://apps.apple.com/us/app/id1453643910](https://apps.apple.com/us/app/id1453643910)  ### Preview  <center><a href=""http://www.youtube.com/watch?feature=player_embedded&v=GT63VkgRins"" target=""_blank""><img src=""http://img.youtube.com/vi/GT63VkgRins/0.jpg"" alt=""audiobook app react-native demo"" width=""480"" height=""360"" border=""10"" /></a></center>  ### Screenshots  <img src=""https://raw.githubusercontent.com/homielab/audiobookapp/main/screenshots/1.png"" width=""300""> <img src=""https://raw.githubusercontent.com/homielab/audiobookapp/main/screenshots/2.png"" width=""300""> <img src=""https://raw.githubusercontent.com/homielab/audiobookapp/main/screenshots/3.png"" width=""300""> <img src=""https://raw.githubusercontent.com/homielab/audiobookapp/main/screenshots/4.png"" width=""300""> <img src=""https://raw.githubusercontent.com/homielab/audiobookapp/main/screenshots/5.png"" width=""300""> <img src=""https://raw.githubusercontent.com/homielab/audiobookapp/main/screenshots/6.png"" width=""300""> <img src=""https://raw.githubusercontent.com/homielab/audiobookapp/main/screenshots/7.png"" width=""300"">"
Calories Counting App,Mobile Applications,https://github.com/simonoppowa/OpenNutriTracker,"<p align=""center"">   <img alt=""Logo"" src=""assets/icon/ont_logo_square.png"" width=""128"" />   <h1 align=""center"">OpenNutriTracker</h1> </p>  <p align=""center"">   <a href=""https://opensource.org/licenses/MIT"" alt=""License"">         <img src=""https://img.shields.io/badge/license-GPLv3-blue"" /></a>   <a href=""https://github.com/simonoppowa/OpenNutriTracker/stargazers"" alt=""GitHub Stars"">         <img src=""https://img.shields.io/github/stars/simonoppowa/OpenNutriTracker.svg"" /></a>   <a href=""https://github.com/simonoppowa/OpenNutriTracker/issues"" alt=""GitHub Issues"">         <img src=""https://img.shields.io/github/issues/simonoppowa/OpenNutriTracker.svg"" /></a>   <a href=""https://github.com/simonoppowa/OpenNutriTracker/pulls"" alt=""GitHub Pull Requests"">         <img src=""https://img.shields.io/github/issues-pr/simonoppowa/OpenNutriTracker.svg"" /></a>   <a alt=""Version"">         <img src=""https://img.shields.io/badge/version-beta-yellow.svg"" /></a> </p>  ## Description OpenNutriTracker is an open-source mobile application designed to simplify nutritional tracking and management. Whether you are looking to improve your health, lose weight, or simply maintain a balanced diet, OpenNutriTracker provides a minimalistic interface to easily track and analyze your daily nutrition.  [Website](https://simonoppowa.github.io/OpenNutriTracker-Website/)  ## Screenshots <p align=""center"">   <img alt=""Logo"" src=""fastlane/metadata/android/en-US/images/phoneScreenshots/1_en-US.png"" width=""20%"" />   &nbsp;&nbsp;   <img alt=""Logo"" src=""fastlane/metadata/android/en-US/images/phoneScreenshots/2_en-US.png"" width=""20%"" />   &nbsp;&nbsp;   <img alt=""Logo"" src=""fastlane/metadata/android/en-US/images/phoneScreenshots/3_en-US.png"" width=""20%"" />   &nbsp;&nbsp;   <img alt=""Logo"" src=""fastlane/metadata/android/en-US/images/phoneScreenshots/4_en-US.png"" width=""20%"" /> </p>  ## Install [<img src=""fastlane/metadata/android/en-US/images/appstore_banner.png"" width=""30%"">](https://testflight.apple.com/join/j7uKoEDl) [<img src=""fastlane/metadata/android/en-US/images/playstore_banner.png"" width=""30%"">](https://play.google.com/store/apps/details?id=com.opennutritracker.ont.opennutritracker)  ## Key Features - **üçé Nutritional Tracking:** Easily log your meals and snacks, and access a vast database of food items and ingredients to get detailed nutritional information. - **üìì Food Diary:** Maintain a comprehensive food diary to keep track of your daily food consumption, habits, and progress. - **üçΩÔ∏è Custom Meals:** Plan your meals in advance, create personalized meal plans, and optimize them according to your dietary goals. - **üì∑ Barcode Scanner:** Scan barcodes on packaged food items to instantly retrieve their nutritional information. - **üîí Privacy Focused:** OpenNutriTracker prioritizes the privacy its users. It does not collect or share any personal data without your consent. - **üö´üí∞ No Subscription, In-App Purchases, or Ads:** OpenNutriTracker is completely free to use, without any subscription fees, in-app purchases, or intrusive advertisements.  ## Privacy See [Data Protection](https://www.iubenda.com/privacy-policy/53501884) - **Data Encryption**: All collected user data is encrypted and stored locally on your device - **Minimal Data Collection**: OpenNutriTracker only collects the necessary information required for tracking nutrition and providing personalized insights. Your data will not be shared with third parties without your consent. - **Open-Source**: OpenNutriTracker is an open-source application  ## TODOs - Add serving sizes to meals - ~~Add Imperial unit support~~ - Add support for Material You themes  ## Contribution Contributions to OpenNutriTracker are welcome! If you find any issues or have suggestions for new features, please open an issue or submit a pull request. Make sure to follow the project's code style and guidelines.  ## Disclaimer OpenNutriTracker is not a medical application. All data provided is not validated and should be used with caution. Please maintain a healthy lifestyle and consult a professional if you have any problems. Use during illness, pregnancy or lactation is not recommended.  The application is still under construction. Errors, bugs and crashes might occur.  ## Acknowledgments The OpenNutriTracker project was inspired by the need for a simple and effective nutrition tracking tool. The food database used in OpenNutriTracker is powered by [Open Food Facts](https://world.openfoodfacts.org/) and [Food Data Central](https://fdc.nal.usda.gov/).  ## License This project is licensed under the GNU General Public License v3.0 License. See the [LICENSE](LICENSE) file for more information.  ## Contact For questions, suggestions, or collaborations, feel free to contact the project maintainer:  Simon Oppowa  - GitHub: [@simonoppowa](https://github.com/simonoppowa) - Email: [opennutritracker-dev@pm.me](mailto:opennutritracker-dev@pm.me)"
Restaurant App (with API),Mobile Applications,https://github.com/CristianoYL/RestaurantAndroidApp,"# Introduction This is an Android online restaurant ordering app. User may register and login into their account and starts online ordering food. The app communicates to a set of REST APIs exposed by server. A sample back-end providing this service can be found at [my other GitHub repo here.](https://github.com/CristianoYL/RestaurantAppAPI) # Configurations ## IDE The project is developed using **Android Studio 3.0**. It is recommended that you use **Android Studio** instead of other IDEs for simpler project import. [You may download Android Studio from the official website here](https://developer.android.com/studio/index.html).  ## Dependencies This service relies on several other services as well. For example, it uses [Google Maps API](https://developers.google.com/maps/documentation/android-api/) for location services and maps, [Gson](https://sites.google.com/site/gson/gson-user-guide) for data serialization, more specifically, for jsonifying data, and [Stripe](https://stripe.com/docs) for live credit card charges etc.  Since **Android Studio** uses **gradle build**, you don't need to worry about dependencies when importing the code. # SDK * The minimum SDK is 15 * The target and compile SDK is 26 * All details can be found in the module's ```build.gradle``` file  # User Guide Here's a basic demo how this app works.  ## Login/Register In order to use the app, first register an account with your email and password.  <img src=""https://github.com/CristianoYL/RestaurantAndroidApp/blob/master/screenshot/Screenshot_20171120-175008.png"" width=""200"">  ## Restaurant Listing After logging in, you will see a listing of all restaurants. Basic info, such as delivery fee, promotions and minimum charges will be presented here.  <img src=""https://github.com/CristianoYL/RestaurantAndroidApp/blob/master/screenshot/Screenshot_20171120-175511.png"" width=""200"">  ## Menu & Order After selecting a restaurant, you'll be navigated to the restaurant's menu page. You may edit your order using the ""-"" and ""+"" buttons. You can see a more detailed description for each menu item by clicking on them, a view containing the picture and detail of the item will expand/fold on click.  <img src=""https://github.com/CristianoYL/RestaurantAndroidApp/blob/master/screenshot/Screenshot_20171120-175606.png"" width=""200""> <img src=""https://github.com/CristianoYL/RestaurantAndroidApp/blob/master/screenshot/Screenshot_20171120-175533.png"" width=""200"">  ## Placing Your Order After finishing your selections, click on the price tag on the upper-right corner to proceed to check out. You'll be navigated to a order summary page where you may confirm your order, provide delivery and payment details and finally place the order.  <img src=""https://github.com/CristianoYL/RestaurantAndroidApp/blob/master/screenshot/Screenshot_20171120-175629.png"" width=""200"">  There is built-in Google Maps that locates you to help you fill in the delivery address, however, you can manually input your desired address as well, and we will provide you suggestions according to your input to make sure your address is valid.  <img src=""https://github.com/CristianoYL/RestaurantAndroidApp/blob/master/screenshot/Screenshot_20171120-175700.png"" width=""200"">  When clicking on ""PAYMENT METHOD"" button, you'll be prompt with a new window asking for your Card detail. If you have already added a card to your account before, you can simply select from the list.  <img src=""https://github.com/CristianoYL/RestaurantAndroidApp/blob/master/screenshot/Screenshot_20171120-183507.png"" width=""200""> <img src=""https://github.com/CristianoYL/RestaurantAndroidApp/blob/master/screenshot/Screenshot_20171120-175735.png"" width=""200"">  If you have concerns on the security of your card info, [here's how our service makes sure it is safe.](https://github.com/CristianoYL/RestaurantAppAPI#security-of-payment)  After filling all necessary information on your order, you can place your order now!  <img src=""https://github.com/CristianoYL/RestaurantAndroidApp/blob/master/screenshot/Screenshot_20171120-175818.png"" width=""200"">"
Expense Tracker App,Mobile Applications,https://github.com/Abhishek111883/Android-project,"# Expense Tracker Android Application  The Expense Tracker Android Application is a feature-rich and user-friendly mobile app designed to help you manage your finances efficiently. With an intuitive interface, advanced features, and robust security, this app makes tracking your expenses a breeze.  ![Expense Tracker App](""C:\Users\divia\OneDrive\Desktop\Screenshot_2023-09-24-18-08-51-60_3a70945bc5b3aad97b1cb9bb49321db3.png"")  ## Features  ### 1. Expense Logging Easily record your expenses with customizable categories and payment methods. Keep a detailed record of every transaction to gain insight into your spending habits.  ### 2. Budget Management Set monthly or weekly budgets to stay on top of your financial goals. The app provides real-time updates on your spending relative to your budget.  ### 3. Income Tracking Track your income sources to understand your cash flow better. The app calculates your total income and expenses, giving you a clear overview of your financial status.  ### 4. Secure Authentication - **Login Functionality**: Securely log in with your registered credentials. - **Forgot Password**: Easily reset your password if you forget it. - **Registration**: New users can register for an account with ease.  ### 5. Expense Analysis Visualize your spending patterns with insightful charts and reports. Understand where your money goes and make informed financial decisions.  ### 6. Receipt Scanning Upload and store receipts for your expenses, making it convenient to keep digital records.  ### 7. Multi-Currency Support For international users, the app supports multiple currencies, ensuring accurate expense tracking.  ### 8. Data Security Your financial data is protected and securely stored using Firebase Database. Rest easy knowing your information is safe.  ### 9. Number Formatting The app uses number formatting to display currency values in a user-friendly format.  ### 10. Offline Mode Receive a pop-up message when there's no internet connection, ensuring you're aware of your app's status.  ### 11. Double Tap Exit Exit the app with a double-tap gesture for quick and convenient navigation.  ### 12. Effective Logout Logout from your account securely with just a few taps.  ## Technologies Used  - **Android Studio**: The app is developed using Android Studio, making use of the Android SDK for seamless integration with Android devices.  - **Firebase Database**: Firebase is used to store and manage user data securely.  - **Java**: The app is built using Java, following object-oriented programming principles for efficient code management.  ## Getting Started  To start using the Expense Tracker Android Application:  1. Clone this repository to your local machine. 2. Open the project in Android Studio. 3. Build and run the app on your Android device or emulator.  ## Contributing  We welcome contributions to improve this Expense Tracker app. Feel free to fork this repository, make your changes, and submit a pull request. For major changes, please open an issue first to discuss the proposed changes.  "
Smart Education App,Mobile Applications,https://github.com/Hash-Studios/e-learning-app,"# <img src=""android/app/src/main/res/mipmap-xxhdpi/ic_launcher.png"" alt=""icon"" width=30> E-Learning  E-Learn is a beautiful open-source education app for Android. It is built with Dart on top of Google's Flutter Framework. <img alt='E-Learn UI Mockup' src='demo/Frame 1.png'/>  ## List of Contents  1. [Demo](#demo) 2. [Support](#support) 3. [Dependencies](#dependencies) 4. [Usage](#usage) 5. [Contributing](#contributing) 6. [License](#license) 7. [Contributors](#contributors)  ## Demo  **Screens**  | ![](demo/Screen/Home.png) | ![](demo/Screen/Calendar.png) | ![](demo/Screen/Home/Overlay.png) | ![](demo/Screen/Videos.png) | ![](demo/Screen/Stats/Local.png) | | :-------------: | :-------------:  | :-------------:  | :-------------:  | :-------------:  | |     Explore     |    Planner    |    Overlay     |     Videos       |     Leaderboard     |   | ![](demo/Screen/Stats/Global.png) | ![](demo/Screen/NavDrawer.png) | ![](demo/Screen/Home/Search.png) | ![](demo/Screen/Home/Search/Results.png) | ![](demo/Screen/Profile.png) | | :-------------: | :-------------:  | :-------------:  | :-------------:  | :-------------:  | |     Global Leaderboard    |    NavDrawer    |    Search     |     Search Results      |     Profile     |   | ![](demo/Screen/Onboarding/1.png) | ![](demo/Screen/Onboarding/2.png)| ![](demo/Screen/Onboarding/3.png)       | ![](demo/Screen/Subjects/Close.png)  | ![](demo/Screen/Subjects/Open.png)| | :-------------:  | :-------------: | :-------------:       | :-------------:  | :-------------: | |  Onboarding 1    |    Onboarding 2       |  Sign-in Screen    | Subjects Close   | Subjects Close  |  | ![](demo/Screen/Video/Open.png) | ![](demo/Screen/Forum.png) | ![](demo/Screen/Help.png) | ![](demo/Screen/Settings.png) | ![](demo/Screen/Test.png) | | :-------------: | :-------------:  | :-------------:  | :-------------:  | :-------------:  | |     Video Info    |    Forum    |    Help     |     Settings       |     Test     |   ## Support  If you like what we do, and would want to help us continue doing it, consider sponsoring this project.  <a href=""https://www.buymeacoffee.com/HashStudios"" target=""_blank""><img src=""https://cdn.buymeacoffee.com/buttons/default-orange.png"" alt=""Buy Me A Coffee"" height=51 width=217></a>  ## Dependencies  The following packages are needed for the development of this application.  - `provider: ^4.1.3` for caching data, and state management - `fluttertoast: ^4.0.1` for toast notifications - `shared_preferences: ^0.5.7` for storing settings like app state - `firebase_core: ^0.4.4+3` for firebase core - `firebase_auth: ^0.16.0` for user auth - `google_sign_in: ^4.4.4` for Google sign in support - `flare_splash_screen: ^3.0.1` for the animated splash screen - `flutter_svg: 0.17.4` for svg assets - `firebase_analytics: ^5.0.16` for analytics  More details about these can be found in the [`pubspec.yaml`](https://github.com/Hash-Studios/e-learning-app/tree/master/pubspec.yaml) file.  ## Usage  More information about the releases can be found in the [Release](https://github.com/Hash-Studios/e-learning-app/releases) tab.  ## Contributing  First off, thank you for considering contributing to e-learning app. It's people like you that make e-learning app such a great app.  To start your lovely journey with e-learning app, first read the [`contributing guidelines`](https://github.com/Hash-Studios/e-learning-app/tree/master/CONTRIBUTING.md) and then fork the repo to start contributing!  ## License  This app is licensed under the [`BSD 3-Clause License`](https://github.com/Hash-Studios/e-learning-app/tree/master/LICENSE.txt). Any Usage of the source code must follow the below license.  ``` BSD 3-Clause License  Copyright (c) 2020 Hash Studios All rights reserved.  Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:  1. Redistributions of source code must retain the above copyright notice, this    list of conditions and the following disclaimer.  2. Redistributions in binary form must reproduce the above copyright notice,    this list of conditions and the following disclaimer in the documentation    and/or other materials provided with the distribution.  3. Neither the name of the copyright holder nor the names of its    contributors may be used to endorse or promote products derived from    this software without specific prior written permission.  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. ```  ``` DISCLAIMER: Google Play and the Google Play logo are trademarks of Google LLC. ```  ## Contributors  <a href=""https://github.com/Hash-Studios/e-learning-app/graphs/contributors"">   <img src=""https://contributors-img.web.app/image?repo=Hash-Studios/e-learning-app"" /> </a>"
First Aid App,Mobile Applications,https://github.com/Vedanth29/First-Aid-App-AndroidProject-,"# First-Aid-App using (Android Studio):  **A Mobile Application Development Mini Project on: First Aid Application**  # Abstract->  In this report, we present a first aid android app that gives assistance to any person suffering a sudden illness and injury in emergency case. This app provides simple step by step instructions for guide user through everyday first aid scenario. Next, first aid apps provide user the videos and animation for easy learning. The user can call 999 the emergency number for the app at any time. Once the user downloads this android app from the Google Play Store then they can use this app without using internet connection. Preloaded content means the users have instant access to all safety information at any time even without reception or an Internet connection as offline mode. Then, this app provides the interactive quizzes that allow the user to earn rank that they can share with their friends and show off their lifesaving knowledge In emergency case, the user will use what type of illness or injury that happen then simple step by step instructions, animation and videos will provide for each scenario. The user can call 999 the emergency number if illness or injury is more critical and need ambulance. The user can use the step as safety tips and treatment for minor conditions while wait for ambulance arrive at the place. There are many situations that require first aid app and it can be used by untrained person. # ----------------------------------------------------------------- # Problem Statement->  We may have only few seconds to save a life when we are in an emergency. Is anything that we can do in an emergency until help arrives? We should do because the only person we may have to depend on is ourselves. First aid is the emergency care given to a victim of injury or sudden illness before professional medical help arrives. Cardiopulmonary resuscitation (CPR) is a lifesaving method or technique useful in many emergencies such as heart attack which someone's breathing or heartbeat has stopped. It is method of combining chest compressions with rescue breathing to maintain a flow of oxygen-rich blood to the brain while the heart is not working.   In Malaysia, the number one killer in terms of diseases and health problems is heart attack. According to the World Health Organization (WHO), the total number of deaths in Malaysia resulted from coronary heart disease was at 22,701 in 2010. This makes to about 22.18 percentages of the total deaths in the country. Heart disease is different from stroke which is the second top killer in Malaysia. So, it can make different when everybody know how to do Cardiopulmonary resuscitation (CPR).   Imagine one day that we find our friend lying on the floor and unconscious. We have no idea how long he has been there and then run to call the ambulance. While the ambulance is on their way that person is helpless and we can‚Äôt do anything for help him. Wouldn‚Äôt you want to possess the skills needed to help save him? Therefore learning CPR skills are important and it could mean the difference between life and death. Furthermore, most popular injuries happen among teenagers and high school students were sling. When our friends break their arms outside, first we have to do is getting the person to support the injured arm with his hand. Next, take the lower end of the bandage up over the hand forearm and tie it in the hollow just above the collarbone. Then, pin the point near the elbow or twist and tuck it in. First aid skill and knowledge is really important in our daily life. # ----------------------------------------------------------------- # Functional requirements->  In software engineering, a functional requirement defines a function of a software system or its component. A function is described as a set of inputs, the behaviour, and outputs (see also software). Functional requirements may be calculations, technical details, data manipulation and processing, and other specific functionality that define what a system is supposed to accomplish. Behavioural requirements describing all the cases where the system uses the functional requirements are captured in use cases.  *	Android Phone To perform and display the functionality of the project. *	Android studio To create, design, test, debug and run the android project. *	Mouse To navigate through the emulator ---------------------------------------------------------------------------------------------------------------------------------------------------------- * **Basic system requirements for Android Studio---** Microsoft, Windows, Mac, Linux. * **Operating System Versio---** Microsoft Windows 7/8/10 (32- or 64-bit).  * **The Android Emulator only supports** 64-bit Windows	Mac OS X 10.10 (Yosemite) or higher, only up to 10.14 (macOS Mojave) * GNOME or KDE desktop Tested on Linux based on Debian (4.19.67-2 rodete2). * **Random Access Memory (RAM)---**	4 GB RAM minimum; 8 GB RAM recommended. * **Free digital storage---**	2 GB of available digital storage minimum, 4 GB Recommended (500 MB for IDE + 1.5 GB for Android SDK and emulator system image). * **Minimum required---** JDK version	Java Development Kit 8 * **Minimum screen---** resolution	1280 x 800 # ----------------------------------------------------------------- # Objective->   Everybody should know how to manage first aid because it is one of the most valuable skills we can ever have. However, not all of us would endeavor to have first aid certification and most would probably think that they don‚Äôt need to be certified. After all, common situations like a child falling down would just entail a mother cleaning up the wound and applying a plaster to the wound. But these are just the very basic first aid techniques that everyone knows how to do.   The main objective for the author to develop this application:--  * To develop an application that giving first aid knowledge to the user by using this application.  * To provide global positioning system (GPS) to the nearest hospital.  * To test the user knowledge first aid by provide test for some situation in the application.  At this point, it makes sense for everyone to learn first aid skills and require having this knowledge so we can help each other. All we need to do is take advantage of them.  # SNAPSHOTS # ----------------------------------------------------------------- <img src=""https://user-images.githubusercontent.com/65438508/131843902-aabfaac0-f181-44e9-bac7-cf506cfb09e0.png"" alt=""HOME PAGE"" width=""300"" height=""600"">  <img src=""https://user-images.githubusercontent.com/65438508/131845532-5bd1cad3-3afb-4ed1-9439-72988eb929b5.png"" alt=""HOME PAGE"" width=""300"" height=""600"">  <img src=""https://user-images.githubusercontent.com/65438508/131845849-fc3850d8-a377-4bb1-93c6-82d332fa5e25.png"" alt=""AID"" width=""300"" height=""600""> <img src=""https://user-images.githubusercontent.com/65438508/131846004-3ee07751-d7f4-4bc6-8d6f-e2bd29fe98e0.png"" alt=""AID"" width=""300"" height=""600""> <img src=""https://user-images.githubusercontent.com/65438508/131848391-634f80f4-2e67-4265-a0fa-4618aae730a6.png"" alt=""AID"" width=""300"" height=""600"">  # ----------------------------------------------------------------- "
Fitness Tracker,Mobile Applications,https://github.com/Fitness-Guys/Fitness-Tracker-App,"# Fitness Tracker  ## Table of Contents 1. [Overview](#Overview) 1. [Product Spec](#Product-Spec) 1. [Wireframes](#Wireframes) 2. [Schema](#Schema)  ## Overview ### Description Allows users to create profiles to track their fitness goals. This is a self reporting fitness app that will allow the user to input their workout history and track their activity. User will be displayed analytics based on the amount of input of workout history recorded into the app. Will also implement tutorial videos section to help users learn new workouts, or even for beginners to get started. A social media aspect of this app will allow users to share with friends and family; while promoting healthy competion and motivating users to work out.   ### App Evaluation [Evaluation of your app across the following attributes] - **Category:** Fitness  - **Mobile:** This app would differ from a glorified website by creating a easy to use interface that would quickly provide data to the user - **Story:** The value on the app will vary on users. The app would be particularly useful for those who like to keep track of their progress. It would also be useful for those who work out frequently. We feel that peers who workout would react positively to this app as it would serve to be a useful tool for them.  - **Market:** This app will appeal to users who typically like to see their past workouts. Also those who like to track their history and see the progression over time - **Habit:** Depending on the users workout schedule. The user will be creating data to track their workouts, and the user will also be able to view videos on specific workouts. The user will be able to create posts, so friends could also view thei fitness progress. - **Scope:** We already have most of the resources to create a barebones version, but the challenging aspect to this is learning how to use new API's and new libraries.  ## Product Spec  ### 1. User Stories (Required and Optional)  **Required Must-have Stories** User to sign up - [x] name, username, password,wt,ht      User to login - [x] username - [x] password  Profile Screen - [x] username, ht, weight(maybe?)  User to home screen - [x] display past days workouts - [x] display total burned calories over a given duration  Have a workout activity page that will begin to track the users activities.  - [x] This will have a checkbox for the premade list of exercises.  - [X] a start/stop button(starts timer) - [x] Finish button should go to summary - [x] In summary, calculated calories should be displayed      Video tutorials fragment. - [x] ListView displaying categories. - [x] Youtube API calls and create model data - [x] RecyclerView of the videos returning from category query - [x] Video Playback activity - [x] Improve astetics of the activities to meet professional look - [X] Autoplay videos in new activity   In workout screen - [x] display timer(to calc burned calories) - [x] display select workout - [x] workout completed button -> display calorie and time results. W/ a positive message. Close screen back to home page.     **Optional Nice-to-have Stories**  * Include a social aspect where the user could post the activities they have completed * Workout ""how to"" videos * Hiking trails (Social workout mode :) * In Profile: Followers(count) and Follows(count) * in Home Screen: display workouts from with in friends network  ### 2. Screen Archetypes  * Login / Register     * After Login and Registration, User is then moved to the home screen * Stream     * Home screen will be a dashboard interface, that will display certain data such as past workouts and calories burnt     * A feature where the user could access workout videos.      * Social media aspect will display the workout of friends * Creation     * The user will be inputing data based on the workout they're completing/tracking. The input will then be saved so that the user could view their progress.      * The user will also have the option to post their workouts for others to see. This feature may be presented once the user has finished inputting data from their workout * Detail     * This will be used when displaying data to the user, such as calories burned, distance walked,   * Profile     * the user will have to create a profile in order to keep track of their progress. The profile activity, will display the users information, and more features may be added during development   ### 3. Navigation  **Tab Navigation** (Tab to Screen)  * Login * Profile * Home Screen * Workout Activity     * workout * Workout videos   **Flow Navigation** (Screen to Screen)  * Login Screen     * --> Home * Registration screen     * --> Home * Homescreen     * --> provide data to the user using a Detail Screen Archetypes once one of the charts get clicked on     * --> provide the friends feed to the user, might be implemented on another activity  ## Wireframes <img src=""WireFrame.png"" width=600>   ### [BONUS] Digital Wireframes & Mockups  ### [BONUS] Interactive Prototype  ## Schema  [This section will be completed in Unit 9] ### Models [Add table of models] ### Networking - [Add list of network requests by screen ] - [Create basic snippets for each Parse network request] - [OPTIONAL: List endpoints if using existing API such as Yelp]  ## Schema  ### Models #### User     | Property      | Type     | Description |    | ------------- | -------- | ------------|    | objectId      | String   | unique id for the user post (default field) |    | author        | Pointer to User| image author |    | image         | File     | profile image |    | Weight       | Number   | weight of the user thats inputted during sign up |    | Height | String   | Height of the user thats inputted during sign up |    | Password    | String   | Password |    | Username     | String | username used to sign in |    | lastWorkout | Pointer to Workout | Date & time of last workout |    | Email | String | email for user    | createdAt | DateTime | date and time when profile created     #### Workout     | Property      | Type     | Description |    | ------------- | -------- | ------------|    | object ID | String | unique id for the workout    | createdAt | DateTime | date and time when the workout was initialized    | start | DateTime | time when the workout was started    | end | DateTime |time when the workout was completed    | duration | String | calculated from start and end to display total time of workout    | WorkoutType | String | name of the specific workout    | Calories | Number | Carlories burned for the workout     #### Date Summary    | Property      | Type     | Description |    | ------------- | -------- | ------------|    | Date | DateTime | The date     | Weather | String | Data gathered from weather API     #### Current Summary    | Property      | Type     | Description |    | ------------- | -------- | ------------|    | Object ID | String | unique ID for the day Summary    | TotalDuration | Number | calculated duration of workouts from the same day    | Calories | Number | calculated Calories of workouts from the same day              ### Networking #### List of network requests by screen    - Home Screen       - (Read/GET) Query Date and Weather, total duration, total calories, workouts done sameday ``` java     Date d = new Date();     SimpleDateFormat dateForm = new SimpleDateFormat(""MM/dd/YY"");     String date = dateForm.format(date);     Weather weather = new Weather() //Sudo code for weather API call     ParseQuery<Workout> query = ParseQuery.getQuery(Workout.class);     query.findInBackground(new FindCallback<Workout>() {         ....     })  ```      - User Sign up ``` java      ParseUser user = new ParseUser();      user.setUsername(""catstevens"");     user.setPassword(""space#cowboy"");     user.setEmail(""email@example.com"");     user.put(""phone"", ""650-253-0000"");     user.signUpInBackground(new SignUpCallback() {       public void done(ParseException e) {         if (e == null) {          } else {           // Sign up didn't succeed. Look at the ParseException           // to figure out what went wrong         }       }     });  ```     - Profile Screen       - (Read/GET) Query logged in user object       - (Read/GET) Summary of workouts, use graphs too       - (Update/PUT) Update user profile image ``` java      ParseQuery<ParseObject> query =     ParseQuery.getQuery(Workout.class);          // Include the post data with each comment     query.include(""Author""); // the key which the associated object was stored          // Execute query with eager-loaded owner     query.findInBackground(new FindCallback<ParseObject>()     {      ....     }                   ```                                 - Workout        - (Create/Post) Create a new Workout Object ``` java     @ParseClassName(""Workout"")     public class Workout extends ParseObject {    /**objectId      * createdAt DateTime     * start = createdAt     * end     * duration     * workoutType enum - General, Strength, Run, Walk, Yoga     * calories - calc: Weight- duration- MET     *      *      * */     }  ```       ### Version 1 Progress ![](progress.gif)  ### Current Progress ![](current.gif)   #### [OPTIONAL:] Existing API Endpoints ##### Youtube Data API - Base URL - [https://www.googleapis.com/youtube/v3/](https://www.googleapis.com/youtube/v3/)     HTTP Verb | Endpoint | Description    ----------|----------|------------     `GET`    | /search | states that we are about to search     `GET`    | /max   | states the max results, in this case it's 40     `GET`    | /order | orders the search by relevance     `GET`    | /""search details"" | this is where we input the query keywords for the search  ##### Back4App API - Base URL - [https://parseapi.back4app.com/classes/Workout](https://parseapi.back4app.com/classes)    HTTP Verb | Endpoint | Description    ----------|----------|------------     `POST`    | /classes/Workout | we post to the workout object when the user finishes a workout     `GET`    | /classes/Workout   | we retrieve the workout data when the user is in home fragment     `GET`    | /users | we get user details when we are logging in, and the Profile fragment     `POST`    | /users | we post when the user is signing up for a new profile     "
Class Notes Sharing App,Mobile Applications,https://github.com/RivaanRanawat/notefynd,"<img src=""../master/screenshots/appIcon.jpeg"" alt=""logo"" width=""144"" height=""144"" align=""right"" />  # Notefynd Notefynd is a platform by, for and of students where people can share and view notes and articles to learn better, and help others do the same! [Lets go to Landing Page](https://www.notefynd.com)   ## Features  - Login & Signup With Email & Password - Google Sign in - Cross Platform - Adding & Editing Profile - PDF Notes Sharing     - Liking & Commenting     - Deleting     - Editing     - Attaching URL for Extra Resources - Video Sharing     - Liking & Commenting     - Deleting - Searching Users - Displaying User Profile     - Displaying Following & Followers of Users     - Displaying Total Likes Received By Users     - Displaying the PDF Shared By User on Profile - Requesting And Viewing Notes - Light,Dark,Default Mode Toggle - Creator Switching Option - Creator Studio     - Displays number of notes shared     - Displays number of Followers     - Displays Recent Posts of Users - Showing Articles Written By Admin - Admin Panel:     - Deleting Any Creator's Notes PDF     - Verifying PDF Notes     - Writing Articles     - Viewing Requested Notes & Removing Notes that have been shared.     - Displaying number of users and number of posts shared by users. - Pagination - Sign Out    ## Screenshots (Android)  ### User <img src=""https://github.com/RivaanRanawat/notefynd/blob/master/screenshots/user/user1.jpeg"" width=""200"" /> <img src=""https://github.com/RivaanRanawat/notefynd/blob/master/screenshots/user/user2.jpeg"" width=""200"" /> <img src=""https://github.com/RivaanRanawat/notefynd/blob/master/screenshots/user/user3.jpeg"" width=""200"" /> <img src=""https://github.com/RivaanRanawat/notefynd/blob/master/screenshots/user/user4.jpeg"" width=""200"" /> <img src=""https://github.com/RivaanRanawat/notefynd/blob/master/screenshots/user/user5.jpeg"" width=""200"" /> <img src=""https://github.com/RivaanRanawat/notefynd/blob/master/screenshots/user/user6.jpeg"" width=""200"" /> <img src=""https://github.com/RivaanRanawat/notefynd/blob/master/screenshots/user/user7.jpeg"" width=""200"" /> <img src=""https://github.com/RivaanRanawat/notefynd/blob/master/screenshots/user/user8.jpeg"" width=""200"" /> <img src=""https://github.com/RivaanRanawat/notefynd/blob/master/screenshots/user/user9.jpeg"" width=""200"" /> <img src=""https://github.com/RivaanRanawat/notefynd/blob/master/screenshots/user/user10.jpeg"" width=""200"" />  ### Admin <img src=""https://github.com/RivaanRanawat/notefynd/blob/master/screenshots/admin/admin1.jpeg"" width=""200"" /> <img src=""https://github.com/RivaanRanawat/notefynd/blob/master/screenshots/admin/admin2.jpeg"" width=""200"" /> <img src=""https://github.com/RivaanRanawat/notefynd/blob/master/screenshots/admin/admin3.jpeg"" width=""200"" /> <img src=""https://github.com/RivaanRanawat/notefynd/blob/master/screenshots/admin/admin4.jpeg"" width=""200"" /> ## Demo  Notefynd is available on: - [Android (Play Store)](https://play.google.com/store/apps/details?id=com.rivaan.notefynd) - [Web](https://app.notefynd.com)     ## Tech Stack  **Client:** Flutter, Provider  **Server:** Firebase     ## Feedback  If you have any feedback, please reach out to me at namanrivaan@gmail.com    "
Insta - Buy App.,Mobile Applications,https://github.com/rdowdle10/InstaBuy/tree/master,"# instabuy Be sure to import the Project into Android Studio and make changes. If anything, an APK (app-debug.apk) is available for a straight install for testing purposes."
Barcode Reader App,Mobile Applications,https://github.com/journeyapps/zxing-android-embedded,"# ZXing Android Embedded  Barcode scanning library for Android, using [ZXing][2] for decoding.  The project is loosely based on the [ZXing Android Barcode Scanner application][2], but is not affiliated with the official ZXing project.  Features:  1. Can be used via Intents (little code required). 2. Can be embedded in an Activity, for advanced customization of UI and logic. 3. Scanning can be performed in landscape or portrait mode. 4. Camera is managed in a background thread, for fast startup time.  A sample application is available in [Releases](https://github.com/journeyapps/zxing-android-embedded/releases).  By default, Android SDK 24+ is required because of `zxing:core` 3.4.x. SDK 19+ is supported with additional configuration, see [Older SDK versions](#older-sdk-versions).  ## Adding aar dependency with Gradle  Add the following to your `build.gradle` file:  ```groovy // Config for SDK 24+  repositories {     mavenCentral() }  dependencies {     implementation 'com.journeyapps:zxing-android-embedded:4.3.0' } ```  ## Older SDK versions  By default, only SDK 24+ will work, even though the library specifies 19 as the minimum version.  For SDK versions 19+, one of the changes below are required. Some older SDK versions below 19 may work, but this is not tested or supported.  ### Option 1. Downgrade zxing:core to 3.3.0  ```groovy repositories {     mavenCentral() }  dependencies {     implementation('com.journeyapps:zxing-android-embedded:4.3.0') { transitive = false }     implementation 'com.google.zxing:core:3.3.0' } ```  ### Option 2: Desugaring (Advanced)  This option does not require changing library versions, but may complicate the build process.  This requires Android Gradle Plugin version 4.0.0 or later.  See [Java 8+ API desugaring support](https://developer.android.com/studio/write/java8-support#library-desugaring).  Example for SDK 21+:  ```groovy android {     defaultConfig {         minSdkVersion 21     }      compileOptions {         // Flag to enable support for the new language APIs         coreLibraryDesugaringEnabled true         // Sets Java compatibility to Java 8         sourceCompatibility JavaVersion.VERSION_1_8         targetCompatibility JavaVersion.VERSION_1_8     } }  dependencies {     implementation 'com.journeyapps:zxing-android-embedded:4.3.0'      coreLibraryDesugaring 'com.android.tools:desugar_jdk_libs:1.1.5' } ```  SDK 19+ additionally requires multiDex. In addition to these gradle config changes, the Application class must also be changed. See for details: [Configure your app for multidex](https://developer.android.com/studio/build/multidex#mdex-gradle).  ```groovy android {     defaultConfig {         multiDexEnabled true         minSdkVersion 19     }      compileOptions {         // Flag to enable support for the new language APIs         coreLibraryDesugaringEnabled true         // Sets Java compatibility to Java 8         sourceCompatibility JavaVersion.VERSION_1_8         targetCompatibility JavaVersion.VERSION_1_8     } }  dependencies {     implementation 'com.journeyapps:zxing-android-embedded:4.3.0'      coreLibraryDesugaring 'com.android.tools:desugar_jdk_libs:1.1.5'     implementation ""androidx.multidex:multidex:2.0.1"" } ```  ## Hardware Acceleration  Hardware acceleration is required since TextureView is used.  Make sure it is enabled in your manifest file:  ```xml     <application android:hardwareAccelerated=""true"" ... > ```  ## Usage with ScanContract  Note: `startActivityForResult` is deprecated, so this example uses `registerForActivityResult` instead. See for details: https://developer.android.com/training/basics/intents/result  `startActivityForResult` can still be used via `IntentIntegrator`, but that is not recommended anymore.  ```java // Register the launcher and result handler private final ActivityResultLauncher<ScanOptions> barcodeLauncher = registerForActivityResult(new ScanContract(),         result -> {             if(result.getContents() == null) {                 Toast.makeText(MyActivity.this, ""Cancelled"", Toast.LENGTH_LONG).show();             } else {                 Toast.makeText(MyActivity.this, ""Scanned: "" + result.getContents(), Toast.LENGTH_LONG).show();             }         });  // Launch public void onButtonClick(View view) {     barcodeLauncher.launch(new ScanOptions()); } ```  Customize options: ```java ScanOptions options = new ScanOptions(); options.setDesiredBarcodeFormats(ScanOptions.ONE_D_CODE_TYPES); options.setPrompt(""Scan a barcode""); options.setCameraId(0);  // Use a specific camera of the device options.setBeepEnabled(false); options.setBarcodeImageEnabled(true); barcodeLauncher.launch(options); ```  See [BarcodeOptions][5] for more options.  ### Generate Barcode example  While this is not the primary purpose of this library, it does include basic support for generating some barcode types:  ```java try {   BarcodeEncoder barcodeEncoder = new BarcodeEncoder();   Bitmap bitmap = barcodeEncoder.encodeBitmap(""content"", BarcodeFormat.QR_CODE, 400, 400);   ImageView imageViewQrCode = (ImageView) findViewById(R.id.qrCode);   imageViewQrCode.setImageBitmap(bitmap); } catch(Exception e) {  } ```  To customize the generated barcode image, use the `setBackgroundColor` and `setForegroundColor` functions of the `BarcodeEncoder` class with a [`@ColorInt`](https://developer.android.com/reference/androidx/annotation/ColorInt) value to update the background and foreground colors of the barcode respectively. By default, the barcode has a white background and black foreground.   ### Changing the orientation  To change the orientation, specify the orientation in your `AndroidManifest.xml` and let the `ManifestMerger` to update the Activity's definition.  Sample:  ```xml <activity 		android:name=""com.journeyapps.barcodescanner.CaptureActivity"" 		android:screenOrientation=""fullSensor"" 		tools:replace=""screenOrientation"" /> ```  ```java ScanOptions options = new ScanOptions(); options.setOrientationLocked(false); barcodeLauncher.launch(options); ```  ### Customization and advanced options  See [EMBEDDING](EMBEDDING.md).  For more advanced options, look at the [Sample Application](https://github.com/journeyapps/zxing-android-embedded/blob/master/sample/src/main/java/example/zxing/MainActivity.java), and browse the source code of the library.  This is considered advanced usage, and is not well-documented or supported.  ## Android Permissions  The camera permission is required for barcode scanning to function. It is automatically included as part of the library. On Android 6 it is requested at runtime when the barcode scanner is first opened.  When using BarcodeView directly (instead of via IntentIntegrator / CaptureActivity), you have to request the permission manually before calling `BarcodeView#resume()`, otherwise the camera will fail to open.  ## Building locally      ./gradlew assemble  To deploy the artifacts the your local Maven repository:      ./gradlew publishToMavenLocal  You can then use your local version by specifying in your `build.gradle` file:      repositories {         mavenLocal()     }  ## Sponsored by  [JourneyApps][1]   ## License  Licensed under the [Apache License 2.0][7]  	Copyright (C) 2012-2022 ZXing authors, Journey Mobile  	Licensed under the Apache License, Version 2.0 (the ""License""); 	you may not use this file except in compliance with the License. 	You may obtain a copy of the License at  	    http://www.apache.org/licenses/LICENSE-2.0  	Unless required by applicable law or agreed to in writing, software 	distributed under the License is distributed on an ""AS IS"" BASIS, 	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. 	See the License for the specific language governing permissions and 	limitations under the License.    [1]: http://journeyapps.com [2]: https://github.com/zxing/zxing/ [5]: zxing-android-embedded/src/com/journeyapps/barcodescanner/ScanOptions.java [7]: http://www.apache.org/licenses/LICENSE-2.0"
Face Identification App,Mobile Applications,https://github.com/atharvakale31/Real-Time_Face_Recognition_Android,"# Real Time Face Recognition App using TfLite  A minimalistic Face Recognition module which can be easily incorporated in any Android project.  ## [Playstore Link](https://play.google.com/store/apps/details?id=com.atharvakale.facerecognition)  ## Key Features  - Fast and very accurate. - No re-training required to add new Faces. - Save Recognitions for further use. - Real-Time and offline. - Simple UI.  ## Tools and Frameworks used: - Android Studio (Java) - CameraX - ML Kit - TensorFlow Lite  ## Model  - MobileFaceNet : [Research Paper](https://arxiv.org/ftp/arxiv/papers/1804/1804.07573.pdf) - [Implementation](https://github.com/sirius-ai/MobileFaceNet_TF)  ## Installation  Use Import from Version Control in Android Studio or Clone repo and open the project in Android Studio.  ```bash git clone https://github.com/atharvakale31/Face_Recognition_Android.git ``` ### Application file : [Face_Recognition.apk](https://drive.google.com/file/d/1ggOo4acHOodrdCP2MkfUv4DJlL_VDZH4/view?usp=sharing)  ## Usage <table>   <tr>     <td><b>1.Add Face</b></td>      <td><b>2.Import Face</b></td>      <td><b>3.Recognize Face</b></td>         </tr>   <tr>     <td><img src=""demo/add_face.gif"" width=270 height=480></td>   <td><img src=""demo/import photo.gif"" width=270 height=480></td>     <td><img src=""demo/recognize_face.gif"" width=270 height=480></td>      </tr>  </table>       <table>   <tr>     <td><b>Actions</b></td>      <td><b>View Recognitions</b></td>      <td><b>Update Recognitions</b></td>   </tr>   <tr>     <td><img src=""demo/actions.jpeg"" width=270 height=480></td>     <td><img src=""demo/view_reco.jpeg"" width=270 height=480></td>     <td><img src=""demo/update_reco.jpeg"" width=270 height=480></td>   </tr>  </table>   ## Contributing Pull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.    # Action Items - [ ] Improve Performance(Code Optimization) - [ ] Auto face orientation for Import Photo Action. - [ ] iOS application "
Blog App,Mobile Applications,https://github.com/keerthivasaa/Blog-App-in-Android-Studio,"# Blog-App  <img src=""https://user-images.githubusercontent.com/47691119/89096929-0952e880-d3f8-11ea-8d6c-52b68e8c8064.jpg"" width=""500"" height=""500"">    This application brings you closer to the people and things you love.    Connect with peoples, share what you‚Äôre up to, or see what's new from others all over the world.    Explore our community where you can feel free to be yourself and share everything from your daily moments to life's highlights.    # Demo-Link   https://youtu.be/qmvFjkwolvI  # Nickname and Uses   The nickname of this application is VAG.    This application enables you to reach the billions of people that use the Internet.   VAG can help you promote yourself or your business.    VAG works as a method for attracting an audience because it provides something of value to them before asking for anything in return.  # Techinical   This code has been written in Framework of ANDROID STUDIO, Front-End in XML as well as Backend in JAVA using Database has FIREBASE.   You should want to generate an SHA-key in ANDROID STUDIO and paste in FIREBASE of your project application.   # Credits   k3vasan_official (FullStack developer)   contact:-   Insta:k3vasan_official    Prasath Niraj (drawable/images)   contact:-   Insta:prasathniraj  "
Chatting App (similar to WhatsApp),Mobile Applications,https://github.com/Tauseef-Hilal/WhatsUp,"# WhatsUp: The Ultimate Chat App  This project brings you the **closest** experience to WhatsApp, crafted with the power of Flutter and Firebase. With features like real-time conversations, efficient media transmission, voice messaging, and robust security, we've recreated the essence of WhatsApp while adding our unique touch. Discover the familiarity you love, paired with exciting enhancements, all in one package.  [üöÄ **Try the latest release now!**](https://github.com/Tauseef-Hilal/WhatsUp/releases/tag/v0.1.1)  ## Watch The Video  https://github.com/Tauseef-Hilal/WhatsUp/assets/67793598/3fd21809-5f38-4bd8-ab03-487556c4e915  > **Note**: If the video is unplayable here, you can [**watch it on youtube**](https://youtu.be/SYPK5g2zr-A?si=W4MT1vmJ_uJS1LNA).  ## Screenshots  Take a closer look to get to know about some of the app features  <section>     <img src='screenshots/emoji.png?raw=true' alt='Emoji Picker' width='250px' />     <img src='screenshots/home.png?raw=true' alt='Home Page' width='250px' />     <img src='screenshots/chat.png?raw=true' alt='Chat Page' width='250px' />     <img src='screenshots/document.png?raw=true' alt='Document Upload' width='250px' />     <img src='screenshots/voice.png?raw=true' alt='Voice Chat' width='250px' />     <img src='screenshots/gallery.png?raw=true' alt='Gallery' width='250px' /> </section> <br>  > **Note**: If screenshots are not visible, you might need a VPN or a different WIFI  ## Features That Shine  - **Real-time Conversations:** Experience real-time messaging, just like you would on your favorite chat apps like WhatsApp. - **Message Attachments:** Share and receive images, music, videos, and documents with ease. Our **compression** technology optimizes storage and bandwidth consumption. - **Voice Chat:** Capture the essence of your thoughts with voice messages, adding a personal and dynamic touch to your conversations. - **Slick Interface:** Navigate smoothly through our beautifully designed interface, crafted with Flutter to ensure a delightful user experience. - **Stay Updated:** Never miss a beat with push notifications that keep you informed even when the app is in the background. Notification work only on Android because APNs requires a paid subscription. - **Offline Access:** Access your chats seamlessly, whether you're online or offline. - [There's more to come! Stay tuned](#the-future-is-exciting)  ## Fortified by Firebase Firestore üåê  At the heart of WhatsUp is the mighty Firebase Firestore. We rely on Firestore to securely save user data and facilitate seamless message transmission. Each message sent by a user is immediately whisked away to Firestore, ensuring efficient and reliable delivery. Notably, messages sent are promptly removed from Firestore as soon as they are received by the recipient's device, enhancing privacy and security.  ## Your Privacy, Our Priority üõ°Ô∏è  In WhatsUp, messages within a chat are stored exclusively on the phones of the communicating individuals. This design ensures that your conversations remain private and inaccessible to unauthorized parties. Your messages are your own, and we're committed to keeping it that way.  ## Known Issues  - Sometimes attachments that were received while in the chat page are not playable - Seen feature not working sometimes - Performance issues in app camera (especially on Android)  ## Contribute and Elevate ü§ù  We heartily welcome contributions from the community to elevate the project. Share your insights, submit those bug reports, request features, and send those pull requests to make WhatsUp shine even brighter.  ## The Future is Exciting  Exciting enhancements await us on the horizon, including:  - **Voice & Video Calling** Connect face-to-face or chat with your voice using our intuitive video and voice calling features. - **Group Chats:** Coming soon, group chat functionality to engage with multiple contacts at once. - **Status Updates** Share your current mood or updates with your friends using status messages. - **Backups** Safeguard your chats and memories with easy-to-use backup and restore functionality. - **Enhanced Notifications** Stay in the loop with detailed notifications, even when the app is in the background. - **Message Management** Forward, delete, or reply to messages effortlessly to keep conversations organized. - **Encrypted Secrets:** Adding end-to-end encryption for messages, attachments, and voice messages to keep your conversations yours. - **Your Way, Your Look:** Customization options to make your WhatsUp experience uniquely yours.  Stay tuned for these fantastic updates!"
Shooting Target Game,Game Development,https://github.com/Aayush-Gangwar/Living-Dead-Fps_3d_shooting-game,"# Living Dead Game link : https://aayush21.itch.io/living-dead  A Unity based First Person Shooter game with Normal and Zombie modes built with Unity terrain using raycasting for shooting and the core mechanics you'd expect in an FPS with available item pick-ups. Enemies use AI navigation and precise pathfinding to chase down the player.  Implemented my algorithms for AI pathfinding, navigation, raycasting, and other mechanics. Online free assets were mostly used excluding some which've been self-designed customly.  The game is PC (can also be customizable and exported for other platforms). Open this project with Unity and choose build target in build settings to export runnable and play.  ###### Note  Developed with Unity 2020.3.24f1.  ## Gameplay [Gameplay](https://www.youtube.com/watch?v=Xocj5toyDDg)  ## Game Logic and Functionality  - Hunt down all the enemies in the terrain by finding them to win before timer ends. - The Enemies patrols at  waypoints until it detects the player as a target. - If the Player enters the enemies perspective, the enemies starts to chasing the player. - The Enemy starts shooting when it close enough to attack while chasing. - Enemies health will recharge after heeling time ..if left unattacked. - Headshot or Eyeshot will give extra damage to enemies. - Ammo pickups for guns and medikit pickup for the players health are available at certain places in the map. - Two guns are available, Pistol, AK-47 (have different attack range) and grenades with some pre-loaded bullets. To change between guns, use the scroller of your mouse.  ## Game Features * User Interface-for all features/modes.     <img src=""https://user-images.githubusercontent.com/101112022/176936728-722c1085-63b7-458e-b47d-6bc2da399cbf.png"" style=""width:500px""></img> * Game interface   * **Player's HP** on the bottom right corner   * The **Timer** on the top mid, which shows time left in over of game.   * **Weapon sprites** is always shown on the bottom left corner which shows the ammo and grenade counts.   * A white **shooting sight** is always in the front of player weapon aim.   * A  **Mini map** is always top right corner of screen showing status of enemies position around player.   * **Kills** are below of Mini map         <img src=""https://user-images.githubusercontent.com/101112022/176938001-c9e42eff-36e1-4610-9cbf-07ea78ed54f9.png"" style=""width:500px""></img> * Enemy models   * There are three types of player **models**:     * **Zombies**     * **Fly Robots**     * **Soldiers**           <img src=""https://user-images.githubusercontent.com/101112022/176939444-3f4f9ef7-5698-46fc-9a66-e313af5eb119.png"" height=""200px""></img><img src=""https://user-images.githubusercontent.com/101112022/176939490-bed2aef1-cfb2-44fc-a0ab-0e14634babd4.png"" height=""200px""></img> <img src=""https://user-images.githubusercontent.com/101112022/176939463-33b91d21-81c7-4508-87c6-4703f6e7cab8.png"" height=""200px""></img> <img src=""https://user-images.githubusercontent.com/101112022/176939475-e49c0115-2abf-472b-af12-224e6edabd94.png"" height=""200px""></img>  * **Animations**:     * **Walk** towards four different directions     * **Run** towards four different directions     * **Jump** without affecting upper part body (**achieved by unity3d body mask**)     * **Shoot** without affecting lower part body (**achieved by unity3d body mask**)     * **Dying and Headshot**  * **Gun models**:    <img src=""https://user-images.githubusercontent.com/101112022/176951024-fc97b7bf-1872-49f9-a79d-c04779ac171a.png"" height=""200px""> <img src=""https://user-images.githubusercontent.com/101112022/176951039-8a4c2b97-1c1a-4e7b-8f99-8a36e29b213f.png"" height=""200px"">   * **Particle effects**:     * **Blood effect**     * **Player hurt**     * **Enemy finish (red smoke)**     * **Explosions**     * **Bullet effect**          <img src=""https://user-images.githubusercontent.com/101112022/176952175-d6c2178f-4a00-40d7-8b0e-636eaeebdc34.png"" height=""150px""></img>     <img src=""https://user-images.githubusercontent.com/101112022/176952086-94adef45-e5a8-4535-8caf-961e0aed3fe6.png"" height=""150px""></img>      <img src=""https://user-images.githubusercontent.com/101112022/176953031-61b49467-4b7a-410b-8cb3-edc01624c7de.png"" height=""150px""></img>      <img src=""https://user-images.githubusercontent.com/101112022/176952062-1e7bbc77-7da1-4a58-a2ce-897fe4587133.png"" height=""150px""></img>       <img src=""https://user-images.githubusercontent.com/101112022/176952027-20d5dad9-4c70-4e0d-9e07-ead7d8125c29.png"" height=""150px""></img>        * **Audio**:    * All the game features have seperates audio like guns,explosions,player hurt,enemy hurt,player die,enemy die,robot flying audio,enemy attack ,play and pause audio etc.                   ## Controls ##  | Action          | Desktop PC              | | --------------- | ----------------------- | | Move Forward    | W                       | | Move Left       | A                       | | Move Backward   | S                       | | Move Right      | D                       | | Jump            |  space                  | | Run             | Left Shift(hold)        | | Shoot           | Mouse Left Button       | | Aim             | Mouse Right Button      | | Previous Weapon | Q *or* Mouse Wheel Up   | | Next Weapon     | Q *or* Mouse Wheel Down | | Reload          | R (when near ammo box)  | | Health Recharge |Capslock (near medikit)  | |Throw Grenade    |  G                      |  ## Installation - Install the game from [Living Dead](https://aayush21.itch.io/living-dead)     and Enjoy!!"
Endless Runner,Game Development,https://github.com/sunsided/unity-endless-runner,"# Endless Runner  In space, no-one can hear you. (Actual fact.)  ![](.readme/final-frontier.png)  This project implements an endless runner game by following and extending on the ideas in the [Create a 3D Endless Runner from Scratch in Unity](https://www.udemy.com/course/endlessrunner/) Udemy course.  I have to say that while following the course was a really good learning experience, the coding style in the lectures is just horrible. So ... better to follow the course in space.  Since I'm still learning Unity at this point and am not aware of Unity-specific best practices, I'm not sure if I'm was doing the right thing here when trying to keep or  save my sanity. Also, some of the bugs - such as being able to jump multiple times, or being able to cancel the dying animation through jumping - are not fixed here. Anyway, maybe someone does find this project helpful regardless of that.  ## Implementation  Core ideas:  - Tiles are being spawned by a ""virtual character"" moving ahead of the player. - The player isn't moving horizontally at all, the world is. - Platforms are spawned from an object pool and everything behind the player is recycled immediately.  This project uses:  - Object pooling, - The animation system with blended animations, - Animation triggers (shooting magic, footstep sounds, ...), - A bunch of particle systems, - Overlapping colliders with initially deactivated physics for exploding wall bricks.  The following video shows the continuous world generation by using physics collisions triggers:  ![Platform generation video](.readme/early-animation.webp)  Here's some gameplay footage:  ![Gameplay video](.readme/late-game.webp)  A platform prefab showing the triggers used for spawning new platforms ahead of the player:  ![](.readme/box-triggers.png)  One of the bigger obstacles in the game, the wall, being blasted away by the magic spell projectile:  ![](.readme/blasting-boxes.jpg)"
Car Racing Game,Game Development,https://github.com/VehanRajintha/Unity-Full-Project-Car-Racing-Game-,"# Unity-Full-Project-Car-Racing-Game-   ![pic](pic.png)   This project was done by me for one of my campus projects and here the full project has been uploaded as a Mediafire Link so that it can be downloaded only and anyone can download it by following the link below.  Mediafire Link :  https://www.mediafire.com/file/axg48uoidiyzwrf/Project.zip/file (Compressed Size 1.2GB )  Zip File¬†Password¬†:¬†1490  Unity Editor Version :- Unity 2021.3.25f1   Note :- MUST READ THIS .....üëá  ""There may be small mistakes in this project, for example, there are small mistakes in car movements and car sounds.  You just have to open it and you have full freedom to change the things you want and make¬†it¬†as¬†you¬†like""  All the Coding Scripts related to this project have been uploaded above and they are also included in the Full¬†Project¬†Zip."
Warfare Game,Game Development,https://github.com/dreadwing5/laser-warfare-unity,# Laser Warfare  ## An original take on the classic arcade shooter game Galaxy Attack: Alien Shooter.  This game was created using Unity Engine. You can play the game here :  https://dreadwing5.itch.io/laser-warfare  Have Fun!
Free Fall Game,Game Development,https://github.com/arijoon/FreeFall-Unity3D-Game,"# Freefall game made in UNITY3D  Source code distributed for reference if anyone is interested. Using Zenject as main bootstrapper so please check out the documentation for zenject for further info: https://github.com/modesttree/Zenject  # Assets and credits  Credits are given in the respected download page for the app. Not all assets are under the same license as the source code. If they are not owned by me, they'll have their own license  # Download App   - [Google Play](https://play.google.com/store/apps/details?id=net.yaraee.freefall)  - [App Store](https://itunes.apple.com/us/app/free-fall-sky-free-falling/id1163543558?ls=1&mt=8)"
Doodle Jump Clone,Game Development,https://github.com/OsmanFrat/doodle-jump-clone,# doodle-jump-clone Unity 2D doodle jump clone tutorial\ Play link: https://osmanfrat.itch.io/goodle-jump  ![image](https://github.com/OsmanFrat/doodle-jump-clone/assets/69113898/5638ac01-11e2-40fe-a3ef-fcbbf10deaf4) 
Simon Says,Game Development,https://github.com/IvoDeK/SimonSays,# SimonSays A simple Simon Says spin off game made in unity with c#  Game starts up directly after pressing start.
Tic Tac Toe,Game Development,https://github.com/ivuecode/TicTacToe,"# Tic Tac Toe :x: :o: The classic game noughts and crosses created in Unity.    Using UI elements and a simple game controller we re-created Tic-tac-toe. Players can set there names in the input fields and then you just mark three in a row to win!  ## Preview :eyes: ![gif demo](https://giant.gfycat.com/ViciousThriftyCarpenterant.gif)    ## Getting Started :page_with_curl: Clone or download this repository and open the project with your favourite flavour of Unity.   _This project was built with Unity 2018.2_  ## Optimizations :pencil2: Checkecing for any current win condition can probably be optimised, {EndTurn();} however this is such a simple game the overhead just is not there to worry about  ## Contributing :muscle: Looking to contribute something to this project? **Here's how you can help.**   If you believe something needs to be immediately fixed please open an issue and document the problem. Fork this project and create a pull request with your solution to the problem. Thank you.    [VueCode YouTube](https://www.youtube.com/channel/UCtP-1zQ2g_jpgYvvBqkWltA)   [VueCode Twitter](https://twitter.com/VueCode/)   [VueCode Discord](https://discord.gg/qWpEtR3)"
Pong,Game Development,https://github.com/zigurous/unity-pong-tutorial,"# Pong (2D)  > Pong is a table tennis-themed arcade video game, featuring simple two-dimensional graphics, manufactured by Atari and originally released in 1972. It was one of the earliest arcade video games; it was created by Allan Alcorn as a training exercise assigned to him by Atari co-founder Nolan Bushnell. Bushnell based the game's concept on an electronic ping-pong game included in the Magnavox Odyssey, the first home video game console.  - **Topics**: Physics, AI - **Version**: Unity 2019.4 (LTS) - [**Download**](https://github.com/zigurous/unity-pong-tutorial/archive/refs/heads/main.zip) - [**Watch Video**](https://youtu.be/AcpaYq0ihaM)"
Procedural Mesh Generation,Game Development,https://github.com/BastiaanOlij/gdprocmesh,"Procedural Mesh Generation GDNative module ==========================================  This is my attempt at creating a procedural mesh generator for the [Godot Game Engine](https://godotengine.org/).  ![example](https://github.com/BastiaanOlij/gdprocmesh/raw/master/example.png)   The ""normal"" way of creating 3D assets for your game involves using modeling software such as Maya, Blender, 3DSMax or any of the other tools out there and for many of the assets you'll want to use in your game this is by far the prefered way of creating assets. There is however one limitation to using these types of software to create 3D assets and that is that they are essentially static. Yes you can animate them but if you create a 2 story house, you'll have to create a new asset if you wish to have a 3 story house.  This is the type of problem procedural modeling attempts to solve. Instead of focussing on an end result the goal is to tell the computer how an asset is to be constructed.  This is best explained with a video: [Timelapse - Procedural modeling in Godot](https://youtu.be/X574IIBgOko)  That video shows the construction of a simple rope bridge where the length and width can be adjusted and instead of the bridge getting stretched, as would happen if you scale the bridge, it actually adds ropes and boards.  You will need Godot 3.1 or later to run this module. A copy of the 64bit Windows DLL is included in the demo folder, use at your own risk.  Downloading and compiling ========================= You will need a C++ compiler, python and scons installed on your machine, same build tools as Godot so see [compiling](http://docs.godotengine.org/en/latest/development/compiling/index.html). Instructions below have only been tested on Windows and the scons file may need tweaking on other platforms.  It is best to clone this repository from a terminal like so: ``` git clone --recursive https://github.com/BastiaanOlij/gdprocmesh.git cd gdprocmesh ```  This project relies on [godot_headers](https://github.com/GodotNativeTools/godot_headers) and [godot-cpp](https://github.com/GodotNativeTools/godot-cpp) which have been submoduled in this repository.  These dependencies will need to be compiled: ``` cd godot-cpp scons platform=windows generate_bindings=yes cd .. ``` (custom api is temporary as we needed a newer api file)  You can add ```-j<n>``` to this to speed up compiling, replace ```<n>``` with the number of cores your machine has.  Now we can compile our module: ``` scons platform=windows ```  Note that you may want to compile both godot-cpp and our module with the added switch ```target=release``` to create a release build.  Trying out the test project =========================== After successful compilation the dynamic library should be placed in the addons folder of the example demo project. Again I've only configured things for windows so you may need to tweak the gdprocmesh.gdnlib file for other platforms.  Simply import the demo project into Godot and have fun.  If you want to use this for your own projects, simply copy the addons folder into your project and make sure that the plugin is turned on.  Now create a ```MeshInstance``` and create an ```ArrayMesh``` for this node. You can now drag the gdprocmesh.gdns file into the script property of the ArrayMesh. This will create a simple box as a starting template. Simply re-select the ArrayMesh in the property manager and you'll see that you're given a new graph editor.  Everything is an array ====================== Well, almost everything. Most nodes are designed to take arrays as input and you can do some funky things with this.   For instance, on face value GDProcScale will scale an array of vertices. So your input is an array of vertices, you specify a vertex by which to scale this input, and your output results in an array of vertices that have been scaled according to the 2nd input. However the 2nd input can be an array too. Let's say you input 5 vertices with 3 scales: Vertice 1 will be scaled by the first scale Vertice 2 will be scaled by the second scale Vertice 3 will be scaled by the third scale Vertice 4 will be scaled by the first scale Vertice 5 will be scaled by the second scale So the scales are applied in order and it simply repeats the pattern until all vertices are scaled.  Many of the nodes work in this way where the different input arrays are combined to create the output.   Similarly primitives like the vector and euler nodes will output arrays if the input is an array of value.  Rotation ======== To rotate elements of your mesh there are various nodes that take a rotation as input. The current input options all center around euler angles but internally these are converted to quaternions. Due to lacking a quaternion array construct we're using a vec3 array and passing uniform quaternions around where w is assumed to be sqrt(1.0 - (x¬≤ + y¬≤ + z¬≤)). It is then these quaternions that are passed from node to node. Additional nodes will be added in the near future that will allow you to construct rotations on other inputs like a rotation around an axis.  License ======= I've made the source in this repository available under an MIT license. If you find it useful, it would always be nice to get a mention.  The following 3rd party libraries are used:  * [Fast Quadric Mesh Simplication](https://github.com/sp4cerat/Fast-Quadric-Mesh-Simplification) - MIT licensed  We also make use of the algorithm explained here:  * [Lengyel, Eric. ‚ÄúComputing Tangent Space Basis Vectors for an Arbitrary Mesh‚Äù. Terathon Software, 2001](http://www.terathon.com/code/tangent.html)  Note that for the demo:  * HRDI texture is from [HDRI Haven](https://hdrihaven.com) * Rope and plank texture is from [CC0 Textures.com](https://cc0textures.com) * Not sure where the numbered textures came from  Please visit these sites for further license details  About this repository --------------------- This repository was created by and is maintained by Bastiaan Olij a.k.a. Mux213  You can follow me on [Twitter](https://twitter.com/mux213) for regular updates.  Videos about my work with Godot can by found on my [Youtube page](https://www.youtube.com/BastiaanOlij)"
Temple Run,Game Development,https://github.com/kenmaz/TempleRun-Unity,TempleRunUnity ==============  TempleRun clone with Unity
Snake Game,Game Development,https://github.com/zigurous/unity-snake-tutorial,"# Snake (2D)  > Snake is the common name for a video game concept where the player maneuvers a line which grows in length, with the line itself being a primary obstacle. The concept originated in the 1976 arcade game Blockade, and the ease of implementing Snake has led to hundreds of versions (some of which have the word snake or worm in the title) for many platforms. After a variant was preloaded on Nokia mobile phones in 1998, there was a resurgence of interest in the snake concept as it found a larger audience.  - **Topics**: Grid Movement, Data Structures - **Version**: Unity 2019.4 (LTS) - [**Download**](https://github.com/zigurous/unity-snake-tutorial/archive/refs/heads/main.zip) - [**Watch Video**](https://youtu.be/U8gUnpeaMbQ)"
Sudoku,Game Development,https://github.com/Umar-Eh/Sudoku-Unity,"# Sudo app #  ### Instructions ### 1. If you do not have Unity, and visual studio installed, access to script (code) may be difficult   #### To play the game #### - download the <b>SudoGame.exe </b> - play the game!  #### if you have unity downloaded, and wish to build the game yourself ####  - Go into the <b> FinaleGame </b> folder - Go into <b> Assets </b>  - Go into <b> Scenes </b> - Open <b>Main.unity </b> in unity      #### if you wish to see the code using text editor without unity ####  - Go to the <b> FinaleGame </b> folder  - Go in to <b> Assets</b> - Go into <b> Scripts </b> and C# files are available  ## Demo ## **Starting menu:** <br><br> <a href=""https://gyazo.com/625467430139525f87455f3ca88e93ca""><img src=""https://i.gyazo.com/625467430139525f87455f3ca88e93ca.png"" alt=""https://gyazo.com/625467430139525f87455f3ca88e93ca"" width=""742""/></a>  **Board Generation:** <br><br> <a href=""https://gyazo.com/d64ef115aa35599b385b193db503dede""><img src=""https://i.gyazo.com/d64ef115aa35599b385b193db503dede.png"" alt=""https://gyazo.com/d64ef115aa35599b385b193db503dede"" width=""674""/></a>  **Inserting a value:** <br><br>  <a href=""https://gyazo.com/9c757a10d505a34d38f1023cbffc5c0d""><img src=""https://i.gyazo.com/9c757a10d505a34d38f1023cbffc5c0d.png"" alt=""https://gyazo.com/9c757a10d505a34d38f1023cbffc5c0d"" width=""607""/></a>  **Validation:** <br><br>  <a href=""https://gyazo.com/a98471c49a81f71cf0b1d1691167d1fb""><img src=""https://i.gyazo.com/a98471c49a81f71cf0b1d1691167d1fb.png"" alt=""https://gyazo.com/a98471c49a81f71cf0b1d1691167d1fb"" width=""709""/></a>  **Invalid value detection:** <br><br> <a href=""https://gyazo.com/2ff15ae8228fe5f77cf73c30fd610cdc""><img src=""https://i.gyazo.com/2ff15ae8228fe5f77cf73c30fd610cdc.png"" alt=""https://gyazo.com/2ff15ae8228fe5f77cf73c30fd610cdc"" width=""607""/></a> "
Game like STACK (By KetchApp Games),Game Development,https://github.com/zeroboo/stack-game,"# stack-game A demo of stack game by ketchapp (https://www.youtube.com/watch?v=x3FeMFMAj9o]).   Created in Unity.   ## Build Open project with Unity Editor, choose Scene 'Game'.   ## TODO - Implement object trim by plane intersec"
Open World Game (Small Version),Game Development,https://github.com/sevketbinali/3D-Survival-Game-Unity,"# 3D-Survival-Game-Unity Open-world 3D survival game made with Unity. üéÆ   <p align=""center""> <img align=""center"" alt=""PNG"" src=""https://github.com/sevketbinali/3D-Survival-Game-Unity/blob/master/Screenshots/Images/main_menu.jpg?raw=true"" /> </p>  <h4 align=""center"">  	3D Survival Game (Geriye D√∂n√º≈ü) - Main Menu  </h4>  <br>     <p align=""center""> <a href=""https://unity3d.com""> <img alt=""Made with Unity"" src=""https://img.shields.io/badge/Made%20with-Unity-57b9d3.svg?logo=unity"">      <a href=""https://github.com/sevketbinali/3D-Survival-Game-Unity/commits""> <img alt=""Last commit"" src=""https://img.shields.io/github/last-commit/sevketbinali/3D-Survival-Game-Unity""> </p>   <p align=""center""> <a href=""https://github.com/sevketbinali?tab=followers""> <img alt=""Last commit"" src=""https://img.shields.io/github/followers/sevketbinali?style=social""> <a href=""https://github.com/sevketbinali/3D-Survival-Game-Unity/stargazers""> <img alt=""Last commit"" src=""https://img.shields.io/github/stars/sevketbinali/3D-Survival-Game-Unity.svg?style=social&label=Star&maxAge=2592000"">  </p> </a> 	 <br>  The project is using the following version of Unity: [Unity 2020.3.35f1](https://unity3d.com/get-unity/download/archive). <br>  Get the build version for windows_x64 from this [link](https://github.com/sevketbinali/3D-Survival-Game-Unity/releases/tag/alpha-v1.0). <br> You can find more details [here](https://github.com/sevketbinali/3D-Survival-Game-Unity/blob/master/Documentation.pdf).     ### Setup project in Unity : `git clone https://www.github.com/sevketbinali/3D-Survival-Game-Unity.git` <br> Open the cloned repository in unity.  ### Requirements for Windows :   Go to [Unity's download page](https://store.unity.com/download) and click ‚ÄúDownload Installer for Windows‚Äù.<br>  Install UnityHub.<br>  With UnityHub opened click **ADD**<br>  Select the project directory<br>  Wait unity setup your project <br>  This project also uses URP. <br>  You need to add Universal RP in unity package manager.  ### Controls :  Move  :&nbsp;&nbsp; W, A, S, D or arrow keys. <br>  Jump :&nbsp;&nbsp; Space button. <br>   Open/Close Inventory:&nbsp;&nbsp; ""TAB"". <br>  Interact:&nbsp;&nbsp; ""E"". <br>  Attack or Gather:&nbsp;&nbsp; Left Mouse Click button. 	 ### Gameplays : 	 #### Inventory System :	  ![Gif](https://github.com/sevketbinali/3D-Survival-Game-Unity/blob/master/Screenshots/Gifs/inventory-and-pickup.gif) 	 #### Gathering Woods :  	 ![Gif](https://github.com/sevketbinali/3D-Survival-Game-Unity/blob/master/Screenshots/Gifs/gathering-woods.gif) 	 #### Gathering Stones :  	 ![Gif](https://github.com/sevketbinali/3D-Survival-Game-Unity/blob/master/Screenshots/Gifs/gathering-stones.gif) 	 #### Crafting System :  	 ![Gif](https://github.com/sevketbinali/3D-Survival-Game-Unity/blob/master/Screenshots/Gifs/crafting.gif) 	 #### Building System :  ![Gif](https://github.com/sevketbinali/3D-Survival-Game-Unity/blob/master/Screenshots/Gifs/building-and-sleep.gif) 	 #### Attacking System :   ![Gif](https://github.com/sevketbinali/3D-Survival-Game-Unity/blob/master/Screenshots/Gifs/attacking.gif) 	 #### Underwater Effect :   ![Gif](https://github.com/sevketbinali/3D-Survival-Game-Unity/blob/master/Screenshots/Gifs/underwater-effect.gif)   ### Credits :   Created by ≈ûevket Binali - sevketbinali@gmail.com  Kenney assets : [Survival Kit](https://www.kenney.nl/assets/survival-kit) &nbsp; [Animated Characters](https://www.kenney.nl/assets/animated-characters)  Lmhpoly assets : [Low Poly Pack](https://www.lmhpoly.com/unity-game-assets)  N-hance assets : [Animals](https://assetstore.unity.com/packages/3d/characters/animals/stylized-wild-animals-pack-159154)  SFX : [Zapsplat](https://www.zapsplat.com/?s=bear+&post_type=music&sound-effect-category-id=) &nbsp; [OpenGameArt](https://opengameart.org)  <a href=""https://github.com/sevketbinali/3D-Survival-Game-Unity/stargazers"">      <img alt=""Last commit"" src=""https://img.shields.io/github/stars/sevketbinali/3D-Survival-Game-Unity?style=plastic""> 	 "
Car Racing Game,Game Development,https://github.com/lucasctnh/street-racing-unity,"# Street Racing - Unity Game  ![Game Gif](https://github.com/lcscout/street-racing-unity/blob/main/SR-Gif_Compressed.gif ""Game Gif"")    Open sourced under the [MIT license](https://github.com/lcscout/street-racing-unity/blob/main/LICENSE).    ## What is it about  This is a racing game where you try to beat your own time, this is a submission project for the Unity course: [Junior Programmer](https://learn.unity.com/pathway/junior-programmer).    If you'd like to know more about the project and the development process, check it's [page on my website](https://lucascoutinho.dev/projects/street-racing).    ## Play the game  Only the [WebGL](https://get.webgl.org/) build is available, you can play it online [here]([https://play.unity.com/mg/other/builds-cl-1](https://play.unity.com/pt/games/fac255fe-31fa-47a7-8b3a-b4de448346b4/street-racing)).    ## Credits  The **unmodified** game assets are available for free at the [Unity Asset Store](https://assetstore.unity.com/), in particular,  - The cars models: [Cartoon Car - Vehicle Pack](https://assetstore.unity.com/packages/3d/vehicles/cartoon-car-vehicle-pack-180962)  - And the roads: [Low Poly Road Pack](https://assetstore.unity.com/packages/3d/environments/roadways/low-poly-road-pack-67288)    ## Requirements  This project requires [Unity3D](https://unity.com/) (2020.3+) to build and run. Clone this repository and open the Programming-Theory folder in Unity.    ## Credits  Created, designed, and developed by [Lucas Coutinho](https://lucascoutinho.dev). Open sourced under the [MIT](https://github.com/lcscout/street-racing-unity/blob/main/LICENSE) license ‚ù§Ô∏è. "
Shooting Games,Game Development,https://github.com/JFroggo-Gaming/Unity-FPS-game," # Unity 3D - First Person Shooter game <br>Made in Unity: 2021.3.5f1  <br>Unity 3D - First Person Shooter game - https://youtu.be/xiVQJyfHhlM     (25/06/2023)   <br><code>In this game you will find:</code>  <br>-The scene contains various destructible objects, each built with a unique material. <br><br>-Destruction of objects is accompanied by sound effects, animations, and particle effects (e.g., wood, metal, stone, leather). <br><br>-The player has 3 different weapons, each dealing different damage depending on the material being shot. <br><br>-Animations for running, shooting, reloading, aiming, etc. <br><br>-Scripts include comments in the code. <br><br>-Movement Instructions:  <br><code>Controls:</code>  <br>- W, S, A, D - character movement <br><br>- 1, 2, 3 - switch weapons / mouse scroll <br><br>- Left Mouse Button (LMB) - shoot / Ctrl <br><br>- Right Mouse Button (RMB) - zoom / Alt <br><br>- P - pause <br><br>- Space - jump <br><br>- C - crawl <br><br>- W, S, A, D + Shift - sprint"
Zelda-like RPG Game,Game Development,https://github.com/DavBraga/UnityGame-Zelda-Like,"# Dungeon Fall [PT-BR Readme](readme_PT_BR.md)  Game on itch.io: [Dungeon Fall on itch.io](https://bragadavi.itch.io/dungeon-fall)  Game Code Version on Git: 0.5.1  ## About  Dungeon Fall is a game developed in the Unity Engine as the final project for the [Unity 3D Game Developer Course](https://www.dio.me/curso-unity-3d) offered by DIO.me.  The project has been delivered and certified, meeting the course completion requirements. [Check the certificate here](https://www.dio.me/certificate/94C79951/share).  This game pays homage to classics like Zelda, Brave Fencer Musashiden, and Alundra, featuring a top-down camera perspective that aims to bring elements of exploration, combat, power-ups, cinematics, valuable treasures, and hidden secrets within a dungeon filled with mysteries.  ### Player Actions  - Move around, - Attack enemies, - Jump, - Use consumables, - Interact with nearby objects, - Utilize the bomb tool to destroy obstacles.  ## Additional Development  Although intended as a showcase of technical capabilities as a game developer, programmer, and Game Designer, I aimed to deliver a complete experience with a clear beginning, middle, and end. To achieve this, additional features were gradually incorporated. Several enhancements have been introduced to enrich the game. Here's a list of what's included in this project.  ### Technical Features  - Camera system using CineMachine for top-down view. - Character movement with custom physics. - Support for gamepads and virtual controls. - Ported to PC Windows, Browser (WebGL), and Android platforms. - Localization system with two implemented languages and support for expansion: PT-BR, EN. - Audio and graphical quality control menus. - Use of post-processing for scene and area atmosphere. - Customized particles for in-game effects. - State machines for various character and creature states.  ### Gameplay Features  - Combat against monsters with simple behavior scripts and NAVMESH navigation. - Scripted final battle with unique mechanics. - Power-ups. - Map and minimap systems. - Random drop system. - Cutscenes implemented using Unity Timeline.  ## Code Refinement  With the objectives and features now achieved and implemented, the code is undergoing a review to enhance human readability, modularity, scalability, and to reduce code coupling. This aims to facilitate the reuse of code components in other projects, streamline the code optimization process, and accommodate future expansion.  This process applies principles of Clean Code and SOLID in a flexible manner, targeting critical areas where the need is most apparent.  The first area being modified is the `PlayerController`, which has exhibited high coupling and an accumulation of functionalities, making it the largest script in the project. The applied design involves the use of events and delegates to partition each aggregated functionality within the script into independent scripts. These scripts will then utilize the `PlayerController` as an interface for actions, interactions, and reactions.  #### PlayerController Refactoring  The previous `PlayerController` script has been restructured into two separate scripts: `PlayerController` and `PlayerAvatar`. This division of functionality allows for clearer communication and organization within the game architecture.  - `PlayerController`: This script now handles player character interactions related to game rules and player input domains. It serves as the communication hub for interactions related to these aspects.  - `PlayerAvatar`: Responsible for managing the player avatar in the world and its physics interactions. it takes care of animation, rendering, physics and sounds.It serves as the communication hub for interactions related to these aspects.  These scripts are assigned to different game objects. The communication between them is bidirectional, ensuring that interactions are properly synchronized.  To achieve this refactor, the functionalities of the old `PlayerController` have been distributed among several new scripts:  - `PlayerPhysics`: Manages player physics interactions, such as movements and collisions.  - `PlayerCombat`: Handles combat-related mechanics, such as attacks, defenses, and damage calculations.  - `PlayerLifeCycle`: Manages the player's lifecycle events, including death, respawning, health gain and loss.  - `PlayerPowerUps`: Deals with the application and effects of power-ups collected by the player.  Alongside these new scripts, existing scripts that previously communicated with the old `PlayerController` have been adapted to work seamlessly with the new structure. This adaptation ensures that interactions between the revamped `PlayerController` and `PlayerAvatar` remain cohesive and effective. By distributing and redefining the responsibilities, the overall game architecture has been improved, leading to more organized and maintainable code.  ## Third Party Assets  ### Assets de Terceiros  Authors of the Third Party Assets used in this project:  - YouFulca - CatBorg Studio - Dungeon Mason - Just Labbing - Polygonal Stuff - Kenny - Leohpaz - Imphenzia   Feel free to reach out for clarifications or inquiries."
Pokemon clone RPG Game,Game Development,https://github.com/TU-AI-For-Games/Unity-AI-Pokemon-Clone,"# AI Powered Pok√©mon Clone  [![](https://markdown-videos.deta.dev/youtube/Zkf9cAFNilE)](https://youtu.be/Zkf9cAFNilE)  # Project Details The artefact created is a Pok√©mon battle simulator. The player navigates the world with a randomly selected team of 6 monsters from the original 151 monsters present in the 1996 video game.   In the game world, there are several ‚Äúwild pok√©mon‚Äù, who wander around the area using various pathfinding algorithms such as *A\* Pathfinding* and *Breadth-First Search*. In the sky, there are **Pidgey** who flock together. In another area, there are **Rattata** who use flocking with some bounds constraints to show off the *boids* algorithm in more depth.   <img src=""https://user-images.githubusercontent.com/48756858/235752937-d2fbf489-b3f3-43da-99e0-2ca612989b1f.png"" width=""80%"">  When the player encounters a wild Pokemon, a battle begins. During the wild Pokemon battles, the player‚Äôs chosen actions are recorded, which is used as training data for the Neural Network implementation. After each battle, the player‚Äôs Pokemon are reset to full health, making them ready to battle again.  Elsewhere in the world, there are NPC characters - or *trainers* - who, when interacted with, challenge the player to a battle. The NPC decision-making uses Behaviour Trees informed by an *Artificial Neural Network*. The game is completed by winning every battle with the trainers.   ## Pathfinding Implementation - [Emil Walseth](https://github.com/emilwalseth)  Our pathfinding solution uses a grid of navigation points covering our walkable surface. Agents generate a path from their closest grid point, to their target's closest grid point.  <img src = ""https://user-images.githubusercontent.com/48756858/235754250-cf86433e-752c-4802-8fb7-fd8785a4e4be.png"" width=80%>  When generating, each grid point traces towards the surface to get surface information. This information allows for the calculation of surface angles. If the angle exceeds a certain threshold, the node is marked as blocked.  # Pathfinding Methods To generate a path, algorithms consider the cost of movement, which may include both the ""hCost"" (distance from current to target position) and the ""gCost"" (distance from current to next node). Our implementation allows for different pathfinding algorithms.  ## Breadth First Search (BFS) Adds neighbouring nodes to a Queue, when selecting the next node, it chooses the next node in the queue.  ## Depth First Search (DFS) Adds each of the neighbouring nodes to a Stack, when selecting the next node, it chooses the top node of the Stack.  <img src = ""https://user-images.githubusercontent.com/48756858/235754456-c902fc4f-ab53-4e8b-8cd5-2d9ff1ce6f43.png"" width=""50%"">  ## Best First Search Checks the hCost of all our neighbours. Lowest hCost is chosen as the next node.  <img src = ""https://user-images.githubusercontent.com/48756858/235754625-cf318147-01b9-4a6f-9212-8832fcf00c92.png"" width=""30%"">  ## Dijkstra Checks the gCost of all our neighbours. Lowest gCost is chosen as the next node.  <img src = ""https://user-images.githubusercontent.com/48756858/235754773-37ed3d3c-8eb0-40f2-a15f-95f4ac3ee756.png"" width=""50%"">   ## A Star Checks both the gCost and the hCost of all our neighbours. The neighbour with the lowest combination of these is chosen as the next node.  <img src = ""https://user-images.githubusercontent.com/48756858/235754928-077aba22-23d8-406b-953f-234495572c9f.png"" width=""50%"">  Displaying all searched nodes helps identify the most efficient pathfinding algorithm. The A star algorithm is typically the fastest and most efficient, and is therefore our default implementation.   ### A Star Searched Nodes  <img src = ""https://user-images.githubusercontent.com/48756858/235755089-2c3d79ad-a5b9-4a4e-888e-cc329e653b7a.png"" width=""50%"">  ### Dijkstra Searched Nodes <img src = ""https://user-images.githubusercontent.com/48756858/235755136-c673a9c8-5a76-431b-8367-215ad3e7c66d.png"" width=""50%"">  # Modularity  Our implementation is highly customizable. All parameters are adjustable through the details panel. For debugging, users can choose to display grid points, paths, and which nodes are searched by our pathfinding algorithm.  <img src = ""https://user-images.githubusercontent.com/48756858/235755282-009aa00c-3b36-4b86-923e-143186afa858.png"" width=""50%"">  # Weight Zones Weight zones can help the user define areas that require extra effort to navigate. By adding these  throughout the world, the user can specify how much additional cost should be added to the grid points.  <img src = ""https://user-images.githubusercontent.com/48756858/235755385-2f2722c9-7014-4b3f-866c-565e305dd681.png"" width=""50%"">  # Obstacles  If one of our nodes overlaps an object on a layer marked as unwalkable, the grid will mark the corresponding node as blocked.  <img src = ""https://user-images.githubusercontent.com/48756858/235755735-ae8e04d3-af85-493b-af57-9da1d5ab5f60.png"" width=""50%"">  # In the map  Here is a picture of the nav-grid in our scene. River is marked as un-walkable, weight zones are added in the forests.  <img src = ""https://user-images.githubusercontent.com/48756858/235755893-e4738d72-f774-4bb6-917d-547053e84989.png"" width=""50%"">  ## Pathfinding Critical Evaluation: The navigation system has demonstrated impressive performance. By adjusting the number of grid points and experimenting with pathfinding algorithms, we can test various levels of performance.   Its modularity is a significant advantage, offering flexibility in parameters and weight zones to suit specific project requirements. Debugging is simplified, as the grid points, paths, and searched nodes can be easily displayed.  However, this implementation's reliance on a grid system may make it less suitable for complex or uneven terrain. Furthermore, the algorithm's efficiency decreases as the number of nodes on the grid increases. There are several potential solutions to these problems, including implementing more advanced pathfinding algorithms, such as hierarchical pathfinding. Another potential improvement could be to multithread the path generation process, particularly for larger grids, which could enhance the system's efficiency and reduce path generation time.  # Artificial Neural Network Implementation - [Tom Scott](http://www.github.com/tomdotscott)  Responsible for the Learning side of a project, in addition to the majority of the gameplay programming, I developed systems to collect data and implemented an Artificial Neural Network (ANN).  <img src = ""https://user-images.githubusercontent.com/48756858/235756881-e0bf6c4b-1b2d-4808-b5ed-3ee20a33b5e4.png"" width=40%>  The algorithm used to implement the neural network was a Feed-Forward Neural Network, using backpropagation to learn. This algorithm is well-suited to learning from datasets and has been shown to be effective in a variety of applications. Using multiple hidden layers, it calculates the stochastic gradient of descent with respect to the weights of the neurons. This includes Sigmoid, ReLU, and TanH activation functions for the neurons and He, Xavier, and Random neuron initialization methods.  <img src = ""https://user-images.githubusercontent.com/48756858/235757461-c1160086-8cd6-41f4-97fc-585b800a0e08.png"" width=""50%"">  <img src = ""https://user-images.githubusercontent.com/48756858/235757511-18eac834-f7d6-4883-872d-3294826ab2bd.png"" width=""50%"">  The ANNs can be saved and loaded in .csv format, allowing players to save time by not having to train the network every time they play the game. The shape and size of the hidden layers are described in the saved network, which includes the number of inputs, outputs, and the size of each of the hidden layers. After this, the layers get serialised line by line. Arrays are represented by numbers separated by commas within curly brackets, and 2D arrays, such as for the neuron weights and the delta weights, are just arrays of arrays represented by curly brackets within curly brackets.  <img src = ""https://user-images.githubusercontent.com/48756858/235757580-d46235f1-ee78-4da3-a711-031c0d6dc2df.png"" width=""70%"">  Training the network needed data, which was collected through a combination of player analytics and cleansing pre-existing data. Custom Python scripts were used to collate and normalise the collected data and also to cleanse the data. For example, to train the neural network on the type advantages in the game, the typeAdvantages.csv file (which the BattleManager uses to calculate damage) was split into several files to train the ANN.  <img src = ""https://user-images.githubusercontent.com/48756858/235757666-4c44e405-c453-417d-b5b6-5a38749f7b0c.png"" width=""50%"">   <img src = ""https://user-images.githubusercontent.com/48756858/235757756-6fb2936b-a6da-4440-843c-2cc0abbf4d00.png"" width=""50%"">  <img src = ""https://user-images.githubusercontent.com/48756858/235757910-c0ae6911-5db0-4a9f-9fa0-44c2aca80e18.png"" width=""50%"">  <img src = ""https://user-images.githubusercontent.com/48756858/235757937-59d763d0-8ca6-4655-a764-2c9f8cf30328.png"" width=""50%"">  A system was also created to record the player's chosen actions against wild Pokemon within the game. This data was normalized and collated, then used to train the decision-making ANN.  <img src = ""https://user-images.githubusercontent.com/48756858/235758546-525e0ea5-3498-4f86-943e-6c286a0b8998.png"" width=""50%"">  <img src = ""https://user-images.githubusercontent.com/48756858/235759072-7b878b16-49ac-46db-8c5d-b9f693f623f5.png"" width=""50%"">  <img src = ""https://user-images.githubusercontent.com/48756858/235759114-8739c8cf-fd84-4573-8e16-9b528b745bb0.png"" width=""50%"">  <img src = ""https://user-images.githubusercontent.com/48756858/235759230-4710e1a1-9152-4591-936f-724ad385b45b.png"" width=""30%"">  ## Machine Learning Critical Evaluation: In terms of performance, the **TypeLearner** was found to be 100% accurate in comparing the learned data to the dataset. This is likely due to the fact that there are only 289 entries in the dataset, which all have unique inputs and outputs. Contrastingly, the **MoveDecisionLearner** was found to be slightly less efficient, achieving about 77.3% accuracy. Tweaking the parameters of the ANN could potentially improve this accuracy. The current dataset, however, is ~600 entries. Collecting more data, including more Pokemon combinations, would improve accuracy. Despite this, the decision-making of the ANN is quite good in-game, and it has even been able to win a few times against me!  Using *softmax* calculations such as *binary cross-entropy loss* (BCEL) could also increase accuracy. BCEL works out the difference between the true and the predicted probability values for categorical tasks. The efficiency of the ANNs could be improved by offloading the workload to the GPU or using prewritten neural networks and libraries like *SciSharp* and Unity's *MLA*.   Overall, the mistakes add to the game's realism, as human Pokemon players are also not likely to make the optimal choice in Pokemon battles.  # Flocking Implementation - [Kieron Killingsworth](https://github.com/AzureSun7)  This section allowed for the setting of the model to be used, the amount of them that would be spawned in, and the size of the boundary between each model.   ![image37](https://user-images.githubusercontent.com/48756858/235760412-06a5cfec-d674-4ae2-987a-d4c25c65ba1e.png)  Underneath, the speed at which the models would move around can be set, where the speed would be randomised between the minimum and maximum.  ![image15](https://user-images.githubusercontent.com/48756858/235760461-6bcd0569-657c-460c-a0ab-3e2f2745ccf3.png)  In the first image above is the main code that will allow the flock to spawn in, taking into account their boundaries, positions and a randomised rotation, while underneath it, is where the movement vector is calculated using each of the behaviour vectors.  ![image42](https://user-images.githubusercontent.com/48756858/235760822-dbdd34ce-72af-48bf-8267-dffe92af4282.png)  ![image7](https://user-images.githubusercontent.com/48756858/235760872-ab06022b-9b90-4788-ba66-0a29422986db.png)  The distances and weights are set, including a *limit*, which is for managing how high aerial creatures can go. In Unity, each value can be adjusted to how much you want the AI to take them into consideration, for example, if *align* on both is maximised, then the models will stay identical in terms of their rotation.  ![image20](https://user-images.githubusercontent.com/48756858/235761370-d8ed0191-8d8c-466e-94b6-2f5abd7a3e62.png)  Smooth damp is used to make sure that the models gradually move towards being at the set angle over time as smoothly as possible. The ‚Äúis bird‚Äù is a simple check to see whether the selected model is an aerial creature, and if so, will allow it to fly, otherwise the model will be locked to the ground.  ![image10](https://user-images.githubusercontent.com/48756858/235761710-972aa0d6-7b6b-4fb7-81ce-66ba0a1c7bb7.png)  ![image35](https://user-images.githubusercontent.com/48756858/235761743-4b469eaf-a428-435b-83d6-7238e9389110.png)  ## Flocking Implementation Critical Evaluation:   Depending on how many members of the flock you have, the performance is quite stable with little to no issues to the game itself, however, if there are too many, then the frame rate will drop significantly. An alternative that I could have used to the algorithm I used was one that incorporated boids instead of model prefabs, which may have made the overall code a bit simpler.  I feel as though the method I used was more beneficial to the game we decided to go with as boids may not have worked well with the set of models we were using, whereas using set prefabs made it quite simple to implement.   My flocking algorithm worked fairly well for what I set out to accomplish with it, allowing for both air and ground Pok√©mon to have flocks that work slightly differently, however, when seeing the models in motion, they do rotate fairly erratically when they aren‚Äôt spread out, so adjusting the rotation calculation may have made that less of an issue. The only real issue I had during the development of this algorithm itself was trying to understand how to control how the flock acted when the weights and distances of the behaviours were altered. For example, making sure that each member of the flock would not collide into one another.  The randomness of the movement, however, does work well when they‚Äôre separated more, and looks much more natural, instead of being hard-coded movements.  # Decision Making - [Jay Bunch](https://github.com/orgs/TU-AI-For-Games/people/goodeveningjay)  I was responsible for the Decision Making AI component of our project. The intended purpose of the decision making AI component was to drive the trainer AI combat and to work in conjunction with the neural network. Unfortunately I fell behind and Tom had to proceed without my contribution being implemented.  ## Implementation  I briefly considered using a Finite State Machine as in a Pokemon battle there are only four actions that can be taken at any given time. However because these are actions that should be the result of decisions made, rather than states that are transitioned between, it quickly became obvious it shouldn‚Äôt be used here.  After I dismissed the idea of a FSM I approached Tom and we discussed all the components of a Pokemon battle that we wanted our AI to consider. For this exercise we considered how competitive players think when competing in tournaments. The outcome of these discussions were a set of 24 parameters we wanted our AI to consider, which Tom eventually whittled down to the 9 most important. This exercise informed how Tom designed the learning component and how I should design the decision making component.  ![image28](https://user-images.githubusercontent.com/48756858/235762506-cc0aa837-79ee-4133-b8d2-0d8bc149ef5b.jpg)  Initially I attempted drafting a Decision Tree as they are fast to implement and relatively simple and efficient. As the complexity of the tree grew it became clear that it was not ideal for the sort of battle simulation we wanted (above image). This isn‚Äôt to say that a decision tree would not be appropriate, in fact the implementation in the main franchise games is likely done with decision trees. We wanted our AI to behave similarly to a human player though and so I was going to need to make use of the task driven structure of a behaviour tree.  I used [this](https://opensource.adobe.com/behavior_tree_editor/#/dash/home) visual editor to create my draft Behaviour Tree diagram:  ![image25](https://user-images.githubusercontent.com/48756858/235763166-77eeaa2f-ec44-4365-991a-738a0660bcb9.jpg)  Unfortunately around the middle of March my progress stalled. The behaviour tree diagram I made was a useful brainstorming tool but was several iterations away from a final draft. I ran out of time and needed to begin programming an implementation so I only kept this first draft. I programmed the framework I was planning to use to create my behaviour tree. These scripts outlined the architecture for generating a behaviour tree. BTNode.cs, Selector.cs, Sequence.cs, and Tree.cs.  ![image40](https://user-images.githubusercontent.com/48756858/235763661-127d3769-5e56-4de0-9bf5-ad0462437435.jpg)  Tree.cs contains a reference to a root node that recursively contains the entire rest of the tree. Upon start this script would then create a behaviour tree according to a SetupTree() function that I was going to define, and then in update if it has a tree it would evaluate continuously.  Sequence.cs and Selector.cs are composite nodes that behave like the AND logic gate and the OR logic gate respectively.  ## Decision-Making Critical Evaluation: The next step should have been implementing individual task scripts and building the tree but I did not achieve this. I think had I managed to complete an implementation it ultimately still would have been replaced by Tom‚Äôs neural net, but I should have been more proactive in catching up.  "
Candy Crush (Match-three-like) Game,Game Development,https://github.com/dgkanatsios/MatchThreeGame,[![unofficial Google Analytics for GitHub](https://gaforgithub.azurewebsites.net/api?repo=Match3StyleGame)](https://github.com/dgkanatsios/gaforgithub)  # Match-3 game   ![Match three game](https://dgkanatsios.files.wordpress.com/2015/02/image_5062f746.png)  A match-3 game in Unity (like Candy Crush and Bejeweled). Tutorial and source code commenting can be found in the blog post: http://dgkanatsios.com/2015/02/25/building-a-match-3-game-in-unity-3/  ### Assets Graphics: http://opengameart.org/content/candy-pack-1 Sounds: http://freesound.org/people/volivieri/sounds/37171/
Tetris clone,Game Development,https://github.com/hyunuk/Tetris,"# Tetris ## About World's one of the most famous games, Tetris project with C# and Unity. Implemented most features that is needed for the contemporary Tetris series.  ## Main Features ### Stage / Infinity Mode The game can start with two different modes. Infinity mode is a classic one, but in the Stage mode, you need to find and clear gems (the orange blocks) to clear the stage and go to the next level.  ### Score When a line is cleared, the score increases based on the number of lines cleared at once.   ### Preview next piece You can see the next piece of a tetromino upper-right side of the screen.  ### Ghost Piece A tetromino will land if it is allowed to drop into the bottom field. As you moves the falling piece, the ghost piece moves below it.  ### Hard/Soft Drop If you push the space bar, a Tetromino drops instantly to where the Ghost Piece is. It can't be moved or rotated afterwards. It causes the Score to go up. If you need more sensitive control, you can push the down arrow button to drop softly.  ### Background Music and Sound effect Implement the classic BGM as well as the sound effects for Tetris arcade version.  ## Demo ![demo]  [demo]: screenshots/demo.gif"
AR Chemistry (Markerbased),Augmented Reality,https://github.com/MiroslavShard/chemistry-ar,"# Chemistry AR AR mobile application for my school project. There are 7 chemical elements (atoms) and one reaction (molecule). Built for Android platform, but has iOS support. Requires Android 7.0 and later. To open this project you need Unity 2021.3.28f1 (LTS) or later.   ## Screenshots <img src=""https://github.com/MiroslavShard/chemistry-ar/blob/main/Chemistry%20AR/Media/Photos/1.jpg"" width=""49%"" height=""49%""> <img src=""https://github.com/MiroslavShard/chemistry-ar/blob/main/Chemistry%20AR/Media/Photos/6.jpg"" width=""49%"" height=""49%""> <img src=""https://github.com/MiroslavShard/chemistry-ar/blob/main/Chemistry%20AR/Media/Photos/7.jpg"" width=""49%"" height=""49%""> <img src=""https://github.com/MiroslavShard/chemistry-ar/blob/main/Chemistry%20AR/Media/Photos/8.jpg"" width=""49%"" height=""49%"">  <img src=""https://github.com/MiroslavShard/chemistry-ar/blob/main/Chemistry%20AR/Media/Photos/9.jpg"" width=""49%"" height=""49%""> <img src=""https://github.com/MiroslavShard/chemistry-ar/blob/main/Chemistry%20AR/Media/Photos/13.jpg"" width=""49%"" height=""49%"">  <img src=""https://github.com/MiroslavShard/chemistry-ar/blob/main/Chemistry%20AR/Media/Photos/15.jpg"" width=""49%"" height=""49%"">   ## Videos <a href=""https://www.youtube.com/watch?v=7lIZQ3iKIzc""><img src=""https://img.youtube.com/vi/7lIZQ3iKIzc/0.jpg"" width=""49%"" height=""49%""></a> <a href=""https://www.youtube.com/watch?v=nW2wcJIgf4A""><img src=""https://img.youtube.com/vi/nW2wcJIgf4A/0.jpg"" width=""49%"" height=""49%""></a>   ## Can I use this project? Of course yes! Also feel free to ask any questions about the project, I will try to answer asap üòâ I will be very grateful if you improve this application (for example, add new atoms and reactions) and make a pull request üí™   ## How can I add new atoms? 1. Generate a marker for the new element using the [ArUco markers generator](https://chev.me/arucogen/). 2. Make the a card with a marker (if you want to use my design, you can use my [Figma project](https://github.com/MiroslavShard/chemistry-ar/tree/1.1.0/Chemistry%20AR/Figma)). 3. Import a card into the Unity project. 4. Add a card to the `ReferenceImageLibrary` file, set the element name and physical size of the card. 5. Make a prefab for a new element (you can copy an already created element and redesign it). 6. Add a prefab to the scene and add it to the `Object library`. 7. Double check the element name in the `ReferenceImageLibrary` and in the `Object Library`, because they must be the same. 8. If you want to have 2+ identical elements on the screen, just create a new one with a number (like I created Hydrogen).   ## Important notes 1. Never use spaces in the Unity XR project path. Unity cannot create XR Reference Image Library if project path contains spaces. 2. Never use QR codes for AR image tracking. Instead of QR codes you can use AR markers. The marker generator I used in this project is [ArUco markers generator](https://chev.me/arucogen/).   ## Build with ‚Ä¢ Unity 2021.3.28f1 (LTS)<br> ‚Ä¢ Google ARCore & Apple ARKit   ## Contacts <a href=""https://www.instagram.com/miroslavshard/""><img src=""https://img.shields.io/badge/instagram-%23E4405F.svg?&style=for-the-badge&logo=instagram&logoColor=white""></a> <a href=""mailto:miroslavshard@gmail.com""><img src=""https://img.shields.io/badge/Gmail-D14836?&style=for-the-badge&logo=gmail&logoColor=white""></a>   <br><i>Made in Ukraine under fire</i><br> <b>¬© Miroslav Stetsiuk - 2023</b>"
AR Book (Markerbased),Augmented Reality,https://github.com/malithpriyashan/Augmented-reality-book-reading-app,"# Augmented-reality-book-reading-app This project is an android application that uses marker-based augmented reality. This project aims to make reading interactive. By using this app, users can go to websites, interact with 3D objects, do cognitive improvement activities, watch videos, create 3D objects, and many more. Graphical 3D models, audio, video, 3D text, UI buttons, virtual buttons are some of the elements used in this project. Two physical reading materials are used in this project: a real estate magazine and a children‚Äôs book. The pages of these act as markers. This app demonstrates how AR can be used for smart marketing by using simulations, videos, images, and audios in an interactive and educational way. The user can get an idea of how buildings look, cost, and feel, and can decide if that‚Äôs what they want. In the children‚Äôs book, there are various activities designed to develop the different cognitive abilities of the child, including games that focus on math, recognizing patterns, and creating and coloring a forest.  ![image3](https://github.com/malithpriyashan/Augmented-reality-book-reading-app/assets/66530522/9ce36760-4cae-45ba-a45c-fc928e1f508e) ![image4](https://github.com/malithpriyashan/Augmented-reality-book-reading-app/assets/66530522/1e04b0a8-9e11-442b-a5fb-47360813cfe5) ![image5](https://github.com/malithpriyashan/Augmented-reality-book-reading-app/assets/66530522/57dd5a1d-3915-474d-95de-dd257d5de676) ![image6](https://github.com/malithpriyashan/Augmented-reality-book-reading-app/assets/66530522/7694348b-ea69-4a2c-bdb8-dcb2d5b794e8) ![image7](https://github.com/malithpriyashan/Augmented-reality-book-reading-app/assets/66530522/91690b0e-ef2d-43ea-8545-9c4cb8ed61aa) ![image8](https://github.com/malithpriyashan/Augmented-reality-book-reading-app/assets/66530522/3898f5e0-2c30-4783-ba03-0ade2e5b92ec) ![image9](https://github.com/malithpriyashan/Augmented-reality-book-reading-app/assets/66530522/4f7d8f8a-4797-4580-9151-35aac3753f0e) ![image10](https://github.com/malithpriyashan/Augmented-reality-book-reading-app/assets/66530522/bd5f2b61-07d1-444d-985a-c58f5d808dbf) ![1](https://github.com/malithpriyashan/Augmented-reality-book-reading-app/assets/66530522/73dddf3f-1446-4105-8cda-018c4ec7ca69) ![2](https://github.com/malithpriyashan/Augmented-reality-book-reading-app/assets/66530522/8a765c75-ab7d-452b-8120-15bc9cba05ae) ![3](https://github.com/malithpriyashan/Augmented-reality-book-reading-app/assets/66530522/221bef97-a7df-46c2-90ee-d01b885323bb)"
AR Bottle Shooter (Markerless),Augmented Reality,https://github.com/rudrajikadra/ARShoot-Game-Markerless-Augmented-Reality-Unity3D-iOS-Android/tree/master,# ARShoot-Game-Markerless-Augmented-Reality-Unity3D-iOS-Android Augmented Reality iOS and Android Project using Markerless Augmented reality for markerless tracking  in Unity 3D for creating a FPS (First Person Shooter) game. Where there will be spacesipts coming from around in your real environment that will be captured by the device camera and the spaceship will be augmented in a few places and they will be moving. User need to aim to the spaceship and tap on the screen so that it will fire and shoot the spaceship.  #### Reference Tutorial : https://www.youtube.com/watch?v=T6bd_MQ2ass&t=968s  ### Note It is a Unity 3D Project for both iOS as well as Android. When you create for iOS it will create another folder with this Unity Project and open it in xcode for running it on an iOS Device.  ## How does it looks like (Gif)  ![ezgif com-optimize](https://user-images.githubusercontent.com/15246084/41500963-133318aa-71b9-11e8-9a27-ec1e6917a5da.gif)
Reading Book App,Augmented Reality,https://github.com/epcm18/interactive-book-reader-with-augmented-reality-content,"# PIXIE: Interactive Reading Platform with Augmented Reality Content  ## Introduction  PIXIE is an innovative interactive book reader with augmented reality (AR) content, specially designed for children aged 3-10 years. The main goal of this project is to revolutionize the way children engage with learning and reading. By incorporating AR visualizations into children's books, the mobile app aims to create an immersive and enjoyable reading experience. We believe that this technology will inspire young readers to fall in love with reading and learning while having fun.  ## Scope  PIXIE is a comprehensive platform that includes both a mobile app and a web app. The mobile app allows users to read e-books with AR content visualization, utilize text-to-speech features, and perform actions such as highlighting, commenting, and bookmarking. On the other hand, the web app provides a platform for publishers to upload AR content for sale and allows users to purchase books. Additionally, the web app supports functionalities such as user registration, login, and profile management.  ### Product Functions  #### Web App Functionalities  - User Registration: Allows users, including both publishers and readers, to create accounts for accessing the system. - User Login: Enables users to log into the system using their registered credentials. - Publish an EBook: Empowers publishers to upload and publish their e-books for readers to access. - Manage Publications: Allows publishers to manage various aspects of their published materials, including updates, modifications, pricing, and availability. - Search the Catalog of Books: Enables users to search for specific books within the platform's catalog. - Purchase Books: Allows users to purchase books from the platform's catalog securely. - Edit User Account Details: Provides users with the ability to manage their account settings and update profile information. - Assign Publisher Rights: Enables administrators to assign publisher rights to users, granting them the capability to publish their e-books. - Logout: Allows users to securely log out from their accounts.  #### Mobile App Functionalities  - User Registration: Allows users to conveniently register using the mobile app. - User Login: Enables users to log into the system using the mobile app. - Access Purchased Books: Allows users to access and read the books they have purchased through the web platform. - AR Content Visualization: Utilizes AR technology to bring characters and scenes from kids' books to life, enhancing the reading experience. - Text-To-Speech: Allows users to have the text content of e-books read aloud to them, providing an alternative mode of consuming content. - Access Digital Dictionaries: Enables users to search for definitions of unfamiliar words while reading. - Highlight, Comment, and Bookmark Pages: Allows users to interact with e-book content by highlighting important sections, adding comments, and bookmarking pages for future reference. - View Saved Definitions: Allows users to revisit and view the meanings of words they have previously saved during their reading sessions.  ## Demo https://www.figma.com/proto/4ud6IpdCK66jkpNYMCDmRT/SEP?type=design&node-id=2-11&t=rF2uvVPC7vOivjg7-1&scaling=min-zoom&page-id=0%3A1&starting-point-node-id=2%3A11&mode=design https://youtu.be/J9XWompkwo0"
"Service App (Hospital, Business, etc.)",Augmented Reality,https://github.com/31Sanskrati/Health-Harbor-AR-app,"<h1 align=""center""> Welcome to Health Harbor üëã</h1>   <p align=""center"">   <em>ü§ñ An Augmented Reality (AR) mobile app designed to raise awareness about the impact of alcohol on various organs. ü§ñ   </em> </p>  <h3 align=""center""> 	<a href=""https://drive.google.com/drive/folders/108UH74vj3_pRrwTnWDGTwkIHQ6TKgYV_"">Demonstration</a> 	<span> | </span> 	<a href=""https://github.com/31Sanskrati/Virtual-Reality-Project"">Virtual Reality Project</a> </h3>  <div style=""text-align: center;"">   <img     width=""1080""   	style=""display: block; margin-left: auto; margin-right: auto;""     class=""block dark:hidden""     src=""/Images/health-harbor-high-resolution-logo-white-transparent.png""     alt=""App Logo""   /> </div>   ## Project Description This app utilizes AR technology to provide users with a visual guide to understanding how alcohol consumption affects the human body. With the help of models created using Blender.  ## Key Features :point_down:  - **3D Organ Models:** The app showcases 3D models of organs created using Blender, allowing users to explore them in augmented reality. - **Impact Visualization:** Different models depict the effects of alcohol on various organs, such as the liver (healthy, alcoholic hepatitis, fatty liver disease, and cirrhosis). - **Personalized Scan Mode:** Users can enter personal data like age, gender, and alcohol consumption habits. The app estimates the potential condition of their organs after a certain period of continued drinking.    ## üöÄ Tech Stack - ‚úÖ **[ARCore](https://developers.google.com/ar)** - Google's framework for building Augmented Reality experiences on Android - ‚úÖ **[ARFoundation](https://docs.unity3d.com/Manual/com.unity.xr.arfoundation.html)** - Unity's cross-platform framework for building AR experiences - ‚úÖ **[Unity](https://unity.com/)** - Popular game engine used for creating 2D, 3D, VR, and AR experiences - ‚úÖ **[Blender](https://www.blender.org/)** - Free and open-source 3D creation suite for modeling, animation, simulation, and rendering  > The project is in its final stages of development and will be available soon!  ## Demonstration [Link for Prototype 1](https://drive.google.com/drive/folders/108UH74vj3_pRrwTnWDGTwkIHQ6TKgYV_) [Link for App Demonstration](https://drive.google.com/file/d/1Vd4hu2JCYS_zRxXrCt4Nryv-uGO1jcjJ/view?usp=drive_link)  ## License This project is licensed under the [MIT License](https://github.com/31Sanskrati/Health-Harbor-AR-app/blob/main/LICENSE).  <p align=""center""><i>Cant wait to hear your opinion...</i></p>"
Shop App,Augmented Reality,https://github.com/akshatkverma/ShopOn,"# ShopOn This is an Android Native application which is developed as a solution for _**HackOn with Amazon 2022**_ for the problem statement related to shopping experience. The detailed problem statement and proposed solution are mentioned furthur in the document.   Click [here](https://youtu.be/ebxUM9ym_Bw) for the video presentation with app demo.  ## Problem Statement: What is that one thing that has refrained customers from buying online for a long time? One of the prominent differences between traditional and online shopping has been physically experiencing the products, confirming its size, all of which has been a pain point for a very long time while buying any product online. If we could solve this problem of product trials, checking the product dimensions by comparing it with real world objects, we can onboard many customers who doesn‚Äôt buy online because of this gap.  ## Proposed Solution: We intend to solve these problems by using the _Augmented Reality (AR) Technology_, through which we‚Äôll target on providing customers with virtual representation analysing the product‚Äôs size, fitment, colour theme sync, etc. With the feature of trying out their products virtually through their mobile phones, the gap between the customer and the product will be minimized.  ## Screenshots of the application :  ![Application Screenshots1](./doc_assets/ScreenShot1.png) ![Application Screenshots2](./doc_assets/ScreenShot2.png) ![Application Screenshots3](./doc_assets/ScreenShot3.png)  ## How does it work? We will be using **SceneViewer** by triggering a explicit intent to it.  Scene Viewer is an immersive viewer that enables 3D and AR experiences from our Android app. It lets users of our application easily preview, place, view, and interact with web-hosted 3D models in their environment. It uses **ARCore**, which is Google‚Äôs platform for building augmented reality experiences. Using different APIs, ARCore enables our application to sense its environment, understand the world and interact with information.  ARCore uses three key capabilities to integrate virtual content with the real world as seen through your phone's camera: 1. **Motion tracking** allows the phone to understand and track its position relative to the world. 2. **Environmental understanding** allows the phone to detect the size and location of all type of surfaces: horizontal, vertical and angled surfaces like the ground, a coffee table or walls. 3. **Light estimation** allows the phone to estimate the environment's current lighting conditions.  ## How quick can this technology be implemented ? Our solution can be instantly be brought to production with the existing shopping applications for different products.  ## What is the impact of this solution ? The impact of adding this feature will be huge, as we‚Äôve now reduced the gap between our customers and the actual product. More and more people would be eager to try the product and ultimately increase the sales by a very large factor.  ## Is the solution scalable ? The solution is highly scalable as it can be implemented with the existing shopping applications.  ## Business Relevance : After bringing the idea to production, more and more users will be tempted to check out the products and the transition barrier from traditional offline buying to online would be reduced, thus increasing the sales.  ## Use this application :   1. You can download the [APK](https://drive.google.com/file/d/1jXiykk2tw-S18ywyPzSPNj8OBwTRiybX/view?usp=sharing), and then install it (you might have to enable installation from unknown sources).  2. Or you can clone the repository and import in Android Studio to see the code + build the APK.  ```bash git clone https://github.com/akshatkverma/ShopOn.git ```  ## Future Scope: The option for virtual try on/ see in your room can be made available for more and more products by making a separate application for making 3D models by just scanning the object by mobile phone‚Äôs camera, which currently requires a professional to make 3D models using heavy graphical softwares. Continuous improvement can be made to the AR technology, making the items even more realistic and improving the overall experience.  # Team Details : **Team Name** : Lifetime Error  **Team Members**: 1. [Akshat Kumar Verma](https://www.linkedin.com/in/akshatkumarverma/) 2. [Mayur Ray](https://www.linkedin.com/in/raymayur9/) 3. [Afzal Ansari](https://www.linkedin.com/in/afzal-ansari-910b38200/) 4. [Sharad Shaiwal](https://www.linkedin.com/in/sharad-shaiwal-590b89201/)"
AR Furniture App (Ikea Style),Augmented Reality,https://github.com/ariel-zilber/couch-mirage,"# Couch Mirage  <img src=""docs/images/logo.png"" width=""200"" align=""right"" />  <b>Welcome to Couch Mirage!</b>   Couch mirage is an augmented reality furniture design planner app similar to [IKEA Place](https://play.google.com/store/apps/details?id=com.inter_ikea.place&hl=en_US). Couch Mirage lets you place 3D models in your place. By utilizing ARCore and Sceneform libraries by google it allows you to visualize how furniture would  fit to your home without ever going to the shop.    Currently Couch Mirage works only in Israel.   Couch Mirage is the final project in the university course #67625 at The Hebrew University in Jerusalem. See : https://shnaton.huji.ac.il/index.php/NewSyl/67625/2/2020/  ## Release The app can be found on Google Play:  [https://play.google.com/store/apps/details?id=com.huji.couchmirage](https://play.google.com/store/apps/details?id=com.huji.couchmirage)  <b>Latest version is v1.0.2</b> ## Features - Measuring real world  location - Searching furniture with  maximum measumrent - Placing 3D model of <b>real furnitures!</b>  at home - Taking picture/ vidoe of model at your home - help page showing how to use the app  ## Supported devices  Go to :  [https://developers.google.com/ar/discover/supported-devices](https://developers.google.com/ar/discover/supported-devices)    ## Demo ![](docs/images/demo_1.gif)  ## Screenshots  ### Main page searching for plane ![screenshot 1](docs/images/screenshot_1.jpeg) ### Main page plane discovered ![screenshot 2](docs/images/screenshot_2.jpeg) ### Main page measuring 3d box ![screenshot 3](docs/images/screenshot_3.jpeg) ### Furniture Catalog main papge ![screenshot 4](docs/images/screenshot_4.jpeg) ### Item catalog by category screen ![screenshot 5](docs/images/screenshot_5.jpeg) ### Item description screen ![screenshot 6](docs/images/screenshot_6.jpeg) ### The model has been placed!! ![screenshot 7](docs/images/screenshot_7.jpeg)  ### Help page 1 ![screenshot 8](docs/images/screenshot_8.jpeg)  ### Help page 2 ![screenshot 8](docs/images/screenshot_9.jpeg)    ## project structure ``` . ‚îú‚îÄ‚îÄ ar                                      # arcore related logic ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ CameraFacingNode.kt                 # a type of node always facing the camera ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ MyArFragment.kt                     # custom arframgent   ‚îú‚îÄ‚îÄ catalog                                 # contains the catalog screens logic ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ CatalogFrontActivity.kt             # main page of the catalog(the one with the categories) ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ DepartmentActivity.kt               # department page activit ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ Department.kt                       # department item  ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ DepartmentRecyclerAdapter.kt        # department adapter ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ DepartmentSourceData.kt             # holds the department images ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ Furniture.kt                        # data class representing a furniture ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ ItemDetailsActivity.kt              #  activity showing details about ites.                                                Also allows downloading a model to show ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ ItemRecyclerAdapter.kt              # recycle adapter for items ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ LoadingDialogFragment.kt            # loading aimation fragment  ‚îú‚îÄ‚îÄ greetings                               # contains logic regearding screens displaying on first app usage ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ AboutAppFrag.kt                     # fragemnt showing details about the app ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ AppDescriptionFrag.kt               # show how to use the app ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ AppTechnicalDescriptionFrag.kt      # shows technical notes regardign ar ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ GreetingActivity.kt                 # the activity showed to the user on first usage ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ HomeFrag.kt                         # first page  showed in the greetnig fragment ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ ObservableWebView.java              # webviewcustom   used to show the privacy notes ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ PrivacyPolicyFrag.kt                # fragments show the privacy policy ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ SquareImageButton.java              # sqaure image button.Extends Button class ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ VideoAppFrag.kt                     # video framenget of the video                                                shown to the user in the greetings pages  ‚îú‚îÄ‚îÄ Help                                    # use help activit'ies related logic ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ HelpActivity.kt                     # actvity that disaplys usage help to the user ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ HelpDescriptionFrag.kt              # show how to use the app ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ HelpTechnicalDescriptionFrag.kt     # shows technical notes regardign ar ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ HelpVideoFrag.kt                    # video framenget of the videds                                               shown to the user in the help pages  ‚îú‚îÄ‚îÄ InfoFAB.kt                              # the infofab shows no the camera activity.Top left panel ‚îú‚îÄ‚îÄ MeasurementBox.kt                       # holds logic regearding creating the 3d cube used to                                               measure a reald world 3d sapcec ‚îú‚îÄ‚îÄ OpenCameraActivity.kt                   # the main activity.Holds the AR logic  ‚îî‚îÄ‚îÄ utils                                   # utility classes     ‚îú‚îÄ‚îÄ PhotoSaver.kt                       # saves ar scene photo     ‚îî‚îÄ‚îÄ VideoRecorder.java                  # records ar scene video ```  ## 3D models Our app uses Firebase as backend.  The models were takes from multiple sources. They were preprocessed using blender software and uploaded to firebase using python. See : https://github.com/arikzilWork/CouchMirageFirebaseLoader  ### List of 3d models sources     * https://free3d.com/3d-models/ikea      * https://www.polantis.com/ikea/arstid-wall-lamp      * https://www.turbosquid.com/Search/3D-Models/free/ikea      * https://clara.io/view/615465b6-822c-4e2a-9b7d-e77b816001a7#      * https://sketchfab.com/tags/ikea      * http://www.proviz.info/blog3dfreemodels/3dmodels-ikeafalster      * http://ikea.csail.mit.edu/      * https://3dbrute.com/33-ikea-sofa/      * https://cgtricks.com/high-quality-3d-models-ikea-proviz/      * https://3dwarehouse.sketchup.com/collection/ue790c74c-6a82-4077-92cd-7f6d2a5f1661/ikea?sortBy=createTime%20DESC&searchTab=model&hl=en&login=true      * https://3dwarehouse.sketchup.com/collection/c03129ceab823647aac2578a7b9ddb8c/2-IKEA-SOFA?sortBy=createTime%20DESC&searchTab=model&hl=en      * https://cgdownloads.com/download-slect-ikea-slakt-ikea-3d-model-free/      * http://www.cgchannel.com/2014/03/download-10-free-models-of-ikea-furniture-from-proviz/      * https://archibaseplanet.com/download/6f86e1ec.html  ## List of tools used  1. [Blender](https://www.blender.org/) - used to edit/prepare models for ARCore to be displayed  2. [adobe after effects](https://www.adobe.com/il_en/products/aftereffects.html?gclid=Cj0KCQjw8fr7BRDSARIsAK0Qqr48Zn77ZN5bvCHuYUj-A8n33hjqkPH6LINI5eQyFHOb9LtmSKOBRUcaAoBNEALw_wcB&sdid=8DN85NTY&mv=search&skwcid=AL!3085!3!340845921979!e!!g!!adobe%20after%20effects&ef_id=Cj0KCQjw8fr7BRDSARIsAK0Qqr48Zn77ZN5bvCHuYUj-A8n33hjqkPH6LINI5eQyFHOb9LtmSKOBRUcaAoBNEALw_wcB:G:s&s_kwcid=AL!3085!3!340845921979!e!!g!!adobe%20after%20effects!1464058443!56263479985) - used to prepare the instruction videos   3. [free logo design ](https://editor.freelogodesign.org/?lang=en&logo=2895af34-67d1-42d7-8c42-0ddedb973ba0&companyname=&category=0) - used to create the icon   ## Credits [Ariel Zilbershtyin ](https://www.linkedin.com/in/ariel-zilberstein-cmc/)- Computer Engineering student at The Hebrew university University mail : ariel.zilbershtey@mail.huji.ac.il  [Brahan Wassan  ](https://www.linkedin.com/in/brahan-wassan/)- Math & Computer Science student at The Hebrew university University mail : Brahan.Wassan@mail.huji.ac.il"
Hair Cutting App,AI/ML,https://github.com/iantv/AR-Hair,# AR Hair Hairstyle application in AR created in Qt  ## Demo https://youtu.be/JNBwUjA2mGQ  [![IMAGE ALT TEXT HERE](https://img.youtube.com/vi/JNBwUjA2mGQ/0.jpg)](https://www.youtube.com/watch?v=JNBwUjA2mGQ)  ## Third-party libraries  1. OpenCV 3.2.0 with extramodule opencv_contrib 2. Dlib 19.6 (also download shape_predictor_68_face_landmarks.dat to Resources directory)  Specify absolutely paths of your opencv and dlib directories in INCLUDEPATH and LIBS in the .pro file  
Handwriting Recogniser,AI/ML,https://github.com/githubharald/SimpleHTR,"# Handwritten Text Recognition with TensorFlow  * **Update 2023/2: a [web demo](https://githubharald.github.io/text_reader.html) is available** * **Update 2023/1: see [HTRPipeline](https://github.com/githubharald/HTRPipeline) for a package to read full pages** * **Update 2021/2: recognize text on line level (multiple words)** * **Update 2021/1: more robust model, faster dataloader, word beam search decoder also available for Windows** * **Update 2020: code is compatible with TF2**   Handwritten Text Recognition (HTR) system implemented with TensorFlow (TF) and trained on the IAM off-line HTR dataset. The model takes **images of single words or text lines (multiple words) as input** and **outputs the recognized text**. 3/4 of the words from the validation-set are correctly recognized, and the character error rate is around 10%.  ![htr](./doc/htr.png)   ## Run demo  * Download one of the pretrained models   * [Model trained on word images](https://www.dropbox.com/s/mya8hw6jyzqm0a3/word-model.zip?dl=1):      only handles single words per image, but gives better results on the IAM word dataset   * [Model trained on text line images](https://www.dropbox.com/s/7xwkcilho10rthn/line-model.zip?dl=1):     can handle multiple words in one image * Put the contents of the downloaded zip-file into the `model` directory of the repository   * Go to the `src` directory  * Run inference code:   * Execute `python main.py` to run the model on an image of a word   * Execute `python main.py --img_file ../data/line.png` to run the model on an image of a text line  The input images, and the expected outputs are shown below when the text line model is used.  ![test](./data/word.png) ``` > python main.py Init with stored values from ../model/snapshot-13 Recognized: ""word"" Probability: 0.9806370139122009 ```  ![test](./data/line.png)  ``` > python main.py --img_file ../data/line.png Init with stored values from ../model/snapshot-13 Recognized: ""or work on line level"" Probability: 0.6674373149871826 ```  ## Command line arguments * `--mode`: select between ""train"", ""validate"" and ""infer"". Defaults to ""infer"". * `--decoder`: select from CTC decoders ""bestpath"", ""beamsearch"" and ""wordbeamsearch"". Defaults to ""bestpath"". For option ""wordbeamsearch"" see details below. * `--batch_size`: batch size. * `--data_dir`: directory containing IAM dataset (with subdirectories `img` and `gt`). * `--fast`: use LMDB to load images faster. * `--line_mode`: train reading text lines instead of single words. * `--img_file`: image that is used for inference. * `--dump`: dumps the output of the NN to CSV file(s) saved in the `dump` folder. Can be used as input for the [CTCDecoder](https://github.com/githubharald/CTCDecoder).   ## Integrate word beam search decoding  The [word beam search decoder](https://repositum.tuwien.ac.at/obvutwoa/download/pdf/2774578) can be used instead of the two decoders shipped with TF. Words are constrained to those contained in a dictionary, but arbitrary non-word character strings (numbers, punctuation marks) can still be recognized. The following illustration shows a sample for which word beam search is able to recognize the correct text, while the other decoders fail.  ![decoder_comparison](./doc/decoder_comparison.png)  Follow these instructions to integrate word beam search decoding:  1. Clone repository [CTCWordBeamSearch](https://github.com/githubharald/CTCWordBeamSearch) 2. Compile and install by running `pip install .` at the root level of the CTCWordBeamSearch repository 3. Specify the command line option `--decoder wordbeamsearch` when executing `main.py` to actually use the decoder  The dictionary is automatically created in training and validation mode by using all words contained in the IAM dataset (i.e. also including words from validation set) and is saved into the file `data/corpus.txt`. Further, the manually created list of word-characters can be found in the file `model/wordCharList.txt`. Beam width is set to 50 to conform with the beam width of vanilla beam search decoding.   ## Train model on IAM dataset  ### Prepare dataset Follow these instructions to get the IAM dataset:  * Register for free at this [website](http://www.fki.inf.unibe.ch/databases/iam-handwriting-database) * Download `words/words.tgz` * Download `ascii/words.txt` * Create a directory for the dataset on your disk, and create two subdirectories: `img` and `gt` * Put `words.txt` into the `gt` directory * Put the content (directories `a01`, `a02`, ...) of `words.tgz` into the `img` directory  ### Run training  * Delete files from `model` directory if you want to train from scratch * Go to the `src` directory and execute `python main.py --mode train --data_dir path/to/IAM` * The IAM dataset is split into 95% training data and 5% validation data   * If the option `--line_mode` is specified,    the model is trained on text line images created by combining multiple word images into one   * Training stops after a fixed number of epochs without improvement  The pretrained word model was trained with this command on a GTX 1050 Ti: ``` python main.py --mode train --fast --data_dir path/to/iam  --batch_size 500 --early_stopping 15 ```  And the line model with: ``` python main.py --mode train --fast --data_dir path/to/iam  --batch_size 250 --early_stopping 10 ```   ### Fast image loading Loading and decoding the png image files from the disk is the bottleneck even when using only a small GPU. The database LMDB is used to speed up image loading: * Go to the `src` directory and run `create_lmdb.py --data_dir path/to/iam` with the IAM data directory specified * A subfolder `lmdb` is created in the IAM data directory containing the LMDB files * When training the model, add the command line option `--fast`  The dataset should be located on an SSD drive. Using the `--fast` option and a GTX 1050 Ti training on single words takes around 3h with a batch size of 500. Training on text lines takes a bit longer.   ## Information about model  The model is a stripped-down version of the HTR system I implemented for [my thesis]((https://repositum.tuwien.ac.at/obvutwhs/download/pdf/2874742)). What remains is the bare minimum to recognize text with an acceptable accuracy. It consists of 5 CNN layers, 2 RNN (LSTM) layers and the CTC loss and decoding layer. For more details see this [Medium article](https://towardsdatascience.com/2326a3487cd5).   ## References * [Build a Handwritten Text Recognition System using TensorFlow](https://towardsdatascience.com/2326a3487cd5) * [Scheidl - Handwritten Text Recognition in Historical Documents](https://repositum.tuwien.ac.at/obvutwhs/download/pdf/2874742) * [Scheidl - Word Beam Search: A Connectionist Temporal Classification Decoding Algorithm](https://repositum.tuwien.ac.at/obvutwoa/download/pdf/2774578) "
Chatbot for Healthcare Management,AI/ML,https://github.com/arvindsis11/Ai-Healthcare-Chatbot,"# flask-chatbot Built on python 3.6 Flask==0.11 chatterbot==0.8.4 SQLAlchemy==1.1.11  #### A web implementation of [ChatterBot](https://github.com/gunthercox/ChatterBot) using Flask.  ## Local Setup:  1. Open command prompt and locate folder. run 'pip install -r requirements.txt'  2. Run *train.py*  3. Run *run.py*  4. Demo will be live at http://localhost:5000/  5. please refer this for setup -    https://github.com/user-attachments/assets/e050d0f2-7c6d-420c-a13a-d59a762aa047       ## Git push cmd- for reference  ```java  echo ""# MyRestApi all crud operations using spring boot framework"" >> README.md git init git add . git commit -m ""initial commit"" git branch -M main git remote add origin https://github.com/arvindsis11/MyRestApi.git git push -u origin main git rm -r --cached . //////////////////////////////////////// or push an existing repository from the command line git remote add origin https://github.com/arvindsis11/springJPAdemo.git git branch -M main git push -u origin main https://github.com/arvindsis11/angular-todomanagement-app.git ///////////////////////////////////// common git error: use this: git pull --rebase origin main git push origin main url:https://stackoverflow.com/questions/24114676/git-error-failed-to-push-some-refs-to-remote  ```  ## License This source is free to use, but ChatterBot does have a license which still applies and can be found on the [LICENSE](https://github.com/gunthercox/ChatterBot/blob/master/LICENSE) page."
Spam Email Detection System,AI/ML,https://github.com/Apaulgithub/oibsip_taskno4,"# Email Spam Detection with Machine Learning  **Oasis Infobyte Internship Project** - [**Credentials**](https://drive.google.com/file/d/1uDjGZcWln07jb0dL60Yuz_33Ck78DQXF/view?usp=drive_link)  ![MasterHead](https://www.easyspace.com/blog/wp-content/uploads/2019/03/spam-1.png)  <font size=""1"">Image Courtesy: https://www.easyspace.com/blog/wp-content/uploads/2019/03/spam-1.png</font>  Click on the following link to checkout the colab file. - [Colab](https://colab.research.google.com/drive/1ggzEa68D7EJ3zerlhMqZAXv5_kC85bYg?usp=sharing)   ---  ## Problem Statement  Email spam, or junk mail, remains a persistent issue, flooding inboxes with unsolicited and often malicious content. These emails may contain cryptic messages, scams, or, most dangerously, phishing attempts. Our task, undertaken during an engaging data science internship provided by Oasis Infobyte, is to create an effective email spam detection system using Python and machine learning.  **Project Objectives:**  1. **Data Preprocessing:** Our project begins with the preprocessing of a substantial email dataset, encompassing tasks such as data cleaning, handling missing values, and converting text data into a format suitable for machine learning.  2. **Email Feature Engineering:** Email data presents unique characteristics. We focus on engineering specific email features, such as the sender's address, recipient list, subject line, and email body, to create meaningful inputs for our spam detection model.  3. **Machine Learning Model Selection:** We aim to design and evaluate a robust spam detection model. Our choice of machine learning algorithms, including decision trees, support vector machines, and neural networks, seeks to maximize the model's effectiveness.  4. **Model Evaluation:** To assess the model's performance, we employ metrics like accuracy, precision, recall, F1-score, and ROC-AUC to ensure a comprehensive understanding of its effectiveness.  5. **Hyperparameter Tuning:** The project involves fine-tuning model hyperparameters to optimize predictive accuracy and minimize false positives, which can have a significant impact in the context of email spam detection.  6. **Cross-Validation and Generalization:** Rigorous cross-validation techniques and testing on dedicated datasets are applied to confirm the model's ability to generalize to new, previously unseen email data.  7. **Practical Application:** We explore practical deployment strategies, considering how the spam detection model could be integrated into email filtering systems, improving email security, and enhancing user experience.  8. **Ethical Considerations:** The project addresses ethical concerns related to privacy and data security by ensuring that email content and sender identities are handled with sensitivity.  9. **Challenges and Future Work:** Identifying potential challenges in email spam detection, including evasive techniques used by spammers, and proposing avenues for future work and research in this domain.  This project encapsulates the power of machine learning in addressing real-world challenges and promises a future where spam emails will no longer plague our inboxes.  ---  ## Project Summary  In today's digital age, the challenge of combating spam emails is more pressing than ever. Spam emails, or junk mail, inundate our inboxes with unsolicited and often malicious content, ranging from cryptic messages to scams and phishing attempts. To address this issue, we embarked on an exciting data science internship project offered by Oasis Infobyte.  **Project Highlights:**  1. **Data Preprocessing:** Our journey began with the preprocessing of a sizable dataset of emails. This phase involved data cleaning, handling missing values, and transforming text data into a suitable format for machine learning.  2. **Feature Extraction:** We explored various techniques for feature extraction, striving to capture the essential characteristics of spam emails. This process was crucial in preparing the data for model training.  3. **Machine Learning Models:** We employed a range of machine learning algorithms to train and evaluate the spam detection model. These models included decision trees, support vector machines, and more.  4. **Evaluation Metrics:** To ensure the model's effectiveness, we carefully selected evaluation metrics such as accuracy, precision, recall, and F1-score. These metrics provided valuable insights into the model's performance.  5. **Tuning and Optimization:** Fine-tuning hyperparameters and optimizing the model was a critical step to enhance its predictive accuracy.  6. **Validation:** Rigorous cross-validation and validation on a test dataset were performed to verify the model's ability to generalize to new, unseen data.  7. **Deployment:** We discussed potential deployment strategies for the spam detection model, highlighting its real-world applicability in email filtering.  The completion of this project not only equipped us with practical data science skills but also contributed to the ongoing battle against email spam. The project's success was a testament to the power of machine learning in addressing real-world challenges.  ---  ## Conclusion  In the world of email communication, the battle against spam messages is an ongoing challenge. Our journey in this project was to develop a robust email spam detector using Python and machine learning techniques. We wanted to equip users with a tool that can distinguish between legitimate emails (ham) and unsolicited, often harmful, spam emails.  **Key Insights:**  - Our dataset revealed an interesting distribution, with approximately 13.41% of messages being categorized as spam and the remaining 86.59% as ham. This distribution served as a crucial starting point for our analysis.  - During the EDA process, we identified common keywords frequently found in spam messages, such as 'free,' 'call,' 'text,' 'txt,' and 'now.' These words often trigger spam filters and were important features for our machine learning model.  - Our journey through machine learning brought us to a standout performer - the Multinomial Naive Bayes model. This model exhibited exceptional accuracy, achieving an impressive score of 98.49% on the recall test set. This outcome signifies the model's exceptional ability to accurately identify and filter out spam emails, thereby contributing to enhanced email security and a superior user experience.  In conclusion, this project has demonstrated that machine learning, combined with effective feature engineering and model selection, can be a powerful tool in the ongoing battle against email spam. By implementing this spam detection system, we've taken a significant step towards minimizing the impact of spam messages on email users' lives.  Email inboxes are now a safer place, thanks to the successful implementation of our email spam detection system. As we conclude this project, we look forward to continued improvements and innovations in email security.  Let's keep our inboxes spam-free and our communications secure.  ---  ## Author  - [Arindam Paul](https://www.linkedin.com/in/arindam-paul-19a085187/)  ---  ## Reference  - [Oasis Infobyte](https://oasisinfobyte.com/)"
Public Sentiment Analyser for Election,AI/ML,https://github.com/Naveenpandey27/Indian_Election_Sentiment_Analysis_using_NLP,"# Sentiment Analysis on Political Reviews  This Python script performs sentiment analysis on political reviews, specifically those related to Narendra Modi and Rahul Gandhi. It utilizes various libraries and techniques to clean, preprocess, and analyze the sentiment of text data.  ## Libraries Used The following libraries are imported at the beginning of the script: - `re`: Regular expression library for text cleaning. - `numpy`: Library for numerical operations. - `pandas`: Data manipulation library. - `nltk`: Natural Language Toolkit for text processing. - `textblob`: TextBlob library for sentiment analysis. - `plotly.graph_objects`: Plotly library for data visualization.  ## Data Loading Two CSV files, 'modi_reviews.csv' and 'rahul_reviews.csv', are read into DataFrames using `pandas`. These files contain political reviews.  ## Data Exploration The script explores the loaded data by displaying the first few rows and checking the data dimensions for both Modi and Rahul datasets.  ## Text Cleaning Text cleaning is performed to prepare the text data for sentiment analysis. The cleaning process includes the following steps:  ### Common Preprocessing Function A common preprocessing function is defined to perform the following tasks on text data: 1. Convert text to lowercase. 2. Handle missing values by dropping rows with missing text. 3. Convert the text column to strings. 4. Remove special characters, emojis, and emoticons. 5. Remove stopwords (common words like 'and', 'the', etc.). 6. Perform stemming (reducing words to their root form). 7. Remove numbers.  ### Modi Data Cleaning The preprocessing function is applied to clean the Modi dataset ('modi_data'). The 'Unnamed: 0' column is also dropped from the DataFrame.  ### Rahul Data Cleaning The same preprocessing function is applied to clean the Rahul dataset ('rahul_data'). The 'Unnamed: 0' column is dropped as well.  ## Sentiment Analysis Sentiment analysis is performed using the TextBlob library to calculate the polarity of each review. The polarity represents the sentiment score, where positive values indicate positive sentiment, negative values indicate negative sentiment, and zero indicates neutral sentiment.  ### Sentiment Labeling Based on the polarity scores, sentiment labels ('positive', 'negative', 'neutral') are assigned to the reviews for both Modi and Rahul datasets. Reviews with a polarity score of 0 are labeled as 'neutral.'  ### Removal of Neutral Reviews Neutral reviews are removed from both datasets to focus on reviews with discernible sentiment.  ### Balancing the Dataset To balance the dataset, a random selection of reviews is removed. The number of reviews removed is determined to ensure that both positive and negative sentiments are equally represented.  ## Prediction about Election To analyze sentiment regarding the politicians, this script calculates the percentage of negative and positive reviews for both Narendra Modi and Rahul Gandhi. This data is visualized using a bar chart, where 'Negative' sentiment is marked in red and 'Positive' sentiment is marked in green.  The chart provides an overview of the sentiment analysis results for the two politicians.  ![chart](https://github.com/Naveenpandey27/Indian_Election_Sentiment_Analysis_using_NLP/assets/66298494/68e3b14e-92e5-476d-812f-ab89257687b1)  Feel free to use this code and make changes according to your sentiment analysis project on political reviews."
Parking Ticket App,AI/ML,https://github.com/francopiloto/ParkingTicket,"# Parking Ticket App  An app that lets parking enforcement officers detect parking infraction and issue tickets. Developed for IOS and Android.  ## Technologies:  * Java, Android Studio * IOS, Swift, XCode * UI Components:  Segues, Intents, Fragments, RecyclerViews, TableViews,  ## Demo  <img src = ""demo.gif"">"
Interactive American Sign Language Translator,AI/ML,https://github.com/kevinjosethomas/sign-language-processing,"https://github.com/kevinjosethomas/sign-language-translation/assets/46242684/27868867-aaed-4213-b595-bcffb357479c    # ‚úåÔ∏è ASL ‚≠§ English Translation with MediaPipe, PointNet, ThreeJS and Embeddings  American Sign Language (ASL) is a complete, natural language that exhibits the same linguistic complexities as spoken languages, including its own syntax, morphology, and grammar that significantly differ from English. However, most existing tools that aim to bridge ASL and English are often developed under the misconception that ASL is identical to English. These tools often prioritize the needs of hearing individuals, simply offering Text-to-Speech (TTS) and Speech-to-Text capabilities (STT). When Deafness is viewed as a ""disability"", most of these *translation* tools are simply developed to assist with the inability to hear, but they do not serve any purpose in actually translating between ASL and English.   This project is a prototype that enables translation between American Sign Language (ASL) and English, facilitating communication between ASL signers and individuals who do not understand ASL. It is still far from fully translating the nuances of visual language, but it is designed to respect and preserve ASL as the primary language. The interface provides two main functionalities:  <table style=""width: 100%"">   <tr>     <th style=""width: 50%"">ASL Fingerspelling ‚Üí English Translation</th>     <th style=""width: 50%"">English ‚Üí ASL Sign Translation</th>   </tr>   <tr>     <td>Translates ASL fingerspelling into written English, which is then spoken aloud. Removes the need for Deaf individuals to translate their thoughts into English, and then write them out.</td>     <td>Translates spoken English into ASL signs, which are then signed by a moving avatar. Removes the need for Deaf individuals to read written English, and then translate it into visual language.</td>   </tr> </table>  You can watch a full explanation of the project [here](https://youtu.be/uuPxMWQRoXc). You can read my paper documenting the technical aspects on [arXiv](https://arxiv.org/abs/2408.09311).   # Table of Contents  > [!NOTE] > As a hearing student with limited ASL proficiency, I recognize that my perspective as a hearing person is limited. My role has been to listen carefully and integrate feedback from the Deaf community, and I have done my best to approach this project with a mindset of learning and understanding, rather than assuming. This project would not have been possible without the active involvement and advice of Deaf individuals and ASL experts who have generously shared their insights. Please see [acknowledgements](#acknowledgements).   - [Motivation](#motivation) - [Language](#language) - [Technology](#technology)   - [Receptive](#receptive)     - [Detection](#detection)     - [Classification](#classification)     - [Synthesis](#synthesis)   - [Expressive](#expressive) - [Future Work](#future-work)   - [Limitations](#limitations)   - [Vision](#vision) - [Acknowledgements](#acknowledgements)   - [People](#people)   - [Projects](#projects)   # Motivation  For over eight years, I tried learning multiple languages, from Sanskrit and Spanish to Hindi and French, yet I could barely maintain a fluent conversation in any of them. When I moved to Vancouver in 2021, I joined Burnaby South Secondary School, which shares its campus with the British Columbia Secondary School for the Deaf (BCSD). This gave me the unique opportunity to study a new kind of language ‚Äì a visual language ‚Äì in high school.  ASL wasn't like any of the other languages I had attempted to learn before: It wasn't just about words or pronunciation, but rather learning how to fully express yourself without the tools you typically use. Over the last three years, our ASL class has shown me how I take communication for granted and also helped me notice the many hurdles that are faced by the Deaf community in our hearing-centric society. From my very first week at Burnaby South, I have had many experiences that suddenly remind me of the reasons we learn about Deaf culture and accessibility in ASL class. The [mission](#future-work) below is ultimately what I hope to achieve with this project.   # Language  Below, on the left is the typical flow of conversation between two people speaking the same language. On the right is the typical flow of a conversation between two individuals who speak different languages. This features either a tool that can translate between the two languages (Google Translate) or a person who knows both languages and serves as a translator. This is also an ASL interpreter's role in a typical conversation between a Deaf and hearing individual.  <img height=""167"" src=""https://github.com/kevinjosethomas/sign-language-translation/assets/46242684/a001e197-0992-40c3-9866-805eb74092ed"" /> <img height=""167"" src=""https://github.com/kevinjosethomas/sign-language-translation/assets/46242684/43a07102-7a0b-4115-b75d-3b6131d44a7b"" /> &ensp;  In contrast, below is the typical flow of conversation between a Deaf and hearing individual when an interpreter is not present. The most commonly utilized process is writing/typing to communicate, or using a TTS/STT tool that assists the hearing individual.  <img src=""https://github.com/kevinjosethomas/sign-language-translation/assets/46242684/966b5fb3-9ba5-47ee-b6b9-db977c3bf2d9"" /> &ensp;  The idea that ASL is merely a visual representation of English is a widespread misconception. In reality, ASL is a distinct language with its own syntax, grammar, and cultural nuances that differ significantly from spoken English. Most existing direct translation tools, similar to the writing/typing translation process I visualized above, are built with this understanding, failing to capture the depth of ASL communication. Furthermore, these tools put an even larger burden on the Deaf individual, requiring them to accommodate and adapt to meet the needs of the hearing individual.  While ASL signers do have a grasp of English, it is often incredibly hard to constantly translate thoughts from ASL into English, and vice-versa. Essentially, most of these translation tools simply transcribe text, and don't serve any real ""translation"" purpose. They are built to assist deafness as a disability, but not as a culture or as a language.   On the left, I added a visual of the 5 fundamental parameters that define ASL, as well as some examples of each (there are hundreds!). On the right, I added an example of the grammatical difference between the same sentence in English and in ASL. I would recommend reading about [ASL Parameters](https://www.handspeak.com/learn/397/), and also about [ASL Gloss](https://www.handspeak.com/learn/3/) & [Grammar](https://www.handspeak.com/learn/37/).  <img height=""300"" src=""https://github.com/kevinjosethomas/sign-language-translation/assets/46242684/a5255c8b-e09d-4338-89c9-f99c6a86c142"" /> <img height=""300"" src=""https://github.com/kevinjosethomas/sign-language-translation/assets/46242684/fe4514ae-879d-4cb9-80c6-65ec6955eb22"" /> &ensp;  The goal of this project has been to eliminate the extra steps that Deaf people have to take because of how hearing-centric our society is. From the 4 extra steps visualized above, our tool eliminates three 3 steps to a sufficient degree. Here is the new flow below:  <img src=""https://github.com/kevinjosethomas/sign-language-translation/assets/46242684/b459cabf-850e-449b-9e0a-8c6086be7ba3"" />   # Technology  This project uses computer vision, machine learning, and web animation to create a two-way ASL-English translation system. There are two main components to the project: - **Receptive:** Ability to interpret fingerspelling and express as spoken English - **Expressive:** Ability to interpret spoken English and express as ASL signs  ## Receptive The receptive component of the project focuses on translating ASL fingerspelling into English. It involves several stages: 1. **Detection:** Identifying and tracking hands within the frame 2. **Classification:** Recognizing ASL alphabets with normalized points of the hand 3. **Synthesis:** Synthesizing singular alphabets to reduce inaccuracies and form complete sentences  This is what the overall flow looks like:  <img alt=""Inference"" src=""https://github.com/kevinjosethomas/sign-language-translation/assets/46242684/209b75c8-da4a-4d16-8f9b-a9237d89bcd0"">  ### Detection This stage involves the real-time recognition identification and tracking of hand movements and positions using the Google MediaPipe Hand Landmark model. The model captures 21 3D hand key points per frame, and it provides detailed information on hand orientation and finger position. Furthermore, it can run entirely locally and does not require significant computing resources to run in a realtime setting. Below is an image of the 21 points detected by the MediaPipe Hand Landmark model.  <img alt=""Detection"" src=""https://github.com/kevinjosethomas/sign-language-translation/assets/46242684/3c632724-27b3-4509-8c4c-53be4367157a"">  By using these points instead of images of hands (like I previously tried!), the classification process becomes significantly more powerful: 1. It is not affected by different backgrounds, hand sizes, skin tones and other factors that will make a typical image classification model significantly more incapable  2. It only needs to process a set of 63 numbers (3 for each point!) for each frame instead of an entire image, making it significantly more efficient for real-time use 3. It looks a lot cooler  To ensure that the model is not affected by distance from the camera, I normalize each point to be relative to the bounds of the hand itself. When training the model, this provides more standardized data that will help increase accuracy and reliability.  ### Classification  Once hand landmarks are captured, the data is fed into a Keras PointNet model, which I trained on over 120,000 labelled images of ASL fingerspelling. PointNet is a deep learning model architecture developed with the intent of classifying 3D point clouds, similar to how the detected hands are now represented.  The PointNet model classifies the input data into one of the ASL alphabet signs (except for J and Z, which include movement of the hand to properly express). Below is a demonstration of the training process of the PointNet model.  <img alt=""Classification"" src=""https://github.com/kevinjosethomas/sign-language-translation/assets/46242684/5df6c3ed-c06a-48f2-800d-318a4b63925d""> &ensp;  I started out by downloading multiple ASL Fingerspell datasets from Kaggle: [One](https://www.kaggle.com/datasets/lexset/synthetic-asl-alphabet) [Two](https://www.kaggle.com/datasets/danrasband/asl-alphabet-test) [Three](https://www.kaggle.com/datasets/debashishsau/aslamerican-sign-language-aplhabet-dataset)  I used MediaPipe to identify hand landmarks on every image in the three datasets. Subsequently, I normalized the landmarks and stored the point clouds in NumPy files, creating a combined augmented dataset. Below is a visualized form of the point cloud dataset <img width=""1136"" alt=""image"" src=""https://github.com/kevinjosethomas/sign-language-translation/assets/46242684/d2f87230-199d-4dec-bf96-30945d2c5830"">  Once I had the augmented dataset, I trained a PointNet model on it. So far, I've trained the model around 6 times, playing with parameters, mostly adjusting epoch count, bath size, and learning rate. Here is training information from the most accurate model currently trained:  <img alt=""image"" src=""https://github.com/kevinjosethomas/sign-language-translation/assets/46242684/af4472da-8737-4766-bdbb-3412ea4f3274""> <img alt=""image"" src=""https://github.com/kevinjosethomas/sign-language-translation/assets/46242684/7f892d1a-f8d2-4685-a09a-74130fd124d2"">  As seen in the confusion matrix, similar fingerspelled alphabets like K-V, U-R and E-S are often misclassified for each other. This is expected and can be improved with additional training data. There also seems to be an issue in the validation data, as seen in a sudden spike in validation loss in one of the epochs.  ### Synthesis The final stage synthesizes the classified characters into coherent words and sentences. This involves error correction to adjust for common misrecognition, as well as contextual synthesis to form sentences based on the classified letters.  First, the program uses conditionals to differentiate between commonly misrecognized letters. For instance, the letters A-T-M-N-S are commonly mistaken for each other, and this can be fixed by checking the relative positioning of certain key coordinates (like the thumb). After ensuring the classified letter is accurate, the program ensures that the recognized letter has been demonstrated for multiple consecutive frames, to ensure that the alphabet is properly recognized. The program also ensures that the same letter is not recognized more than two times consecutively, preventing each individual frame from adding a letter to the synthesized text. Finally, it uses an LLM to synthesize the cleaned information into a meaningful sentence. This synthesis process also applies grammatical rules to form sentences that are syntactically correct in English.  ## Expressive The expressive component of this project focuses on translating spoken English into ASL, which is visually represented through a 2D animated avatar using ThreeJS.   To begin, I created a database of over 9000+ words and their corresponding ASL signs. Once again, I used MediaPipe's Pose and Hand Landmark Models to identify body points for each frame in videos of these signs. Furthermore, I also used the ``all-MiniLM-L6-v2`` model to create embeddings for each word. I stored all words, their corresponding embeddings, and their corresponding ASL sign point animations in a PostgreSQL database using ``pgvector``. Although the database only has about 9500 words, using cosine similarity for semantic search allows me to drastically increase the word count by using contextually similar signs to replace certain words. Here is a demonstration of how the expressive aspect of the project works:  <img src=""https://github.com/kevinjosethomas/sign-language-translation/assets/46242684/7fb66399-55ca-4233-a740-05bef3e1fd57"" />  The interface begins by using ``react-speech-recognition`` to transcribe spoken text into words (this can be replaced with OpenAI Whisper for more accuracy). When the speaker stops speaking for a certain amount of time, it transmits the transcription to the backend through websockets. The backend iterates through the words and creates embeddings for each of them. For each word, it queries the database using the cosine similarity function to fetch the animation of the sign with the closest meaning to the spoken word. If there are no similar signs in the database, it generates a fingerspelling animation using the individual letters in the word. After fetching all the animation points, it transmits them back to the client through websockets. The client then uses ThreeJS to accurately animate the points to control the 2D avatar that signs each word correspondingly.    # Future Work  ## Limitations - As seen [above](#language), out of the four fundamental obstacles that are present in ASL ‚≠§ English translation, this project only solves three to a sufficient degree. ASL signers still need to fingerspell words to actually communicate information. Fingerspelling only accounts for 25-30% of signed ASL - The receptive model is still not perfect, and can definitely be improved with more training data and ML expertise - The expressive part of the project still doesn't convey all five parameters of ASL, and is simply a stick figure that might be hard to comprehend  ## Vision - Use RNNs/LSTMs might be a potential way to interpret ASL signs (instead of just fingerspelling) - Look into using ASL parameters and databases like [ASL-LEX](https://asl-lex.org/) or [SLPAA](https://www.youtube.com/watch?v=o4C4hibTW1o) to narrow down a human signing into specific parameters (like location, orientation, erc) and then use a model to classify the sign - Rig and control a 3D model that is used to express ASL, instead of a simple 2D stick figure  My goal for this project has changed as I have progressed, and my vision for it has grown over time. When I initially started working on simply recognizing individual ASL alphabets, I did not expect to get very far, let alone develop something capable of sustaining two-way communication between ASL and English. Regardless, I had one main goal for the project: - Once this project is sufficiently capable, I want to set up a desk/TV somewhere between the BCSD and Burnaby South hallways, or maybe even at the main entrance to our school. With the interface running on this device, I hope that students from BCSD and Burnaby South stop by to talk to each other without a human interpreter assisting the conversation. While this project is far from properly capturing all expressive aspects of ASL, I hope that the novelty of the fingerspell recognition and avatar visualization will bring some students together.  Now, since the project is more capable, my vision for it has also grown: - I want to run an instance that is publicly accessible, so anyone in the world can just open a website and mess around with it. Ideally, it will draw more attention to ASL translation itself and people take the technology further. I also hope it encourages more people to look into ASL and consider learning the language. Maybe the platform will provide people with a fun and interactive method to learn the language. - I also want to modularize the code and make both the receptive and expressive aspects of the project function by themselves. While this project is mostly a proof of concept, I see many potential tools arising from this concept of ASL translation, and I want to work on developing some of them. Hopefully, it will also encourage more people to develop viable accessibility technology. If you happen to be interested in developing such technology, here are some of the potential projects that I have thought of during the development process:   - **ASL Interpreter Webcam Client:** A lightweight modular application that adds an ASL interpreter avatar to the top right of a user's webcam. It will sign all input from the microphone. This will allow people to join meetings or make YouTube videos with live English ‚Üí ASL translation at no cost. This is already possible for more tech-savvy individuals, i.e. I implement it by adding an OBS Browser Source and serving it through a Virtual Camera.   - **ASL YouTube Captions:** A browser extension that adds an ASL option for YouTube captions, essentially adding a 2d avatar to the top right that simply signs everything that is said in the YouTube video. This will allow Deaf individuals to better experience YouTube videos if they prefer signs over captions.   # Acknowledgements ## People - My Computer Science Teacher, who is always excited about my projects and inspires me to do more, for sharing this project and doing their very best to find more opportunities for me to grow this project - My ASL Teacher, for always taking the time out of their day to provide me with feedback, insight, and guidance for improving this project. And also for finding amazing people and opportunities where I can get more feedback! - Ishir Rao, my (goated) childhood friend, for getting me interested in the AI/ML space and providing me with a starting point to get ahead!  ## Projects - These Kaggle datasets, where all my training data was augmented from: [One](https://www.kaggle.com/datasets/lexset/synthetic-asl-alphabet), [Two](https://www.kaggle.com/datasets/danrasband/asl-alphabet-test), [Three](https://www.kaggle.com/datasets/debashishsau/aslamerican-sign-language-aplhabet-dataset) - [The HandSpeak Project](https://www.handspeak.com/), for a lot of additional knowledge about the workings of ASL, as well as a source of sign language videos for the Expressive aspect of this project - [The SignWave Project](https://github.com/tan-ad/SignWave), an amazing tool developed by high-schoolers that provided me with my initial approach towards the Expressive aspect of this project"
Sentiment Analysis,AI/ML,https://github.com/RichardRivaldo/Sentiment-Analysis/tree/main,# Sentiment-Analysis Sentiment Analysis Classifier with Machine Learning Models  ### Topics and Contents This repository contains my projects on Sentiment Analysis from customer reviews.  ### Environment I did all of this project on Google Colaboratory. Check it out in this link:  [Sentiment Analysis on Google Colab](https://colab.research.google.com/drive/1Jb0-XtSdEoTIYw6suN4nzlao4vaXX0JY?usp=sharing)  ### Reference Most of the references and descriptions of this project are in the notebooks.  ### Notes Information about the dataset used in the project is also in the notebook. I also put the dataset (in CSV format) in this repository.
Predicting Student Performance,AI/ML,https://github.com/William-Laverty/ML-Student-Performance-Predictor,"# üöÄ Machine Learning: Student Performance Predictor   Welcome to the **Machine Learning for Student Performance Predictor**. This is a machine learning algorithm for predicting student performance using the Linear Regression technique. The goal of this program is to forecast the final grades of students based on their academic performance and other related factors.  In this algorithm, we use the `student-mat.csv` dataset, which is part of the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Student+Performance). The dataset contains information about student performance in mathematics. The features include attributes such as first-period grade, second-period grade, weekly study time, school type, family size, parent's occupation, and more.  **The project consists of two main components:**  - [`GUI.ipynb`](https://github.com/William-Laverty/ML-Student-Performance-Predictor/blob/main/gradePredictor%20-%20GUI.ipynb) This Jupyter Notebook contains a user-friendly graphical user interface (GUI) that allows users to input student data and obtain grade predictions through an interactive and intuitive interface.   - [`TERMINAL.ipynb`](https://github.com/William-Laverty/ML-Student-Performance-Predictor/blob/main/gradePredictor%20-%20TERMINAL.ipynb) This Jupyter Notebook provides a command-line interface (CLI) version of the grade prediction tool, enabling users to input student data through the terminal and receive grade predictions.  The [terminal version](https://github.com/William-Laverty/ML-Student-Performance-Predictor/blob/main/gradePredictor%20-%20TERMINAL.ipynb) includes a more detailed explanation of the prediction process, accompanied by graphs and visualizations to provide users with a deeper understanding of the model's performance and results. Users can explore the data analysis and model evaluation in the terminal version to gain insights into the grade prediction tool's performance.  ## üéØ Steps Performed by the Code  The Student Grade Predictor is a tool that uses a Linear Regression model to predict the final grade of a student based on their first-period grade (G1), second-period grade (G2), and weekly study time. The model is trained on a dataset containing student information, and the user can input values for G1, G2, and study time through an interactive Graphical User Interface (GUI) to obtain the predicted final grade for a new student.  The predictor uses one-hot encoding for categorical variables and is trained on a dataset (assuming the dataset is in the same directory as the script) that is preprocessed to handle missing values or categorical variables.  1. **Data Loading:** The code reads the `""student-mat.csv""` file, which contains the student performance data, using the pandas library. The data is loaded into a DataFrame for further processing.  2. **Data Preprocessing:** The dataset may have missing values or categorical variables that need handling. The code preprocesses the data, converting categorical variables into numerical form using one-hot encoding. This transformation is necessary because most machine learning algorithms, including Linear Regression, require numerical inputs.  3. **Data Splitting:** The data is split into training and testing sets using the `train_test_split()` function from sklearn. This ensures that the model is trained on a subset of the data and evaluated on unseen data to assess its generalization performance.  4. **Model Training:** The Linear Regression model from sklearn is created and trained on the training data using the `fit()` method. The model aims to learn the relationships between the features and the target variable (final grade).  5. **Model Evaluation:** After training, the model's performance is evaluated using the test data. Two common evaluation metrics used are Mean Squared Error (MSE) and R-squared (R2). MSE measures the average squared difference between the predicted and actual grades, while R2 indicates how well the model explains the variance in the target variable.  6. **Example Prediction with GUI:** The code features an interactive GUI that allows users to input the first-period grade, second-period grade, and weekly study time of a new student. The model will predict their final grade (G3) based on these inputs, providing a convenient and user-friendly way to utilize the predictor.  ## üî® Install the required packages  These packages are essential for different aspects of the project, from data handling and machine learning to creating an interactive GUI within the Jupyter notebook environment.  - **Pandas**  # Data manipulation and analysis - **Numpy**   # Fundamental package for numerical computations - **Scikit-learn**  # Machine learning library - **IPywidgets**    # Interactive widgets for Jupyter notebooks - **Ttkthemes**     # Theming extension for Tkinter  To install the required packages, execute the following commands:  ```bash pip install pandas numpy scikit-learn ipywidgets ttkthemes numpy matplotlib.pyplot ```  ## üßë‚Äçüíª Usage  1. Clone the repository and navigate to the project directory.  ```bash git clone https://github.com/cgs-ist/student-grade-predictor-William-Laverty.git cd ML-Student-Performance-Predictor ```  2. Ensure you have the required packages installed (see the Installation section).  3. Run the Jupyter notebook `""Student_Performance_Predictor.ipynb""` and interact with the GUI to predict student performance.  ### üìù License  The code and documentation in this repository are licensed under the MIT License. You can find the full license text in the [LICENSE](LICENSE) file.  ### üìö References  Cortez, Paulo. (2014). Student Performance. UCI Machine Learning Repository. [Link](https://doi.org/10.24432/C5TG7T)"
Digit Recogniser,AI/ML,https://github.com/aakashjhawar/handwritten-digit-recognition,"MNIST Handwritten Digit Classifier ==================================  An implementation of multilayer neural network using keras with an accuracy of 98.314% and using tensorflow with an accuracy over 99%.  ### About MNIST dataset: The MNIST database (Modified National Institute of Standards and Technology database) of handwritten digits consists of a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. Additionally, the black and white images from NIST were size-normalized and centered to fit into a 28x28 pixel bounding box and anti-aliased, which introduced grayscale levels.   ### Structure of Neural Network: A neural network is made up by stacking layers of neurons, and is defined by the weights  of connections and biases of neurons. Activations are a result dependent on a certain input.  This structure is known as a feedforward architecture because the connections in the network flow forward from the input layer to the output layer without any feedback loops. In this figure:  * The input layer contains the predictors. * The hidden layer contains unobservable nodes, or units. The value of each hidden unit is some function of the predictors; the exact form of the function depends in part upon the network type and in part upon user-controllable specifications. * The output layer contains the responses. Since the history of default is a categorical variable with two categories, it is recoded as two indicator variables. Each output unit is some function of the hidden units. Again, the exact form of the function depends in part on the network type and in part on user-controllable specifications. ![Small Labelled Neural Network](http://i.imgur.com/HdfentB.png)   #### Summary of Sequential model   ![Summary](https://github.com/aakashjhawar/handwritten-digit-recognition/blob/master/assets/model/model_summary.png)  ## Getting Started  How to use ```     git clone https://github.com/aakashjhawar/Handwritten-Digit-Recognition.git cd Handwritten-Digit-Recognition pip3 install -r requirements.txt  python3 tf_cnn.py ``` * You can also run the `load_model.py` to skip the training of NN. It will load the pre saved model from `model.json` and `model.h5` files. ``` python3 load_model.py <path/to/image_file> ``` For example ``` python3 load_model.py assets/images/1a.jpg  ```   ## Prerequisites  - Python 3.5 - OpenCV ``` sudo apt-get install python-opencv ```  ## Result: Following image is the prediction of the model. ![Result of CNN model](https://github.com/aakashjhawar/Handwritten-Digit-Recognition/blob/master/result.png) "
Predicting Music Genres,AI/ML,https://github.com/jsalbert/Music-Genre-Classification-with-Deep-Learning,"# Music Genre Classification with Deep Learning  [![DOI](https://zenodo.org/badge/74898449.svg)](https://zenodo.org/badge/latestdoi/74898449)  ## Abstract  In this project we adapt the model from [Choi et al.](https://github.com/keunwoochoi/music-auto_tagging-keras) to train a custom music genre classification system with our own genres and data. The model takes as an input the spectogram of music frames and analyzes the image using a Convolutional Neural Network (CNN) plus a Recurrent Neural Network (RNN). The output of the system is a vector of predicted genres for the song.   We fine-tuned their model with a small dataset (30 songs per genre) and test it on the GTZAN dataset providing a final accuracy of 80%.   ## Slides and Report  - [Slides](https://github.com/jsalbert/music-genre-classification/blob/master/Slides.pdf)  - [Report](https://github.com/jsalbert/music-genre-classification/blob/master/Music_genre_recognition.pdf)  ## Code   In this repository we provide the scripts to fine-tune the pre-trained model and a quick music genre prediction algorithm using our own weights.   Currently the genres supported are the [GTZAN dataset](http://marsyasweb.appspot.com/download/data_sets/) tags:  - Blues - Classical - Country - Disco - HipHop - Jazz - Metal - Pop - Reggae - Rock  ### Prerequisites  We have used Keras running over Theano to perform the experiments. Was done previous to Keras 2.0, not sure if it will work with the new version. It should work on CPU and GPU.  - Have [pip](https://pip.pypa.io/en/stable/installing/)  - Suggested install: [virtualenv](https://virtualenv.pypa.io/en/stable/)  Python packages necessary specified in *requirements.txt* run: ```  # Create environment  virtualenv env_song  # Activate environment  source env_song/bin/activate  # Install dependencies  pip install -r requirements.txt   ```  ### Example Code  Fill the folder music with songs. Fill the example list with the song names.  ```  python quick_test.py   ```  ## Results  ### Sea of Dreams - Oberhofer [![Sea of Dreams - Oberhofer](https://github.com/jsalbert/Music-Genre-Classification-with-Deep-Learning/blob/master/figs/sea.png?raw=true)](https://www.youtube.com/watch?v=mIDWsTwstgs) ![fig_sea](https://github.com/jsalbert/Music-Genre-Classification-with-Deep-Learning/blob/master/figs/seaofdreams.png?raw=true)  ![Results](https://github.com/jsalbert/Music-Genre-Classification-with-Deep-Learning/blob/master/figs/output.png?raw=true)  ### Sky Full of Stars - Coldplay [![Sky Full of Stars- Coldplay](https://github.com/jsalbert/Music-Genre-Classification-with-Deep-Learning/blob/master/figs/sky.png?raw=true)](https://www.youtube.com/watch?v=zp7NtW_hKJI)  ![fig_sky](https://github.com/jsalbert/Music-Genre-Classification-with-Deep-Learning/blob/master/figs/skyfullofstars.png?raw=true)   "
Music Analysis,AI/ML,https://github.com/mir-aidj/all-in-one/tree/main,"# All-In-One Music Structure Analyzer  [![Visual Demo](https://img.shields.io/badge/Visual-Demo-8A2BE2)](https://taejun.kim/music-dissector/) [![arXiv](https://img.shields.io/badge/arXiv-2307.16425-B31B1B)](http://arxiv.org/abs/2307.16425/) [![Hugging Face Space](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-f9f107)](https://huggingface.co/spaces/taejunkim/all-in-one/) [![PyPI - Version](https://img.shields.io/pypi/v/allin1.svg)](https://pypi.org/project/allin1) [![PyPI - Python Version](https://img.shields.io/pypi/pyversions/allin1.svg)](https://pypi.org/project/allin1)  This package provides models for music structure analysis, predicting: 1. Tempo (BPM) 2. Beats 3. Downbeats 4. Functional segment boundaries 5. Functional segment labels (e.g., intro, verse, chorus, bridge, outro)   -----   **Table of Contents**  - [Installation](#installation) - [Usage for CLI](#usage-for-cli) - [Usage for Python](#usage-for-python) - [Visualization & Sonification](#visualization--sonification) - [Available Models](#available-models) - [Speed](#speed) - [Advanced Usage for Research](#advanced-usage-for-research) - [Concerning MP3 Files](#concerning-mp3-files) - [Training](TRAINING.md) - [Citation](#citation)  ## Installation  ### 1. Install PyTorch  Visit [PyTorch](https://pytorch.org/) and install the appropriate version for your system.  ### 2. Install NATTEN (Required for Linux and Windows; macOS will auto-install) * **Linux**: Download from [NATTEN website](https://www.shi-labs.com/natten/) * **macOS**: Auto-installs with `allin1`. * **Windows**: Build from source: ```shell pip install ninja # Recommended, not required git clone https://github.com/SHI-Labs/NATTEN cd NATTEN make ```  ### 3. Install the package  ```shell pip install git+https://github.com/CPJKU/madmom  # install the latest madmom directly from GitHub pip install allin1  # install this package ```  ### 4. (Optional) Install FFmpeg for MP3 support  For ubuntu:  ```shell sudo apt install ffmpeg ```  For macOS:  ```shell brew install ffmpeg ```   ## Usage for CLI  To analyze audio files: ```shell allin1 your_audio_file1.wav your_audio_file2.mp3 ``` Results will be saved in the `./struct` directory by default: ```shell ./struct ‚îî‚îÄ‚îÄ your_audio_file1.json ‚îî‚îÄ‚îÄ your_audio_file2.json ``` The analysis results will be saved in JSON format: ```json {   ""path"": ""/path/to/your_audio_file.wav"",   ""bpm"": 100,   ""beats"": [ 0.33, 0.75, 1.14, ... ],   ""downbeats"": [ 0.33, 1.94, 3.53, ... ],   ""beat_positions"": [ 1, 2, 3, 4, 1, 2, 3, 4, 1, ... ],   ""segments"": [     {       ""start"": 0.0,       ""end"": 0.33,       ""label"": ""start""     },     {       ""start"": 0.33,       ""end"": 13.13,       ""label"": ""intro""     },     {       ""start"": 13.13,       ""end"": 37.53,       ""label"": ""chorus""     },     {       ""start"": 37.53,       ""end"": 51.53,       ""label"": ""verse""     },     ...   ] } ``` All available options are as follows: ```shell $ allin1 -h  usage: allin1 [-h] [-o OUT_DIR] [-v] [--viz-dir VIZ_DIR] [-s] [--sonif-dir SONIF_DIR] [-a] [-e] [-m MODEL] [-d DEVICE] [-k]               [--demix-dir DEMIX_DIR] [--spec-dir SPEC_DIR]               paths [paths ...]  positional arguments:   paths                 Path to tracks  options:   -h, --help            show this help message and exit   -o OUT_DIR, --out-dir OUT_DIR                         Path to a directory to store analysis results (default: ./struct)   -v, --visualize       Save visualizations (default: False)   --viz-dir VIZ_DIR     Directory to save visualizations if -v is provided (default: ./viz)   -s, --sonify          Save sonifications (default: False)   --sonif-dir SONIF_DIR                         Directory to save sonifications if -s is provided (default: ./sonif)   -a, --activ           Save frame-level raw activations from sigmoid and softmax (default: False)   -e, --embed           Save frame-level embeddings (default: False)   -m MODEL, --model MODEL                         Name of the pretrained model to use (default: harmonix-all)   -d DEVICE, --device DEVICE                         Device to use (default: cuda if available else cpu)   -k, --keep-byproducts                         Keep demixed audio files and spectrograms (default: False)   --demix-dir DEMIX_DIR                         Path to a directory to store demixed tracks (default: ./demix)   --spec-dir SPEC_DIR   Path to a directory to store spectrograms (default: ./spec) ```  ## Usage for Python  Available functions: - [`analyze()`](#analyze) - [`load_result()`](#load_result) - [`visualize()`](#visualize) - [`sonify()`](#sonify)  ### `analyze()` Analyzes the provided audio files and returns the analysis results.  ```python import allin1  # You can analyze a single file: result = allin1.analyze('your_audio_file.wav')  # Or multiple files: results = allin1.analyze(['your_audio_file1.wav', 'your_audio_file2.mp3']) ``` A result is a dataclass instance containing: ```python AnalysisResult(   path='/path/to/your_audio_file.wav',    bpm=100,   beats=[0.33, 0.75, 1.14, ...],   beat_positions=[1, 2, 3, 4, 1, 2, 3, 4, 1, ...],   downbeats=[0.33, 1.94, 3.53, ...],    segments=[     Segment(start=0.0, end=0.33, label='start'),      Segment(start=0.33, end=13.13, label='intro'),      Segment(start=13.13, end=37.53, label='chorus'),      Segment(start=37.53, end=51.53, label='verse'),      Segment(start=51.53, end=64.34, label='verse'),      Segment(start=64.34, end=89.93, label='chorus'),      Segment(start=89.93, end=105.93, label='bridge'),      Segment(start=105.93, end=134.74, label='chorus'),      Segment(start=134.74, end=153.95, label='chorus'),      Segment(start=153.95, end=154.67, label='end'),   ]), ``` Unlike CLI, it does not save the results to disk by default. You can save them as follows: ```python result = allin1.analyze(   'your_audio_file.wav',   out_dir='./struct', ) ```  #### Parameters:  - `paths` : `Union[PathLike, List[PathLike]]`   List of paths or a single path to the audio files to be analyzed.    - `out_dir` : `PathLike` (optional)   Path to the directory where the analysis results will be saved. By default, the results will not be saved.    - `visualize` : `Union[bool, PathLike]` (optional)   Whether to visualize the analysis results or not. If a path is provided, the visualizations will be saved in that directory. Default is False. If True, the visualizations will be saved in './viz'.    - `sonify` : `Union[bool, PathLike]` (optional)   Whether to sonify the analysis results or not. If a path is provided, the sonifications will be saved in that directory. Default is False. If True, the sonifications will be saved in './sonif'.    - `model` : `str` (optional)   Name of the pre-trained model to be used for the analysis. Default is 'harmonix-all'. Please refer to the documentation for the available models.    - `device` : `str` (optional)   Device to be used for computation. Default is 'cuda' if available, otherwise 'cpu'.    - `include_activations` : `bool` (optional)   Whether to include activations in the analysis results or not.    - `include_embeddings` : `bool` (optional)   Whether to include embeddings in the analysis results or not.    - `demix_dir` : `PathLike` (optional)   Path to the directory where the source-separated audio will be saved. Default is './demix'.    - `spec_dir` : `PathLike` (optional)   Path to the directory where the spectrograms will be saved. Default is './spec'.    - `keep_byproducts` : `bool` (optional)   Whether to keep the source-separated audio and spectrograms or not. Default is False.    - `multiprocess` : `bool` (optional)   Whether to use multiprocessing for extracting spectrograms. Default is True.  #### Returns:  - `Union[AnalysisResult, List[AnalysisResult]]`   Analysis results for the provided audio files.   ### `load_result()`  Loads the analysis results from the disk.  ```python result = allin1.load_result('./struct/24k_Magic.json') ```   ### `visualize()`  Visualizes the analysis results.  ```python fig = allin1.visualize(result) fig.show() ```  #### Parameters:  - `result` : `Union[AnalysisResult, List[AnalysisResult]]`   List of analysis results or a single analysis result to be visualized.  - `out_dir` : `PathLike` (optional)   Path to the directory where the visualizations will be saved. By default, the visualizations will not be saved.  #### Returns:  - `Union[Figure, List[Figure]]` List of figures or a single figure containing the visualizations. `Figure` is a class from `matplotlib.pyplot`.   ### `sonify()`  Sonifies the analysis results. It will mix metronome clicks for beats and downbeats, and event sounds for segment boundaries to the original audio file.  ```python y, sr = allin1.sonify(result) # y: sonified audio with shape (channels=2, samples) # sr: sampling rate (=44100) ```  #### Parameters:  - `result` : `Union[AnalysisResult, List[AnalysisResult]]`   List of analysis results or a single analysis result to be sonified. - `out_dir` : `PathLike` (optional)   Path to the directory where the sonifications will be saved. By default, the sonifications will not be saved.  #### Returns:  - `Union[Tuple[NDArray, float], List[Tuple[NDArray, float]]]`   List of tuples or a single tuple containing the sonified audio and the sampling rate.   ## Visualization & Sonification This package provides a simple visualization (`-v` or `--visualize`) and sonification (`-s` or `--sonify`) function for the analysis results. ```shell allin1 -v -s your_audio_file.wav ``` The visualizations will be saved in the `./viz` directory by default: ```shell ./viz ‚îî‚îÄ‚îÄ your_audio_file.pdf ``` The sonifications will be saved in the `./sonif` directory by default: ```shell ./sonif ‚îî‚îÄ‚îÄ your_audio_file.sonif.wav ``` For example, a visualization looks like this: ![Visualization](./assets/viz.png)  You can try it at [Hugging Face Space](https://huggingface.co/spaces/taejunkim/all-in-one).   ## Available Models The models are trained on the [Harmonix Set](https://github.com/urinieto/harmonixset) with 8-fold cross-validation. For more details, please refer to the [paper](http://arxiv.org/abs/2307.16425). * `harmonix-all`: (Default) An ensemble model averaging the predictions of 8 models trained on each fold. * `harmonix-foldN`: A model trained on fold N (0~7). For example, `harmonix-fold0` is trained on fold 0.  By default, the `harmonix-all` model is used. To use a different model, use the `--model` option: ```shell allin1 --model harmonix-fold0 your_audio_file.wav ```   ## Speed With an RTX 4090 GPU and Intel i9-10940X CPU (14 cores, 28 threads, 3.30 GHz), the `harmonix-all` model processed 10 songs (33 minutes) in 73 seconds.   ## Advanced Usage for Research This package provides researchers with advanced options to extract **frame-level raw activations and embeddings**  without post-processing. These have a resolution of 100 FPS, equivalent to 0.01 seconds per frame.  ### CLI  #### Activations The `--activ` option also saves frame-level raw activations from sigmoid and softmax: ```shell $ allin1 --activ your_audio_file.wav ``` You can find the activations in the `.npz` file: ```shell ./struct ‚îî‚îÄ‚îÄ your_audio_file1.json ‚îî‚îÄ‚îÄ your_audio_file1.activ.npz ``` To load the activations in Python: ```python >>> import numpy as np >>> activ = np.load('./struct/your_audio_file1.activ.npz') >>> activ.files ['beat', 'downbeat', 'segment', 'label'] >>> beat_activations = activ['beat'] >>> downbeat_activations = activ['downbeat'] >>> segment_boundary_activations = activ['segment'] >>> segment_label_activations = activ['label'] ``` Details of the activations are as follows: * `beat`: Raw activations from the **sigmoid** layer for **beat tracking** (shape: `[time_steps]`) * `downbeat`: Raw activations from the **sigmoid** layer for **downbeat tracking** (shape: `[time_steps]`) * `segment`: Raw activations from the **sigmoid** layer for **segment boundary detection** (shape: `[time_steps]`) * `label`: Raw activations from the **softmax** layer for **segment labeling** (shape: `[label_class=10, time_steps]`)  You can access the label names as follows: ```python >>> allin1.HARMONIX_LABELS ['start',  'end',  'intro',  'outro',  'break',  'bridge',  'inst',  'solo',  'verse',  'chorus'] ```   #### Embeddings This package also provides an option to extract raw embeddings from the model. ```shell $ allin1 --embed your_audio_file.wav ``` You can find the embeddings in the `.npy` file: ```shell ./struct ‚îî‚îÄ‚îÄ your_audio_file1.json ‚îî‚îÄ‚îÄ your_audio_file1.embed.npy ``` To load the embeddings in Python: ```python >>> import numpy as np >>> embed = np.load('your_audio_file1.embed.npy') ``` Each model embeds for every source-separated stem per time step,  resulting in embeddings shaped as `[stems=4, time_steps, embedding_size=24]`: 1. The number of source-separated stems (the order is bass, drums, other, vocals). 2. The number of time steps (frames). The time step is 0.01 seconds (100 FPS). 3. The embedding size of 24.  Using the `--embed` option with the `harmonix-all` ensemble model will stack the embeddings,  saving them with the shape `[stems=4, time_steps, embedding_size=24, models=8]`.  ### Python The Python API `allin1.analyze()` offers the same options as the CLI: ```python >>> allin1.analyze(       paths='your_audio_file.wav',       include_activations=True,       include_embeddings=True,     )  AnalysisResult(   path='/path/to/your_audio_file.wav',    bpm=100,    beats=[...],   downbeats=[...],   segments=[...],   activations={     'beat': array(...),      'downbeat': array(...),      'segment': array(...),      'label': array(...)   },    embeddings=array(...), ) ```  ## Concerning MP3 Files Due to variations in decoders, MP3 files can have slight offset differences. I recommend you to first convert your audio files to WAV format using FFmpeg (as shown below),  and use the WAV files for all your data processing pipelines. ```shell ffmpeg -i your_audio_file.mp3 your_audio_file.wav ``` In this package, audio files are read using [Demucs](https://github.com/facebookresearch/demucs). To my understanding, Demucs converts MP3 files to WAV using FFmpeg before reading them. However, using a different MP3 decoder can yield different offsets.  I've observed variations of about 20~40ms, which is problematic for tasks requiring precise timing like beat tracking,  where the conventional tolerance is just 70ms.  Hence, I advise standardizing inputs to the WAV format for all data processing,  ensuring straightforward decoding.   ## Training Please refer to [TRAINING.md](TRAINING.md).  ## Citation If you use this package for your research, please cite the following paper: ```bibtex @inproceedings{taejun2023allinone,   title={All-In-One Metrical And Functional Structure Analysis With Neighborhood Attentions on Demixed Audio},   author={Kim, Taejun and Nam, Juhan},   booktitle={IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)},   year={2023} } ```"
Generate Human Faces with DCGAN,AI/ML,https://github.com/ZSoumia/DCGAN-for-generating-human-faces-/tree/master,"# DCGAN-for-generating-human-faces- ## 1. Project overview :  In this projects I built a Deep convolutional generative adversarial network (DCGAN) to generate new fake images of human faces . <br> The model was trained on (CelebA)](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)   ## 2. Getting started : In order to reproduce the same results(that can be found in the notebook itself) , make sure to follow the following steps :  ### 1. Clone the repository :  `` git clone https://github.com/ZSoumia/DCGAN-for-generating-human-faces-.git       `` ### 2. Download the dataset :  (CelebA)](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)   ### 3. Make sure to use the same tools :  - Pytorch. - Python 3.6.  ### Advices :  GANs are a bit too sensitive so trainning them to reach a low loss is a bit challenging, these are some tips that I discovered from training this neural network :  - The BCEWithLogits performed better than the mean of squared errors - In the optimizer's parameters generally setting beta1 to have a value between 0.2 and 0.35 seemed to improve my results - Setting the learning rate of the discriminator to be 4 times greater than the learning rate of the generator helped speeding up the training process - Setting a mini batch to a small value like 16 or 32 also enhanced the results .  ## License :  This project is licensed under the GNU [LICENSE](https://github.com/ZSoumia/DCGAN-for-generating-human-faces-/blob/master/LICENSE) file for details."
Text Summariser,AI/ML,https://github.com/LunaticPrakash/Text-Summarization,"# Text-Summarization Using Spacy and NLTK module with TF-IDF algorithm for text-summarisation. This code will give you the summary of inputted article. You can input the text by typing (or copy-paste) or from Txt file, PDF file or from Wikipedia Page Url.  ## Purpose :-   To save time while reading by summarizing a large article or text into fewer lines.    ## Description :-  It usage Term Frequency-Inverse Document Frequency (TF-IDF) algorithm for summarising the article.  ## Features :-  You can read the text of your long article in 4 ways :-  ![InputTextWays](https://user-images.githubusercontent.com/56812557/212475484-5bd0addf-1b14-4820-b4e2-b21565de8b71.png)    - By typing text on your own (or copy-paste).   - Reading the text from **.txt file**.   - Reading the text from **.pdf file**.(You can choose either to get summary of entire pdf or select any page interval).      ![PdfInput](https://user-images.githubusercontent.com/56812557/212475479-d012f433-8ebd-4283-9c18-c1ebf552accf.png)    - Reading the text from **wikipedia page** (All you have to do is to provide the url of that page. Program will automatically scrap the text and summarise it for you).    Don't worry about Code length xD. It might look lengthy but there are lot of comments for explaination of code(almost 70 comments) and extra spacing for more readability.     ## Output :-       - This is some of the summary text return by the program. Main article was loaded by Wikipedia Page Url -> https://en.wikipedia.org/wiki/Artificial_intelligence        ![Summary](https://user-images.githubusercontent.com/56812557/212475483-5fe99afd-5016-428e-877d-e1e0b9406786.png)        - Comparison of Original Content vs Summarized content.        ![OriginalvsSummaryWordCount](https://user-images.githubusercontent.com/56812557/212475485-d06beadf-1805-49e2-a906-a2745d06b832.png)          ## Requirements :-  - Python3  - Spacy Module (short, medium, or long any type is sufficient) - NLTK Module - PyPdf2 - Beautiful Soup (bs4) - urllib (already available with python itself, no need for external installation)   ## How to install Requirements :-  1. Python3 can be installed from their official site https://www.python.org/ . Or you can use anaconda environment. 2. Spacy can be installed by For Anaconda Environment >  ``` conda install -c conda-forge spacy  python3 -m spacy download en ``` For other environments >  ``` pip3 install spacy  python3 -m spacy download en ``` 3. NLTK can be installed by For Anaconda Environment >  ``` conda install -c anaconda nltk ``` For other environments >  ``` pip3 install nltk ```  4. PyPdf2 can be installed by For Anaconda Environment >  ``` conda install -c conda-forge pypdf2 ``` For other environments >  ``` pip3 install PyPDF2 ```  5. Beautiful Soup (bs4) For Anaconda Environment >  ``` conda install -c anaconda beautifulsoup4 ``` For other environments >  ``` pip3 install beautifulsoup4` ``` ## Getting Started :-  - Download or clone repository.  - Open cmd or terminal in same directory where **Text-Summarizer.py** file is stored and then run it by followng command :-  ``` python3 Text-Summarizer.py ``` - Now just follow along with the program.   ## Bugs and Improvements :-  - No known bugs. Summary can't be as perfect as humans can do. - Audio feature will be added soon, so that you can listen the summary too if you want.   ## Dev :- Prakash Gupta"
Text Analyzer using NLP (Natural Language Processing text) and spaCy,AI/ML,https://github.com/JonathanReeve/workshop-text-analysis-spacy/tree/master,"# Workshop in Text Analysis with Python and SpaCy  This is the third revision of an intermediate / advanced workshop in text analysis with Python and SpaCy, first given at NYCDH in 2017, later that year at PyData (Microsoft), and most recently through Foundations for Research Computing at the Columbia University libraries.  Everything here *should* work out of the box if you import this notebook into a Google Colab notebook.  "
Sudoku Solver using Backtracking and OpenCV,AI/ML,https://github.com/AabidPatel/Sudoku-Solver-with-OpenCV,"# Sudoku Solver using OpenCV Python  This Sudoku Solver uses a camera to search for a 9 by 9 Sudoku puzzle in the frame by extracting the matrix. It extracts the sudoku matrix by preprocessing the image and finding the largest contour of the frame. To preprocess the image, the image converted to a binary color space then a thresholding vlaue is applied to remove the unwanted noise. Then it classifies the digits using the keras model to identify which cell of the matrix is empty or filled and assigns the number classified to the corrosponding cell.   The matrix is then solved using Backtracking algorithm. Backtracking is an algorithmic-technique for solving problems recursively by trying to build a solution incrementally, one piece at a time, removing those solutions that fail to satisfy the constraints of the problem at any point of time. The solution is then overlayed on the original frame.  ![Sudoku-Github](https://user-images.githubusercontent.com/73630123/116782132-067b4600-aaa5-11eb-8f65-07cab96e15e5.jpg)"
Human Expressions Classifier,AI/ML,https://github.com/mlsmall/Facial-Expression-Detection,"# Video Classification: Facial Expression Detection  Learning to recognize a person's facial expressions on a video takes two steps.  The first step is object detection.  The algorithm should be able to see a video and and be able to recognize a person's face. The object detection section of the algorithm is done with the Open CV (Computer Vision) library from Python.  After the face is detected, the algorithm uses a CNN model to classify the person's facial expression.  ## Data  The dataset used was from the Kaggle facial expression recognition competition, which can be found [here](https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data). It contains over 35,000 images, with each one classified with one of the following 7 emotions: (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral).  ## Models The Pytorch model uses a ResNet 50 architecture with transfer learning.  The Keras model uses a simple 4-layer convolutional neural network (CNN) architecture.  The accuracy of both models is roughly the same (around 40%), but when we use the OpenCV library to detect and capture human faces, the algorithm becomes very accurate as you can see see by images of the video captures below.  ## Results <img src=""https://github.com/mlsmall/Facial-Expression-Detection/blob/master/Expressions/happy.png"" width=""360"" /> <img src=""https://github.com/mlsmall/Facial-Expression-Detection/blob/master/Expressions/surprised.png"" width=""360"" /> <img src=""https://github.com/mlsmall/Facial-Expression-Detection/blob/master/Expressions/sad.png"" width=""360"" /> <img src=""https://github.com/mlsmall/Facial-Expression-Detection/blob/master/Expressions/scared.png"" width=""360"" /> <img src=""https://github.com/mlsmall/Facial-Expression-Detection/blob/master/Expressions/angry.png"" width=""360"" /> <img src=""https://github.com/mlsmall/Facial-Expression-Detection/blob/master/Expressions/neutral.png"" width=""360"" />   ## References  The following sources were used to help create the models in this repo:  https://github.com/omar178/Emotion-recognition https://github.com/MauryaRitesh/Facial-Expression-Detection-V2 "
Geographical analysis of Images,AI/ML,https://github.com/NiveditaSureshK/Geospatial-Analysis/blob/main/Zomato_EDA.ipynb,"# Geospatial-Analysis  ## Objective  The basic idea of analyzing the Zomato dataset is to get a fair idea about the factors affecting the establishment of different types of restaurants at different places in Bengaluru. This Zomato data aims at analyzing demography of the location. Most importantly it will help new restaurants in deciding their theme, menus, cuisine, cost, etc for a particular location. It also aims at finding similarity between neighborhoods of Bengaluru on the basis of food.  ## Problem Statement  Observations on the following are made: 1. Top restaurant chains in Bangaluru 2. How does the price differ between restaurants that accept online orders and those that don't? 3. How many restaurants offer table reservations compared to those that do not? 4. Types of restaurants available 5. Top-rated restaurants 6.  Restaurants located at various locations around Bangalore 7. Approximate cost for 2 people 8. How does the vote on restaurants accepting online orders compare to those refusing to accept them? 9. In what restaurant does the most costly rate for two people exist? What is the dish involved? The most popular dish to eat there? 10. Top ten most expensive and cheapest restaurants, based on an estimate for two people 11. Restaurants under 500 (budget hotels) 12. Budget-friendly restaurants with rating >4 13. Overall number of restaurants that have ratings >4 and are under budget (less than 500) 14. Hotels at various locations with affordable rates 15. Foodie's hotspots 16. Heatmap of North Indian and South Indian restaurants 17. Chains with the most popularity for casual dining 18. Favorite dishes in various cuisines represented by a word cloud  ## Dataset  The dataset contains 17 columns as shown below: - **url** - url of the restaurant in the zomato website - **address** - address of the restaurant in Bengaluru - **name** - name of the restaurant - **online_order** - whether online ordering is available in the restaurant or not - **book_table** - table booking option available or not - **rate** - overall rating of the restaurant out of 5 - **votes** - total number of rating for the restaurant as of the above mentioned date - **phone** - phone number of the restaurant - **location** - neighborhood in which the restaurant is located - **rest_type** - restaurant type - **dish_liked** - dishes people liked in the restaurant - **cuisines** - food styles, separated by comma - **approx_cost(for two people)** - approximate cost of meal for two people - **reviews_list** - list of tuples containing reviews for the restaurant - **menu_item** - list of menus available in the restaurant - **listed_in(type)** - type of meal - **listed_in(city)** - neighborhood in which the restaurant is listed  ## Data Analysis Using Python  Work flow of process: 1. Data Collection 2. Data Cleaning 3. Performing EDA 4. Performing Geospatial Analysis 5. Performing Sentiment Analysis  <p align=""center""><img width=""436"" alt=""image"" src=""https://user-images.githubusercontent.com/71536311/194030212-c6501431-385d-401a-8b15-32c9eaf2864a.png""></p>   ### Data Collection - The Dataset ‚ÄúZOMATO BANGALORE RESTAURANTS‚Äù is publicly available on Kaggle website with 51,717 records and 17 attributes as shown under the dataset section.  ### Data Cleaning - This is an essential step to perform before creating a visualization.  - Clean, consistent data will be much easier to visualize. - As a result, missing values are filled, data are filtered accordingly, and inappropriate data are removed.     ### Exploratory Data Analysis - There are different types of charts Bar, Pie, Line, Scatter Plot, Column chart etc. which can visually present the data in a more understandable way. - Below **bar chart** shows the most famous restaurant chains in Bangalore with number of outlets.  <p align=""center""><img width=""571"" alt=""image"" src=""https://user-images.githubusercontent.com/71536311/194037000-d06ac267-2a96-494e-867b-b85187c80e5a.png""></p>  - The following **pie chart** shows the percentage of online orders accepted by restaurants.  <p align=""center""><img width=""200"" alt=""image"" src=""https://user-images.githubusercontent.com/71536311/194037223-0b6008bd-fa97-40db-a14f-b7cd5294c31c.png""></p>   - The below figure represents the **bar chart** for different types of restaurants.  <p align=""center""><img width=""657"" alt=""image"" src=""https://user-images.githubusercontent.com/71536311/194037589-e36863c3-5416-4e32-8417-2a4374569322.png""></p>   - **Bar graph** of different varieties of cuisines in Bangalore.  <p align=""center""><img width=""436"" alt=""image"" src=""https://user-images.githubusercontent.com/71536311/194037795-00b5ec12-1978-4cbb-8bf9-c4bb6d376793.png""></p>   - Below **scatter plot** with X axis denotes the ratings of the restaurants and Y axis denotes the approximate cost for 2 people.   <p align=""center""><img width=""512"" alt=""image"" src=""https://user-images.githubusercontent.com/71536311/194037948-7d08a98d-3fd9-4797-b96e-bb8e702969ae.png""></p>   - **Box plot** depicting the price difference between restaurants that accept online orders and those that do not  <p align=""center""><img width=""341"" alt=""image"" src=""https://user-images.githubusercontent.com/71536311/194038465-a933fa89-a360-474e-b8ef-bc8dec129d17.png""></p>   ### Geospatial Analysis - Geospatial Analysis is useful for locating the geographical area in a particular region.   #### Heatmap of Restaurants in Bengaluru city - For locating the restaurants in geographical map, we need latitudes, longitudes and count of restaurants.  - Extract the **""Latitude""** and **""Longitude""** w.r.t. different Locations using **Python's Geopy library**. - Generate a **""BaseMap""** of Bangalore using **Python's Folium library**.  ![geo analysis](https://user-images.githubusercontent.com/71536311/194526173-42f7e43b-227c-4260-8b01-dbb505b80710.gif)  - Plot a **HeatMap** based on variety of use cases with the help of **Python's Folium ""HeatMap"" Plugins**. - The heatmap below depicts the clutter of restaurants in Bengaluru.  ![heatmap of blore](https://user-images.githubusercontent.com/71536311/194525572-e888e715-84ea-4a83-a304-a2ac19843997.gif)  #### Heatmap of North Indian restaurants  ![hm of ni](https://user-images.githubusercontent.com/71536311/194525587-eca90c7e-3d6b-4f09-b813-6b28a61ee010.gif)  ### Sentiment Analysis - Here are the **Wordclouds developed using the built-in function in python called ‚ÄúWordCloud‚Äù** for 9 different types of restaurants where customers left feedback.    - To generate the below pictured wordclouds using **Python**, feedbacks are preprocessed, null values are dropped and all characters and spaces are removed except alphabets.  <p align=""center""><img width=""698"" alt=""image"" src=""https://user-images.githubusercontent.com/71536311/194528121-df60fde3-4dd3-4c6c-b4d6-c3d79f0b7982.png""></p>  <p align=""center""><img width=""690"" alt=""image"" src=""https://user-images.githubusercontent.com/71536311/194528242-581f6dbc-7b5b-4950-a865-643fc7f295df.png""></p>  <p align=""center""><img width=""690"" alt=""image"" src=""https://user-images.githubusercontent.com/71536311/194528323-45b59f60-8a12-476e-a641-1734b9088412.png""></p>   ## Tools Used ![Jupyter Notebook](https://img.shields.io/badge/jupyter-%23FA0F00.svg?style=for-the-badge&logo=jupyter&logoColor=white)   ![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54)   ![Pandas](https://img.shields.io/badge/pandas-%23150458.svg?style=for-the-badge&logo=pandas&logoColor=white)   ![NumPy](https://img.shields.io/badge/numpy-%23013243.svg?style=for-the-badge&logo=numpy&logoColor=white)   ![Matplotlib](https://img.shields.io/badge/Matplotlib-%23#ffffff.svg?style=for-the-badge&logo=Matplotlib&logoColor=white)   ![Plotly](https://img.shields.io/badge/Plotly-%233F4F75.svg?style=for-the-badge&logo=plotly&logoColor=white)  - **Jupyter Notebook** is used as IDE. - Among the **Python libraries**, **Pandas** and **NumPy** are used for handling data, preprocessing, and mathematical operations, respectively. - **Plotly, Seaborn**, and **Matplotlib** are used for visualizing plots.  For more details, please go through the Jupyter Notebook attached above.  ## Conclusion  - ***Cafe Coffee Day*** dominates the restaurant chain landscape followed by Onesta and then Empire. - ***Online orders are accepted by 64.4%*** of restaurants, whereas ***35.6% of restaurants do not accept them***. - The city of Bangalore is known as a high-tech hub of India, and people who live a busy and modern life are inclined to choose ***Quick Bites***. - The ***most common cuisines are North Indian, Chinese, and South Indian***. Bangalore is therefore influenced more by the cultures of the north than those of the south. - Having reviewed the above scatterplot, we can conclude that ***most of the highest-rated restaurants accept online orders and are budget-friendly as well***. - In the box plot, it can be seen that there is a discrepancy between the median number of votes for both categories. The Zomato application gives customers the option to rate restaurants after they've ordered through it. This will lead to more votes for the restaurants accepting online orders. - The ***majority of the restaurants are priced under 1000***, which means they are affordable and few are luxurious. - The ***most no. of eateries are found in BTM, HSR, and Koranmangala 5th block***. BTM dominates the section by having more than 4000 restaurants. - It is evident that ***eateries are primarily located in the central Bangalore region***. As we get farther from the center of the city, the number of restaurants decreases. Therefore, prospective restaurateurs can consult this to identify suitable places for their business.  **Check out the notebook above to learn more**"
Cancer/Tumor detection using Image Segmentation,AI/ML,https://github.com/MohamedAliHabib/Brain-Tumor-Detection,"# Brain-Tumor-Detector Building a detection model using a convolutional neural network in Tensorflow & Keras.<br> Used a brain MRI images data founded on Kaggle. You can find it [here](https://www.kaggle.com/navoneel/brain-mri-images-for-brain-tumor-detection).<br>  **About the data:**<br> The dataset contains 2 folders: yes and no which contains 253 Brain MRI Images. The folder yes contains 155 Brain MRI Images that are tumorous and the folder no contains 98 Brain MRI Images that are non-tumorous.  # Getting Started  **Note:** sometimes viewing IPython notebooks using GitHub viewer doesn't work as expected, so you can always view them using [nbviewer](https://nbviewer.jupyter.org/).  ## Data Augmentation:  **Why did I use data augmentation?**  Since this is a small dataset, There wasn't enough examples to train the neural network. Also, data augmentation was useful in taclking the data imbalance issue in the data.<br>  Further explanations are found in the Data Augmentation notebook.  Before data augmentation, the dataset consisted of:<br> 155 positive and 98 negative examples, resulting in 253 example images.  After data augmentation, now the dataset consists of:<br> 1085 positive and 980 examples, resulting in 2065 example images.  **Note:** these 2065 examples contains also the 253 original images. They are found in folder named 'augmented data'.  ## Data Preprocessing  For every image, the following preprocessing steps were applied:  1. Crop the part of the image that contains only the brain (which is the most important part of the image). 2. Resize the image to have a shape of (240, 240, 3)=(image_width, image_height, number of channels): because images in the dataset come in different sizes. So, all images should have the same shape to feed it as an input to the neural network. 3. Apply normalization: to scale pixel values to the range 0-1.  ## Data Split:  The data was split in the following way: 1. 70% of the data for training. 2. 15% of the data for validation. 3. 15% of the data for testing.  # Neural Network Architecture  This is the architecture that I've built:  ![Neural Network Architecture](convnet_architecture.jpg)  **Understanding the architecture:**<br> Each input x (image) has a shape of (240, 240, 3) and is fed into the neural network. And, it goes through the following layers:<br>  1. A Zero Padding layer with a pool size of (2, 2). 2. A convolutional layer with 32 filters, with a filter size of (7, 7) and a stride equal to 1. 3. A batch normalization layer to normalize pixel values to speed up computation. 4. A ReLU activation layer. 5. A Max Pooling layer with f=4 and s=4. 6. A Max Pooling layer with f=4 and s=4, same as before. 7. A flatten layer in order to flatten the 3-dimensional matrix into a one-dimensional vector. 8. A Dense (output unit) fully connected layer with one neuron with a sigmoid activation (since this is a binary classification task).  **Why this architecture?**<br>  Firstly, I applied transfer learning using a ResNet50 and vgg-16, but these models were too complex to the data size and were overfitting. Of course, you may get good results applying transfer learning with these models using data augmentation. But, I'm using training on a computer with 6th generation Intel i7 CPU and 8 GB memory. So, I had to take into consideration computational complexity and memory limitations.<br>  So why not try a simpler architecture and train it from scratch. And it worked :)  # Training the model The model was trained for 24 epochs and these are the loss & accuracy plots:   ![Loss plot](Loss.PNG)   ![Accuracy plot](Accuracy.PNG)  The best validation accuracy was achieved on the 23rd iteration.  # Results  Now, the best model (the one with the best validation accuracy) detects brain tumor with:<br>  **88.7%** accuracy on the **test set**.<br> **0.88** f1 score on the **test set**.<br> These resutls are very good considering that the data is balanced.  **Performance table of the best model:**  | <!-- -->  | Validation set | Test set | | --------- | -------------- | -------- | | Accuracy  | 91%            | 89%      | | F1 score  | 0.91           | 0.88     |   # Final Notes  What's in the files?  1. The code in the IPython notebooks. 2. The weights for all the models. The best model is named as 'cnn-parameters-improvement-23-0.91.model'. 3. The models are stored as *.model* files. They can be restored as follows:   ``` from tensorflow.keras.models import load_model best_model = load_model(filepath='models/cnn-parameters-improvement-23-0.91.model') ```  4. The original data in the folders named 'yes' and 'no'. And, the augmented data in the folder named 'augmented data'.   Contributes are welcome! <br>Thank you!   "
Plant Disease Detection,AI/ML,https://github.com/sumanismcse/Plant-Disease-Identification-using-CNN,# Plant-Disease-Identification-using-CNN # Plant Disease Identification Using Convulutional neural Network  Here is how I built a Plant Disease Detection model using a Convolutional Neural Network .  # For those having issues  For finding DataSet;Go to Kaggle and download the PlantVillage Dataset.  I have included a running version of my code in kaggle link. If you have any problem please refer to that. 
Create a Consumer Relationship Management System,Java,https://github.com/machowina/CRM,"# CRM - Customer Relationship Managment  Simple CRM system, made as a final project for Java coding bootcamp in CodersLab.  Technologies used: Java 8, Spring Boot, Spring MVC, Spring Data, Hibernate, Spring Security, MySQL, Multithreading, JUnit and Thymeleaf. Libraries used: Lombok, OpenCSV,  iText  Note: in this project a free bootstrap templete [Vali Admin](https://github.com/pratikborsadiya/vali-admin) is used.  ## Main features: - User roles: employee, manager, owner and admin - Adding and editing users by admin - Adding and editing clients - Searching clients from employee city by employee - Searching all clients in database by manager - Import and export CSV files with clients data - Generating contracts with client - Accepting contracts depending on contract value and user role - Printing PDF files - Adding events with client - Generating notifications for today's events - API for clients  ## Authors  Katarzyna Machowina - [machowina](https://github.com/machowina)"
bFit Cognitive and Memory Testing Game,Java,https://github.com/Dk35840/bFit-A-Cognitive-Game,"# **Project Information**   ## Project Title   bFit-A Cognitive Game   ## Project Description Ever played brainy mobile games? Had fun? In this project you will be building one such game. This game is a reflex-memory testing game. Bascially the user has to follow along and click the colored (green) tiles as they keep popping. There will be a scoreboard keeping track of how good you click :p You can get more about the project at (CRIO project)[https://www.crio.do/projects/java-android-game/]  ## Author   dk35840@gmail.com <br>   ## Collaborator(s)   kevinpaulose35@gmail.com, mridula@criodo.com <br>   ## Project Language(s)   Java <br>   ## Difficulty Intermediate <br>   ## **Duration** 30 hours   ## **Prerequisite(s)** Core Java, Multithreading with some concepts of Android.   ## **Skills to be learned** Java, Splash screens, Multithreading, Mobile App Development   # **Project Metadata**   ## **Project Id** PROJECT_ANDROID_COGNITIVE_GAME   ## **Slug URL**   java-android-game   ## **Keywords** java, android, game   ## **Category** Java,App Development,Android   ## **Focus**  Mobile App Development, Android, Splash screens     # **Overview**   ## **Objective** This project is focussed on creating a simple, fun, interactive brain training android game (using Java).   ## Project Context   Android is a mobile operating system based on a modified version of the Linux kernel and some other open source softwares, designed primarily for touchscreen mobile devices such as smartphones and tablets. Android is developed by a consortium of developers known as the Open Handset Alliance and commercially sponsored by Google.   <br>   About 70 percent of Android smartphones run Google's ecosystem. Android was primarily built on Java (and Kotlin) language. Java is the best option when it comes to building core native android apps. Hence, here we will be using Java.   <br>   People spend hours at the gym, lifting weights, doing cardio exercises and following proper diets to keep their bodies physically fit. But what do people do to keep their brains in shape? Usually, when people are done at the gym they crash in front of a TV and put their brain into a passive mode where all the skills the mind has, i.e. memory, thinking and logical reasoning stagnates. Your brain is the most important part of your body and to not keep it fit is to invite laziness and lethargy into your life. Brain exercise is an approach to train the brain to perform at optimum levels and be sharp and strong even as old age acts to deteriorate your memory. In our daily life, we always feel stressed out. We exercise for the physical wellness of our body. But what do we actually do for the mental health? This game is like an exercise for the brain. Games of these kinds are categorically termed as [cognitive games](https://www.cognifit.com/brain-games).    <br>   The game's walkthrough is shown below:   <br>  [![https://www.youtube.com/watch?v=gauJ-nwoD0M](images/GamePlay.gif)](https://www.youtube.com/watch?v=gauJ-nwoD0M ""bFit-A-Cognitive-Game - Click to Watch!"")   <br>   The app can also be found at the [Amazon store](https://www.amazon.com/dp/B06VXYFNK3/ref=apps_sf_sta). If you face some issue with the Amazon store, you can download `Bfit_2.32` from [here](https://drive.google.com/file/d/0B_mUfZvrxlA5UDFqcE5xVk03S3c/view?usp=sharing) instead.   <br>   So do try the app first!!     ## Project Stages   The project can be broadly classified into 4 sections:- 1) Setting up the UI for the game with buttons and other game screens 2) Adding logic to the game 3) Creating a walkthrough/guided tour of the game to introduce first-time users to our app. 4) Creating a splash screen to load important files in the background.   ![Project Stage](images/projectStage.jpg) <br>   Each of these stages are covered in detailed multiple milestones ahead.     ## High-Level Approach    - Firstly we build the UI for the game. Ideally, it can be 12 x 10 clickable boxes. Also, three more buttons for the start, stop and help section should be optional. - Build the controller of the app or the logic i.e. how it works. - Add game levels that keep up the user's excitement. - Then learn how to create a walkthrough of the game to introduce first-time users to our app. - Create a splash screen. It is the introductory screen that shows an image when we load the game for the first time ever; to actually download important files in the background. - Finally, we will deploy the app by using ""best practice of deploying Android apps"" like obfuscating, modifying apps for multiple screens etc.     ## Applications    - It can be used as an alternate for the cognitive game. - These types of cognitive i.e. brain-training games are de-stressing activites. So they can serve as excellent relaxants. - Improves attention and concentration. - Improves the brain's speed. - Enhances multitasking skills.   ## Credits    This is an android app adaptation of the [brainmetrix](https://www.brainmetrix.com/).     # **Task_id = 1**   ## Task Title   Environment setup   ## Setting up the development environment   First you need to set up the development environment for the android. For this, you need to install Android Studio, Java 8 or higher.   ## Requirements   - Install Java (JDK 1.8) - Install Android Studio - Do the environment setup for Java on your local machine - Create a blank android project by choosing the Empty Activity template. - Provide the project name and the base package and Minimum SDK of API . - Make sure the [version](https://source.android.com/setup/start/build-numbers) used is API Level 23 or later. - Make sure you have minimum RAM of 8 GB installed on your system and SSD is preferable.     <br>   Check the version of java to know whether you have successfully installed it or not-   ```java java -version ```   Congratulations!!! Your initial set up is done!   ### References   - [Installing Java](https://docs.oracle.com/javase/8/docs/technotes/guides/install/install_overview.html) - [Installing Android Studio](https://developer.android.com/studio) - [What is API level?](http://www.dre.vanderbilt.edu/~schmidt/android/android-4.0/out/target/common/docs/doc-comment-check/guide/appendix/api-levels.html)     ## Expected Outcome   You should be able to set up the initial development environment required to develop the game.   For more instruction visit my Projet at (CRIO project)[https://www.crio.do/projects/java-android-game/]   "
Network Packet Sniffer Analyzer Software,Java,https://github.com/HassanAdamm/packet-sniffer,# packet-sniffer This is a project in computer networks course for CSE students at faculty of engineering Ain-shams university. This is a simple wireshark-like program used to capture packets and analyze them. The project is developed using Java Jpcap library on netbeans IDE.
Create a Criminal Face Detection System,Java,https://github.com/prasadus92/Face-Recognition,"# Face Recognition  A Java application for face recognition under expressions, occlusions and pose variations.  ## Description:  - This is a prototype with the goal of improving recognition accuracy and reliability under un-cooperative scenarios like expressions, occlusions (obstacles like spectacles) and pose variations (<60deg).     - The project is tested with Bosphorous Database (http://bosphorus.ee.boun.edu.tr/default.aspx).  ## Running the project:  1. Install latest version of JDK.  2. Install NetBeans.  3. Go to Control Panel->Add or Remove Programs(Uninstall Programs) and check any other databases are installed (e.g. Microsoft SQL or simply MySQL). If so, uninstall all.  4. Install Microsoft Visual C++ Redistributable Package (vcredist_x86.exe).  5. Intall WAMP Server and start it.  6. Open NetBeans and create a new Java project. Delete the default package created.    (For example, if the project name is abc, then NetBeans automatically creates a package with the same name along with .java file. Delete the complete package in Source Packages section )  7. Open the code and copy full ""src"" folder(Parent folder of all .java files) and paste it on Source Packages in NetBeans.  8. Now right click on Libraries and choose Add JAR/Folder option and open the ""libs"" folder that came with code. Select all(CTRL+A) and click Open.  9. Copy all .dll files came with code in the ""dll"" folder to C:\Windows folder [ONLY FOR 32 BIT SYSTEMS].  10. Install Java 3D API(java3d_1_5...exe).  11. Go to C:\Program Files\Java\Java3D\bin and copy all DLLs to C:\Windows [ONLY FOR 64 BIT SYSTEMS].  12. Open System Tray->Wamp Server green icon->Left Click->MySQL->MySQL Console. Press Enter. You should see mysql> prompt.      Enter the following commands: ```mysql    create database 3dface;  use 3dface;  create table users(UserName varchar(30),Email varchar(50),Phone varchar(10),Password varchar(20));  exit;    ```  13. Again open system tray->Wamp green icon left click->phpMyAdmin. Browser will be opened. Database list on left side. Click on ""3dface"". You can see ""users"" table and get confirmed with the same.  12. Run the project."
Wordcount Tools in Java,Java,https://github.com/lucassrg/javawc,"# javawc  The `Java Word Count` ia a Java application built with the purpose of showcasing the usage of some Java 8 features, e.g. Streams, Lambdas.  This application is very similar to Linux 'wc' tool, which basically display some statistics related to one file (number of lines, words, etc).  ## Requirements - Java 8  ## Usage This application requires to set up JAVA_HOME env variable and you also need [gradle](http://gradle.org/). But don't worry because you don't need to download Gradle to start using it. This project is configured to use  the [Gradle Wrapper](https://docs.gradle.org/current/userguide/gradle_wrapper.html) which is a Gradle feature which allows a developer to execute the build without having to manually install Gradle, which also makes your build more reliable since you don't need to worry about dependencies and versions and, as a developer, you only need to focus on writing your code.  When running `gradlew` command, all the project dependencies will be automatically downloaded and the build process started.   So, just run `gradlew` for start building your application: ```bash $ ./gradlew ```  After that, you are ready to run your `javawc` application. Then go to folder `/build/install/JavaWC/bin` and run `./wc <filename>`.  ```bash $ ./wc file.txt Lines: 16 Words: 16 average letters per word: 5.00 most common letter: l ```  ## Generate distribution package (.zip and .tar) You can find a distribution package in both .zip and .tar format under the `/build/distribution` folder which contains the source code, documentation, binaries and script files for running this application.  They are organized in the following folders: - `bin`: script files (.bat or binary) - `lib`: javawc binaries (.jar & .jar dependencies) - `src`: source code - `javadoc`: javadoc   ## Project Structure This project was designed using NetBeans IDE 8.1 with JDK 8.  - The application root package is `com.oracle.javawc` - main method for running this application as a standalone one can be found in the `com.oracle.javawc.main.Main` class. - the logic for reading the file from the file system is available in the `com.oracle.javawc.entities.shell.FileLoader` class. - the logic for calculating the statistics is available in the `com.oracle.javawc.entities.shell.WordCount` class."
Exam Seating Arrangement System in Java,Java,https://github.com/chabedalam11/Exam-Seating-Arrangement-System-Using-JSP-Servlet,"# Exam-Seating-Arrangement-System-Using-JSP-Servlet This web project build using Netbeans IDE. Frame Work use Java Servlet and Jsp. Database Mysql.  Installation instructions ==>  1.	install netbeans.(NetBeans IDE 8.0.2) or other version 2.	install mysql database password (apcl123456) that use in project (you can also change project password see point 13,14) 3.	mysql workbench  Database Configure  ==>  4.	open mysql workbench  5.	unzip the project folder(ESRS.zip) 6.	open btrs.sql from ESRS\DatabaseScript\esas.sql) location 7.	copy all file text and past it in mysql Workbance workspace and run it       8.	now esas database in available in database  Project Configure  ==>  9.	open netbeans IDE 10.	GlassFish server by default add in netbeans, if not add GlassFish server 11.	Open project in netbeans from (ESRS\Project) location 12.	for change database password, host or anything 13.	open Connect Class from com package 14.	now change batabase configure what you want 15.	run project.  Porject Login  ==>  16.	username = admin 	password = test "
Password Generator using Java,Java,https://github.com/KZarzour/Password-Generator,"# Password-Generator  This project is a Java Console Application to generate random passwords and perform password strength checks.  ## Introduction  I decided to build this project during the Winter Break of my second year after taking the Object-Oriented Effective Java Programming course. I wanted to build something interesting with Java to practice and see what I could do on my own. However, I still wasn't sure what I wanted to do. Then one night, while explaining to my father the importance of having a strong password for his social media accounts, I got the idea of creating a random password generator. A week later, it was done.   While working on it, I decided to include a password strength checker feature that checks the overall strength of the entered password. I was pretty happy with how it turned out, but I realized that it was not very straightforward to use for someone who does not know how it is supposed to work. So, I decided to create a GUI for the application for the next step, which is available in the Password-Services repository.  ## Functionalities  ### 1. Generating a Password:  - The user answers with ""Yes"" or ""No"" to questions about using uppercase letters, lowercase letters, numbers, or symbols. - The user then enters the desired length of the password. - A password alphabet is generated based on the user's answers, which is a string containing the chosen characters. - Random characters from the password alphabet are selected and combined to form a completely random string according to the user's preferences. - The randomly generated password is then displayed on the console.  ### 2. Checking a Password's Strength:  The strength check is based on the following criteria: - The password uses uppercase letters. - The password uses lowercase letters. - The password uses numbers. - The password uses symbols. - The length of the password is 8 or more (8 is often the minimum required length for a decent password). - The length of the password is 16 or more (16 is considered to be the minimum length for a good password).  These criteria are used to calculate a score for the password, which determines the message displayed to the user indicating the strength of the password (weak/medium/good/great).  ### 3. Displaying Useful Information:  This is a minor feature that displays information for the user on the console about password security, such as avoiding using the same password twice, avoiding character repetition, keyboard patterns, dictionary words, letter or number sequences, etc."
Online Survey System,Java,https://github.com/kodekracker/Online-Survey-System,"Survey-Board ====================  SurveyBoard is an online survey system for create and publish online surveys in minutes, and view results graphically and in real time.  "
Online Resume Builder,Java,https://github.com/meetakbari/CV-Resume-Builder,"# CV-Resume-Builder A Java based application developed to help you build a professional, fully formatted CV and Resume in various design formats.  #### For more information regarding the project, please visit [Project_Report](https://github.com/meetakbari/CV-Resume-Builder/blob/master/Project_Report.pdf) presentation file available in the repository.   ## Contributors [![](https://avatars1.githubusercontent.com/u/56075605?s=50&u=bf99d5c66a0749903135b279cf00e8ecf0e26d77&v=4)](https://github.com/meetakbari) [![](https://avatars0.githubusercontent.com/u/55320599?s=50&v=4)](https://github.com/MayankkumarTank) "
Snake Game using Java,Java,https://github.com/janbodnar/Java-Snake-Game,# Java-Snake-Game Java Snake game source code  https://zetcode.com/javagames/snake/    ![Snake game screenshot](snake.png)
Snake Game using Java,Java,https://github.com/janbodnar/Java-Snake-Game,# Java-Snake-Game Java Snake game source code  https://zetcode.com/javagames/snake/    ![Snake game screenshot](snake.png)
Data Visualization Software,Java,https://github.com/gavalian/groot,"# twig  twig is the reincarnation of groot and is the project that is actively maintained. Please visit the  [twig repository](https://github.com/gavalian/twig) to get the package.  # Data Visualization and Analysis Software  Powerfull data analysis and visualization tool writte in pure Java. Can be included in the application. Twig library is evolution of groot, which was initially developped for small data visualization while developing data reconstruction codes, and since became very improtant part of CLAS12 online and offline software.This project is actively developped.  # Visualization  <img src=""https://github.com/gavalian/twig/blob/main/tutorials/images/twig-demo-0.0.4.png"" width=""900"">  Examples produced by [twig repository](https://github.com/gavalian/twig)  # Usage  Include twig project in your pom using:  ```html  <!-- github repository for TWIG Library -->  <repositories>     <repository>       <id>twig-github</id>       <url>https://maven.pkg.github.com/gavalian/twig</url>     </repository>  </repositories> <!-- TWIG Library (Java Data Visualization and Analysis) --> <dependency>   <groupId>j4np</groupId>   <artifactId>twig</artifactId>   <version>0.0.4</version> </dependency> ```  Check out the distribution site for newer versions: (click on ""Packages"" on the right menu bar)  # Tutorials  The tutorials for some of the graph types can be found in directory tutorials/plotting, and direcotry  tutorials/io containds examples of data IO., such as saving histograms and reading and plotting, also  reading data from CSV and Text files and plotting graphs.  To run tutorials compile the library:  ``` prompt> git clone https://github.com/gavalian/twig.git prompt> mvn install ```  then run command (depnding what version you got)  ``` prompt> jshell jshell --class-path target/twig-0.0.4-core.jar --startup etc/imports.jshell tutorials/plotting/advanced_bar_chart.java ``` the example scripts are writte to run in JSHELL, if you'd like to include them in your Java program, you must add the imports found in etc/imports.jshell file. If you'd like a specific example of your favorite graph type, please, submit an issue and I will create the desired example.  # Gallery  Gallery from [twig repository](https://github.com/gavalian/twig)  <table class=""center"" width=""100%"">     <tr>         <td width=""50%"">             <figure>                 <a href=""https://github.com/gavalian/twig/blob/main/tutorials/plotting/advanced_graph_fitting.java"">                  <img src=""https://github.com/gavalian/twig/blob/main/tutorials/images/figure_advanced_graph_fitting.png"" alt=""""></a>                 <figcaption><h2></h2></figcaption>             </figure>         </td>         <td width=""50%"">              <a href=""https://github.com/gavalian/twig/blob/main/tutorials/plotting/advanced_bar_chart.java"">               <img src=""https://github.com/gavalian/twig/blob/main/tutorials/images/figure_advanced_bar_chart.png"" alt=""""></a>                 <figcaption><h2></h2></figcaption>         </td>     </tr>     <tr>           <td width=""50%"">             <figure>                 <a href=""https://github.com/gavalian/twig/blob/main/tutorials/plotting/confusion_matrix.java"">                  <img src=""https://github.com/gavalian/twig/blob/main/tutorials/images/figure_confusion_matrix.png"" alt=""""></a>                 <figcaption><h2></h2></figcaption>             </figure>         </td>         <td width=""50%"">              <a href=""https://github.com/gavalian/twig/blob/main/tutorials/images/figure_slice_graph_3d.png"">               <img src=""https://github.com/gavalian/twig/blob/main/tutorials/images/figure_slice_graph_3d.png"" alt=""""></a>                 <figcaption><h2></h2></figcaption>         </td>     </tr> </table>   # Example of creating tuple from text file (Appendix)  We start from file that constains events each presented in two lines  ``` awk '{print $2,$5,$6,$7}' extractedDataPred2.txt > epip_hb.txt ```  this command takes two lines from the input and joins them into one line  ``` paste - - < epip_hb.txt > epip_hb_joined.txt ``` "
Electricity Billing System,Java,https://github.com/Adarsh9616/Electricity_Billing_System,"# Electricity Billing System This is a GUI made using Java Swing. It lets User perform multiple operations like:-   1- User can Create his Personal login for security purposes.  2- User can Add customers and Calculate their Electricity Bill.  3- User can Pay Electricity Bills.  4- User can Generate Bill.  ## About Project: This Java application was created using Intelli J . Additional library was added for the support of JDBC (Required to setup the connection between the Database and Java Application). It contains 9 different classes which works together to create a better user experience .  ->Splash Screen class  ->Login Screen class  ->Main System class  ->Add Customer class  ->Pay Bill class  ->Generate Bill class  ->Show Details class  ->Last Bill class  ->Connection Setup class(JDBC - MySQL)  ## Database (MySQL) Database for this Electricity Billing System contains 4 Tables   ->Login Table (UserName,Password)  ->Bill Table(MeterNumber,Units,Month,Amount)  ->Emp Table(Name, MeterNumber, Address, State, City, Email, Phone)  ->Tax Table(MeterLocation,MeterType,PhaseCode,BillType,Days,MeterRent,MCB_Rent,ServiceRent,GST)   Java communicates with MySQL tables using JDBC which stands for Java Database Connectivity.  ## Screenshots:  ## Login  <img src=""https://github.com/Adarsh9616/Electricity_Billing_System/blob/master/ScreenShots/Login.JPG"" width=""400"" height=""300"">  ## Main Page  <img src=""https://github.com/Adarsh9616/Electricity_Billing_System/blob/master/ScreenShots/Main.JPG"" width=""600"" height=""500"">  ## Add Customer  <img src=""https://github.com/Adarsh9616/Electricity_Billing_System/blob/master/ScreenShots/AddC.JPG"" width=""500"" height=""500"">  ## Calculate Bill  <img src=""https://github.com/Adarsh9616/Electricity_Billing_System/blob/master/ScreenShots/CalculateBill.JPG"" width=""500"" height=""500"">  ## Details  <img src=""https://github.com/Adarsh9616/Electricity_Billing_System/blob/master/ScreenShots/Details.JPG"" width=""800"" height=""300"">  ## Generate Bill  <img src=""https://github.com/Adarsh9616/Electricity_Billing_System/blob/master/ScreenShots/GenerateBill.JPG"" width=""400"" height=""700"">"
Web Medical Management System,Java,https://github.com/mokarrom/medical-center,"# Medical Center Management System A web based Online Medical Center Management System (for SUST Medical Center).  Problem Statement ----------------- Currently, SUST Medical Centre uses manual (primitive) Management System for maintaining the patient demography and distributing medicine to the patient. In the existing system, doctors and other employees have to spend a lot of time to provide services to the patient because a lot of papers are used to record information. Here, all the tasks (e.g., prescribing patient, delivering medicine, maintaining medicine stock, retrieving records etc.) are tremendously manual and paper dependent. Therefore, an automated online management system needs to be developed.  Project Overview ---------------- This system will provide a graphical user interface to maintain the whole system, including prescribing patient, delivering medicine, maintaining medicine stock etc. Moreover, the new system will be accessible from terminals within the Medical Center and also through the internet from computers outside the Medical Center. Besides, the patients (both student and staff) can view their prescription through internet from anywhere.   Users ----- 1) **General User (Patient):** Student, Staff.  2)	**Administrative User:** Doctor, Pharmacist, Store Officer, Medicine Distributor.  Major Functions --------------- The main facilities will be available in this project are:-  - Total existing management system will be computerized. - Maintaining patients diagnosis details, advised tests to be done.  - Maintaining patient‚Äôs prescription, medicine, medication instructions, precautions and diet advice details. - Providing and maintaining all records of stock medicine through two subcategory central-store and sub-store.  -	The system will keep all tracks of newly purchased medicine and also monitoring their flow. -	Billing report for the patient who are employee and Report generation. -	The system will able to provide a proposed list of medicine that should be purchased in upcoming month. -	If user forgets his/her password then it can be retrieved by hint question.  System Interfaces ----------------- - **Client on Internet:** Web Browser, Operating System (any). - **Client on Intranet:** Client Software, Web Browser, Operating System (any). - **Web Server:** Apache Tomcat, Operating System (any). - **DataBase:** MySQL.  User Interface -------------- User interfaces for all users are graphical user interfaces (GUI). These GUI could be both web based and desktop based which is connect to the medical central terminal. The user interfaces are pretty simple and straight-forward.  Communication Interface ----------------------- - Client on Internet will be using HTTP/HTTPS Protocol. - Client on intranet will be using TCP/IP protocol.  Requirements & Installation --------------------------- **Requirements:**  1. jdk-6u1-windows-i586-p 2. netbeans-6.9.1-ml-windows 3. MySql Database   1. mysql-5.1.39-win32_2   2. mysql-connector-odbc-5.1.6-win32   3. mysql-gui-tools-5.0-r17-win32 4. mysql-connector-java-5.1.6-bin.jar 5. Apache Tomcat 6.0.26 (integrated with netbaens) 6. Firefox  (3.6.3 or higher)  **Installation:**  1. Install JDK. 2. Install NetBeans IDE (Version 6.9.1). 3. We used MySQL database in this project. We have to install  three components of MySQL. In our case `root` and `admin` are the username and password respectively for MySQL. (If you want to change the username & password then open `database.java` file from *project\MedicalCentre\src\java\medicalcenter* location and change username or  password according to your MySQL database username and password). 4. Install Firefox web browser. 5. Open NetBeans IDE and import the `MedicalCentre` project as web project. 6. Add *mysql-connector-java-5.1.6-bin.jar* in Libraries if not added before.  **Known Issue:**  Although you have installed and configured everyting properrly, you might have faced problem regarding database. In that case you may take help from `DBQuery.sql` file for database query (i.e., trigger, procedure, event etc.).  A few dummy users are (available in the system):  User Name  | Password | Uset Type ------------- | ------------- | ----------- doctor  | d | Doctor doctor2	| d | Doctor pharmacist |	p | Pharmacist md |	md1 | Medicine Distributor a-cse	| p | Patient (Employee)	 b-cse	| p | Patient (Employee) 2007331039 |	mokarrom | Patient (Student) 2007331023 |	p | Patient (Student)	  Reference --------- A detail technical report which explains different phases (e.g. requirement analysis, system design, architecture, coding, debugging and testing etc.) of software engineering, is avaialable in the `docs` directory. "
Supply Chain Management System,Java,https://github.com/sonnyhcl/Backend,"# README ## Table Of Contents -   [Video](#video) -   [Run in Intellij IDEA (recommend)](#run-in-intellij-idea) -   [Run in Spring Tool Suite](#run-in-spring-tool-suite)  ## Repo tree ```bash com/zbq/                 # Implementation of global cache and message queue ‚îú‚îÄ‚îÄ ACTFEvent.java ‚îú‚îÄ‚îÄ EventType.java ‚îú‚îÄ‚îÄ GlobalEventQueue.java ‚îú‚îÄ‚îÄ GlobalVariables.java ‚îî‚îÄ‚îÄ VWFEvent.java  supplychain/activiti     # Closely related to the activiti engine our bussiness process execution. ‚îú‚îÄ‚îÄ conf                 # Java configuration files including configuring activiti engine and web context. ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ ActivitiEngineConfiguration.java ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ AsyncConfiguration.java ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ Bootstrapper.java ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ ContentStorageConfiguration.java ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ CustomSecurityConfig.java ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ DatabaseConfiguration.java ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ EmailConfiguration.java ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ JacksonConfiguration.java ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ MyApplicationConfiguration.java ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ MyCorsRegistration.java ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ RestApiConfiguration.java ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ RestTemplateConfiguration.java ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ SchedulingConfiguration.java ‚îú‚îÄ‚îÄ coord              # Impletation of 4 Coordinator Service , among them , VWC is the most complex one. ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ MSCoordinator.java ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ SWCoordinator.java ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ VMCoordinator.java ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ VWCoordinator.java ‚îú‚îÄ‚îÄ listener           # Some of the exection/task listeners set in process model. ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ AnchorStartListener.java ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ DockTaskEndListener.java ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ FlowIntoVoyaListener.java ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ InitListener.java ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ InitWeagonListener.java ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ RunEndListener.java ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ RunListener.java ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ SendArraInfoToSWC.java ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ SendMsgToVWC.java ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ SendOrderToMSC.java ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ VoyagingListener.java ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ VoyaTaskStartListener.java ‚îú‚îÄ‚îÄ rest ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ service ‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ api       # Some convertors between custom type defined by myself and RestVariable Type build in engine. ‚îÇ¬†¬†         ‚îú‚îÄ‚îÄ CustomActivitiTaskActionService.java ‚îÇ¬†¬†         ‚îú‚îÄ‚îÄ CustomArrayListRestVariableConverter.java ‚îÇ¬†¬†         ‚îú‚îÄ‚îÄ CustomBaseExcutionVariableResource.java ‚îÇ¬†¬†         ‚îú‚îÄ‚îÄ CustomBaseVariableCollectionResource.java ‚îÇ¬†¬†         ‚îú‚îÄ‚îÄ CustomDateRestVariableConverter.java ‚îÇ¬†¬†         ‚îú‚îÄ‚îÄ CustomRestResponseFactory.java ‚îÇ¬†¬†         ‚îú‚îÄ‚îÄ LocationRestVariableConverter.java ‚îÇ¬†¬†         ‚îú‚îÄ‚îÄ VPortRestVariableConverter.java ‚îÇ¬†¬†         ‚îú‚îÄ‚îÄ WeagonRestVariableConverter.java ‚îÇ¬†¬†         ‚îî‚îÄ‚îÄ WPortRestVariableConverter.java ‚îú‚îÄ‚îÄ service ¬† ¬† ¬† ¬†  # Some JavaDelegate Class bound to Service Task in process. ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ CustomActivitiTaskFormService.java ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ SendApplyToVMCService.java ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ SendMsgToAWSService.java ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ SendMsgToWVCService.java ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ UpdateVesselInfoService.java ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ UpdateWeagonInfoService.java ‚îî‚îÄ‚îÄ servlet         # About other web configuration      ‚îú‚îÄ‚îÄ MyApiDispatcherServletConfiguration.java   #servlet dispatcher for url pattern /api/*     ‚îú‚îÄ‚îÄ MyCorsFilter.java        # used to solve CORS problem.     ‚îú‚îÄ‚îÄ MyWebConfigurer.java     # configure two servlet dispatcher , respectively for '/api/*' and '/app/*'.     ‚îú‚îÄ‚îÄ SystemWebSocketHandler.java     # websocket haven't been used , you can ignore them.     ‚îî‚îÄ‚îÄ WebsocketHandshakeInterceptor.java  supplychain/web/        # Implementation of REST interfaces to provide endpoints to front-end or AWS to access. ‚îú‚îÄ‚îÄ AbstractController.java ‚îú‚îÄ‚îÄ CoordController.java ‚îú‚îÄ‚îÄ ProcessInstancesResource.java ‚îú‚îÄ‚îÄ TaskController.java ‚îú‚îÄ‚îÄ TaskFormResource.java ‚îú‚îÄ‚îÄ VesselController.java ‚îî‚îÄ‚îÄ VesselProcessInstanceVariableDataResource.java  supplychain/entity/     # Some custom Types for process model. ‚îú‚îÄ‚îÄ IotVessel.java ‚îú‚îÄ‚îÄ Location.java ‚îú‚îÄ‚îÄ Path.java ‚îú‚îÄ‚îÄ VesselVariablesResponse.java ‚îú‚îÄ‚îÄ VPort.java ‚îú‚îÄ‚îÄ Weagon.java ‚îî‚îÄ‚îÄ WPort.java ```  ## Video [Demo video](https://www.dropbox.com/s/kvhavse90hicdi9/cases_demo.mp4?dl=0)  ## Run in `Intellij IDEA` 1.  Download development tool:      IDE: [Intellij IDEA](https://www.jetbrains.com/idea/download)          Web Container: [Tomcat](http://tomcat.apache.org/)          Database: [MYSQL](https://www.mysql.com/)  2.  Clone Source Code:      `git clone --recursive git@github.com:sonnyhcl/Backend.git`  3.  Import Project in IDEA:        -   Choose `Import Project`.         ![import](image/idea_import_project.png)      -   Choose the `directory path` which you clone the source code     -   Choose import project from external model `maven`     -   Pass `select profiles`     -   Pass `select maven projects to import`.     -   Carefully choose `project JDK`     -   `Finish`     -   Wait for IDEA to download maven dependencies  4.  Configure Mysql      First make sure you have installed `mysql` and your mysql has a empty database called `act6`          Or you can edit your own mysql `databasename/username/password` in [activiti-app.properties](https://github.com/sonnyhcl/Backend/blob/master/src/main/resources/META-INF/activiti-app/activiti-app.properties)  5.  Configure Tomcat:     -   Click `Run`         ![run_idea](image/run_idea.png)     -   Edit `Run Configuration`     -   Click the `plus` in left-up corner     -   Choose `Tomcat Server`     -   Choose `local`     -   Configure tomcat in idea         ![tomcat_idea](image/tomcat_idea.png)      -   Configure artifact         ![artifact](image/artifact.png)      -   Configure application-context: `/activiti-app`         ![application-context](image/application-context.png)  7.  Run Activiti      Run Activiti and visit [https://localhost:8080/activiti-app](https://localhost:8080/activiti-app) in the browser     ```     username: admin     password: test     ```     ![activiti](image/activiti.png)  8.  Import bpmn:      After running the project and logging in  the system , upload process models under the root directory `src/main/resources`, such as `Supply_Chain_pool.bpmn`, `Weagon_Test.bpmn`, then create app for it  and publish your app, then  go to processes page to start the selected process.     ![import process](image/import_process.png)  ## Run in `Spring Tool Suite` 1.  Download development tool:      IDE: [Spring-Tool-Suite-3.9.0.RELEASE](https://spring.io/tools/sts/all)          Web Container: [Tomcat](http://tomcat.apache.org/)          Database: [Mysql](https://www.mysql.com/)  2.  Clone Source Code:      `git clone --recursive git@github.com:sonnyhcl/Backend.git`  3.  Import Project:        To import Backend as a Maven project in IDEA, follow the steps below :     `File -> Import -> Maven -> Existing Maven Projects -> Select Root Directory -> Finish`     ![import](image/idea_import_project) 4.  Update Maven Dependencies:      Right click the project name `activiti-app`, then enter -> `Maven` -> `Update Project`, waiting to load dependencies.     ![maven](image/maven_update.png)  5.  Configure Tomcat:      After loading is completed, right-click the project name, followed by `Run As `-> `Run On Server` -> `choose Tomcat Server` -> `Finish`.      ![runas](image/runas.png)      ![runas](image/run_on_server.png)  6.  Configure Mysql      First make sure you have installed mysql. You can edit your own mysql `databasename/username/password` in [activiti-app.properties](https://github.com/sonnyhcl/Backend/blob/master/src/main/resources/META-INF/activiti-app/activiti-app.properties)  7.  Run Activiti      Visit [https://localhost:8080/activiti-app](https://localhost:8080/activiti-app) in the browser, enter the login page,     ```     username: admin     password: test     ```     ![activiti](image/activiti.png)  8.  Develop:      After running the project and logging in  the system , upload process models under the root directory `src/main/resources`, such as [Supply_Chain_pool.bpmn20.xml](https://github.com/sonnyhcl/Backend/blob/master/src/main/resources/Supply_Chain_pool.bpmn20.xml), then create app for it  and publish your app, then  go to processes page to start the selected process.     ![import process](image/import_process.png)  ## Dependencies -   [`Vessel Frontend`](https://www.github.com/sonnyhcl/Frontend)     > Attention: In order to perform as [demo](#demo) shows, The `Activiti Backend` project must be coordinated with the `Vessel Frontend` project."
Bookshop Management System Using C++,C++,https://github.com/aryan-khanijo/Bookshop-Management-System-CPP-Project,# BookShop Management System Create a MySQL Database  DataBase Name : Management  And Tables :   Books      int id			// Primary Key 	string name 	string auth 	int price 	int qty  suppliers  	int id				//Primary Key 	string name 	long int phn 	string addr_line1 	string addr_line2 	string addr_city 	string addr_state  purchases  	int ord_id			//Primary Key 	int book_id		    //FK ref (books) 	int sup_id			//FK ref (suppliers) 	int qty 	date dt_ordered 	int eta 	char received		// Check(T or C or F) def (F) 	int inv  employees  	int id				//Primary Key 	string name 	string addr_line1 	string addr_line2 	string addr_city 	string addr_state 	long int phn 	date date_of_joining 	long int salary 	string mgr_status	//check(T or F) def f  members      int id				//Primary Key 	string name 	string addr_line1 	string addr_line2 	string addr_city 	string addr_state 	long int phn 	date beg_date 	date end_date 	string valid  sales  	int invoice_id		//Primary key     int member_id		//FK ref member(id) 	int book_id		//FK ref books(id) 	int qty 	int amount 	date date_s
Bank Management System In C++,C++,https://github.com/wkhaliddev/Bank-Management/tree/master/DSAproject,# Bank-Management In this project there a three basic portions: 1-admin 2-customer 3-staff so all these have different levels of authorizations like   1-customer can : perform transaction and like that  3-staff can: add amount in account deduct amount from account request to open account  3-admin can: see all accounts  can read all accounts can give permission to open account have cresidentials of all accounts
Student Record Management System,C++,https://github.com/Code-Recursion/Student-Record-Management-System,"# Student Record Management System A minimalist desktop app  to managae students record. **record are saved directly on an excel file to decrease data Entry labour cost**  and improves efficiency.  >launch the srm.exe file directly to use the App.  >for testing purpose both passwords are set to ""password"".  >Admin have full access to manipulate the data.  >Student account can only view the data. ## `Login` <img src=""ScreenShot/Home.JPG"">  ## `Logged in as Admin` <img src=""ScreenShot/AdminFunction.JPG"">  ## `Viewing Students Records` <img src=""ScreenShot/ViewData.JPG"">  ## ``Records Saved in Excel File`` <img src=""ScreenShot/ViewExcel.JPG"">  ### Running on Local machine  - Install a C++ compiler (e.g., GCC). - Write your C++ code and save it with the .cpp extension. - Open the terminal (Linux/Mac) or Command Prompt (Windows). - Navigate to the directory containing your C++ source code. - Compile the code using the command: `g++ -o output_filename source_file.cpp`. - Run the executable using: `./output_filename` (Linux/Mac) or `output_filename.exe` (Windows). - Test and verify your program's functionality. "
Contact Management System Using C++,C++,https://github.com/JeremyWu917/ContactManagementSystem,# ContactManagementSystem C++ project
Car Rental System In C++,C++,https://github.com/Ellie-Y/CarRentalSystem,"## Car Rental System  <img src = ""https://github.com/Ellie-Y/CarRentalSystem/blob/master/presentation.gif"" width = ""800px"" />    This project used Qt creator 5.8.0, and it divided into 8 classes, the start one is login class.   - The class of Sign_In implement the function of  sign-up. (I made a typo)  - It includes database, please use absolute path of your database in the ""connectData"" method, which located in Login, Sign_In and findRidePage classes."
Credit Card Validator,C++,https://github.com/karancodes/credit-card-validator,# Credit Card Validator A simple C++ program I wrote in my first computing course @SFU which validates credit card numbers using Luhn's Algorithm.
Sudoku Game,C++,https://github.com/AlexIzydorczyk/sudoku,"# sudoku C++ sudoku game  ## Program Use ##  Game can be started by compiling (with makefile) and running sudoku  1. Default action is to start interactive, 9x9 game 2. To fill in cell, enter the column number, row number, and value you want to enter (1 based). Separate with spaces, commas, or any other delimiter (besides an integer of course) 3. At any time, enter ""Solve"" to have the backtracer solve the game for you based on your current position. 4. If you've walked yoursel into an impossible configuration you will be prompted to first clear the board 5. Once solved, you will be asked if you want to play again  6. Run speed test / alternate game configurations using command flags below  Command line flags can be used to configure the game  -s --seed Set the random seed to replicate results -u --Unittest Runs unit (speed) test with both solvers. Specify number of times to run -g --gamesize Integer value to specify game size 				ie 9 for 9x9 game, 16 for 16x16 etc. -n --nobs Specify number of pre-filled values to include (ie. immutable Sudoku cells) -v --verbose Add flag with no argument for verbose output after every unit test. No effect if game played in interactive mode   Notes:  - Puzzles are always generating using a sort of reverse backtrace (ie. filling in diagonal using random permutation, solving puzzle, and then removing cells). This means that sometimes generating the puzzle takes longer than solving it (especially for big puzzles). The unit test times only the solver function - During the class demo, in the speed test, our first puzzle took an order of magnitude longer to solve for the backtracer than the other 9 puzzles. Upon investigation, it turns out that this was likely because of the constant random seed (there is significant variability in time to solve puzzles for backtracer, the Alternating Projections approach seems to be less variable for given puzzle configuration)   Items to grade:  1. Game Logic/ Gameplay of the interactive game + command line tools/speed tests 2. Back-tracking solver (the ""solve"" function in solve.cpp) 3. Alternating projection solver (the ""DR"" function in altproj.cpp)  ## Project Contributions ##  1. Matt:     * implemented two Sudoku solvers: backtracking and alternating projections     * implemented function to generate a Sudoku puzzle  2. Alex:     * implements actual game-play and game-play logic (taking user input, printing/formatting to screen etc.)     * implemented comamnd line flags and associated functions(ie. unit testing / timing functions)  ## Notes on Building Project ##  Build reqiures the `armadillo` library.  You can download it from the website http://arma.sourceforge.net/.  ## References ##  1. Pseudo-code for backtracking algorithm     - http://moritz.faui2k3.org/en/yasss  2. Idea to use Douglas-Rachford splitting to solve Sudoku     - ""Recent Results on Douglas‚ÄìRachford Methods for Combinatorial Optimization Problems""  3. Function timing code 	- http://stackoverflow.com/questions/29719999/testing-function-for-speed-performance-in-cpp "
Trading Application Project In C++,C++,https://github.com/JulyIghor/QtBitcoinTrader,"## Qt Bitcoin Trader  This software helps you open and cancel orders very fast. Real time data monitoring. Automated trading using JavaScript powered scripts.  Developed on pure Qt, uses OpenSSL, AES 256 key and secret protection.  ## Download binaries  * https://sourceforge.net/projects/bitcointrader/  `Win32`,`Mac`,`Linux` * http://www.softpedia.com/get/Others/Finances-Business/Bitcoin-Trader.shtml `Win32` * http://mac.softpedia.com/get/Finance/Qt-Bitcoin-Trader.shtml `Mac`  ## Compilation on Linux * `sudo apt-get install g++ libssl-dev libglu1-mesa-dev qt5-qmake qtscript5-dev qtmultimedia5-dev git` * `git clone https://github.com/JulyIGHOR/QtBitcoinTrader.git` * `cd ./QtBitcoinTrader/src` * `QT_SELECT=5 qmake QtBitcoinTrader_Desktop.pro` * `make && make install && make clean`  ## Demos  * http://www.youtube.com/watch?v=C--P258DQkk  ## Forums  * https://bitcointalk.org/index.php?topic=201062 `ENG` * https://bitcointalk.org/index.php?topic=218044 `RUS`  ## Social  * http://www.facebook.com/QtBitcoinTrader * http://twitter.com/QtBitcoinTrader  ## Change Log  v1.42.22 Released!  - Fixed Bitfinex history - Fixed Bitstamp depth - Pairs synchronised - Removed Indacoin exchange  v1.42.21 Released!  - Poloniex date format hot fix  v1.42.20 Released!  - Updated API integration for Poloniex - Fixed trade history issue in Bitfinex - Synchronized currency pairs  v1.42.10 Released!  - Updated API integration to fix Bitstamp and Poloniex - Synchronized currency pairs  v1.42.00 Released!  - Fixed crash while running some scripts/rules - Updated API integration to fix Bitstamp, Poloniex and Bittrex - Fixed a bug when trade history stays empty - Fixed UI bugs  v1.41.00 Released!  - Fixed time syncronization in Binance - Translation fixes, added Korean language - Fixed bug in trade history of Bittrex exchange - Improved request interval limits for Poloniex - Qt 6 support, using QJSEngine instead of QtScript - Optimized CPU usage - Updated currency pairs list  v1.40.55 Released!  - Fixed trader.groupStop(..) script command - Synchronized currency pairs  v1.40.54 Released!  - Fixed trader.say(..) script command - Fixed crash on unix systems  v1.40.53 Released!  - Fixed critical bug causing missing public trade history items - Fixed bug when zero/incorrect balance reached script/rules events - Fixed missing script events/wrong rule state for Bitfinex - Fixed own trade history items missing for Bitfinex - Fixed rounding of buy/sell total calculations - Fixed Hitbtc exchange api keys impossible to add - Fixed translation issues. French language updated - Optimized memory usage - Removed exchanges bitmarket.pl, okcoin.cn, btcchina.com, goc.io  v1.40.52 Released!  - Enabled experimental vnc server on a Linux - Fixed yobit api, if you ever get nonce error, re-create API keys - Improved minimum request interval for Binance to prevent IP bans  v1.40.51 Released!  - Fixed hitbtc balance - Fixed binance balance  v1.40.50 Released!  - Fixed hitbtc authorization bug - Fixed transaction history in bitstamp - Smart paste for spinbox values - Synchronized currency pairs - Improved text to speech engine  v1.40.43 Released!  - Request interval now can be set less than 0.5 seconds - Fixed bug when timers wasn't stopped on group stop - Fixed crash when open Debug dialog - Fixed incorrect fee value in Bitfinex - Fixed filter open orders in Bitfinex - Fixed tray icon in Linux - Fixed bug caused long delay after network down - Fixed on top mode for dock widgets - Fixed bug when password was incorrectly detected as weak - Currency pairs synchronized in all exchanges  v1.40.42 Released!  - Fixed start window position - Removed duplicated info in log view - Fixed Own transactions display in Bitstamp - Fixed problem with false time - Added option to change hostname and port for exchange profile - Fixed wrong order amount for Bitfinex, when order partly filled - Minor UI fixes, fixed crash in log view  v1.40.41 - Bitstamp API integration fixed - Rules generator dialog critical bug fixed when price with fee used - Time sync bug fixed - Binance orderbook table fixed - Removed Wex exchange - Added alternative domain for Yobit - Fixed dock panel issue caused blinks - Many small fixes  v1.40.40 - New exchange Poloniex - Fixed script language command cancel ask and bid for Binance - Fixed bug caused forever API down in some cases - Minor fixes  v1.40.30 - Added new exchange HitBTC - Added more decimals for Fee Calculator - Fixed canceling order via rule or script for Binance - Fixed bug in charts - Fixed wex mirror - Minor cleanup and optimizations  v1.40.23 - Added new exchange Bittrex - Portable mode for Linux and macOS - Secure auto update for Linux x86_64 - Uninstall option in help menu - Fixed Binance own trade history order - Time Sync bug fixed - HDPI bug fixed - Minor fixes  v1.40.22 - Added new exchange Binance - Many small fixes  v1.40.21 - Fixed bug of Rules dialog - Fixed HiDPI mode - Fixed Script and Rules enable-disable command - Fixed Last Buy and Sell price event bug - Memory optimisations, refractoring - Minor fixes  v1.40.12 - Dropped support of Windows XP and macOS 10.9, but you still can compile it manually - Removed update size limit for future bigger updates - Fixed Bitfinex trade history bug - Fixed Bitstamp certificate issue - Switched to OpenSSL 1.1, no UPX in release binaries - Minor improvements  v1.40.09 - Fixed FeeCalculator crash - Fixed account open orders filtering and calculations - UI layout bugs fixed  v1.40.08 - Added WEX exchange, removed BTC-e - Fixed crash on app closing - Added confirmation message on script editor clear - Minor fixes  v1.40.07 - Release builds for Win64 - Fix balance for OKCoin - Fix script for Bitfinex - Fix order type for Bitfinex - JL Script logs now recognize endline and tabulation - Fixed bug when silent auto update wont work  v1.40.06 - Fixed fee calculator issue - Added new currencies - Minor improvements and fixes  v1.40.03 - Bitfinex support fixes - Secure auto update now works via Proxy - YoBit support fixes - Added button to force resync currencies  v1.4 - New Exchange YoBit.net - Currency pair synchronization on startup! - Fixed issue with functions getPriceByVolume and getVolumeByPrice in JL Script - Add HiDPI enable or disable settings - Now you can search for currency pairs by keyword - Improved many elements of interface - Fixed bug with certificate that caused error messages - Many other small fixes  v1.30.04 - Add new pair for BitStamp - Fixd fee calculations in Bitfinex - Main window title now shows middle price instead of last trade price - Fixed issue when app can't start  v1.30.03 - High screen resolution support for Windows - Fixed fee for BTCChina and OKCoin - Add new pairs for Bitfinex - Fixed authorization for Bitfinex - Completed the Norwegian translation - Fixed bug on restoring Workspace from previous state - Fixed account data and add new pairs for bitstamp - Fixed minor bugs  v1.3 - macOS Sierra support - JL Script file read/write support - Syncronised currency pairs of Bitfinex, Bitstamp, BTC-e - Fixed bugs of history and currency pair in Bitstamp - Improved Proxy settings - Fixed network stat information - Fixed display of balance in bitfinex, bitstamp, btcchina - Fix tonce in btcchina - Add reducing interface elements spacing (optional in settings) - Add inactive start script button - Fixed JL Script groups bug - Fixed open order/cancel bug in bitstamp - Fixed time synchronization bug   v1.07.01 - Hotfix. High CPU load fix. Nonce error fix  v1.07 - Improved connection stability - API engine rewrited - Added balances triggers to rules - Fixed RUR currency in BTC-e - Fixed auto-scroll mode in last trades - Fixed many minor bugs - Now all binaries is digitally signed  v1.06 - Bugfix release - Fixed critical bug in Mt.Gox engine - Fixed Mac OS X ""New window"" bug - Fixed minor bugs - Improved buy/sell/cancel stability  v1.05 - Critical bugs fixed - Calculations fixes - Ui fixes - Added PPC, FTC, RTC support - Added proxy support  v1.04 - Fixed UI bugs - Added NMC and NVC to BTC-e - Now last price displays in title - Stability improvements in BTC-e exchange - Many small improvements - Translation fixes  v1.03 - SSL security fix - Fixed translations - Small ui fix  v1.02 - BTC-E.com support added. Now you can trade with LTC - Added trade history display - Multi-monitor support (Detachable windows) - Improved connection stability - Added German translation - Fixed UI bugs, translation bugs and some minor bugs  v1.01 - Fixed traffic heavy load. - Added trades fetch to update last values faster.  v1.00 - Api engine rewrited. Now works faster. - Api lag improvements. - Fixed minor bugs. - Added Norwegian translation.  v0.99 - Fixed minor bugs - Added Spanish translation - Translation engine updated - Checking update engine rewrited. Secure autoinstall for Mac and Win feature added. - Added portable mode for Windows (Just create folder QtBitcoinTrader near exe file)  v0.98 - Added translation engine. Now you can translate application to your native language - Fixed ui bugs - Enhanced rules. Now there is two modes, Sequential and Concurrent  v0.97 - Added profit calculation to main window - Optimized ui  v0.96 - Fixed minor bugs - Optimized ui - Mac version released  v0.95 - Fixed fee calculator bug - Now supports resolution down to 1024x700 - Enhanced rules feature - Minor bugs fixed  v0.94 - Rules finally working! - Fixed some minor bugs  v0.93 - Fixed critical ui bug where was wrong field in orders table  v0.92 - Fixed minor bugs in ui and currencies  v0.91  - Added Profiles  - Fixed Ui Bugs  v0.90 - Added all currencies supported by Mt.Gox - Minor fixes  v0.89 - Fixed critical bug  v0.88 - Addes Mt.Gox key and secret encryption with AES 256 - Fixed some ui bugx - Minor fixes  v0.87 - Match more faster engine - Fixed some bugs - Improved socket stability - Tested on Linux, thanks to macman31  v0.86 - Fixed bug in Orders Log - Fixed bug in fee calculation - Fixed ui dialogs  v0.85 - Added SSL switcher - Fixed some dialogs  v0.84 - Interlaced software lag performance  v0.83 - Added Fee Calculator  v0.82 - Improved socket stability  v0.81 - Minor bugs fixed - Improved stability - Added packet priority for buying and selling  v0.8 - First public release"
Casino Number Guessing Game,C++,https://github.com/KevinVillania/Baccarat-Casino-Number-Game,"# Casino number game  BACCARAT - Casino Number Game Third project by KEVIN VILLANIA villaniakevin07@gmail.com  Player chooses between two sides either 'banker' or 'player'. After drawing two cards the one who has the nearest number to 9 wins.  Whenever sum overboards to 9, 1 card will be drawn from the deck. However if at the first two draw results a sum of 8 and 9, the dealer won't draw additional card anymore.  ![](game.PNG)"
Sales Management System,C++,https://github.com/whoeverxd/sales,# sales Real Estate Sales Management (RESMS) System
Face Detection App with C++,C++,https://github.com/elador/FeatureDetection,FeatureDetection ================  A library (and demo applications) for face- and facial feature detection and real-time tracking in 2d-images
Digital Calculator,C++,https://github.com/christopher-siewert/cpp-calculator,"Ôªø# C++ Command Line Calculator    This is an extremely simple calculator I made while learning C++.    Download the repo, compile and run it:  ```  $git clone https://github.com/christopher-siewert/cpp-calculator.git  $cd cpp-calculator  $make  ```    For Windows users, I included my Code::Blocks project file, `Calculator.cbp`.  Open this with the [Code::Blocks](http://www.codeblocks.org/) IDE.    It has two modes, regular and scientific, that a user can switch between.    ### Overview    The program has 2 classes. A simple calculator class called Calculator, and a class that inherits Calculator called Scientific.    Some of the member functions of Calculator are virtual and are overridden in the Scientific class.    The program polymorphically calls the appropriate member functions depending on the current mode (scientific vs simple).    The program flow is:  1. Prompt user for the desired operation (polymorphically determine the message based on the current mode; show more options while in scientific mode)  2. Get additional data from the user (if doing addition, get 2 numbers)  3. Perform calculation, print to screen  4. Repeat from (1)    #### Calculator Class    This is the base class for the calculator program. It has data members and member functions that will display messages to the screen, prompt for user input, and perform calculations.    Specifically, Calculator has 2 protected data members, `result` and `mem`. `result` stores the result of the last computation. `mem` is used by the user to store a number in memory and access it later for further computation. They are both doubles, and they are initialized to 0 in the constructor.    Public member functions `add`, `subtract`, `multiply`, `divide`, `square`, and `sqrt` perform all the calculations. They prompt the user for either 1 or 2 numbers depending on the function, perform the required calculation, store this in the protected variable result, and print out the result to the screen.    Calculator has 2 public member functions in order to use `mem`, a get and a set.    Then some helper member functions will be needed.  `welcome()` will print out the available options that can be entered. This is virtual so the derived class can output its own options.    `parseOperation()` takes in the user input string and calls the appropriate member function. This is to enable the user entering ""+"" or ""log"" and having the correct function called. This is also virtual so it can be overridden for the extra functions in the scientific calc.    `parseInput()` is used for the extra `result`/`mem` feature. I wanted the user to be able to type the words result and mem as if they are numbers. `parseInput()` looks for those strings and gets the values from memory to be used in any calculations.    As this class contains virtual functions, a virtual destructor will be included.    #### Scientific Class    Scientific inherits from the base class calculator. It calls the Calculator constructor to initialize `result` and `mem`.  It has many additional operation functions. `sin`, `cos`, `tan`, `ln`, `log`, `abs`, and `pow` all prompt the user for input, perform a calculation, save it in result, and print that result.    It overrides the 2 virtual functions of class Calculator. `welcome()` is overridden to show all the options for the scientific calculator, and `parseOperation()` is overridden to access the additional calculation functions.    A virtual destructor is also included.    #### Main    The class set up allows the program to polymorphically call object methods depending on the type of the object. The main program will be one user entry loop. In the loop, a base class Calculator pointer (`calcPtr`) pointed at a Calculator object is used to call the Calculator member functions.    The switch to scientific mode will be made by pointing this Calculator class pointer at a derived Scientific class object. Now the program dynamically calls different member functions based on the state.    This simplifies the main code as the statement `calcPtr->welcome()` will automatically call the right welcome function, detailing out the proper options for the current mode.    #### Nifty Features    1. Users are able to enter a range of inputs that correspond to each operation. For example: ‚Äú+‚Äù, ‚Äúadd‚Äù, ‚Äúaddition‚Äù, ‚Äúsum‚Äù all trigger the addition operation.  2. This calculator has a memory feature. The result of a previous calculation is always stored and can be accessed using the keyword `result`. This keyword can be used as a number. Thus, the user could enter: `sub` to choose the subtraction operation then type in `result` and `1` to take one away from the previous result. The `mem` keyword allows a user to store a number for later use. This is highly useful when doing long calculations. An important result can be stored into `mem`, then accessed later by using `mem` as an entry in your calculations. "
